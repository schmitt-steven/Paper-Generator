{
  "Introduction": [
    {
      "chunk": {
        "chunk_id": "mock_0",
        "paper": {
          "id": "user_2404.15822v1",
          "title": "Recursive Backwards Q-Learning in Deterministic Environments",
          "published": "2024-04-24",
          "authors": [
            "Jan Diekhoff",
            "Jorn Fischer"
          ],
          "summary": "Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.",
          "pdf_url": "",
          "doi": "10.48550/arXiv.2404.15822",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "arXiv.org",
          "citation_count": 0,
          "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n",
          "markdown_text": "## RECURSIVE BACKWARDS Q-LEARNING IN DETERMINISTIC ENVIRONMENTS\n\n\n\n**Jan Diekhoff**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: jan.diekhoff@web.de\n\n\n\n**Jörn Fischer**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: j.fischer@hs-mannheim.de\n\n\n\n**ABSTRACT**\n\n\nReinforcement learning is a popular method of finding optimal solutions to complex problems.\nAlgorithms like Q-learning excel at learning to solve stochastic problems without a model of their\nenvironment. However, they take longer to solve deterministic problems than is necessary. Q-learning\ncan be improved to better solve deterministic problems by introducing such a model-based approach.\nThis paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds\na model of the environment. After reaching a terminal state, it recursively propagates its value\nbackwards through this model. This lets each state be evaluated to its optimal value without a lengthy\nlearning process. In the example of finding the shortest path through a maze, this agent greatly\noutperforms a regular Q-learning agent.\n\n\n_**Keywords**_ Q-learning _·_ deterministic _·_ recursive _·_ reinforcement learning\n\n\n**1** **Introduction**\n\n\nMachine learning and reinforcement learning are increasingly popular and important fields in the modern age. There are\nproblems that reinforcement learning agents can learn to solve more efficiently and consistently than any human when\ngiven enough time to practice. However, modern approaches like Q-learning run into issues when facing certain types\nof problems. Their approach to solving problems in combination with not using a model of the environment causes\nthem to take longer than is necessary to learn to solve problems that are deterministic in nature. By working without\nmodel of the environment, information that is available and help the learning process is ignored.\n\n\nThis paper introduces an adapted Q-learning agent called the _recursive backwards Q-Learning (RBQL) agent_ . It solves\nthese types of problems by building a model of its environment as it explores and recursively applying the Q-value\nupdate rule to find an optimal policy much quicker than a regular Q-learning agent. This agent is shown to work with\nthe example of finding the fastest path through a maze. Its results are compared to the results of a regular Q-learning\nagent.\n\n\n**2** **Reinforcement Learning**\n\n\nReinforcement learning is one of the main fields of machine learning. It is commonly used for optimizing solutions to\nproblems. At its most fundamental level, a reinforcement learning method is an implementation of an agent for solving\na Markov decision process [1] by interacting with an environment. Markov decision processes describe problems as\na set of states _S_, a set of actions _A_ and a set of rewards _R_ . For every time step _t_, the agent chooses an action _a ∈_ _A_\nand receives a new state _s ∈_ _S_ and a reward _r ∈_ _R_ for the action [2]. Rewards may be positive or negative, depending\non the outcome of the action, to encourage or discourage taking that action in the future [3]. The process of the agent\ninteracting with the environment is called an episode which ends when a terminal state is reached which resets the\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nenvironment and agent to their original configuration for the start of a new episode [3]. For the purposes of this paper,\nonly finite Markov decision processes are considered, meaning the environment has at least one terminal state.\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-1-0.png)\n\n\n_St_ +1\n\n\nFigure 1: Basic agent-environment relationship in a Markov decision process. The agent chooses an action _At_ and the\nenvironment returns a new state _St_ +1 and a reward _Rt_ +1. The dotted line represents the transition from step _t_ to step\n_t_ + 1 [3].\n\n\nReinforcement learning agents learn an optimal strategy for a given Markov decision process by estimating the value of\neither being in a state or taking a certain action in a certain state. They do this through a value function or action-value\nfunction respectively. The aim of the agent is to maximize the reward they receive in an episode [3]. To achieve this,\nvalue estimations do not only consider the immediate action the agent takes but also consider all future states and actions\nthat may occur when taking the original action. Agents follow so-called policies according to which they choose which\nactions to take. Through gaining knowledge, they continuously adapt this policy in order to eventually reach an optimal\npolicy - a policy which chooses the optimal action at every step. To explore, agents have to balance between exploration\nand exploitation [3]. Exploration is the act of following suboptimal actions to attempt to find an even better policy. On\nthe other hand, exploitation is following the actions that will yield the currently highest estimated value. An agent that\nonly exploits acts _greedily_ . To ensure continual exploration so that all actions get updated given enough time, agents\ncan choose policies that are mostly greedy but choose to explore sometimes [2]. To this end, an approach like _ϵ_ -greedy\nmay be used. Here, _ϵ_ is the probability of choosing a random action and 1 _−_ _ϵ_ is the probability of acting greedily.\n\n\nA widely used modern approach to RL is temporal difference learning [4], more specifically Q-learning [2]. Q-learning\nworks with the Q-learning update formula to update its policies:\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + _α ·_\n\n[ _Rt_ +1 + _γ ·_ max _Q_ ( _St_ +1 _, a_ ) _−_ _Q_ ( _St, At_ )] (1)\n_a_\n\n\n_Q_ ( _St, At_ ) is the estimated value for any given state-action pair. The equation shows how it is updated after taking\naction _At_ from state _St_ . _Rt_ +1 represents the reward gained, max _Q_ ( _St_ +1 _, a_ ) is the value estimation of the best action\n_a_\n_a ∈_ _At_ +1 that can be taken from _St_ +1 according to the current policy, the state resulting from action _At_ . _α_ is a step-size\nparameter, also known as the _learning rate_ . Its value lies between 0 and 1 and it determines how importantly the agent\nvalues new information against the current estimate it already has. A value of 0 completely ignores new information\nwhile a value of 1 completely overrides the preexisting value estimate. _γ_ is the discount factor, weighing future rewards\nless than immediate ones. It also lies between 0 and 1, where 1 weighs the best future action equally to the current one\nand 0 does not consider it at all.\n\n\n**3** **Recursive Backwards Q-Learning**\n\n\n**3.1** **Idea**\n\n\nQ-learning agents are very widespread in modern reinforcement learning. Working free of a model allows them to\nbe generally applicable to many problems. However, some Markov decision processes take longer to solve than is\nnecessary because the agent ignores readily available information. This is noticeable in deterministic, episodic tasks\nwhere a positive reward is only given when reaching a terminal state. Before this state is reached for the first time, the\nagent appears to be moving entirely at random. Looking at figure 2, the issue becomes apparent. Even when following\nthe optimal path at every step, it still takes multiple episodes for the reward of the terminal state to propagate back to the\nstarting state. In fact, the optimal paths value estimation gets worse before it gets better. If every step has a reward of\n\n\n2\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_−_ 1, values along the optimal path get worse if they do not lead to a state that has already been reached by the terminal\nstate’s positive reward as it travels backwards.\n\n\nIn this paper, grid worlds [3] are used as an example Markov decision process for the agent to solve. Grid worlds are a\ntwo-dimensional grid in which every tile represents a state and the actions are limited to walking up, down, left or right.\nGrid worlds are useful in that they are very simple to understand and to display, they have a limited set of actions and\ntheir set of states can be as small or large as is desired. Additionally, showing the value or optimal policy for each state\nis as easy as writing a number or drawing an arrow on the corresponding tile. Actions that would place the agent off of\nthe grid simply return the state the agent is already in, but may still give a reward. Special tiles can also be defined, such\nas walls that act like the grid edge or pits that are terminal fail states because the agent cannot leave them once it has\nfallen in. Every grid world tile gives a reward of _−_ 1 to punish taking unnecessary actions in favor of taking the fastest\npath to the goal.\n\n\n_Q_ greedy policy\nw.r.t. _Q_\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -0.8\n\n\n-1\n\n\n-1\n\n\n-1 0.52\n\n\n-1\n\n\n-1\n\n\n-1 1.99\n\n\n-1\n\n\n-1\n\n\n-1 3.27\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 0.78\n\n\n-1\n\n\n-1\n\n\n-1 3.15\n\n\n-1\n\n\n-1\n\n\n-1 4.96\n\n\n-1\n\n\n-1\n\n\n-1 6.17\n\n\n-1\n\n\n-1\n\n\n-1 6.93\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 4.5\n\n\n-1\n\n\n-1\n\n\n-1 7.25\n\n\n-1\n\n\n-1\n\n\n-1 8.63\n\n\n-1\n\n\n-1\n\n\n-1 9.32\n\n\n-1\n\n\n-1\n\n\n-1 9.66\n\n\n-1\n\n\n-1\n\n\n-1 9.83\n\n\n-1\n\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n\nep. 0\n\n\nep. 1\n\n\nep. 2\n\n\nep. 3\n\n\nep. 4\n\n\nep. 5\n\n\nep. 6\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -1.6\n\n\n-1\n\n\n-1\n\n\n-1 -1.66\n\n\n-1\n\n\n-1\n\n\n-1 -1.1\n\n\n-1\n\n\n-1\n\n\n-1 -0.15\n\n\n-1\n\n\n\nFigure 2: Q-learning in a one-dimensional grid world. All Q-values are initialized as _−_ 1. Actions that lead to the\nterminal state reward 10. All other actions reward -1. The discount rate _γ_ is set to 0 _._ 9. The learning rate _α_ is set to 0 _._ 5.\nThe value of _ϵ_ is irrelevant as the only action the agent takes is _→_ .\n\n\nFigure 2 is a very simple grid world and it still takes six episodes to reach an optimal policy, even when taking the\noptimal action at every step. This problem will only grow worse and add noticeably more episodes of training for grid\nworlds that are not as trivial to solve, or even more complex tasks with more variables to consider. As stated, the issue\nis that the agent has no source of direction until it has randomly stumbled across the terminal state, its only source of\npositive rewards. The larger the state space, the longer it is blindly searching.\n\n\nReinforcement learning agents that work with a model of their environment are known as _model-based_ reinforcement\nlearning agents. They can either work with a preexisting model or, more commonly, build their own. The way they\nconstruct their models is important as having perfect knowledge of an environment is neither feasible nor sensible. In\nthe case of a grid world it is no problem, but imagining a more complex scenario like a self-driving car makes this fact\napparent. When trying to drive from one city to another, knowing every centimeter of the road with every possible place\nother cars might be on the route is resource intensive and unnecessary. Instead, an agent should attempt to simplify its\nmodel as much as possible. Instead of every bit of road, long stretches going straight can be clumped together. Similar\nsituations like a car in front slowing down can be treated the same wherever they occur.\n\n\nThe purpose of this paper is to introduce and evaluate a new type of model-based agent called the RBQL agent. The\nRBQL agent solves deterministic, episodic tasks that positively reward only the terminal state more efficiently than a\nregular Q-learning agent. It functions by building a model of its environment through exploration. When it reaches a\nterminal state, it recursively travels backwards through all previously explored states, applying a modified Q-learning\nupdate rule, the RBQL update rule. By setting the learning rate _α_ to 1, equation (1) can be simplified as such:\n\n\n3\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + 1 _·_ [ _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )]\n= _Q_ ( _St, At_ ) + _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )\n= _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n\n(2)\n\n\n\nAs can be seen in formula (2), the Q-value now exclusively depends on the reward and the discounted reward of the\nbest neighbor. Because the algorithm applies this formula starting with what is guaranteed to be the highest value of the\nenvironment and working its way away from it, the best possible neighbor for any given state is always the previously\nevaluated state.\n\n\nEvaluating all states at the end of the episode is reminiscent of dynamic programming [5] or Monte Carlo methods [3]\nand is a point of critique for those approaches. However, as will be shown in chapter 4, this evaluation method is so\neffective in RBQL that evaluating all known states in one go is still cost effective. RBQL also differs in comparison\nto dynamic programming and Monte Carlo in a few major ways. In contrast with dynamic programming, it does not\nstart out with a perfect model but has to build its own. It also propagates its reward throughout all states much more\nquickly and it uses an action-value function, not a state-value function. In contrast with Monte Carlo, it does not use\nexploring starts to guarantee exploration. It also does not only update the values that were seen in an episode. Instead,\nto facilitate exploration, it always prioritizes visiting unexplored actions, only following the greedy path when there\nare none. Because this mode of exploration still results in unexplored actions, the _ϵ_ -greedy approach is adapted for\nRBQL. Instead of exploring steps, the agent has exploration episodes. _ϵ_ serves the same purpose as before, marking\nthe probability of taking an exploration episode while 1 _−_ _ϵ_ is the probability of taking an exploitation episode. In an\nexploration episode, the agent randomly chooses an unexplored action anywhere in its model, navigates the model to\nput itself in a position to take that action and then continues to explore until it finds a known path again or the episode\nends.\n\n\nIn this paper, finding an optimal path through a randomly generated grid world maze is used as an example task for\nRBQL to solve. It is also used to compare the performance of RBQL to Q-learning.\n\n\n**3.2** **Implementation**\n\n\nTo implement RBQL [1], the Godot game engine v. 3.5 [2] was used. Godot is a free, open source engine used mainly for\nvideo game development. Its main language is GDScript, an internal language that is very similar in syntax to Python,\nthough it also supports C, C++, C# and VisualScript. Because Python is very popular for machine learning development,\nthe implementation is written in GDScript so that it is easily readable for interested parties. Godot uses a hierarchical\nstructure of objects called _nodes_ . In the implementation, there are two main nodes: the agent and the environment.\n\n\n**3.2.1** **Environment**\n\n\nThe environment is of the type `TileMap` [3] – a class designed for creating maps in grid-based environments like grid\nworlds. Before starting the first episode, the environment generates a maze given a width _w_ and a height _h_ using a\nrecursive backtracking algorithm [6]. The starting point for the agent is always (0 _,_ 0) and the goal it attempts to reach –\nthe only terminal state – is ( _w −_ 1 _, h −_ 1). To ensure that the agent has the ability to improve even after finding the goal\nin the first episode, a maze with multiple paths is needed. Because a maze generated with recursive backtracking only\nhas one path to the terminal state, a number of alternate paths are generated by taking _w · h/_ 4 random positions and a\ndirection for each position. If the position has a wall in that direction, it is removed. If not, nothing happens.\n\n\nThe environment has a function `step(state,action)` that serves as the only way for the agent to interact with it.\nThe possible moves are `UP`, `DOWN`, `LEFT` and `RIGHT` . The state is described as a coordinate of the current position. In\nGodot, the class `Vector2(x,y)` [4] is used for this purpose. `step()` checks if taking the given action from the given\nstate results in hitting a wall or not. If not, the agent moves to a new position. There are three different rewards: _−_ 1 for\nany normal tile, _−_ 5 for hitting a wall and 10 for reaching the terminal state. _−_ 1 is awarded at every step to discourage\nagents from taking unnecessary steps. Walls give _−_ 5 to quickly teach the agent to ignore them. After taking an action,\n\n\n1 The source code can be downloaded at `[https://github.com/JanDiekhoff/BackwardsLearner](https://github.com/JanDiekhoff/BackwardsLearner)`\n2 Godot v. 3.5 can be downloaded at `[https://godotengine.org/download/archive/3.5-stable/](https://godotengine.org/download/archive/3.5-stable/)`\n3 `[https://docs.godotengine.org/en/3.5/classes/class_tilemap.html](https://docs.godotengine.org/en/3.5/classes/class_tilemap.html)`\n4 `[https://docs.godotengine.org/en/3.5/classes/class_vector2.html](https://docs.godotengine.org/en/3.5/classes/class_vector2.html)`\n\n\n4\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nthe new state and reward are returned to the agent, as well as a notification if the episode has ended or not and if the\nagent has hit a wall or not.\n\n\nThe `TileMap` has a tile for each combination of having or not having a wall in each of the four directions, totaling 2 [4] or\n16 total possible tiles. Another option would be to just have a floor tile and a wall tile. However, that would make a\nmaze with an equivalent wall layout much larger, leading to a larger state set and longer solving times. To determine if a\nwall is in a certain direction, the id of each tile from 0 to 15 acts as a four-bit flag. Each direction is assigned one of the\nbits ( `UP` = 0, `RIGHT` = 1, `DOWN` = 2 and `LEFT` = 3). If the flag is set, there is a wall in the corresponding direction. The\nid for an L-shaped tile for example would be 2 [2] + 2 [3] = 12 as `DOWN` and `LEFT` have walls. The process for determining\nif the agent can move in a given direction _d_ from a position _p_ is ( _¬idp_ ) & (2 _[d]_ ), where _idp_ is the id of the tile at _p_ .\n\n\n**3.2.2** **RBQL Agent**\n\n\nThe RBQL agent is represented by a `Sprite` [5] object – a 2D image – so it can be observed while solving a maze. During\nits runtime, the agent keeps track of a few key things:\n\n\n    - A model of the environment ( `explored_map` )\n\n\n    - A list of rewards for each state-action pair ( `rewards` )\n\n\n    - The last reward received ( `reward` )\n\n\n    - A list of steps taken per episode ( `steps_taken` )\n\n\n    - The Q-table ( `qtable` )\n\n\n    - The current state ( `current_state` )\n\n\n    - The previous state ( `old_state` )\n\n\n    - The last taken action ( `action` )\n\n\nThe model of the environment starts out as an empty dictionary. Every time a new state is discovered, an entry for that state is made and initialized as an empty array. When an action is taken from this state, the resulting new state is entered into the previous state’s array at the index of the taken action’s designated number\n( `explored_map[old_state][action] = current_state` ). When hitting a wall, the “new” state is the same as the\nstate from which the action was taken. Similarly, when an action is taken, the resulting reward is saved in the rewards\nlist ( `rewards[old_state][action] = reward` ). Because the agent uses state-action values, not state values, the\ntiles are treated like nodes in a directed graph. Going from tile A to tile B might result in a different reward than when\ngoing from B to A, so when the agent learns the reward of going from A to B, it does not also learn the reward of going\nfrom B to A.\n\n\nBeing a Q-learner makes it simpler to generalize the agent for other tasks, but it causes a lot of exploratory steps and\nexploratory episodes to only explore one position at a time. If an exploration episode chooses an unexplored state-action\npair that results in hitting a wall, the exploration episode immediately ends with little information gained. To alleviate\nthis problem, the agent takes exploratory “look-ahead” steps. After entering a tile, it takes a step in every direction but\nonly saves the result if it hits a wall. This guarantees that exploratory episodes always take new paths and not just hit a\nwall and continue on the best known path.\n\n\nThe agent also keeps track of a list of the actions it has taken – except for when hitting a wall – for the case that it\nreaches a dead end, or rather a state with no unexplored neighbors. In this case, the agent would normally follow the\noptimal path until it finds a new unexplored path or reaches the terminal state. However, if the path the agent is on has\nnot been explored before it has not yet been evaluated and there is no optimal path to follow. In this case, the agent\nbacktracks by taking the opposite action of the most recent in the list, then removes it from the list, until an unexplored\ntile or an evaluated path to follow is found.\n\n\nFinally, when the terminal state is reached, the Q-table is updated with the rewards saved in `rewards` according to the\nRBQL update rule.\n```\n             qtable[state][action] =\n\n```\n\n`rewards[state][action] + discount_rate` _·_\n\n```\n             qtable[explored_map[state][action]].max()\n\n```\n\nTo do this, a copy of `explored_map` is inverted to be able to traverse it in reverse. This is then done with a breadth-first\nsearch algorithm, starting at the terminal state, and the Q-value is calculated for each state. Breadth-first search is chosen\n\n\n5 `[https://docs.godotengine.org/en/3.5/classes/class_sprite.html](https://docs.godotengine.org/en/3.5/classes/class_sprite.html)`\n\n\n5\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nover a depth-first search algorithm so that each state must only be visited once as the value is directly proportional to\nthe distance from the terminal state. With breadth-first search, each state gets the highest possible value on its first visit\nbecause it is visited from its highest possible valued neighbor.\n\n\nWhen all known states have been evaluated, a new episode begins. After the first episode, episodes are chosen to be\neither exploratory or exploitative, similar to how an _ϵ_ -greedy policy may choose exploratory actions. In an exploitative\nepisode, the agent simply follows the best path it knows, choosing at random if two states are equally good, but still\nalways exploring unknown states directly adjacent to the path above all else. In an exploratory episode, a random state\nwith an unexplored neighbor is chosen. The agent navigates to this state with the help of the A* search algorithm [7]\nand follow the unexplored path from there until it finds a known state again. This exploratory excursion may only find\none new state or it may find a vastly superior path to what was known before. _ϵ_ is decreased after every episode as\nfollows:\n_ϵ_ = `min_epsilon + (max_epsilon - min_epsilon)`\n\n\n_· e_ [(] _[−]_ `[decay_rate]` _[ ·]_ `[ current_episode]` [)]\n\n\nwhere `min_epsilon`, `max_epsilon` and `decay_rate` can be any value within a range of [0 _,_ 1] and `current_episode`\nis the number of the current episode starting with 0. Once every state is explored, the agent is guaranteed to have found\nthe optimal path, or paths, through the maze. In its entirety, the algorithm can be expressed like this:\n\n\n**Algorithm 1** Backwards Q-Learning Algorithm\n\nSet exploration_episode to false\n**while** true **do**\n\n**if** exploration_episode **then**\n\nFind unexplored path\nTravel to unexplored path\n**end if**\n**while** episode is not over **do**\n\n**if** current position has an unexplored neighbor **then**\n\nVisit unexplored neighbor\nUpdate model\nSave reward\n\n**if** no wall hit **then**\n\nSave action in action queue\n**end if**\n**else if** there is an optimal path to follow **then**\n\nVisit best neighbor\n**end if**\n**while** current pos. has no unexplored neighbor **do**\n\nBacktrack\n\n**end while**\n\n**end while**\nCreate state queue with breadth-first search\n**for** state in queue **do**\n\nApply RBQL formula\n**end for**\nSet exploration_episode to random() _<_ = _ϵ_\nApply decay to _ϵ_\n**end while**\n\n\n**3.2.3** **Q-learning agent**\n\n\nA standard Q-learning agent has been implemented in Godot as well to compare the performance of the RBQL agent to.\nThis agent is comparatively simple:\n\n\n**4** **Tests and Results**\n\n\nTo compare the performance of the two agents, three sets of tests have been done for different maze sizes: 5 _×_ 5, 10 _×_ 10\nand 15 _×_ 15. All variables have been set to common values. The decay rate is set somewhat high to account for the\nrelatively low episode amount:\n\n\n6\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**Algorithm 2** Q-Learning Algorithm\n\n\n**while** true **do**\n\n**if** random() _<_ = _ϵ_ **then**\n\nChoose random action\n\n**else**\n\nChoose greedy action\n**end if**\n\nTake action\n\nReceive new state and reward\nUpdate Q-table for old state and action\n**if** terminal state reached **then**\n\nStart new episode\n**end if**\nApply decay to _ϵ_\n**end while**\n\n\n    - _γ_ = 0 _._ 9\n\n\n    - _α_ = 0 _._ 1 (RBQL has _α_ = 1 as explained in equation (2))\n\n\n    - `min_epsilon` = 0 _._ 01\n\n\n    - `max_epsilon` = 1\n\n\n    - `decay_rate` = _−_ 0 _._ 01\n\n\nFor every maze size, each agent is given the same set of 50 randomly generated mazes. Each agent is given 25 episodes\nper maze to train. These values are chosen to offer a reasonably large sample size without requiring an enormous\namount of time to compute. Agents are compared by the number of steps taken per episode, with less steps taken being\na more desirable outcome. The step counter is increased every time `step()` is called, including the look-ahead steps of\nthe RBQL. For a sense of perspective, the best possible solution to any square maze of size _s_ [2] is 2 _s −_ 2. Assuming a\nmaze with no walls, the shortest distance between two points _A_ and _B_ can be expressed as their Manhattan distance\n_|AX −_ _BX_ _|_ + _|AY −_ _BY |_ [8]. In the corners of a square, it holds that _AX_ = _AY_ and _BX_ = _BY_, so the distance can\nbe simplified as 2 _· |A −_ _B|_ . Setting _A_ = 0 and _B_ = _s −_ 1, this further simplifies to 2 _s −_ 2. This means that while\nthe amount of states (and thereby state-action pairs) increases quadratically, the best possible solution only increases\nlinearly. This in turn means that the amount of states that are not on the optimal path that the agent has to evaluate will\noften increase drastically with the size of the maze.\n\n\nLooking at the results, a few things can be observed. First of all, the average number of steps the RBQL agent takes\nis consistently lower than the Q-learning agent in all three maze sizes. It also has much less variation in step counts,\nwhich can be seen when looking at the areas of lighter hue. The light red areas are much more sporadic and spike\nfurther away from the average. The green areas stick much closer together. If the highest two step counts per episode\nwere not removed, RBQL would also have a few small spikes. These spikes would represent exploratory episodes\nwhere a new path is explored, resulting in a higher step count. In cases where the line is flat for a long period of time, it\ncan be assumed that the optimal solution is found. This can be seen in all three figures, where both the average and\nthe min/max range become a straight line close to the minimum. Important to note is that every maze has a different\noptimal solution, hence why the average sits above the blue line which denotes the lowest possible step count in any\nmaze of this size. It can also be observed that none of the lines ever go below this boundary, as is to be expected.\n\n\nSecond, even when removing the highest two step counts per episode, many of the Q-learning agent’s step counts are so\nlarge that scaling the graphs to fit them makes the RBQL agent’s data and the lower boundary difficult to see in the\ngraphs for the larger mazes. The highest step count values that have not been cut are 858 steps in figure 3, 7,585 in\nfigure 4 and 21,147 in figure 5, while the highest in total are 3,716 steps in figure 3, 20,553 in figure 4 and 26,315 in\nfigure 5.\n\n\nThird, it is interesting to see how the differences in average step counts evolve with the grid size. Table 1 shows this\ndifference in the first and last episode. The difference between the average step counts in the last episode especially is\nstriking, as it is close to doubling from each size to the next. Further, looking at the improvement of each agent as seen\nin table 2, one can see that the factor by which RBQL improves massively increases the bigger the maze becomes while\nthe Q-learner only slightly improves its performance in comparison. Additionally, most of the improvement of RBQL is\ndone in the first two episodes, while the Q-learner has a more gradual learning curve.\n\n\n7\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**1** _**,**_ **000**\n\n\n**900**\n\n\n**800**\n\n\n**700**\n\n\n**600**\n\n\n**500**\n\n\n**400**\n\n\n**300**\n\n\n**200**\n\n\n**100**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 3: Number of steps taken to find the goal in a randomly generated grid world maze of size 5 _×_ 5. The blue line is\nthe minimum step threshold for any maze of this size. The light red area shows the range of Q-learning agent’s highest\nand lowest step count, excluding the highest and lowest two. The red line shows the average performance. Similarly, the\nlight green area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest\ntwo, and the green line shows the average performance.\n\n\n8\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-7-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**8** _**,**_ **000**\n\n\n**6** _**,**_ **000**\n\n\n**5** _**,**_ **000**\n\n\n**4** _**,**_ **000**\n\n\n**3** _**,**_ **000**\n\n\n**2** _**,**_ **000**\n\n\n**1** _**,**_ **000**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 4: Number of steps taken to find the goal in a randomly generated grid world maze of size 10 _×_ 10. The light\nred area shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two.\nThe red shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n9\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-8-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 5: Number of steps taken to find the goal in a randomly generated grid world maze of size 15 _×_ 15. The light red\narea shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two. The\nred line shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n10\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-9-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nTable 1: Difference in average step counts of the Q-learner and RBQL. The difference expresses how many times more\nsteps the Q-learner took compared to RBQL.\n\n\nGrid size Q-learner steps RBQL steps Difference\n**Episode 0**\n5 _×_ 5 278.06 191.84 1.45\n\n10 _×_ 10 3,308.46 843.52 3.92\n15 _×_ 15 7,180.98 1,965 3.65\n**Episode 24**\n5 _×_ 5 49.14 9.62 5.11\n\n10 _×_ 10 281.44 23.68 11.89\n\n15 _×_ 15 778.68 35.96 21.65\n\n\nLastly, the RBQL agent seems to find an optimal policy at around episode 4 for the 5 _×_ 5, episode 6 for the 10 _×_ 10 and\nepisode 10 for the 15 _×_ 15 grid. As the previous figures show, the Q-learning agent does not come close to similarly\nlow step counts and therefore does not reach an optimal policy at all with the same amount of training.\n\n\nTable 2: Difference in average step counts of the Q-learner and RBQL. Improvement shows the factor by which the\namount of steps is reduced from episode 0 to 24.\n\n\nGrid size Steps in episode 0 Steps in episode 24 Improvement\n**Q-learning agent**\n5 _×_ 5 278.06 49.14 5.66\n\n10 _×_ 10 3,308.46 281.44 11.76\n15 _×_ 15 7,180.98 778.68 9.22\n**RBQL agent**\n5 _×_ 5 191.84 9.62 19.94\n\n10 _×_ 10 843.52 23.68 35.62\n\n15 _×_ 15 1,965 35.96 90.76\n\n\nTo further show RBQL’s efficiency, it has also been tested under the same parameters in a grid of size 50 _×_ 50. The\nresults can be seen in figure 6. This test is done to demonstrate that even such a large maze can be explored by RBQL.\nAs with the previous examples, by far the largest policy improvement still happens in the first episode. With mazes of\nsuch a large size, a lot more spikes in step counts are seen in later episodes because there are more states to explore. The\ndifference in average step counts goes from 20,811.08 in episode 0 to 344.9 in episode 24, an improvement by a factor\nof 60.34. This is worse than the improvement in the 15 _×_ 15 mazes, but still almost double that of the 10 _×_ 10 mazes.\n\n\n**5** **Discussion**\n\n\nThis chapter explores the practicality of using this algorithm to solve other Markov decision processes. It discusses\nwhich parts of the implementation are and are not specific to the problem of fastest path through a maze, which\nimprovements can be made to make it more applicable for other problems and showcases further points for research in\nthis field. The constraints given in this paper are that the agent will attempt to solve deterministic, episodic tasks with a\nsingle terminal state as its only source of positive rewards. This chapter also discusses which of these constraints can be\ndismissed.\n\n\nThere are a few parts of the implementation as presented in chapter 3.2 that are only applicable to this specific problem.\nThis is not necessarily a bad thing, as the purpose of the RBQL agent is to utilize knowledge of its environment. As a\nresult of this, the only parts that cannot be directly adapted for other problems are the way the agent builds its model.\nIn the grid world maze, it can assume that every state has the same actions it can take and has a neighboring state in\neach direction (though it may sometimes be itself). Further, every action always has an opposite action, going up can\nalways be undone by going down for example. These assumptions allow it to easily build a model of the grid world\nand influence its policy in how it further explores it. They allow the agent to take steps in each direction to check for\nwalls and they allow the agent to backtrack when it is stuck in a dead end. These assumptions cannot be guaranteed\nfor other Markov decision processes or even for grid worlds with more complex behavior like a wind tunnel that if\nwalked through also pushes the agent one tile in the direction the wind is traveling. The way in which the agent builds a\n\n\n11\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**3**\n\n\n**2** _**.**_ **8**\n\n\n**2** _**.**_ **6**\n\n\n**2** _**.**_ **4**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 6: Number of steps taken to find the goal in a randomly generated grid world maze of size 50 _×_ 50. The light\ngreen area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest two,\nand the green line shows the average performance.\n\n\n12\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-11-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nmodel has to either be designed for each environment individually or it has to be abstracted so that it is more broadly\napplicable. Finding such an approach to model building is one area of improvement for RBQL. Importantly though,\nnone of these assumptions are required for the agent to function. Backtracking, opposite steps and the same actions for\nevery state simply make the implementation easier and more efficient. As long as no path of a directed graph would\ncause the agent to be stuck with no way to reach a terminal state, it can be explored and evaluated.\n\n\nAnother improvement to the way the implementation builds its model is to simplify it as far as possible. As the amount\nof states directly influences how long a problem takes to solve, RBQL will become more efficient the more it can\nremove unnecessary states. Currently, every position has its own state. If the agent could detect “hallways” – tiles with\nparallel walls – they could be removed without problem in favor of directly connecting the two tiles at either side of the\nhallway – only the negative rewards for the length of the hallway would have to be implemented into the model. Further,\nif there is a non-forking path that leads into a dead end, the entire path could be treated as a wall and ignored entirely.\nThis would leave only the starting state, terminal state, turns and forking paths to evaluate. Both of these additions leave\nthe key part of the algorithm, traversing the model backwards and applying the RBQL update formula, untouched.\n\n\nRBQL can be easily adapted to include multiple terminal states with the same or different rewards and this is already\nsupported by the implementation. There are two possible ways to do this. First is to create an imaginary state that\nall terminal states lead into from which the backtracking always starts. Second is to remember all terminal states and\nbacktrack from each of them. The first option is much more efficient as each state still only gets evaluated once while\nthe second version avoids having to tamper with the model.\n\n\nFinally, RBQL could be adapted to work in non-deterministic environments. To reiterate, deterministic means that a\nstate-action pair always yields the same state-reward pair. If the agent could, while building its model, also estimate the\ntransition probabilities of a state-action pair to a new state, RBQL could still be used to evaluate the states. The RBQL\nupdate rule can be generalized to\n\n\n\n_Q_ ( _St, At_ ) _←_ �\n\n_s∈St_ +1\n\n\n\n( _Rs_ + _γ_ max _Q_ ( _s, a_ )) _· p_ (3)\n_a_\n� �\n\n\n\nwhere _St_ +1 is the set of possible states when taking _At_ from _St_, _p_ is the probability of reaching _s_ when taking _At_ from\n_St_ and _Rs_ is the reward of reaching _s_ . In a deterministic environment, _St_ +1 only consists of one state with _p_ = 1,\nnegating these additions. Whether RBQL would be as effective in non-deterministic environments as in deterministic\nenvironments is something to be explored in further studies.\n\n\nThe only constraint on the algorithm that cannot easily be circumvented is its episodic nature. Because the agent relies\non a terminal state from which to propagate the rewards backwards from, a continuous task implementation seems\nimpossible to implement.\n\n\n**6** **Conclusion**\n\n\nThis paper has introduced recursive backwards Q-learning, a model-based reinforcement learning algorithm that\nevaluates all known state-action pairs of the model at the end of each episode with the Q-learning update rule. It has\nalso shown how recursive backwards Q-learning relates to, adapts and improves on them. This paper has presented\nan implementation of recursive backwards Q-learning in the Godot game engine to test its performance. Through\nmultiple tests, it has been shown to be superior in finding the shortest path through a randomly generated grid world\nmaze. It has been argued that this algorithm could be adapted to solve other deterministic, episodic tasks more quickly\nthan Q-learning. Further, it has given avenues for further research in adapting recursive backwards Q-learning for\nnon-deterministic problems.\n\n\n**References**\n\n\n[1] Richard Bellman, “A markovian decision process,” _Journal of Mathematics and Mechanics_, vol. 6, no. 5, pp. 679–\n684, 1957. [Online]. Available: `[http://www.jstor.org/stable/24900506](http://www.jstor.org/stable/24900506)` .\n\n[2] Christopher John Cornish Hellaby Watkins, “Learning from delayed rewards,” 1989.\n\n[3] Richard S Sutton and Andrew G Barto, _Reinforcement learning: An introduction_ . MIT press, 2018.\n\n[4] Richard S. Sutton, “Learning to predict by the methods of temporal differences,” _Machine Learning_, vol. 3, no. 1,\npp. 9–44, 1988. DOI: `[10.1007/bf00115009](https://doi.org/10.1007/bf00115009)` .\n\n[5] Richard Bellman, “Dynamic programming,” _Princeton, USA: Princeton University Press_, vol. 1, no. 2, p. 3, 1957.\n\n[6] Peter Gabrovšek, “Analysis of maze generating algorithms,” _IPSI Transactions on Internet Research_, vol. 15,\nno. 1, pp. 23–30, 2019.\n\n\n13\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n[7] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael, “A formal basis for the heuristic determination of minimum\ncost paths,” _IEEE Transactions on Systems Science and Cybernetics_, vol. 4, no. 2, pp. 100–107, 1968. DOI:\n`[10.1109/TSSC.1968.300136](https://doi.org/10.1109/TSSC.1968.300136)` .\n\n[8] Eugene F Krause, “Taxicab geometry,” _The Mathematics Teacher_, vol. 66, no. 8, pp. 695–706, 1973.\n\n\n14\n\n\n",
          "ranking": null,
          "is_open_access": false,
          "user_provided": true,
          "pdf_path": "output/literature/user_2404.15822v1/user_2404.15822v1.pdf"
        },
        "chunk_text": "The increasing complexity of software systems necessitates automated approaches for code quality assurance and bug detection.",
        "chunk_index": 0
      },
      "summary": "The increasing complexity of software systems necessitates automated approaches for code quality assurance and bug detection.",
      "vector_score": 0.7360000000000001,
      "llm_score": 0.92,
      "combined_score": 0.92,
      "source_query": "mock_query_introduction"
    },
    {
      "chunk": {
        "chunk_id": "mock_1",
        "paper": {
          "id": "acda55ebdf39c6634e89a9730ff7d963471f2b0a",
          "title": "Expected Eligibility Traces",
          "published": "2020-07-03",
          "authors": [
            "H. V. Hasselt",
            "Sephora Madjiheurem",
            "Matteo Hessel",
            "David Silver",
            "André Barreto",
            "Diana Borsa"
          ],
          "summary": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state.\nIn this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that some- times substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
          "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17200/17007",
          "doi": "10.1609/aaai.v35i11.17200",
          "fields_of_study": [
            "Computer Science",
            "Mathematics"
          ],
          "venue": "AAAI Conference on Artificial Intelligence",
          "citation_count": 41,
          "bibtex": "@Article{Hasselt2020ExpectedET,\n author = {H. V. Hasselt and Sephora Madjiheurem and Matteo Hessel and David Silver and André Barreto and Diana Borsa},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Expected Eligibility Traces},\n volume = {abs/2007.01839},\n year = {2020}\n}\n",
          "markdown_text": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)\n\n# **Expected Eligibility Traces**\n\n\n**Hado van Hasselt** [1] **, Sephora Madjiheurem** [2] **, Matteo Hessel** [1]\n\n**David Silver** [1] **, Andr´e Barreto** [1] **, Diana Borsa** [1]\n\n1 DeepMind\n2 University College London, UK\n\n\n\n**Abstract**\n\n\nThe question of how to determine which states and actions\nare responsible for a certain outcome is known as the _credit_\n_assignment problem_ and remains a central research question\nin reinforcement learning and artificial intelligence. _Eligibil-_\n_ity traces_ enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not\nto counterfactual sequences that could also have led to the\ncurrent state. In this work, we introduce _expected eligibility_\n_traces_ . Expected traces allow, with a single update, to update\nstates and actions that could have preceded the current state,\neven if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous)\ntraces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide\na way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which\nensures that the resulting algorithm is a strict generalisation\nof TD( _λ_ ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.\n\n\n**Motivation and Summary**\n\n\nAppropriate credit assignment has long been a major research\ntopic in artificial intelligence (Minsky 1963). To make effective decisions and understand the world, we need to accurately associate events, like rewards or penalties, to relevant\nearlier decisions or situations. This is important both for learning accurate predictions, and for making good decisions.\n_Temporal credit assignment_ can be achieved with repeated\ntemporal-difference (TD) updates (Sutton 1988). One-step\nTD updates propagate information slowly: when a surprising value is observed, the state immediately preceding it is\nupdated, but no earlier states or decisions are updated. _Multi-_\n_step_ updates (Sutton 1988; Sutton and Barto 2018) propagate\ninformation faster over longer temporal spans, speeding up\ncredit assignment and learning. Multi-step updates can be\nimplemented online using _eligibility traces_ (Sutton 1988),\nwithout incurring significant additional computational expense, even if the time spans are long; these algorithms have\ncomputation that is independent of the temporal span of the\npredictions (van Hasselt and Sutton 2015).\n\n\nCopyright c _⃝_ 2021, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\nMDP True value TD(0) TD(λ) ET(λ)\n\n\nFigure 1: A comparison of TD(0), TD( _λ_ ), and the new\nexpected-trace algorithm ET( _λ_ ) (with _λ_ = 0 _._ 9). The MDP\nis illustrated on the left. Each episode, the agent moves randomly down and right from the top left to the bottom right,\nwhere any action terminates the episode. Reward on termination are +1 with probability 0.2, and zero otherwise—all\nother rewards are zero. We plot the value estimates after the\nfirst positive reward, which occurred in episode 5. We see\na) TD(0) only updated the last state, b) TD( _λ_ ) updated the\ntrajectory in this episode, and c) ET( _λ_ ) additionally updated\ntrajectories from earlier (unrewarding) episodes.\n\n\nTraces provide temporal credit assignment, but do not assign credit _counterfactually_ to states or actions that _could_\nhave led to the current state, but did not do so this time.\nCredit will eventually trickle backwards over the course of\nmultiple visits, but this can take many iterations. As an example, suppose we collect a key to open a door, which leads\nto an unexpected reward. Using standard one-step TD learning, we would update the state in which the door opened.\nUsing eligibility traces, we would also update the preceding\ntrajectory, including the acquisition of the key. But we would\nnot update other sequences that _could_ have led to the reward,\nsuch as collecting a spare key or finding a different entrance.\nThe problem of credit assignment to counterfactual states\nmay be addressed by learning a model, and using the model\nto propagate credit (cf. Sutton 1990; Moore and Atkeson\n1993; Chelu, Precup, and van Hasselt 2020); however, it\nhas often proven challenging to construct and use models\neffectively in complex environments (cf. van Hasselt, Hessel,\nand Aslanides 2019).\nWe introduce a new approach to counterfactual credit assignment, based on the concept of _expected eligibility traces_ .\nWe present a family of algorithms, which we call ET( _λ_ ), that\nuse expected traces to update their predictions. We analyse\nthe nature of these expected traces, and illustrate their benefits empirically in several settings—see Figure 1 for a first\n\n\n\n9997\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-1.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-3.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-4.png)\n\n\nillustration. We introduce a bootstrapping mechanism that\nprovides a spectrum of algorithms between standard eligibility traces and expected eligibility traces, and also discuss\nways to apply these ideas with deep neural networks. Finally,\nwe discuss possible extensions and connections to related\nideas such as successor features.\n\n\n**Background**\nSequential decision problems can be modelled as Markov\ndecision processes [1] (MDP) ( _S, A, p_ ) (Puterman 1994), with\nstate space _S_, action space _A_, and a joint transition and\nreward distribution _p_ ( _r, s_ _[′]_ _|s, a_ ). An agent selects actions according to its policy _π_, such that _At ∼_ _π_ ( _·|St_ ) where _π_ ( _a|s_ )\ndenotes the probability of selecting _a_ in _s_, and observes random rewards and states generated according to the MDP, resulting in trajectories _τt_ : _T_ = _{St, At, Rt_ +1 _, St_ +1 _, . . ., ST }_ .\nA central goal is to predict _returns_ of future discounted rewards (Sutton and Barto 2018)\n\n\n_Gt ≡_ _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _Rt_ +2 + _γt_ +1 _γt_ +2 _Rt_ +3 + _. . ._\n\n\n\n=\n\n\n\n_T_\n� _γt_ [(] +1 _[i][−]_ [1)] _[R][t]_ [+] _[i][,]_\n\n\n_i_ =1\n\n\n\nwhere _T_ is for instance the time the current episode terminates or _T_ = _∞_, and where _γt ∈_ [0 _,_ 1] is a (possibly constant) discount factor and _γt_ [(] _[n]_ [)] = [�] _[n]_ _k_ =0 _[−]_ [1] _[γ][t]_ [+] _[k]_ [, and] _[ γ]_ _t_ [(0)] = 1.\nThe value _vπ_ ( _s_ ) = E [ _Gt|St_ = _s, π_ ] of state _s_ is the expected return for a policy _π_ . Rather than writing the return as\na random variable _Gt_, it will be convenient to instead write it\nas an explicit function _G_ ( _τ_ ) of the random trajectory _τ_ . Note\nthat _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ).\nWe approximate the value with a function _v_ **w** ( _s_ ) _≈_ _vπ_ ( _s_ ).\nThis can for instance be a table—with a single separate entry\n_w_ [ _s_ ] for each state—a linear function of some input features,\nor a non-linear function such as a neural network with parameters **w** . The goal is to iteratively update **w** with\n\n\n**w** _t_ +1 = **w** _t_ + ∆ **w** _t_\n\n\nsuch that _v_ **w** approaches the true _vπ_ . Perhaps the simplest\nalgorithm to do so is the Monte Carlo (MC) algorithm\n\n\n∆ **w** _t ≡_ _α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._\n\n\nMonte Carlo is effective, but has high variance, which can\nlead to slow learning. TD learning (Sutton 1988; Sutton and\nBarto 2018) instead replaces the return with the current estimate of its expectation _v_ ( _St_ +1) _≈_ _G_ ( _τt_ +1: _T_ ), yielding\n\n\n∆ **w** _t ≡_ _αδt∇_ **w** _v_ **w** ( _St_ ) _,_ (1)\nwhere _δt ≡_ _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ ) _,_\n\n\nwhere _δt_ is called the temporal-difference (TD) error. We\ncan interpolate between these extremes, for instance with\n_λ_ -returns which smoothly mix values and sampled returns:\n\n\n_G_ _[λ]_ ( _τt_ : _T_ ) = _Rt_ +1+ _γt_ +1�(1 _−λ_ ) _v_ **w** ( _St_ +1)+ _λG_ _[λ]_ ( _τt_ +1: _T_ )� _._\n\n\n‘Forward view’ algorithms, like the MC algorithm, use returns\nthat depend on future trajectories and need to wait until the\n\n\n1The ideas in this paper extend naturally to POMDPs (cf. **?** ).\n\n\n\nend of an episode to construct their updates, which can take a\nlong time. Conversely, ‘backward view’ algorithms rely only\non past experiences and can update their predictions online,\nduring an episode. Such algorithms build an _eligibility trace_\n(Sutton 1988; Sutton and Barto 2018). An example is TD( _λ_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ _t,_ with _**e**_ _t_ = _γtλ_ _**e**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ ) _,_\n\n\nwhere _**e**_ _t_ is an accumulating eligibility trace. This trace can\nbe viewed as a function _**e**_ _t ≡_ _**e**_ ( _τ_ 0: _t_ ) of the trajectory of past\ntransitions. The TD update in (1) is known as TD(0), because\nit corresponds to using _λ_ = 0. TD( _λ_ = 1) corresponds to an\nonline implementation of the MC algorithm. Other variants\nexist, using other kinds of traces, and equivalences have been\nshown between these algorithms and their forward views that\nuse _λ_ -returns: these backward-view algorithms converge to\nthe same solution as the corresponding forward view, and can\nin some cases yield equivalent weight updates (Sutton 1988;\nvan Seijen and Sutton 2014; van Hasselt and Sutton 2015).\n\n\n**Expected Traces**\n\n\nThe main idea of this paper is to use the concept of an _ex-_\n_pected eligibility trace_, defined as\n\n\n_**z**_ ( _s_ ) _≡_ E [ _**e**_ _t | St_ = _s_ ] _,_\n\n\nwhere the expectation is over the agent’s policy and the MDP\ndynamics. We introduce a concrete family of algorithms,\nwhich we call ET( _λ_ ) and ET( _λ_, _η_ ), that learn expected traces\nand use them in value updates. We analyse these algorithms\ntheoretically, describe specific instances, and discuss computational and algorithmic properties.\n\n\n**ET(** _λ_ **)**\n\n\nWe propose to learn approximations _**zθ**_ ( _s_ ) _≈_ _**z**_ ( _s_ ), with parameters _**θ**_ _∈_ R _[d]_ (e.g., the weights of a neural network). One\nway to learn _**zθ**_ is by updating it toward the instantaneous\ntrace _**e**_ _t_, by minimizing an empirical loss _L_ ( _**e**_ _t,_ _**zθ**_ ( _St_ )). For\ninstance, _L_ could be a component-wise squared loss, optimized with stochastic gradient descent:\n\n\n_**θ**_ _t_ +1 = _**θ**_ _t_ + ∆ _**θ**_ _t,_ where (2)\n\n\n1\n\n∆ _**θ**_ _t_ = _−β_ _[∂]_\n\n_∂_ _**θ**_ 2 [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))] _[⊤]_ [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))]\n\n= _β_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] ( _**e**_ _t −_ _**zθ**_ ( _St_ )) _,_ (3)\n\n_∂_ _**θ**_\n\n\nwhere _[∂z]_ _**[θ]**_ _∂_ [(] _**θ**_ _[S][t]_ [)] is a _|_ _**θ**_ _| × |_ _**e**_ _|_ Jacobian [2] and _β_ is a step size.\n\nThe idea is then to use _**zθ**_ ( _s_ ) _≈_ E [ _**e**_ _t | St_ = _s_ ] in place\nof _**e**_ _t_ in the value update, which becomes\n\n\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (4)\n\n\nWe call this ET( _λ_ ). Below, we prove that this update can\nbe unbiased and can have lower variance than TD( _λ_ ). Algorithm 1 shows pseudo-code for a concrete instance of ET( _λ_ ).\n\n\n2The Jacobian-vector product can efficiently be computed (e.g.,\nvia auto-differentiation) with computational requirements that are\ncomparable to the computation of the loss.\n\n\n\n9998\n\n\n\n\n**Algorithm 1** ET( _λ_ )\n\n\n1: initialise **w**, _**θ**_\n2: **for** _M_ episodes **do**\n3: initialise _**e**_ = **0**\n\n4: observe initial state _S_\n5: **repeat** for each step in episode _m_\n6: generate _R_ and _S_ _[′]_\n\n7: _δ ←_ _R_ + _γv_ **w** ( _S_ _[′]_ ) _−_ _v_ **w** ( _S_ )\n8: _**e**_ _←_ _γλ_ _**e**_ + _∇_ **w** _v_ **w** ( _S_ )\n\n9: _**θ**_ _←_ _**θ**_ + _β_ _[∂]_ _**[z]**_ _∂_ _**[θ]**_ _**θ**_ [(] _[S]_ [)] ( _**e**_ _−_ _**zθ**_ ( _S_ ))\n\n10: **w** _←_ **w** + _αδ_ _**zθ**_ ( _S_ )\n11: **until** _S_ is terminal\n\n12: **end for**\n\n13: **Return w**\n\n\n**Interpretation and ET(** _λ, η_ **)**\n\nWe can interpret TD(0) as taking the MC update and replacing the return from the subsequent state, which is a function\nof the future trajectory, with a state-based estimate of its expectation: _v_ **w** ( _St_ +1) _≈_ E [ _G_ ( _τt_ +1: _T_ ) _|St_ +1 ]. This becomes\nmost clear when juxtaposing the updates:\n\n\n_α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _,_ (MC)\n_α_ ( _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._ (TD)\n\n\nTD( _λ_ ) also uses a function of a trajectory: the trace _**e**_ _t_ . We\npropose replacing this as well with a function of state: the\nexpected trace _**zθ**_ ( _St_ ) _≈_ E [ _**e**_ ( _τ_ 0: _t_ ) _|St_ ]. Again juxtaposing:\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ ( _τ_ 0: _t_ ) _,_ (TD( _λ_ ))\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (ET( _λ_ ))\n\n\nWe can interpolate smoothly between MC and TD(0) via\n_λ_ . This is often useful to trade off variance of the return with\npotential bias of the value estimate. For instance, we might\nnot have access to the true state _s_, and might instead have to\nrely on features **x** ( _s_ ). Then we cannot always represent or\nlearn the true values _v_ ( _s_ )—for instance different states may\nbe aliased (Whitehead and Ballard 1991).\nSimilarly, when moving from TD( _λ_ ) to ET( _λ_ ) we replaced\na trajectory-based trace with a state-based estimate. This\nmight induce bias and, again, we can smoothly interpolate by\nusing a recursively defined mixture trace _**y**_ _t_, as defined as [3]\n\n\n_**y**_ _t_ = (1 _−_ _η_ ) _**zθ**_ ( _St_ ) + _η_ � _γtλ_ _**y**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ )� _._ (5)\n\n\nThis recursive usage of the estimates _**zθ**_ ( _s_ ) at previous states\nis analogous to bootstrapping on future state values when\nusing a _λ_ -return, with the important difference that the arrow\nof time is opposite. This means we do not first have to convert\nthis into a backward view: the quantity can already be computed from past experience directly. We call the algorithm\nthat uses this mixture trace ET( _λ_, _η_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**y**_ ( _St_ ) _._ (ET( _λ_, _η_ ))\n\n\n3While _**y**_ _t_ depends on both _η_ and _λ_ we leave this dependence\nimplicit, as is conventional for traces.\n\n\n\nNote that if _η_ = 1 then _**y**_ _t_ = _**e**_ _t_ equals the instantaneous\ntrace: ET( _λ_, 1) is equivalent to TD( _λ_ ). If _η_ = 0 then _**y**_ _t_ = _**z**_ _t_\nequals the expected trace; the algorithm introduced earlier\nas ET( _λ_ ) is equivalent to ET( _λ_, 0). By setting _η ∈_ (0 _,_ 1), we\ncan smoothly interpolate between these extremes.\n\n\n**Theoretical Analysis**\n\nWe now analyse the new ET algorithms theoretically. First\nwe show that if we use _**z**_ ( _s_ ) directly and _s_ is Markov then the\nupdate has the same expectation as TD( _λ_ ) (though possibly\nwith lower variance), and therefore also inherits the same\nfixed point and convergence properties.\n\n\n**Lemma 1.** _If s is Markov, then_\n\n\nE [ _δt_ _**e**_ _t | St_ = _s_ ] = E [ _δt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] _._\n\n\n_Proof._ In Appendix .\n\n\n**Proposition 1.** _Let_ _**e**_ _t be any trace vector, updated in any_\n_way. Let_ _**z**_ ( _s_ ) = E [ _**e**_ _t | St_ = _s_ ] _. Consider the ET(λ) algo-_\n_rithm_ ∆ **w** _t_ = _αtδt_ _**z**_ ( _St_ ) _. For all Markov states s the expec-_\n_tation of this update is equal to the expected update under_\n_instantaneous trace_ _**e**_ _t, and its variance is lower or equal:_\n\n\nE [ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] = E [ _αtδt_ _**e**_ _t|St_ = _s_ ] _and_\nV[ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] _≤_ V[ _αtδt_ _**e**_ _t|St_ = _s_ ] _,_\n\n\n_where the second inequality holds component-wise for the_\n_update vector, and is strict when_ V[ _**e**_ _t|St_ ] _>_ 0 _._\n\n\n_Proof._ We have\n\n\nE [ _αtδt_ _**e**_ _t | St_ = _s_ ]\n= E [ _αtδt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] (Lemma 1)\n= E [ _αtδt | St_ = _s_ ] _**z**_ ( _s_ )\n= E [ _αtδt_ _**z**_ ( _St_ ) _| St_ = _s_ ] _._ (6)\n\n\nDenote the _i_ -th component of _**z**_ ( _St_ ) by _zt,i_ and the _i_ -th\ncomponent of _**e**_ _t_ by _et,i_ . Then, we also have\n\n\nE � ( _αtδtzt,i_ ) [2] _|St_ = _s_ � = E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � _zt,i_ [2]\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E [ _et,i|St_ = _s_ ] [2]\n\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ ��E � _e_ [2] _t,i_ _[|][S][t]_ [=] _[ s]_ � _−_ V[ _et,i|St_ = _s_ ]�\n\n_≤_ E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E � _e_ [2] _t,i_ _[|][ S][t]_ [=] _[ s]_ �\n\n= E � ( _αtδtet,i_ ) [2] _| St_ = _s_ � _,_\n\n\nwhere the last step used the fact that _s_ is Markov, and the inequality is strict when V[ _et,i|St_ ] _>_ 0. Since the expectations\nare equal, as shown in (6), the conclusion follows.\n\n\n**Interpretation** Proposition 1 is a strong result: it holds for\nany trace update, including accumulating traces (Sutton 1984,\n1988), replacing traces (Singh and Sutton 1996), dutch traces\n(van Seijen and Sutton 2014; van Hasselt, Mahmood, and\nSutton 2014; van Hasselt and Sutton 2015), and future traces\nthat may be discovered. It implies convergence of ET( _λ_ )\nunder the same conditions as TD( _λ_ ) (Dayan 1992; Peng 1993;\n\n\n\n9999\n\n\n\n\nTsitsiklis 1994) with lower variance when V[ _**e**_ _t|St_ ] _>_ 0,\nwhich is the common case.\nNext, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a\nlearned approximation _**z**_ _t_ ( _s_ ) _≈_ _**z**_ ( _s_ ) that relies solely on\nobserved experience.\n\n**Proposition 2.** _Let_ _**e**_ _t an instantaneous trace vector. Then_\n1 _nt_ ( _s_ )\n_let_ _**z**_ _t_ ( _s_ ) _be the empirical mean_ _**z**_ _t_ ( _s_ ) = _nt_ ( _s_ ) � _i_ _**e**_ _t_ _[s]_ _i_ _[,]_\n_where t_ _[s]_ _i_ _[denotes past times when we have been in state]_\n_s, that is St_ _[s]_ _i_ [=] _[ s][, and][ n][t]_ [(] _[s]_ [)] _[ is the number of visits to][ s]_\n_in the first t steps. Consider the expected trace algorithm_\n**w** _t_ +1 = **w** _t_ + _αtδt_ _**z**_ _t_ ( _St_ ) _. If St is Markov, the expectation of_\n_this update is equal to the expected update with instantaneous_\n_traces_ _**e**_ _t, while attaining a potentially lower variance:_\n\n\nE [ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] = E [ _αtδt_ _**e**_ _t | St_ ] _and_\nV[ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] _≤_ V[ _αtδt_ _**e**_ _t | St_ ] _,_\n\n\n_where the second inequality holds component-wise. The in-_\n_equality is strict when_ V[ _**e**_ _t | St_ ] _>_ 0 _._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** Proposition 2 mirrors Proposition 1 but, importantly, covers the case where we estimate the expected\ntraces from data, rather than relying on exact estimates. This\nmeans the benefits extend to this pure learning setting. Again,\nthe result holds for any trace update. The inequality is typically strict when the path leading to state _St_ = _s_ is stochastic\n(due to environment or policy).\nNext we consider what happens if we do not have Markov\nstates and instead have to rely on, possibly non-Markovian,\nfeatures **x** ( _s_ ). We then have to pick a function class and for\nthe purpose of this analysis we consider linear expected traces\n_**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) and values _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), as convergence for non-linear values can not always be assured even\nfor standard TD( _λ_ ) (Tsitsiklis and Van Roy 1997), without\nadditional assumptions (e.g., Ollivier 2018; Brandfonbrener\nand Bruna 2020).\n\n**Proposition 3.** _When using approximations z_ **Θ** ( _s_ ) = **Θx** ( _s_ )\n_and v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ) _then, if_ (1 _−_ _η_ ) **Θ** + _η_ I _is non-singular,_\n_ET(λ, η) has the same fixed point as TD(λη)._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** This result implies that linear ET( _λ_, _η_ ) converges under similar conditions as linear TD( _λ_ _[′]_ ) for _λ_ _[′]_ = _λ·η_ .\nIn particular, when **Θ** is non-singular, using the approximation _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) in ET( _λ_, 0) = ET( _λ_ ) implies convergence to the fixed point of TD(0).\nThough ET( _λ_, _η_ ) and TD( _λη_ ) have the same fixed point,\nthe algorithms are not equivalent. In general, their updates\nare not the same. Linear approximations are more general\nthan tabular functions (which are linear functions of a indicator vector for the current state), and we have already seen\nin Figure 1 that ET( _λ_ ) behaves quite differently from both\nTD(0) and TD( _λ_ ), and we have seen its variance can be lower\nin Propositions 1 and 2. Interestingly, **Θ** resembles a preconditioner that speeds up the linear semi-gradient TD update,\n\n\n10000\n\n\n\nepisode 5\n1st reward\n\n\n\nepisode 12\n2nd reward\n\n\n\nepisode 100\n20 rewards\n\n\n\nepisode 1K\n~200 rewards\n\n\n\nepisode 10K\n~2K rewards\n\n\n\nFigure 2: In the same setting as Figure 1, we show later value\nestimates after more rewards have been observed. TD(0)\nlearns slowly but steadily, TD( _λ_ ) learns faster but with higher\nvariance, and ET( _λ_ ) learns both fast and stable.\n\n\nsimilar to how second-order optimisation algorithms (Amari\n1998; Martens 2016) precondition the gradient updates.\n\n\n**Empirical Analysis**\n\nFrom the insights above, we expect that ET( _λ_ ) yields lower\nprediction errors because it has lower variance and aggregates information across episodes better. In this section we\nempirically investigate expected traces in several experiments.\nWhenever we refer to ET( _λ_ ), this is equivalent to ET( _λ_, 0).\n\n\n\n**An Open World**\n\nFirst consider the grid world depicted in Figure 1. The agent\nrandomly moves right or down (excluding moves that would\nhit a wall), starting from the top-left corner. Any action in the\nbottom-right corner terminates the episode with +1 reward\nwith probability 0 _._ 2, and 0 otherwise. All other rewards are 0.\nFigure 1 shows value estimates after the first positive reward, which occurred in the fifth episode. TD(0) updated a\nsingle state, TD( _λ_ ) updated earlier states in that episode, and\nET( _λ_ ) additionally updated states from previous episodes.\nFigure 2 additionally shows value estimates after the\nsecond reward (which occurred in episode 12), and after\nroughly 20, 200, and 2000 rewards (or 100, 1000, and 10 _,_ 000\nepisodes, respectively). ET( _λ_ ) converged faster than TD(0),\nwhich propagated information slowly, and faster than TD( _λ_ ),\nwhich exhibited higher variance. All step sizes decayed as\n_α_ = _β_ = ~~�~~ 1 _/k_, where _k_ is the current episode number.\n\n\n**A Multi-Chain**\n\nWe now consider the multi-chain shown in Figure 3. We\nfirst compare TD( _λ_ ) and ET( _λ_ ) with tabular values on various variants of the multi-chain, corresponding to _m ∈_\n_{_ 1 _,_ 2 _,_ 4 _,_ 8 _, ...,_ 128 _}_ parallel chains of length _n_ = 4. The leftmost plot in Figure 4 shows the average root mean squared\nerror (RMSE) of the value predictions after 1024 episodes.\nWe ran 10 seeds for each combination of step size 1 _/t_ _[d]_ with\n_d ∈{_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 1 _}_ and _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ .\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-4-0.png)\n\neither is +1 with probability 0 _._ 9 or _−_ 1 with probability 0 _._ 1.\n\n\nThe left plot in Figure 4 shows value errors for different\n_m_, minimized over _d_ and _λ_ . The prediction error of TD( _λ_ )\n(blue) grew quickly with the number of parallel chains. ET( _λ_ )\n(orange) scaled better, because it updates values in multiple\nchains (from past episodes) upon receiving a surprising reward (e.g., _−_ 1) on termination. The other three plots in Figure\n4 show value error as a function of _λ_ for a subset of problems\ncorresponding to _m ∈{_ 8 _,_ 32 _,_ 128 _}_ . The dependence on _λ_\ndiffers across algorithms and problem instances, but ET( _λ_ )\nconsistently achieved lower error than TD( _λ_ ), especially with\nhigh _λ_ . Further analysis, including on step-size sensitivity, is\nincluded in the appendix.\nNext, we encode each state with a feature vector **x** ( _s_ )\ncontaining a binary indicator vector of the branch, a binary\nindicator of the progress along the chain, a bias that always\nequals one, and two binary features indicating when we are in\nthe start (white) or bottleneck (orange) state. We extend the\nlengths of the chains to _n_ = 16. Both TD( _λ_ ) and ET( _λ_ ) use\na linear value function _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), and ET( _λ_ ) uses a\nlinear expected trace _z_ **Θ** ( _s_ ) = **Θx** ( _s_ ). All updates use the\nsame constant step size _α_ . The left plot in Figure 5 shows the\naverage root mean squared value error after 1024 episodes\n(averaged over 10 seeds). For each point the best constant\nstep size _α ∈{_ 0 _._ 01 _,_ 0 _._ 03 _,_ 0 _._ 1 _}_ (shared across all updates)\nand _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ is selected. ET( _λ_ ) (orange)\nattained lower errors across all values of _m_ (left plot), and\nfor all _λ_ (center two plots, for two specific _m_ ). The right plot\nshows results for smooth interpolations via _η_, for _λ_ = 0 _._ 9\nand _m_ = 16. The full expected trace ( _η_ = 0) performed well\nhere, we expect in other settings the additional flexibility of\n_η_ could be beneficial.\n\n\n**Expected Traces in Deep Reinforcement Learning**\n\n(Deep) neural networks are a common choice of function\nclass in reinforcement learning (e.g., Werbos 1990; Tesauro\n1992, 1994; Bertsekas and Tsitsiklis 1996; Prokhorov and\nWunsch 1997; Riedmiller 2005; van Hasselt 2012; Mnih\net al. 2015; van Hasselt, Guez, and Silver 2016; Wang et al.\n2016; Silver et al. 2016; Duan et al. 2016; Hessel et al. 2018).\nEligibility traces are not very commonly combined with deep\nnetworks (but see Tesauro 1992; Elfwing, Uchibe, and Doya\n2018), perhaps in part because of the popularity of experience\n\n\n10001\n\n\n\nreplay (Lin 1992; Mnih et al. 2015; Horgan et al. 2018).\nPerhaps the simplest way to extend expected traces to deep\nneural networks is to first separate the value function into\na representation **x** ( _s_ ) and a value _v_ ( **w** _,_ _**ξ**_ )( _s_ ) = **w** _[⊤]_ **x** _**ξ**_ ( _s_ ),\nwhere **x** _**ξ**_ is some (non-linear) function of the observations\n_s_ . [4] We can then apply the same expected trace algorithm as\nused in the previous sections by learning a separate linear\nfunction _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) using the representation which is\nlearned by backpropagating the value updates:\n\n\n**w** _t_ +1 = **w** _t_ + _αδ_ _**z**_ **Θ** ( _St_ ) _,_\n\n_**ξ**_ _t_ +1 = _**ξ**_ _t_ + _αδ_ _**e**_ _**[ξ]**_ _t_ _[,]_\n\nwhere _**e**_ _**[ξ]**_ _t_ [=] _[ γ][t][λ]_ _**[e][ξ]**_ _t−_ 1 [+] _[ ∇]_ _**[ξ]**_ _[v]_ [(] **[w]** _[,]_ _**[ξ]**_ [)][(] _[S][t]_ [)] _[,]_\n\n_**e**_ **[w]** _t_ [=] _[ γ][t][λ]_ _**[e]**_ **[w]** _t−_ 1 [+] _[ ∇]_ **[w]** _[v]_ ( **w** _,_ _**ξ**_ ) [(] _[S][t]_ [)] _[,]_\n\n\nand then updating **Θ** to minimise component-wise squared\ndifferences between _**e**_ **[w]** _t_ [and] _**[ z]**_ **[Θ]** _t_ [(] _[S][t]_ [)][, as in (2) and (3).]\nInteresting challenges appear outside the fully linear case.\nFirst, the representation will itself be updated and will have\nits own trace _**e**_ _**[ξ]**_ _t_ [. Second, in the control case we optimise]\nbehaviour: the policy will change. Both these properties of\nthe non-linear control setting imply that the expected traces\nmust track a non-stationary target. We found that being able to\ntrack this rather quickly improved performance: the expected\ntrace parameters **Θ** in the following experiment were updated\nwith a relatively high step size of _β_ = 0 _._ 1.\nWe tested this idea on two canonical Atari games: Pong and\nMs. Pac-Man. The results in Figure 6 show that the expected\ntraces helped speed up learning compared to the baseline\nwhich uses accumulating traces, for various step sizes. Unlike\nmost prior work on this domain, which often relies on replay\n(Mnih et al. 2015; Schaul et al. 2016; Horgan et al. 2018)\nor parallel streams of experience (Mnih et al. 2016), these\nalgorithms updated the values online from a single stream\nof experience. Further details on the experimental setup are\ngiven in the appendix.\nThese experiments demonstrate that the idea of expected\ntraces extends to non-linear function approximation, such as\ndeep neural networks. We consider this to be a rich area of\nfurther investigations. The results presented here are similar\nto earlier results (e.g., Mnih et al. 2015) and are not meant to\ncompete with state-of-the-art performance results, which often depend on replay and much larger amounts of experience\n(e.g., Horgan et al. 2018).\n\n\n**Discussion and Extensions**\n\nWe now discuss various interesting interpretations and relations, and discuss promising extensions.\n\n\n**Predecessor Features**\n\nFor linear value functions the expected trace _z_ ( _s_ ) can be\nexpressed non-recursively as follows:\n\n\n\n�\n\n\n\n_**z**_ ( _s_ ) = E\n\n\n\n_∞_\n� _λ_ [(] _t_ _[n]_ [)] _γt_ [(] _[n]_ [)] **x** _t−n | St_ = _s_\n� _n_ =0\n\n\n\n_,_ (7)\n\n\n\n4Here _s_ denotes observations to the agent, not a full environment\nstate— _s_ is not assumed to be Markovian.\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-1.png)\n\nFigure 4: Prediction errors in the multi-chain. ET( _λ_ ) (orange) consistently outperformed TD( _λ_ ) (blue). Shaded areas depict\nstandard errors across 10 seeds.\n\n\nFigure 5: Comparing value error with linear function approximation a) as function of the number of branches (left), b) as\nfunction of _λ_ (center two plots) and c) as function of _η_ (right). The left three plots show comparisons of TD( _λ_ ) (blue) and ET( _λ_ )\n(orange), showing ET( _λ_ ) attained lower prediction errors. The right plot interpolates between these algorithms via ET( _λ_, _η_ ),\nfrom ET( _λ_ ) = ET( _λ_, 0) to ET( _λ_, 1) = TD( _λ_ ), with _λ_ = 0 _._ 9 (corresponding to a vertical slice indicated in the second plot).\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-3.png)\n\nwhere _γk_ [(] _[n]_ [)] _≡_ [�] _[k]_ _j_ = _k−n_ _[γ][j]_ [. This is interestingly similar to the]\ndefinition of _successor features_ (Barreto et al. 2017):\n\n\n\n�\n\n\n\n_ψ_ ( _s_ ) = E\n\n\n\n_∞_\n� _γt_ [(] +1 _[n][−]_ [1)] **x** _t_ + _n | St_ = _s_\n� _n_ =1\n\n\n\n_._ (8)\n\n\n\nThe summation in (8) is over future features, while in (7)\nwe have a sum over features already observed by the agent.\nWe can thus think of linear expected traces as _predecessor_\n_features_ . A similar connection was made in the tabular setting by Pitis (2018), relating source traces, which aim to\nestimate the source matrix ( _I −_ _γP_ ) _[−]_ [1], to successor representations (Dayan 1993). In a sense, the above generalises\nthis insight. In addition to being interesting in its own right,\nthis connection allows for an intriguing interpretation of _**z**_ ( _s_ )\nas a multidimensional value function. Like with successor\nfeatures, the features **x** _t_ play the role of rewards, discounted\nwith _γ · λ_ rather than _γ_, and with time flowing backwards.\nAlthough the predecessor interpretation only holds in the\nlinear case, it is also of interest as a means to obtain a practical\nimplementation of expected traces with non-linear function\napproximation, for instance applied only to the linear ‘head’\nof a deep neural network. We used this ‘predecessor feature\ntrick’ in our Atari experiments described earlier.\n\n\n**Relation to Model-Based Reinforcement Learning**\n\n\nModel-based reinforcement learning provides an alternative\napproach to efficient credit assignment. The general idea is\nto construct a model that estimates state-transition dynamics,\nand to update the value function based upon hypothetical\n\n\n10002\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-4.png)\n\ntransitions drawn from the model (Sutton 1990), for example\nby prioritised sweeping (Moore and Atkeson 1993; van Seijen\nand Sutton 2013). In practice, model-based approaches have\nproven challenging in environments (such as Atari games)\nwith rich perceptual observations, compared to model-free\napproaches that more directly update the agent’s policy and\npredictions (van Hasselt, Hessel, and Aslanides 2019).\nIn some sense, expected traces also construct a model of\nthe environment—but one that differs in several key regards\nfrom standard state-to-state models used in model-based reinforcement learning. First, expected traces estimate _past_\nquantities rather than _future_ quantities. Backward planning\n(e.g., Chelu, Precup, and van Hasselt 2020) is possible with\nexplicit transition models, but is less common in practice.\nSecond, expected traces estimate the accumulation of _gradi-_\n_ents_ over a multi-step trajectory, rather than trying to learn\nthe full transition dynamics, thereby focusing only on those\naspects that matter for the eventual weight update. Third, expected traces allow credit assignment across these potential\npast trajectories with a single update, without the iterative\ncomputation that is typically required when using a dynamics\nmodel. These differences may be important to side-step some\nof the challenges faced in model-based learning.\n\n\n**Batch Learning and Replay**\n\n\nWe have mainly considered the online learning setting in this\npaper. It is often convenient to learn from batches of data, or\nreplay transitions repeatedly, to enhance data efficiency. A\nnatural extension is replay the experiences sequentially (e.g.\nKapturowski et al. 2018), but perhaps alternatives exist. We\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-1.png)\n\nFigure 6: Performance of Q( _λ_ ) ( _η_ = 1, blue) and QET( _λ_ ) ( _η_ = 0, orange) on Pong and Ms.Pac-Man for various learning rates.\nShaded regions show standard error across 10 random seeds. All results are for _λ_ = 0 _._ 95. Further implementation details and\nhyper-parameters are in the appendix.\n\n\n\nnow discuss one potential extension.\nWe defined a mixed trace _**y**_ _t_ that mixes the instantaneous\nand expected traces. Optionally the expected trace _**z**_ _t_ can\nbe updated towards the mixed trace _**y**_ _t_ as well, instead of\ntowards the instantaneous trace _**e**_ _t_ . Analogously to TD( _λ_ ) we\npropose to then use at least one real step of data:\n\n\n∆ _**θ**_ _t ≡_ _β_ ( _**∇**_ _t_ + _γtλt_ _**y**_ _t−_ 1 _−_ _**zθ**_ ( _St_ )) _[⊤]_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] _,_ (9)\n\n_∂_ _**θ**_\n\n\nwith _**∇**_ _t ≡∇_ **w** _v_ **w** ( _St_ ). This is akin to a forward-view _λ_ return update, with _∇_ **w** _v_ **w** ( _St_ ) in the role of (vector) reward,\nand _**zθ**_ of value, and discounted by _λtγt_, but reversed in time.\nIn other words, this can be considered a sampled Bellman\nequation (Bellman 1957) but backward in time.\nWhen we then choose _η_ = 0, then _**y**_ _t−_ 1 = _z_ _**θ**_ ( _St−_ 1), and\nthen the target in (9) only depends on a single transition.\nInterestingly, that means we can then learn expected traces\nfrom _individual_ transitions, sampled out of temporal order,\nfor instance in batch settings or when using replay.\n\n\n**Application to Other Traces**\n\n\nWe can apply the idea of expected trace to more traces than\nconsidered here. We can for instance consider the characteristic eligibility trace used in REINFORCE (Williams 1992)\nand related policy-gradient algorithms (Sutton et al. 2000).\nAnother appealing application is to the follow-on trace\nor _emphasis_, used in emphatic temporal difference learning\n(Sutton, Mahmood, and White 2016) and related algorithms\n(e.g., Imani, Graves, and White 2018). Emphatic TD was\nproposed to correct an important issue with off-policy learning, which can be unstable and lead to diverging learning\ndynamics. Emphatic TD weights updates according to 1) the\ninherent interest in having accurate predictions in that state\nand, 2) the importance of predictions in that state for updating\n\n\n10003\n\n\n\nother predictions. Emphatic TD uses scalar ‘follow-on’ traces\nto determine the ‘emphasis’ for each update. However, this\nfollow-on trace can have very high, even infinite, variance.\nInstead, we might estimate and use its expectation instead of\nthe instantaneous emphasis. A related idea was explored by\nZhang, Boehmer, and Whiteson (2019) to obtain off-policy\nactor critic algorithms.\n\n\n**Conclusion**\n\n\nWe have proposed a mechanism for efficient credit assignment, using the expectation of an eligibility trace. We have\ndemonstrated this can sometimes speed up credit assignment\ngreatly, and have analyzed concrete algorithms theoretically\nand empirically to increase understanding of the concept.\nExpected traces have several interpretations. First, we can\ninterpret the algorithm as counterfactually updating multiple possible trajectories leading up to the current state. Second, they can be understood as trading off bias and variance,\nwhich can be done smoothly via a unifying _η_ parameter, between standard eligibility traces (low bias, high variance) and\nestimated traces (possibly higher bias, but lower variance).\nFurthermore, with tabular or linear function approximation\nwe can interpret the resulting expected traces as predecessor\nstates or features—object analogous to successor states or features, but time-reversed. Finally, we can interpret the linear\nalgorithm as preconditioning the standard TD update, thereby\npotentially speeding up learning. These interpretations suggest that a variety of complementary ways to potentially\nextend these concepts and algorithms.\nWe have shown expected traces can already be used to\nenhance learning in non-linear settings (i.e., deep reinforcement learning), and in the control setting where we update\nthe policy. Further work is needed to determine the full extent\nof the possibilities of these new algorithms.\n\n\n\n\n**References**\n\n\nAmari, S. I. 1998. Natural gradient works efficiently in\nlearning. _Neural computation_ 10(2): 251–276. ISSN 08997667.\n\n\nBarreto, A.; Dabney, W.; Munos, R.; Hunt, J. J.; Schaul, T.;\nvan Hasselt, H. P.; and Silver, D. 2017. Successor features\nfor transfer in reinforcement learning. In _Advances in neural_\n_information processing systems_, 4055–4065.\n\n\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. _J. Artif. Intell. Res. (JAIR)_ 47:\n253–279.\n\n\nBellman, R. 1957. _Dynamic Programming_ . Princeton University Press.\n\n\nBertsekas, D. P.; and Tsitsiklis, J. N. 1996. _Neuro-dynamic_\n_Programming_ . Athena Scientific, Belmont, MA.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018a. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018b. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBrandfonbrener, D.; and Bruna, J. 2020. Geometric Insights\ninto the Convergence of Non-linear TD Learning. In _Interna-_\n_tional Conference on Learning Representations_ .\n\n\nChelu, V.; Precup, D.; and van Hasselt, H. P. 2020. Forethought and Hindsight in Credit Assignment. In Larochelle,\nH.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H.,\neds., _Advances in Neural Information Processing Systems_,\nvolume 33, 2270–2281.\n\n\nDayan, P. 1992. The convergence of TD( _λ_ ) for general\nlambda. _Machine Learning_ 8: 341–362.\n\n\nDayan, P. 1993. Improving generalization for temporal difference learning: The successor representation. _Neural Com-_\n_putation_ 5(4): 613–624.\n\n\nDuan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel,\nP. 2016. Benchmarking deep reinforcement learning for\ncontinuous control. In _International Conference on Machine_\n_Learning_, 1329–1338.\n\n\nElfwing, S.; Uchibe, E.; and Doya, K. 2018. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_ 107:\n3–11.\n\n\nHennigan, T.; Cai, T.; Norman, T.; and Babuschkin, I. 2020.\nHaiku: Sonnet for JAX. URL http://github.com/deepmind/\ndm-haiku.\n\n\nHessel, M.; Modayil, J.; van Hasselt, H. P.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2018. Rainbow: Combining Improvements in Deep\nReinforcement Learning. _AAAI_ .\n\n\n10004\n\n\n\nHorgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel,\nM.; van Hasselt, H. P.; and Silver, D. 2018. Distributed\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ .\n\n\nImani, E.; Graves, E.; and White, M. 2018. An Off-policy\nPolicy Gradient Theorem Using Emphatic Weightings. In\nBengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; CesaBianchi, N.; and Garnett, R., eds., _Advances in Neural Infor-_\n_mation Processing Systems 31_, 96–106. Curran Associates,\nInc. URL http://papers.nips.cc/paper/7295-an-off-policypolicy-gradient-theorem-using-emphatic-weightings.pdf.\n\n\nKaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1995.\nPlanning and Acting in Partially Observable Stochastic Domains. Unpublished report.\n\n\nKapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and\nDabney, W. 2018. Recurrent experience replay in distributed\nreinforcement learning. In _International conference on learn-_\n_ing representations_ .\n\n\nKingma, D. P.; and Adam, J. B. 2015. A method for stochastic optimization. In _International Conference on Learning_\n_Representation_ .\n\n\nLin, L. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_\n8(3): 293–321.\n\n\nMartens, J. 2016. _Second-order optimization for neural net-_\n_works_ . University of Toronto (Canada).\n\n\nMinsky, M. 1963. Steps Toward Artificial Intelligence.\nIn Feigenbaum, E.; and Feldman, J., eds., _Computers and_\n_Thought_, 406–450. McGraw-Hill, New York.\n\n\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous Methods for Deep Reinforcement Learning. In\n_International Conference on Machine Learning_ .\n\n\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg,\nS.; and Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. _Nature_ 518(7540): 529–533.\n\n\nMoore, A. W.; and Atkeson, C. G. 1993. Prioritized Sweeping: Reinforcement Learning with less Data and less Time.\n_Machine Learning_ 13: 103–130.\n\n\nOllivier, Y. 2018. Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies. _CoRR_\nabs/1805.00869.\n\n\nPeng, J. 1993. _Efficient dynamic programming-based learn-_\n_ing for control_ . Ph.D. thesis, Northeastern University.\n\n\nPeng, J.; and Williams, R. J. 1996. Incremental Multi-step\nQ-learning. _Machine Learning_ 22: 283–290.\n\n\nPitis, S. 2018. Source Traces for Temporal Difference Learning. In McIlraith, S. A.; and Weinberger, K. Q., eds., _Pro-_\n_ceedings of the Thirty-Second AAAI Conference on Artificial_\n_Intelligence_, 3952–3959. AAAI Press.\n\n\n\n\nPohlen, T.; Piot, B.; Hester, T.; Azar, M. G.; Horgan, D.;\nBudden, D.; Barth-Maron, G.; van Hasselt, H. P.; Quan, J.;\nVecerˇ ´ık, M.; Hessel, M.; Munos, R.; and Pietquin, O. 2018.\nObserve and look further: Achieving consistent performance\non Atari. _arXiv preprint arXiv:1805.11593_ .\n\n\nProkhorov, D. V.; and Wunsch, D. C. 1997. Adaptive critic\ndesigns. _IEEE Transactions on Neural Networks_ 8(5): 997–\n1007.\n\n\nPuterman, M. L. 1994. _Markov Decision Processes: Discrete_\n_Stochastic Dynamic Programming_ . John Wiley & Sons, Inc.\nNew York, NY, USA.\n\n\nRiedmiller, M. 2005. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning\nMethod. In Gama, J.; Camacho, R.; Brazdil, P.; Jorge, A.; and\nTorgo, L., eds., _Proceedings of the 16th European Conference_\n_on Machine Learning (ECML’05)_, 317–328. Springer.\n\n\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ . Puerto Rico.\n\n\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering\nthe game of Go with deep neural networks and tree search.\n_Nature_ 529(7587): 484–489.\n\n\nSingh, S. P.; and Sutton, R. S. 1996. Reinforcement Learning\nwith replacing eligibility traces. _Machine Learning_ 22: 123–\n158.\n\n\nSutton, R. S. 1984. _Temporal Credit Assignment in Reinforce-_\n_ment Learning_ . Ph.D. thesis, University of Massachusetts,\nDept. of Comp. and Inf. Sci.\n\n\nSutton, R. S. 1988. Learning to predict by the methods of\ntemporal differences. _Machine learning_ 3(1): 9–44.\n\n\nSutton, R. S. 1990. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Proceedings of the seventh international conference_\n_on machine learning_, 216–224.\n\n\nSutton, R. S.; and Barto, A. G. 2018. _Reinforcement Learning:_\n_An Introduction_ . The MIT press, Cambridge MA.\n\n\nSutton, R. S.; Mahmood, A. R.; and White, M. 2016. An\nEmphatic Approach to the Problem of Off-policy TemporalDifference Learning. _Journal of Machine Learning Research_\n17(73): 1–29.\n\n\nSutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y.\n2000. Policy gradient methods for reinforcement learning\nwith function approximation. _Advances in Neural Informa-_\n_tion Processing Systems 13 (NIPS-00)_ 12: 1057–1063.\n\n\nTesauro, G. 1992. Practical Issues in Temporal Difference\nLearning. In Lippman, D. S.; Moody, J. E.; and Touretzky,\nD. S., eds., _Advances in Neural Information Processing Sys-_\n_tems 4_, 259–266. San Mateo, CA: Morgan Kaufmann.\n\n\nTesauro, G. J. 1994. TD-Gammon, a self-teaching backgammon program, achieves master-level play. _Neural computa-_\n_tion_ 6(2): 215–219.\n\n\n10005\n\n\n\nTsitsiklis, J. N. 1994. Asynchronous stochastic approximation and Q-learning. _Machine Learning_ 16: 185–202.\n\nTsitsiklis, J. N.; and Van Roy, B. 1997. An analysis of\ntemporal-difference learning with function approximation.\n_IEEE Transactions on Automatic Control_ 42(5): 674–690.\n\nvan Hasselt, H. P. 2012. Reinforcement Learning in Continuous State and Action Spaces. In Wiering, M. A.; and\nvan Otterlo, M., eds., _Reinforcement Learning: State of the_\n_Art_, volume 12 of _Adaptation, Learning, and Optimization_,\n207–251. Springer.\n\n\nvan Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver,\nD. 2016. Learning values across many orders of magnitude. In _Advances in Neural Information Processing Systems_\n_29: Annual Conference on Neural Information Processing_\n_Systems 2016, December 5-10, 2016, Barcelona, Spain_, 4287–\n4295.\n\n\nvan Hasselt, H. P.; Guez, A.; and Silver, D. 2016. Deep reinforcement learning with double Q-Learning. In _Proceedings_\n_of the Thirtieth AAAI Conference on Artificial Intelligence_,\n2094–2100.\n\n\nvan Hasselt, H. P.; Hessel, M.; and Aslanides, J. 2019. When\nto use parametric models in reinforcement learning? In _Ad-_\n_vances in Neural Information Processing Systems_, volume 32,\n14322–14333.\n\nvan Hasselt, H. P.; Mahmood, A. R.; and Sutton, R. S. 2014.\nOff-policy TD( _λ_ ) with a true online equivalence. In _Pro-_\n_ceedings of the 30th Conference on Uncertainty in Artificial_\n_Intelligence_, 330–339.\n\nvan Hasselt, H. P.; Quan, J.; Hessel, M.; Xu, Z.; Borsa, D.;\nand Barreto, A. 2019. General non-linear Bellman equations.\n_arXiv preprint arXiv:1907.03687_ .\n\n\nvan Hasselt, H. P.; and Sutton, R. S. 2015. Learning to predict\nindependent of span. _CoRR_ abs/1508.04582.\n\nvan Seijen, H.; and Sutton, R. S. 2013. Planning by Prioritized Sweeping with Small Backups. In _International_\n_Conference on Machine Learning_, 361–369.\n\nvan Seijen, H.; and Sutton, R. S. 2014. True online TD( _λ_ ).\nIn _International Conference on Machine Learning_, 692–700.\n\n\nWang, Z.; de Freitas, N.; Schaul, T.; Hessel, M.; van Hasselt,\nH. P.; and Lanctot, M. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In _International_\n_Conference on Machine Learning_ . New York, NY, USA.\n\nWerbos, P. J. 1990. A menu of designs for reinforcement\nlearning over time. _Neural networks for control_ 67–95.\n\nWhitehead, S. D.; and Ballard, D. H. 1991. Learning to\nperceive and act by trial and error. _Machine Learning_ 7(1):\n45–83.\n\nWilliams, R. J. 1992. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. _Machine_\n_Learning_ 8: 229–256.\n\n\nZhang, S.; Boehmer, W.; and Whiteson, S. 2019. Generalized\noff-policy actor-critic. In _Advances in Neural Information_\n_Processing Systems_, 2001–2011.\n\n\n",
          "ranking": {
            "relevance_score": 0.7493067885948483,
            "citation_score": 0.6302697050551669,
            "recency_score": 0.3906854405837399,
            "final_score": 0.6896372370858013
          },
          "is_open_access": false,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Large language models have demonstrated remarkable capabilities in understanding and generating code across multiple programming languages.",
        "chunk_index": 1
      },
      "summary": "Large language models have demonstrated remarkable capabilities in understanding and generating code across multiple programming languages.",
      "vector_score": 0.7040000000000001,
      "llm_score": 0.88,
      "combined_score": 0.88,
      "source_query": "mock_query_introduction"
    }
  ],
  "Related Work": [
    {
      "chunk": {
        "chunk_id": "mock_3",
        "paper": {
          "id": "user_2404.15822v1",
          "title": "Recursive Backwards Q-Learning in Deterministic Environments",
          "published": "2024-04-24",
          "authors": [
            "Jan Diekhoff",
            "Jorn Fischer"
          ],
          "summary": "Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.",
          "pdf_url": "",
          "doi": "10.48550/arXiv.2404.15822",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "arXiv.org",
          "citation_count": 0,
          "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n",
          "markdown_text": "## RECURSIVE BACKWARDS Q-LEARNING IN DETERMINISTIC ENVIRONMENTS\n\n\n\n**Jan Diekhoff**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: jan.diekhoff@web.de\n\n\n\n**Jörn Fischer**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: j.fischer@hs-mannheim.de\n\n\n\n**ABSTRACT**\n\n\nReinforcement learning is a popular method of finding optimal solutions to complex problems.\nAlgorithms like Q-learning excel at learning to solve stochastic problems without a model of their\nenvironment. However, they take longer to solve deterministic problems than is necessary. Q-learning\ncan be improved to better solve deterministic problems by introducing such a model-based approach.\nThis paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds\na model of the environment. After reaching a terminal state, it recursively propagates its value\nbackwards through this model. This lets each state be evaluated to its optimal value without a lengthy\nlearning process. In the example of finding the shortest path through a maze, this agent greatly\noutperforms a regular Q-learning agent.\n\n\n_**Keywords**_ Q-learning _·_ deterministic _·_ recursive _·_ reinforcement learning\n\n\n**1** **Introduction**\n\n\nMachine learning and reinforcement learning are increasingly popular and important fields in the modern age. There are\nproblems that reinforcement learning agents can learn to solve more efficiently and consistently than any human when\ngiven enough time to practice. However, modern approaches like Q-learning run into issues when facing certain types\nof problems. Their approach to solving problems in combination with not using a model of the environment causes\nthem to take longer than is necessary to learn to solve problems that are deterministic in nature. By working without\nmodel of the environment, information that is available and help the learning process is ignored.\n\n\nThis paper introduces an adapted Q-learning agent called the _recursive backwards Q-Learning (RBQL) agent_ . It solves\nthese types of problems by building a model of its environment as it explores and recursively applying the Q-value\nupdate rule to find an optimal policy much quicker than a regular Q-learning agent. This agent is shown to work with\nthe example of finding the fastest path through a maze. Its results are compared to the results of a regular Q-learning\nagent.\n\n\n**2** **Reinforcement Learning**\n\n\nReinforcement learning is one of the main fields of machine learning. It is commonly used for optimizing solutions to\nproblems. At its most fundamental level, a reinforcement learning method is an implementation of an agent for solving\na Markov decision process [1] by interacting with an environment. Markov decision processes describe problems as\na set of states _S_, a set of actions _A_ and a set of rewards _R_ . For every time step _t_, the agent chooses an action _a ∈_ _A_\nand receives a new state _s ∈_ _S_ and a reward _r ∈_ _R_ for the action [2]. Rewards may be positive or negative, depending\non the outcome of the action, to encourage or discourage taking that action in the future [3]. The process of the agent\ninteracting with the environment is called an episode which ends when a terminal state is reached which resets the\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nenvironment and agent to their original configuration for the start of a new episode [3]. For the purposes of this paper,\nonly finite Markov decision processes are considered, meaning the environment has at least one terminal state.\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-1-0.png)\n\n\n_St_ +1\n\n\nFigure 1: Basic agent-environment relationship in a Markov decision process. The agent chooses an action _At_ and the\nenvironment returns a new state _St_ +1 and a reward _Rt_ +1. The dotted line represents the transition from step _t_ to step\n_t_ + 1 [3].\n\n\nReinforcement learning agents learn an optimal strategy for a given Markov decision process by estimating the value of\neither being in a state or taking a certain action in a certain state. They do this through a value function or action-value\nfunction respectively. The aim of the agent is to maximize the reward they receive in an episode [3]. To achieve this,\nvalue estimations do not only consider the immediate action the agent takes but also consider all future states and actions\nthat may occur when taking the original action. Agents follow so-called policies according to which they choose which\nactions to take. Through gaining knowledge, they continuously adapt this policy in order to eventually reach an optimal\npolicy - a policy which chooses the optimal action at every step. To explore, agents have to balance between exploration\nand exploitation [3]. Exploration is the act of following suboptimal actions to attempt to find an even better policy. On\nthe other hand, exploitation is following the actions that will yield the currently highest estimated value. An agent that\nonly exploits acts _greedily_ . To ensure continual exploration so that all actions get updated given enough time, agents\ncan choose policies that are mostly greedy but choose to explore sometimes [2]. To this end, an approach like _ϵ_ -greedy\nmay be used. Here, _ϵ_ is the probability of choosing a random action and 1 _−_ _ϵ_ is the probability of acting greedily.\n\n\nA widely used modern approach to RL is temporal difference learning [4], more specifically Q-learning [2]. Q-learning\nworks with the Q-learning update formula to update its policies:\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + _α ·_\n\n[ _Rt_ +1 + _γ ·_ max _Q_ ( _St_ +1 _, a_ ) _−_ _Q_ ( _St, At_ )] (1)\n_a_\n\n\n_Q_ ( _St, At_ ) is the estimated value for any given state-action pair. The equation shows how it is updated after taking\naction _At_ from state _St_ . _Rt_ +1 represents the reward gained, max _Q_ ( _St_ +1 _, a_ ) is the value estimation of the best action\n_a_\n_a ∈_ _At_ +1 that can be taken from _St_ +1 according to the current policy, the state resulting from action _At_ . _α_ is a step-size\nparameter, also known as the _learning rate_ . Its value lies between 0 and 1 and it determines how importantly the agent\nvalues new information against the current estimate it already has. A value of 0 completely ignores new information\nwhile a value of 1 completely overrides the preexisting value estimate. _γ_ is the discount factor, weighing future rewards\nless than immediate ones. It also lies between 0 and 1, where 1 weighs the best future action equally to the current one\nand 0 does not consider it at all.\n\n\n**3** **Recursive Backwards Q-Learning**\n\n\n**3.1** **Idea**\n\n\nQ-learning agents are very widespread in modern reinforcement learning. Working free of a model allows them to\nbe generally applicable to many problems. However, some Markov decision processes take longer to solve than is\nnecessary because the agent ignores readily available information. This is noticeable in deterministic, episodic tasks\nwhere a positive reward is only given when reaching a terminal state. Before this state is reached for the first time, the\nagent appears to be moving entirely at random. Looking at figure 2, the issue becomes apparent. Even when following\nthe optimal path at every step, it still takes multiple episodes for the reward of the terminal state to propagate back to the\nstarting state. In fact, the optimal paths value estimation gets worse before it gets better. If every step has a reward of\n\n\n2\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_−_ 1, values along the optimal path get worse if they do not lead to a state that has already been reached by the terminal\nstate’s positive reward as it travels backwards.\n\n\nIn this paper, grid worlds [3] are used as an example Markov decision process for the agent to solve. Grid worlds are a\ntwo-dimensional grid in which every tile represents a state and the actions are limited to walking up, down, left or right.\nGrid worlds are useful in that they are very simple to understand and to display, they have a limited set of actions and\ntheir set of states can be as small or large as is desired. Additionally, showing the value or optimal policy for each state\nis as easy as writing a number or drawing an arrow on the corresponding tile. Actions that would place the agent off of\nthe grid simply return the state the agent is already in, but may still give a reward. Special tiles can also be defined, such\nas walls that act like the grid edge or pits that are terminal fail states because the agent cannot leave them once it has\nfallen in. Every grid world tile gives a reward of _−_ 1 to punish taking unnecessary actions in favor of taking the fastest\npath to the goal.\n\n\n_Q_ greedy policy\nw.r.t. _Q_\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -0.8\n\n\n-1\n\n\n-1\n\n\n-1 0.52\n\n\n-1\n\n\n-1\n\n\n-1 1.99\n\n\n-1\n\n\n-1\n\n\n-1 3.27\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 0.78\n\n\n-1\n\n\n-1\n\n\n-1 3.15\n\n\n-1\n\n\n-1\n\n\n-1 4.96\n\n\n-1\n\n\n-1\n\n\n-1 6.17\n\n\n-1\n\n\n-1\n\n\n-1 6.93\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 4.5\n\n\n-1\n\n\n-1\n\n\n-1 7.25\n\n\n-1\n\n\n-1\n\n\n-1 8.63\n\n\n-1\n\n\n-1\n\n\n-1 9.32\n\n\n-1\n\n\n-1\n\n\n-1 9.66\n\n\n-1\n\n\n-1\n\n\n-1 9.83\n\n\n-1\n\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n\nep. 0\n\n\nep. 1\n\n\nep. 2\n\n\nep. 3\n\n\nep. 4\n\n\nep. 5\n\n\nep. 6\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -1.6\n\n\n-1\n\n\n-1\n\n\n-1 -1.66\n\n\n-1\n\n\n-1\n\n\n-1 -1.1\n\n\n-1\n\n\n-1\n\n\n-1 -0.15\n\n\n-1\n\n\n\nFigure 2: Q-learning in a one-dimensional grid world. All Q-values are initialized as _−_ 1. Actions that lead to the\nterminal state reward 10. All other actions reward -1. The discount rate _γ_ is set to 0 _._ 9. The learning rate _α_ is set to 0 _._ 5.\nThe value of _ϵ_ is irrelevant as the only action the agent takes is _→_ .\n\n\nFigure 2 is a very simple grid world and it still takes six episodes to reach an optimal policy, even when taking the\noptimal action at every step. This problem will only grow worse and add noticeably more episodes of training for grid\nworlds that are not as trivial to solve, or even more complex tasks with more variables to consider. As stated, the issue\nis that the agent has no source of direction until it has randomly stumbled across the terminal state, its only source of\npositive rewards. The larger the state space, the longer it is blindly searching.\n\n\nReinforcement learning agents that work with a model of their environment are known as _model-based_ reinforcement\nlearning agents. They can either work with a preexisting model or, more commonly, build their own. The way they\nconstruct their models is important as having perfect knowledge of an environment is neither feasible nor sensible. In\nthe case of a grid world it is no problem, but imagining a more complex scenario like a self-driving car makes this fact\napparent. When trying to drive from one city to another, knowing every centimeter of the road with every possible place\nother cars might be on the route is resource intensive and unnecessary. Instead, an agent should attempt to simplify its\nmodel as much as possible. Instead of every bit of road, long stretches going straight can be clumped together. Similar\nsituations like a car in front slowing down can be treated the same wherever they occur.\n\n\nThe purpose of this paper is to introduce and evaluate a new type of model-based agent called the RBQL agent. The\nRBQL agent solves deterministic, episodic tasks that positively reward only the terminal state more efficiently than a\nregular Q-learning agent. It functions by building a model of its environment through exploration. When it reaches a\nterminal state, it recursively travels backwards through all previously explored states, applying a modified Q-learning\nupdate rule, the RBQL update rule. By setting the learning rate _α_ to 1, equation (1) can be simplified as such:\n\n\n3\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + 1 _·_ [ _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )]\n= _Q_ ( _St, At_ ) + _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )\n= _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n\n(2)\n\n\n\nAs can be seen in formula (2), the Q-value now exclusively depends on the reward and the discounted reward of the\nbest neighbor. Because the algorithm applies this formula starting with what is guaranteed to be the highest value of the\nenvironment and working its way away from it, the best possible neighbor for any given state is always the previously\nevaluated state.\n\n\nEvaluating all states at the end of the episode is reminiscent of dynamic programming [5] or Monte Carlo methods [3]\nand is a point of critique for those approaches. However, as will be shown in chapter 4, this evaluation method is so\neffective in RBQL that evaluating all known states in one go is still cost effective. RBQL also differs in comparison\nto dynamic programming and Monte Carlo in a few major ways. In contrast with dynamic programming, it does not\nstart out with a perfect model but has to build its own. It also propagates its reward throughout all states much more\nquickly and it uses an action-value function, not a state-value function. In contrast with Monte Carlo, it does not use\nexploring starts to guarantee exploration. It also does not only update the values that were seen in an episode. Instead,\nto facilitate exploration, it always prioritizes visiting unexplored actions, only following the greedy path when there\nare none. Because this mode of exploration still results in unexplored actions, the _ϵ_ -greedy approach is adapted for\nRBQL. Instead of exploring steps, the agent has exploration episodes. _ϵ_ serves the same purpose as before, marking\nthe probability of taking an exploration episode while 1 _−_ _ϵ_ is the probability of taking an exploitation episode. In an\nexploration episode, the agent randomly chooses an unexplored action anywhere in its model, navigates the model to\nput itself in a position to take that action and then continues to explore until it finds a known path again or the episode\nends.\n\n\nIn this paper, finding an optimal path through a randomly generated grid world maze is used as an example task for\nRBQL to solve. It is also used to compare the performance of RBQL to Q-learning.\n\n\n**3.2** **Implementation**\n\n\nTo implement RBQL [1], the Godot game engine v. 3.5 [2] was used. Godot is a free, open source engine used mainly for\nvideo game development. Its main language is GDScript, an internal language that is very similar in syntax to Python,\nthough it also supports C, C++, C# and VisualScript. Because Python is very popular for machine learning development,\nthe implementation is written in GDScript so that it is easily readable for interested parties. Godot uses a hierarchical\nstructure of objects called _nodes_ . In the implementation, there are two main nodes: the agent and the environment.\n\n\n**3.2.1** **Environment**\n\n\nThe environment is of the type `TileMap` [3] – a class designed for creating maps in grid-based environments like grid\nworlds. Before starting the first episode, the environment generates a maze given a width _w_ and a height _h_ using a\nrecursive backtracking algorithm [6]. The starting point for the agent is always (0 _,_ 0) and the goal it attempts to reach –\nthe only terminal state – is ( _w −_ 1 _, h −_ 1). To ensure that the agent has the ability to improve even after finding the goal\nin the first episode, a maze with multiple paths is needed. Because a maze generated with recursive backtracking only\nhas one path to the terminal state, a number of alternate paths are generated by taking _w · h/_ 4 random positions and a\ndirection for each position. If the position has a wall in that direction, it is removed. If not, nothing happens.\n\n\nThe environment has a function `step(state,action)` that serves as the only way for the agent to interact with it.\nThe possible moves are `UP`, `DOWN`, `LEFT` and `RIGHT` . The state is described as a coordinate of the current position. In\nGodot, the class `Vector2(x,y)` [4] is used for this purpose. `step()` checks if taking the given action from the given\nstate results in hitting a wall or not. If not, the agent moves to a new position. There are three different rewards: _−_ 1 for\nany normal tile, _−_ 5 for hitting a wall and 10 for reaching the terminal state. _−_ 1 is awarded at every step to discourage\nagents from taking unnecessary steps. Walls give _−_ 5 to quickly teach the agent to ignore them. After taking an action,\n\n\n1 The source code can be downloaded at `[https://github.com/JanDiekhoff/BackwardsLearner](https://github.com/JanDiekhoff/BackwardsLearner)`\n2 Godot v. 3.5 can be downloaded at `[https://godotengine.org/download/archive/3.5-stable/](https://godotengine.org/download/archive/3.5-stable/)`\n3 `[https://docs.godotengine.org/en/3.5/classes/class_tilemap.html](https://docs.godotengine.org/en/3.5/classes/class_tilemap.html)`\n4 `[https://docs.godotengine.org/en/3.5/classes/class_vector2.html](https://docs.godotengine.org/en/3.5/classes/class_vector2.html)`\n\n\n4\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nthe new state and reward are returned to the agent, as well as a notification if the episode has ended or not and if the\nagent has hit a wall or not.\n\n\nThe `TileMap` has a tile for each combination of having or not having a wall in each of the four directions, totaling 2 [4] or\n16 total possible tiles. Another option would be to just have a floor tile and a wall tile. However, that would make a\nmaze with an equivalent wall layout much larger, leading to a larger state set and longer solving times. To determine if a\nwall is in a certain direction, the id of each tile from 0 to 15 acts as a four-bit flag. Each direction is assigned one of the\nbits ( `UP` = 0, `RIGHT` = 1, `DOWN` = 2 and `LEFT` = 3). If the flag is set, there is a wall in the corresponding direction. The\nid for an L-shaped tile for example would be 2 [2] + 2 [3] = 12 as `DOWN` and `LEFT` have walls. The process for determining\nif the agent can move in a given direction _d_ from a position _p_ is ( _¬idp_ ) & (2 _[d]_ ), where _idp_ is the id of the tile at _p_ .\n\n\n**3.2.2** **RBQL Agent**\n\n\nThe RBQL agent is represented by a `Sprite` [5] object – a 2D image – so it can be observed while solving a maze. During\nits runtime, the agent keeps track of a few key things:\n\n\n    - A model of the environment ( `explored_map` )\n\n\n    - A list of rewards for each state-action pair ( `rewards` )\n\n\n    - The last reward received ( `reward` )\n\n\n    - A list of steps taken per episode ( `steps_taken` )\n\n\n    - The Q-table ( `qtable` )\n\n\n    - The current state ( `current_state` )\n\n\n    - The previous state ( `old_state` )\n\n\n    - The last taken action ( `action` )\n\n\nThe model of the environment starts out as an empty dictionary. Every time a new state is discovered, an entry for that state is made and initialized as an empty array. When an action is taken from this state, the resulting new state is entered into the previous state’s array at the index of the taken action’s designated number\n( `explored_map[old_state][action] = current_state` ). When hitting a wall, the “new” state is the same as the\nstate from which the action was taken. Similarly, when an action is taken, the resulting reward is saved in the rewards\nlist ( `rewards[old_state][action] = reward` ). Because the agent uses state-action values, not state values, the\ntiles are treated like nodes in a directed graph. Going from tile A to tile B might result in a different reward than when\ngoing from B to A, so when the agent learns the reward of going from A to B, it does not also learn the reward of going\nfrom B to A.\n\n\nBeing a Q-learner makes it simpler to generalize the agent for other tasks, but it causes a lot of exploratory steps and\nexploratory episodes to only explore one position at a time. If an exploration episode chooses an unexplored state-action\npair that results in hitting a wall, the exploration episode immediately ends with little information gained. To alleviate\nthis problem, the agent takes exploratory “look-ahead” steps. After entering a tile, it takes a step in every direction but\nonly saves the result if it hits a wall. This guarantees that exploratory episodes always take new paths and not just hit a\nwall and continue on the best known path.\n\n\nThe agent also keeps track of a list of the actions it has taken – except for when hitting a wall – for the case that it\nreaches a dead end, or rather a state with no unexplored neighbors. In this case, the agent would normally follow the\noptimal path until it finds a new unexplored path or reaches the terminal state. However, if the path the agent is on has\nnot been explored before it has not yet been evaluated and there is no optimal path to follow. In this case, the agent\nbacktracks by taking the opposite action of the most recent in the list, then removes it from the list, until an unexplored\ntile or an evaluated path to follow is found.\n\n\nFinally, when the terminal state is reached, the Q-table is updated with the rewards saved in `rewards` according to the\nRBQL update rule.\n```\n             qtable[state][action] =\n\n```\n\n`rewards[state][action] + discount_rate` _·_\n\n```\n             qtable[explored_map[state][action]].max()\n\n```\n\nTo do this, a copy of `explored_map` is inverted to be able to traverse it in reverse. This is then done with a breadth-first\nsearch algorithm, starting at the terminal state, and the Q-value is calculated for each state. Breadth-first search is chosen\n\n\n5 `[https://docs.godotengine.org/en/3.5/classes/class_sprite.html](https://docs.godotengine.org/en/3.5/classes/class_sprite.html)`\n\n\n5\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nover a depth-first search algorithm so that each state must only be visited once as the value is directly proportional to\nthe distance from the terminal state. With breadth-first search, each state gets the highest possible value on its first visit\nbecause it is visited from its highest possible valued neighbor.\n\n\nWhen all known states have been evaluated, a new episode begins. After the first episode, episodes are chosen to be\neither exploratory or exploitative, similar to how an _ϵ_ -greedy policy may choose exploratory actions. In an exploitative\nepisode, the agent simply follows the best path it knows, choosing at random if two states are equally good, but still\nalways exploring unknown states directly adjacent to the path above all else. In an exploratory episode, a random state\nwith an unexplored neighbor is chosen. The agent navigates to this state with the help of the A* search algorithm [7]\nand follow the unexplored path from there until it finds a known state again. This exploratory excursion may only find\none new state or it may find a vastly superior path to what was known before. _ϵ_ is decreased after every episode as\nfollows:\n_ϵ_ = `min_epsilon + (max_epsilon - min_epsilon)`\n\n\n_· e_ [(] _[−]_ `[decay_rate]` _[ ·]_ `[ current_episode]` [)]\n\n\nwhere `min_epsilon`, `max_epsilon` and `decay_rate` can be any value within a range of [0 _,_ 1] and `current_episode`\nis the number of the current episode starting with 0. Once every state is explored, the agent is guaranteed to have found\nthe optimal path, or paths, through the maze. In its entirety, the algorithm can be expressed like this:\n\n\n**Algorithm 1** Backwards Q-Learning Algorithm\n\nSet exploration_episode to false\n**while** true **do**\n\n**if** exploration_episode **then**\n\nFind unexplored path\nTravel to unexplored path\n**end if**\n**while** episode is not over **do**\n\n**if** current position has an unexplored neighbor **then**\n\nVisit unexplored neighbor\nUpdate model\nSave reward\n\n**if** no wall hit **then**\n\nSave action in action queue\n**end if**\n**else if** there is an optimal path to follow **then**\n\nVisit best neighbor\n**end if**\n**while** current pos. has no unexplored neighbor **do**\n\nBacktrack\n\n**end while**\n\n**end while**\nCreate state queue with breadth-first search\n**for** state in queue **do**\n\nApply RBQL formula\n**end for**\nSet exploration_episode to random() _<_ = _ϵ_\nApply decay to _ϵ_\n**end while**\n\n\n**3.2.3** **Q-learning agent**\n\n\nA standard Q-learning agent has been implemented in Godot as well to compare the performance of the RBQL agent to.\nThis agent is comparatively simple:\n\n\n**4** **Tests and Results**\n\n\nTo compare the performance of the two agents, three sets of tests have been done for different maze sizes: 5 _×_ 5, 10 _×_ 10\nand 15 _×_ 15. All variables have been set to common values. The decay rate is set somewhat high to account for the\nrelatively low episode amount:\n\n\n6\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**Algorithm 2** Q-Learning Algorithm\n\n\n**while** true **do**\n\n**if** random() _<_ = _ϵ_ **then**\n\nChoose random action\n\n**else**\n\nChoose greedy action\n**end if**\n\nTake action\n\nReceive new state and reward\nUpdate Q-table for old state and action\n**if** terminal state reached **then**\n\nStart new episode\n**end if**\nApply decay to _ϵ_\n**end while**\n\n\n    - _γ_ = 0 _._ 9\n\n\n    - _α_ = 0 _._ 1 (RBQL has _α_ = 1 as explained in equation (2))\n\n\n    - `min_epsilon` = 0 _._ 01\n\n\n    - `max_epsilon` = 1\n\n\n    - `decay_rate` = _−_ 0 _._ 01\n\n\nFor every maze size, each agent is given the same set of 50 randomly generated mazes. Each agent is given 25 episodes\nper maze to train. These values are chosen to offer a reasonably large sample size without requiring an enormous\namount of time to compute. Agents are compared by the number of steps taken per episode, with less steps taken being\na more desirable outcome. The step counter is increased every time `step()` is called, including the look-ahead steps of\nthe RBQL. For a sense of perspective, the best possible solution to any square maze of size _s_ [2] is 2 _s −_ 2. Assuming a\nmaze with no walls, the shortest distance between two points _A_ and _B_ can be expressed as their Manhattan distance\n_|AX −_ _BX_ _|_ + _|AY −_ _BY |_ [8]. In the corners of a square, it holds that _AX_ = _AY_ and _BX_ = _BY_, so the distance can\nbe simplified as 2 _· |A −_ _B|_ . Setting _A_ = 0 and _B_ = _s −_ 1, this further simplifies to 2 _s −_ 2. This means that while\nthe amount of states (and thereby state-action pairs) increases quadratically, the best possible solution only increases\nlinearly. This in turn means that the amount of states that are not on the optimal path that the agent has to evaluate will\noften increase drastically with the size of the maze.\n\n\nLooking at the results, a few things can be observed. First of all, the average number of steps the RBQL agent takes\nis consistently lower than the Q-learning agent in all three maze sizes. It also has much less variation in step counts,\nwhich can be seen when looking at the areas of lighter hue. The light red areas are much more sporadic and spike\nfurther away from the average. The green areas stick much closer together. If the highest two step counts per episode\nwere not removed, RBQL would also have a few small spikes. These spikes would represent exploratory episodes\nwhere a new path is explored, resulting in a higher step count. In cases where the line is flat for a long period of time, it\ncan be assumed that the optimal solution is found. This can be seen in all three figures, where both the average and\nthe min/max range become a straight line close to the minimum. Important to note is that every maze has a different\noptimal solution, hence why the average sits above the blue line which denotes the lowest possible step count in any\nmaze of this size. It can also be observed that none of the lines ever go below this boundary, as is to be expected.\n\n\nSecond, even when removing the highest two step counts per episode, many of the Q-learning agent’s step counts are so\nlarge that scaling the graphs to fit them makes the RBQL agent’s data and the lower boundary difficult to see in the\ngraphs for the larger mazes. The highest step count values that have not been cut are 858 steps in figure 3, 7,585 in\nfigure 4 and 21,147 in figure 5, while the highest in total are 3,716 steps in figure 3, 20,553 in figure 4 and 26,315 in\nfigure 5.\n\n\nThird, it is interesting to see how the differences in average step counts evolve with the grid size. Table 1 shows this\ndifference in the first and last episode. The difference between the average step counts in the last episode especially is\nstriking, as it is close to doubling from each size to the next. Further, looking at the improvement of each agent as seen\nin table 2, one can see that the factor by which RBQL improves massively increases the bigger the maze becomes while\nthe Q-learner only slightly improves its performance in comparison. Additionally, most of the improvement of RBQL is\ndone in the first two episodes, while the Q-learner has a more gradual learning curve.\n\n\n7\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**1** _**,**_ **000**\n\n\n**900**\n\n\n**800**\n\n\n**700**\n\n\n**600**\n\n\n**500**\n\n\n**400**\n\n\n**300**\n\n\n**200**\n\n\n**100**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 3: Number of steps taken to find the goal in a randomly generated grid world maze of size 5 _×_ 5. The blue line is\nthe minimum step threshold for any maze of this size. The light red area shows the range of Q-learning agent’s highest\nand lowest step count, excluding the highest and lowest two. The red line shows the average performance. Similarly, the\nlight green area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest\ntwo, and the green line shows the average performance.\n\n\n8\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-7-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**8** _**,**_ **000**\n\n\n**6** _**,**_ **000**\n\n\n**5** _**,**_ **000**\n\n\n**4** _**,**_ **000**\n\n\n**3** _**,**_ **000**\n\n\n**2** _**,**_ **000**\n\n\n**1** _**,**_ **000**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 4: Number of steps taken to find the goal in a randomly generated grid world maze of size 10 _×_ 10. The light\nred area shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two.\nThe red shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n9\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-8-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 5: Number of steps taken to find the goal in a randomly generated grid world maze of size 15 _×_ 15. The light red\narea shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two. The\nred line shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n10\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-9-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nTable 1: Difference in average step counts of the Q-learner and RBQL. The difference expresses how many times more\nsteps the Q-learner took compared to RBQL.\n\n\nGrid size Q-learner steps RBQL steps Difference\n**Episode 0**\n5 _×_ 5 278.06 191.84 1.45\n\n10 _×_ 10 3,308.46 843.52 3.92\n15 _×_ 15 7,180.98 1,965 3.65\n**Episode 24**\n5 _×_ 5 49.14 9.62 5.11\n\n10 _×_ 10 281.44 23.68 11.89\n\n15 _×_ 15 778.68 35.96 21.65\n\n\nLastly, the RBQL agent seems to find an optimal policy at around episode 4 for the 5 _×_ 5, episode 6 for the 10 _×_ 10 and\nepisode 10 for the 15 _×_ 15 grid. As the previous figures show, the Q-learning agent does not come close to similarly\nlow step counts and therefore does not reach an optimal policy at all with the same amount of training.\n\n\nTable 2: Difference in average step counts of the Q-learner and RBQL. Improvement shows the factor by which the\namount of steps is reduced from episode 0 to 24.\n\n\nGrid size Steps in episode 0 Steps in episode 24 Improvement\n**Q-learning agent**\n5 _×_ 5 278.06 49.14 5.66\n\n10 _×_ 10 3,308.46 281.44 11.76\n15 _×_ 15 7,180.98 778.68 9.22\n**RBQL agent**\n5 _×_ 5 191.84 9.62 19.94\n\n10 _×_ 10 843.52 23.68 35.62\n\n15 _×_ 15 1,965 35.96 90.76\n\n\nTo further show RBQL’s efficiency, it has also been tested under the same parameters in a grid of size 50 _×_ 50. The\nresults can be seen in figure 6. This test is done to demonstrate that even such a large maze can be explored by RBQL.\nAs with the previous examples, by far the largest policy improvement still happens in the first episode. With mazes of\nsuch a large size, a lot more spikes in step counts are seen in later episodes because there are more states to explore. The\ndifference in average step counts goes from 20,811.08 in episode 0 to 344.9 in episode 24, an improvement by a factor\nof 60.34. This is worse than the improvement in the 15 _×_ 15 mazes, but still almost double that of the 10 _×_ 10 mazes.\n\n\n**5** **Discussion**\n\n\nThis chapter explores the practicality of using this algorithm to solve other Markov decision processes. It discusses\nwhich parts of the implementation are and are not specific to the problem of fastest path through a maze, which\nimprovements can be made to make it more applicable for other problems and showcases further points for research in\nthis field. The constraints given in this paper are that the agent will attempt to solve deterministic, episodic tasks with a\nsingle terminal state as its only source of positive rewards. This chapter also discusses which of these constraints can be\ndismissed.\n\n\nThere are a few parts of the implementation as presented in chapter 3.2 that are only applicable to this specific problem.\nThis is not necessarily a bad thing, as the purpose of the RBQL agent is to utilize knowledge of its environment. As a\nresult of this, the only parts that cannot be directly adapted for other problems are the way the agent builds its model.\nIn the grid world maze, it can assume that every state has the same actions it can take and has a neighboring state in\neach direction (though it may sometimes be itself). Further, every action always has an opposite action, going up can\nalways be undone by going down for example. These assumptions allow it to easily build a model of the grid world\nand influence its policy in how it further explores it. They allow the agent to take steps in each direction to check for\nwalls and they allow the agent to backtrack when it is stuck in a dead end. These assumptions cannot be guaranteed\nfor other Markov decision processes or even for grid worlds with more complex behavior like a wind tunnel that if\nwalked through also pushes the agent one tile in the direction the wind is traveling. The way in which the agent builds a\n\n\n11\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**3**\n\n\n**2** _**.**_ **8**\n\n\n**2** _**.**_ **6**\n\n\n**2** _**.**_ **4**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 6: Number of steps taken to find the goal in a randomly generated grid world maze of size 50 _×_ 50. The light\ngreen area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest two,\nand the green line shows the average performance.\n\n\n12\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-11-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nmodel has to either be designed for each environment individually or it has to be abstracted so that it is more broadly\napplicable. Finding such an approach to model building is one area of improvement for RBQL. Importantly though,\nnone of these assumptions are required for the agent to function. Backtracking, opposite steps and the same actions for\nevery state simply make the implementation easier and more efficient. As long as no path of a directed graph would\ncause the agent to be stuck with no way to reach a terminal state, it can be explored and evaluated.\n\n\nAnother improvement to the way the implementation builds its model is to simplify it as far as possible. As the amount\nof states directly influences how long a problem takes to solve, RBQL will become more efficient the more it can\nremove unnecessary states. Currently, every position has its own state. If the agent could detect “hallways” – tiles with\nparallel walls – they could be removed without problem in favor of directly connecting the two tiles at either side of the\nhallway – only the negative rewards for the length of the hallway would have to be implemented into the model. Further,\nif there is a non-forking path that leads into a dead end, the entire path could be treated as a wall and ignored entirely.\nThis would leave only the starting state, terminal state, turns and forking paths to evaluate. Both of these additions leave\nthe key part of the algorithm, traversing the model backwards and applying the RBQL update formula, untouched.\n\n\nRBQL can be easily adapted to include multiple terminal states with the same or different rewards and this is already\nsupported by the implementation. There are two possible ways to do this. First is to create an imaginary state that\nall terminal states lead into from which the backtracking always starts. Second is to remember all terminal states and\nbacktrack from each of them. The first option is much more efficient as each state still only gets evaluated once while\nthe second version avoids having to tamper with the model.\n\n\nFinally, RBQL could be adapted to work in non-deterministic environments. To reiterate, deterministic means that a\nstate-action pair always yields the same state-reward pair. If the agent could, while building its model, also estimate the\ntransition probabilities of a state-action pair to a new state, RBQL could still be used to evaluate the states. The RBQL\nupdate rule can be generalized to\n\n\n\n_Q_ ( _St, At_ ) _←_ �\n\n_s∈St_ +1\n\n\n\n( _Rs_ + _γ_ max _Q_ ( _s, a_ )) _· p_ (3)\n_a_\n� �\n\n\n\nwhere _St_ +1 is the set of possible states when taking _At_ from _St_, _p_ is the probability of reaching _s_ when taking _At_ from\n_St_ and _Rs_ is the reward of reaching _s_ . In a deterministic environment, _St_ +1 only consists of one state with _p_ = 1,\nnegating these additions. Whether RBQL would be as effective in non-deterministic environments as in deterministic\nenvironments is something to be explored in further studies.\n\n\nThe only constraint on the algorithm that cannot easily be circumvented is its episodic nature. Because the agent relies\non a terminal state from which to propagate the rewards backwards from, a continuous task implementation seems\nimpossible to implement.\n\n\n**6** **Conclusion**\n\n\nThis paper has introduced recursive backwards Q-learning, a model-based reinforcement learning algorithm that\nevaluates all known state-action pairs of the model at the end of each episode with the Q-learning update rule. It has\nalso shown how recursive backwards Q-learning relates to, adapts and improves on them. This paper has presented\nan implementation of recursive backwards Q-learning in the Godot game engine to test its performance. Through\nmultiple tests, it has been shown to be superior in finding the shortest path through a randomly generated grid world\nmaze. It has been argued that this algorithm could be adapted to solve other deterministic, episodic tasks more quickly\nthan Q-learning. Further, it has given avenues for further research in adapting recursive backwards Q-learning for\nnon-deterministic problems.\n\n\n**References**\n\n\n[1] Richard Bellman, “A markovian decision process,” _Journal of Mathematics and Mechanics_, vol. 6, no. 5, pp. 679–\n684, 1957. [Online]. Available: `[http://www.jstor.org/stable/24900506](http://www.jstor.org/stable/24900506)` .\n\n[2] Christopher John Cornish Hellaby Watkins, “Learning from delayed rewards,” 1989.\n\n[3] Richard S Sutton and Andrew G Barto, _Reinforcement learning: An introduction_ . MIT press, 2018.\n\n[4] Richard S. Sutton, “Learning to predict by the methods of temporal differences,” _Machine Learning_, vol. 3, no. 1,\npp. 9–44, 1988. DOI: `[10.1007/bf00115009](https://doi.org/10.1007/bf00115009)` .\n\n[5] Richard Bellman, “Dynamic programming,” _Princeton, USA: Princeton University Press_, vol. 1, no. 2, p. 3, 1957.\n\n[6] Peter Gabrovšek, “Analysis of maze generating algorithms,” _IPSI Transactions on Internet Research_, vol. 15,\nno. 1, pp. 23–30, 2019.\n\n\n13\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n[7] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael, “A formal basis for the heuristic determination of minimum\ncost paths,” _IEEE Transactions on Systems Science and Cybernetics_, vol. 4, no. 2, pp. 100–107, 1968. DOI:\n`[10.1109/TSSC.1968.300136](https://doi.org/10.1109/TSSC.1968.300136)` .\n\n[8] Eugene F Krause, “Taxicab geometry,” _The Mathematics Teacher_, vol. 66, no. 8, pp. 695–706, 1973.\n\n\n14\n\n\n",
          "ranking": null,
          "is_open_access": false,
          "user_provided": true,
          "pdf_path": "output/literature/user_2404.15822v1/user_2404.15822v1.pdf"
        },
        "chunk_text": "Prior work on static analysis tools has shown effectiveness in detecting common bug patterns but struggles with context-dependent issues.",
        "chunk_index": 0
      },
      "summary": "Prior work on static analysis tools has shown effectiveness in detecting common bug patterns but struggles with context-dependent issues.",
      "vector_score": 0.7120000000000001,
      "llm_score": 0.89,
      "combined_score": 0.89,
      "source_query": "mock_query_related work"
    },
    {
      "chunk": {
        "chunk_id": "mock_4",
        "paper": {
          "id": "acda55ebdf39c6634e89a9730ff7d963471f2b0a",
          "title": "Expected Eligibility Traces",
          "published": "2020-07-03",
          "authors": [
            "H. V. Hasselt",
            "Sephora Madjiheurem",
            "Matteo Hessel",
            "David Silver",
            "André Barreto",
            "Diana Borsa"
          ],
          "summary": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state.\nIn this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that some- times substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
          "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17200/17007",
          "doi": "10.1609/aaai.v35i11.17200",
          "fields_of_study": [
            "Computer Science",
            "Mathematics"
          ],
          "venue": "AAAI Conference on Artificial Intelligence",
          "citation_count": 41,
          "bibtex": "@Article{Hasselt2020ExpectedET,\n author = {H. V. Hasselt and Sephora Madjiheurem and Matteo Hessel and David Silver and André Barreto and Diana Borsa},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Expected Eligibility Traces},\n volume = {abs/2007.01839},\n year = {2020}\n}\n",
          "markdown_text": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)\n\n# **Expected Eligibility Traces**\n\n\n**Hado van Hasselt** [1] **, Sephora Madjiheurem** [2] **, Matteo Hessel** [1]\n\n**David Silver** [1] **, Andr´e Barreto** [1] **, Diana Borsa** [1]\n\n1 DeepMind\n2 University College London, UK\n\n\n\n**Abstract**\n\n\nThe question of how to determine which states and actions\nare responsible for a certain outcome is known as the _credit_\n_assignment problem_ and remains a central research question\nin reinforcement learning and artificial intelligence. _Eligibil-_\n_ity traces_ enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not\nto counterfactual sequences that could also have led to the\ncurrent state. In this work, we introduce _expected eligibility_\n_traces_ . Expected traces allow, with a single update, to update\nstates and actions that could have preceded the current state,\neven if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous)\ntraces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide\na way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which\nensures that the resulting algorithm is a strict generalisation\nof TD( _λ_ ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.\n\n\n**Motivation and Summary**\n\n\nAppropriate credit assignment has long been a major research\ntopic in artificial intelligence (Minsky 1963). To make effective decisions and understand the world, we need to accurately associate events, like rewards or penalties, to relevant\nearlier decisions or situations. This is important both for learning accurate predictions, and for making good decisions.\n_Temporal credit assignment_ can be achieved with repeated\ntemporal-difference (TD) updates (Sutton 1988). One-step\nTD updates propagate information slowly: when a surprising value is observed, the state immediately preceding it is\nupdated, but no earlier states or decisions are updated. _Multi-_\n_step_ updates (Sutton 1988; Sutton and Barto 2018) propagate\ninformation faster over longer temporal spans, speeding up\ncredit assignment and learning. Multi-step updates can be\nimplemented online using _eligibility traces_ (Sutton 1988),\nwithout incurring significant additional computational expense, even if the time spans are long; these algorithms have\ncomputation that is independent of the temporal span of the\npredictions (van Hasselt and Sutton 2015).\n\n\nCopyright c _⃝_ 2021, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\nMDP True value TD(0) TD(λ) ET(λ)\n\n\nFigure 1: A comparison of TD(0), TD( _λ_ ), and the new\nexpected-trace algorithm ET( _λ_ ) (with _λ_ = 0 _._ 9). The MDP\nis illustrated on the left. Each episode, the agent moves randomly down and right from the top left to the bottom right,\nwhere any action terminates the episode. Reward on termination are +1 with probability 0.2, and zero otherwise—all\nother rewards are zero. We plot the value estimates after the\nfirst positive reward, which occurred in episode 5. We see\na) TD(0) only updated the last state, b) TD( _λ_ ) updated the\ntrajectory in this episode, and c) ET( _λ_ ) additionally updated\ntrajectories from earlier (unrewarding) episodes.\n\n\nTraces provide temporal credit assignment, but do not assign credit _counterfactually_ to states or actions that _could_\nhave led to the current state, but did not do so this time.\nCredit will eventually trickle backwards over the course of\nmultiple visits, but this can take many iterations. As an example, suppose we collect a key to open a door, which leads\nto an unexpected reward. Using standard one-step TD learning, we would update the state in which the door opened.\nUsing eligibility traces, we would also update the preceding\ntrajectory, including the acquisition of the key. But we would\nnot update other sequences that _could_ have led to the reward,\nsuch as collecting a spare key or finding a different entrance.\nThe problem of credit assignment to counterfactual states\nmay be addressed by learning a model, and using the model\nto propagate credit (cf. Sutton 1990; Moore and Atkeson\n1993; Chelu, Precup, and van Hasselt 2020); however, it\nhas often proven challenging to construct and use models\neffectively in complex environments (cf. van Hasselt, Hessel,\nand Aslanides 2019).\nWe introduce a new approach to counterfactual credit assignment, based on the concept of _expected eligibility traces_ .\nWe present a family of algorithms, which we call ET( _λ_ ), that\nuse expected traces to update their predictions. We analyse\nthe nature of these expected traces, and illustrate their benefits empirically in several settings—see Figure 1 for a first\n\n\n\n9997\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-1.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-3.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-4.png)\n\n\nillustration. We introduce a bootstrapping mechanism that\nprovides a spectrum of algorithms between standard eligibility traces and expected eligibility traces, and also discuss\nways to apply these ideas with deep neural networks. Finally,\nwe discuss possible extensions and connections to related\nideas such as successor features.\n\n\n**Background**\nSequential decision problems can be modelled as Markov\ndecision processes [1] (MDP) ( _S, A, p_ ) (Puterman 1994), with\nstate space _S_, action space _A_, and a joint transition and\nreward distribution _p_ ( _r, s_ _[′]_ _|s, a_ ). An agent selects actions according to its policy _π_, such that _At ∼_ _π_ ( _·|St_ ) where _π_ ( _a|s_ )\ndenotes the probability of selecting _a_ in _s_, and observes random rewards and states generated according to the MDP, resulting in trajectories _τt_ : _T_ = _{St, At, Rt_ +1 _, St_ +1 _, . . ., ST }_ .\nA central goal is to predict _returns_ of future discounted rewards (Sutton and Barto 2018)\n\n\n_Gt ≡_ _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _Rt_ +2 + _γt_ +1 _γt_ +2 _Rt_ +3 + _. . ._\n\n\n\n=\n\n\n\n_T_\n� _γt_ [(] +1 _[i][−]_ [1)] _[R][t]_ [+] _[i][,]_\n\n\n_i_ =1\n\n\n\nwhere _T_ is for instance the time the current episode terminates or _T_ = _∞_, and where _γt ∈_ [0 _,_ 1] is a (possibly constant) discount factor and _γt_ [(] _[n]_ [)] = [�] _[n]_ _k_ =0 _[−]_ [1] _[γ][t]_ [+] _[k]_ [, and] _[ γ]_ _t_ [(0)] = 1.\nThe value _vπ_ ( _s_ ) = E [ _Gt|St_ = _s, π_ ] of state _s_ is the expected return for a policy _π_ . Rather than writing the return as\na random variable _Gt_, it will be convenient to instead write it\nas an explicit function _G_ ( _τ_ ) of the random trajectory _τ_ . Note\nthat _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ).\nWe approximate the value with a function _v_ **w** ( _s_ ) _≈_ _vπ_ ( _s_ ).\nThis can for instance be a table—with a single separate entry\n_w_ [ _s_ ] for each state—a linear function of some input features,\nor a non-linear function such as a neural network with parameters **w** . The goal is to iteratively update **w** with\n\n\n**w** _t_ +1 = **w** _t_ + ∆ **w** _t_\n\n\nsuch that _v_ **w** approaches the true _vπ_ . Perhaps the simplest\nalgorithm to do so is the Monte Carlo (MC) algorithm\n\n\n∆ **w** _t ≡_ _α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._\n\n\nMonte Carlo is effective, but has high variance, which can\nlead to slow learning. TD learning (Sutton 1988; Sutton and\nBarto 2018) instead replaces the return with the current estimate of its expectation _v_ ( _St_ +1) _≈_ _G_ ( _τt_ +1: _T_ ), yielding\n\n\n∆ **w** _t ≡_ _αδt∇_ **w** _v_ **w** ( _St_ ) _,_ (1)\nwhere _δt ≡_ _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ ) _,_\n\n\nwhere _δt_ is called the temporal-difference (TD) error. We\ncan interpolate between these extremes, for instance with\n_λ_ -returns which smoothly mix values and sampled returns:\n\n\n_G_ _[λ]_ ( _τt_ : _T_ ) = _Rt_ +1+ _γt_ +1�(1 _−λ_ ) _v_ **w** ( _St_ +1)+ _λG_ _[λ]_ ( _τt_ +1: _T_ )� _._\n\n\n‘Forward view’ algorithms, like the MC algorithm, use returns\nthat depend on future trajectories and need to wait until the\n\n\n1The ideas in this paper extend naturally to POMDPs (cf. **?** ).\n\n\n\nend of an episode to construct their updates, which can take a\nlong time. Conversely, ‘backward view’ algorithms rely only\non past experiences and can update their predictions online,\nduring an episode. Such algorithms build an _eligibility trace_\n(Sutton 1988; Sutton and Barto 2018). An example is TD( _λ_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ _t,_ with _**e**_ _t_ = _γtλ_ _**e**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ ) _,_\n\n\nwhere _**e**_ _t_ is an accumulating eligibility trace. This trace can\nbe viewed as a function _**e**_ _t ≡_ _**e**_ ( _τ_ 0: _t_ ) of the trajectory of past\ntransitions. The TD update in (1) is known as TD(0), because\nit corresponds to using _λ_ = 0. TD( _λ_ = 1) corresponds to an\nonline implementation of the MC algorithm. Other variants\nexist, using other kinds of traces, and equivalences have been\nshown between these algorithms and their forward views that\nuse _λ_ -returns: these backward-view algorithms converge to\nthe same solution as the corresponding forward view, and can\nin some cases yield equivalent weight updates (Sutton 1988;\nvan Seijen and Sutton 2014; van Hasselt and Sutton 2015).\n\n\n**Expected Traces**\n\n\nThe main idea of this paper is to use the concept of an _ex-_\n_pected eligibility trace_, defined as\n\n\n_**z**_ ( _s_ ) _≡_ E [ _**e**_ _t | St_ = _s_ ] _,_\n\n\nwhere the expectation is over the agent’s policy and the MDP\ndynamics. We introduce a concrete family of algorithms,\nwhich we call ET( _λ_ ) and ET( _λ_, _η_ ), that learn expected traces\nand use them in value updates. We analyse these algorithms\ntheoretically, describe specific instances, and discuss computational and algorithmic properties.\n\n\n**ET(** _λ_ **)**\n\n\nWe propose to learn approximations _**zθ**_ ( _s_ ) _≈_ _**z**_ ( _s_ ), with parameters _**θ**_ _∈_ R _[d]_ (e.g., the weights of a neural network). One\nway to learn _**zθ**_ is by updating it toward the instantaneous\ntrace _**e**_ _t_, by minimizing an empirical loss _L_ ( _**e**_ _t,_ _**zθ**_ ( _St_ )). For\ninstance, _L_ could be a component-wise squared loss, optimized with stochastic gradient descent:\n\n\n_**θ**_ _t_ +1 = _**θ**_ _t_ + ∆ _**θ**_ _t,_ where (2)\n\n\n1\n\n∆ _**θ**_ _t_ = _−β_ _[∂]_\n\n_∂_ _**θ**_ 2 [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))] _[⊤]_ [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))]\n\n= _β_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] ( _**e**_ _t −_ _**zθ**_ ( _St_ )) _,_ (3)\n\n_∂_ _**θ**_\n\n\nwhere _[∂z]_ _**[θ]**_ _∂_ [(] _**θ**_ _[S][t]_ [)] is a _|_ _**θ**_ _| × |_ _**e**_ _|_ Jacobian [2] and _β_ is a step size.\n\nThe idea is then to use _**zθ**_ ( _s_ ) _≈_ E [ _**e**_ _t | St_ = _s_ ] in place\nof _**e**_ _t_ in the value update, which becomes\n\n\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (4)\n\n\nWe call this ET( _λ_ ). Below, we prove that this update can\nbe unbiased and can have lower variance than TD( _λ_ ). Algorithm 1 shows pseudo-code for a concrete instance of ET( _λ_ ).\n\n\n2The Jacobian-vector product can efficiently be computed (e.g.,\nvia auto-differentiation) with computational requirements that are\ncomparable to the computation of the loss.\n\n\n\n9998\n\n\n\n\n**Algorithm 1** ET( _λ_ )\n\n\n1: initialise **w**, _**θ**_\n2: **for** _M_ episodes **do**\n3: initialise _**e**_ = **0**\n\n4: observe initial state _S_\n5: **repeat** for each step in episode _m_\n6: generate _R_ and _S_ _[′]_\n\n7: _δ ←_ _R_ + _γv_ **w** ( _S_ _[′]_ ) _−_ _v_ **w** ( _S_ )\n8: _**e**_ _←_ _γλ_ _**e**_ + _∇_ **w** _v_ **w** ( _S_ )\n\n9: _**θ**_ _←_ _**θ**_ + _β_ _[∂]_ _**[z]**_ _∂_ _**[θ]**_ _**θ**_ [(] _[S]_ [)] ( _**e**_ _−_ _**zθ**_ ( _S_ ))\n\n10: **w** _←_ **w** + _αδ_ _**zθ**_ ( _S_ )\n11: **until** _S_ is terminal\n\n12: **end for**\n\n13: **Return w**\n\n\n**Interpretation and ET(** _λ, η_ **)**\n\nWe can interpret TD(0) as taking the MC update and replacing the return from the subsequent state, which is a function\nof the future trajectory, with a state-based estimate of its expectation: _v_ **w** ( _St_ +1) _≈_ E [ _G_ ( _τt_ +1: _T_ ) _|St_ +1 ]. This becomes\nmost clear when juxtaposing the updates:\n\n\n_α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _,_ (MC)\n_α_ ( _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._ (TD)\n\n\nTD( _λ_ ) also uses a function of a trajectory: the trace _**e**_ _t_ . We\npropose replacing this as well with a function of state: the\nexpected trace _**zθ**_ ( _St_ ) _≈_ E [ _**e**_ ( _τ_ 0: _t_ ) _|St_ ]. Again juxtaposing:\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ ( _τ_ 0: _t_ ) _,_ (TD( _λ_ ))\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (ET( _λ_ ))\n\n\nWe can interpolate smoothly between MC and TD(0) via\n_λ_ . This is often useful to trade off variance of the return with\npotential bias of the value estimate. For instance, we might\nnot have access to the true state _s_, and might instead have to\nrely on features **x** ( _s_ ). Then we cannot always represent or\nlearn the true values _v_ ( _s_ )—for instance different states may\nbe aliased (Whitehead and Ballard 1991).\nSimilarly, when moving from TD( _λ_ ) to ET( _λ_ ) we replaced\na trajectory-based trace with a state-based estimate. This\nmight induce bias and, again, we can smoothly interpolate by\nusing a recursively defined mixture trace _**y**_ _t_, as defined as [3]\n\n\n_**y**_ _t_ = (1 _−_ _η_ ) _**zθ**_ ( _St_ ) + _η_ � _γtλ_ _**y**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ )� _._ (5)\n\n\nThis recursive usage of the estimates _**zθ**_ ( _s_ ) at previous states\nis analogous to bootstrapping on future state values when\nusing a _λ_ -return, with the important difference that the arrow\nof time is opposite. This means we do not first have to convert\nthis into a backward view: the quantity can already be computed from past experience directly. We call the algorithm\nthat uses this mixture trace ET( _λ_, _η_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**y**_ ( _St_ ) _._ (ET( _λ_, _η_ ))\n\n\n3While _**y**_ _t_ depends on both _η_ and _λ_ we leave this dependence\nimplicit, as is conventional for traces.\n\n\n\nNote that if _η_ = 1 then _**y**_ _t_ = _**e**_ _t_ equals the instantaneous\ntrace: ET( _λ_, 1) is equivalent to TD( _λ_ ). If _η_ = 0 then _**y**_ _t_ = _**z**_ _t_\nequals the expected trace; the algorithm introduced earlier\nas ET( _λ_ ) is equivalent to ET( _λ_, 0). By setting _η ∈_ (0 _,_ 1), we\ncan smoothly interpolate between these extremes.\n\n\n**Theoretical Analysis**\n\nWe now analyse the new ET algorithms theoretically. First\nwe show that if we use _**z**_ ( _s_ ) directly and _s_ is Markov then the\nupdate has the same expectation as TD( _λ_ ) (though possibly\nwith lower variance), and therefore also inherits the same\nfixed point and convergence properties.\n\n\n**Lemma 1.** _If s is Markov, then_\n\n\nE [ _δt_ _**e**_ _t | St_ = _s_ ] = E [ _δt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] _._\n\n\n_Proof._ In Appendix .\n\n\n**Proposition 1.** _Let_ _**e**_ _t be any trace vector, updated in any_\n_way. Let_ _**z**_ ( _s_ ) = E [ _**e**_ _t | St_ = _s_ ] _. Consider the ET(λ) algo-_\n_rithm_ ∆ **w** _t_ = _αtδt_ _**z**_ ( _St_ ) _. For all Markov states s the expec-_\n_tation of this update is equal to the expected update under_\n_instantaneous trace_ _**e**_ _t, and its variance is lower or equal:_\n\n\nE [ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] = E [ _αtδt_ _**e**_ _t|St_ = _s_ ] _and_\nV[ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] _≤_ V[ _αtδt_ _**e**_ _t|St_ = _s_ ] _,_\n\n\n_where the second inequality holds component-wise for the_\n_update vector, and is strict when_ V[ _**e**_ _t|St_ ] _>_ 0 _._\n\n\n_Proof._ We have\n\n\nE [ _αtδt_ _**e**_ _t | St_ = _s_ ]\n= E [ _αtδt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] (Lemma 1)\n= E [ _αtδt | St_ = _s_ ] _**z**_ ( _s_ )\n= E [ _αtδt_ _**z**_ ( _St_ ) _| St_ = _s_ ] _._ (6)\n\n\nDenote the _i_ -th component of _**z**_ ( _St_ ) by _zt,i_ and the _i_ -th\ncomponent of _**e**_ _t_ by _et,i_ . Then, we also have\n\n\nE � ( _αtδtzt,i_ ) [2] _|St_ = _s_ � = E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � _zt,i_ [2]\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E [ _et,i|St_ = _s_ ] [2]\n\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ ��E � _e_ [2] _t,i_ _[|][S][t]_ [=] _[ s]_ � _−_ V[ _et,i|St_ = _s_ ]�\n\n_≤_ E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E � _e_ [2] _t,i_ _[|][ S][t]_ [=] _[ s]_ �\n\n= E � ( _αtδtet,i_ ) [2] _| St_ = _s_ � _,_\n\n\nwhere the last step used the fact that _s_ is Markov, and the inequality is strict when V[ _et,i|St_ ] _>_ 0. Since the expectations\nare equal, as shown in (6), the conclusion follows.\n\n\n**Interpretation** Proposition 1 is a strong result: it holds for\nany trace update, including accumulating traces (Sutton 1984,\n1988), replacing traces (Singh and Sutton 1996), dutch traces\n(van Seijen and Sutton 2014; van Hasselt, Mahmood, and\nSutton 2014; van Hasselt and Sutton 2015), and future traces\nthat may be discovered. It implies convergence of ET( _λ_ )\nunder the same conditions as TD( _λ_ ) (Dayan 1992; Peng 1993;\n\n\n\n9999\n\n\n\n\nTsitsiklis 1994) with lower variance when V[ _**e**_ _t|St_ ] _>_ 0,\nwhich is the common case.\nNext, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a\nlearned approximation _**z**_ _t_ ( _s_ ) _≈_ _**z**_ ( _s_ ) that relies solely on\nobserved experience.\n\n**Proposition 2.** _Let_ _**e**_ _t an instantaneous trace vector. Then_\n1 _nt_ ( _s_ )\n_let_ _**z**_ _t_ ( _s_ ) _be the empirical mean_ _**z**_ _t_ ( _s_ ) = _nt_ ( _s_ ) � _i_ _**e**_ _t_ _[s]_ _i_ _[,]_\n_where t_ _[s]_ _i_ _[denotes past times when we have been in state]_\n_s, that is St_ _[s]_ _i_ [=] _[ s][, and][ n][t]_ [(] _[s]_ [)] _[ is the number of visits to][ s]_\n_in the first t steps. Consider the expected trace algorithm_\n**w** _t_ +1 = **w** _t_ + _αtδt_ _**z**_ _t_ ( _St_ ) _. If St is Markov, the expectation of_\n_this update is equal to the expected update with instantaneous_\n_traces_ _**e**_ _t, while attaining a potentially lower variance:_\n\n\nE [ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] = E [ _αtδt_ _**e**_ _t | St_ ] _and_\nV[ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] _≤_ V[ _αtδt_ _**e**_ _t | St_ ] _,_\n\n\n_where the second inequality holds component-wise. The in-_\n_equality is strict when_ V[ _**e**_ _t | St_ ] _>_ 0 _._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** Proposition 2 mirrors Proposition 1 but, importantly, covers the case where we estimate the expected\ntraces from data, rather than relying on exact estimates. This\nmeans the benefits extend to this pure learning setting. Again,\nthe result holds for any trace update. The inequality is typically strict when the path leading to state _St_ = _s_ is stochastic\n(due to environment or policy).\nNext we consider what happens if we do not have Markov\nstates and instead have to rely on, possibly non-Markovian,\nfeatures **x** ( _s_ ). We then have to pick a function class and for\nthe purpose of this analysis we consider linear expected traces\n_**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) and values _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), as convergence for non-linear values can not always be assured even\nfor standard TD( _λ_ ) (Tsitsiklis and Van Roy 1997), without\nadditional assumptions (e.g., Ollivier 2018; Brandfonbrener\nand Bruna 2020).\n\n**Proposition 3.** _When using approximations z_ **Θ** ( _s_ ) = **Θx** ( _s_ )\n_and v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ) _then, if_ (1 _−_ _η_ ) **Θ** + _η_ I _is non-singular,_\n_ET(λ, η) has the same fixed point as TD(λη)._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** This result implies that linear ET( _λ_, _η_ ) converges under similar conditions as linear TD( _λ_ _[′]_ ) for _λ_ _[′]_ = _λ·η_ .\nIn particular, when **Θ** is non-singular, using the approximation _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) in ET( _λ_, 0) = ET( _λ_ ) implies convergence to the fixed point of TD(0).\nThough ET( _λ_, _η_ ) and TD( _λη_ ) have the same fixed point,\nthe algorithms are not equivalent. In general, their updates\nare not the same. Linear approximations are more general\nthan tabular functions (which are linear functions of a indicator vector for the current state), and we have already seen\nin Figure 1 that ET( _λ_ ) behaves quite differently from both\nTD(0) and TD( _λ_ ), and we have seen its variance can be lower\nin Propositions 1 and 2. Interestingly, **Θ** resembles a preconditioner that speeds up the linear semi-gradient TD update,\n\n\n10000\n\n\n\nepisode 5\n1st reward\n\n\n\nepisode 12\n2nd reward\n\n\n\nepisode 100\n20 rewards\n\n\n\nepisode 1K\n~200 rewards\n\n\n\nepisode 10K\n~2K rewards\n\n\n\nFigure 2: In the same setting as Figure 1, we show later value\nestimates after more rewards have been observed. TD(0)\nlearns slowly but steadily, TD( _λ_ ) learns faster but with higher\nvariance, and ET( _λ_ ) learns both fast and stable.\n\n\nsimilar to how second-order optimisation algorithms (Amari\n1998; Martens 2016) precondition the gradient updates.\n\n\n**Empirical Analysis**\n\nFrom the insights above, we expect that ET( _λ_ ) yields lower\nprediction errors because it has lower variance and aggregates information across episodes better. In this section we\nempirically investigate expected traces in several experiments.\nWhenever we refer to ET( _λ_ ), this is equivalent to ET( _λ_, 0).\n\n\n\n**An Open World**\n\nFirst consider the grid world depicted in Figure 1. The agent\nrandomly moves right or down (excluding moves that would\nhit a wall), starting from the top-left corner. Any action in the\nbottom-right corner terminates the episode with +1 reward\nwith probability 0 _._ 2, and 0 otherwise. All other rewards are 0.\nFigure 1 shows value estimates after the first positive reward, which occurred in the fifth episode. TD(0) updated a\nsingle state, TD( _λ_ ) updated earlier states in that episode, and\nET( _λ_ ) additionally updated states from previous episodes.\nFigure 2 additionally shows value estimates after the\nsecond reward (which occurred in episode 12), and after\nroughly 20, 200, and 2000 rewards (or 100, 1000, and 10 _,_ 000\nepisodes, respectively). ET( _λ_ ) converged faster than TD(0),\nwhich propagated information slowly, and faster than TD( _λ_ ),\nwhich exhibited higher variance. All step sizes decayed as\n_α_ = _β_ = ~~�~~ 1 _/k_, where _k_ is the current episode number.\n\n\n**A Multi-Chain**\n\nWe now consider the multi-chain shown in Figure 3. We\nfirst compare TD( _λ_ ) and ET( _λ_ ) with tabular values on various variants of the multi-chain, corresponding to _m ∈_\n_{_ 1 _,_ 2 _,_ 4 _,_ 8 _, ...,_ 128 _}_ parallel chains of length _n_ = 4. The leftmost plot in Figure 4 shows the average root mean squared\nerror (RMSE) of the value predictions after 1024 episodes.\nWe ran 10 seeds for each combination of step size 1 _/t_ _[d]_ with\n_d ∈{_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 1 _}_ and _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ .\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-4-0.png)\n\neither is +1 with probability 0 _._ 9 or _−_ 1 with probability 0 _._ 1.\n\n\nThe left plot in Figure 4 shows value errors for different\n_m_, minimized over _d_ and _λ_ . The prediction error of TD( _λ_ )\n(blue) grew quickly with the number of parallel chains. ET( _λ_ )\n(orange) scaled better, because it updates values in multiple\nchains (from past episodes) upon receiving a surprising reward (e.g., _−_ 1) on termination. The other three plots in Figure\n4 show value error as a function of _λ_ for a subset of problems\ncorresponding to _m ∈{_ 8 _,_ 32 _,_ 128 _}_ . The dependence on _λ_\ndiffers across algorithms and problem instances, but ET( _λ_ )\nconsistently achieved lower error than TD( _λ_ ), especially with\nhigh _λ_ . Further analysis, including on step-size sensitivity, is\nincluded in the appendix.\nNext, we encode each state with a feature vector **x** ( _s_ )\ncontaining a binary indicator vector of the branch, a binary\nindicator of the progress along the chain, a bias that always\nequals one, and two binary features indicating when we are in\nthe start (white) or bottleneck (orange) state. We extend the\nlengths of the chains to _n_ = 16. Both TD( _λ_ ) and ET( _λ_ ) use\na linear value function _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), and ET( _λ_ ) uses a\nlinear expected trace _z_ **Θ** ( _s_ ) = **Θx** ( _s_ ). All updates use the\nsame constant step size _α_ . The left plot in Figure 5 shows the\naverage root mean squared value error after 1024 episodes\n(averaged over 10 seeds). For each point the best constant\nstep size _α ∈{_ 0 _._ 01 _,_ 0 _._ 03 _,_ 0 _._ 1 _}_ (shared across all updates)\nand _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ is selected. ET( _λ_ ) (orange)\nattained lower errors across all values of _m_ (left plot), and\nfor all _λ_ (center two plots, for two specific _m_ ). The right plot\nshows results for smooth interpolations via _η_, for _λ_ = 0 _._ 9\nand _m_ = 16. The full expected trace ( _η_ = 0) performed well\nhere, we expect in other settings the additional flexibility of\n_η_ could be beneficial.\n\n\n**Expected Traces in Deep Reinforcement Learning**\n\n(Deep) neural networks are a common choice of function\nclass in reinforcement learning (e.g., Werbos 1990; Tesauro\n1992, 1994; Bertsekas and Tsitsiklis 1996; Prokhorov and\nWunsch 1997; Riedmiller 2005; van Hasselt 2012; Mnih\net al. 2015; van Hasselt, Guez, and Silver 2016; Wang et al.\n2016; Silver et al. 2016; Duan et al. 2016; Hessel et al. 2018).\nEligibility traces are not very commonly combined with deep\nnetworks (but see Tesauro 1992; Elfwing, Uchibe, and Doya\n2018), perhaps in part because of the popularity of experience\n\n\n10001\n\n\n\nreplay (Lin 1992; Mnih et al. 2015; Horgan et al. 2018).\nPerhaps the simplest way to extend expected traces to deep\nneural networks is to first separate the value function into\na representation **x** ( _s_ ) and a value _v_ ( **w** _,_ _**ξ**_ )( _s_ ) = **w** _[⊤]_ **x** _**ξ**_ ( _s_ ),\nwhere **x** _**ξ**_ is some (non-linear) function of the observations\n_s_ . [4] We can then apply the same expected trace algorithm as\nused in the previous sections by learning a separate linear\nfunction _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) using the representation which is\nlearned by backpropagating the value updates:\n\n\n**w** _t_ +1 = **w** _t_ + _αδ_ _**z**_ **Θ** ( _St_ ) _,_\n\n_**ξ**_ _t_ +1 = _**ξ**_ _t_ + _αδ_ _**e**_ _**[ξ]**_ _t_ _[,]_\n\nwhere _**e**_ _**[ξ]**_ _t_ [=] _[ γ][t][λ]_ _**[e][ξ]**_ _t−_ 1 [+] _[ ∇]_ _**[ξ]**_ _[v]_ [(] **[w]** _[,]_ _**[ξ]**_ [)][(] _[S][t]_ [)] _[,]_\n\n_**e**_ **[w]** _t_ [=] _[ γ][t][λ]_ _**[e]**_ **[w]** _t−_ 1 [+] _[ ∇]_ **[w]** _[v]_ ( **w** _,_ _**ξ**_ ) [(] _[S][t]_ [)] _[,]_\n\n\nand then updating **Θ** to minimise component-wise squared\ndifferences between _**e**_ **[w]** _t_ [and] _**[ z]**_ **[Θ]** _t_ [(] _[S][t]_ [)][, as in (2) and (3).]\nInteresting challenges appear outside the fully linear case.\nFirst, the representation will itself be updated and will have\nits own trace _**e**_ _**[ξ]**_ _t_ [. Second, in the control case we optimise]\nbehaviour: the policy will change. Both these properties of\nthe non-linear control setting imply that the expected traces\nmust track a non-stationary target. We found that being able to\ntrack this rather quickly improved performance: the expected\ntrace parameters **Θ** in the following experiment were updated\nwith a relatively high step size of _β_ = 0 _._ 1.\nWe tested this idea on two canonical Atari games: Pong and\nMs. Pac-Man. The results in Figure 6 show that the expected\ntraces helped speed up learning compared to the baseline\nwhich uses accumulating traces, for various step sizes. Unlike\nmost prior work on this domain, which often relies on replay\n(Mnih et al. 2015; Schaul et al. 2016; Horgan et al. 2018)\nor parallel streams of experience (Mnih et al. 2016), these\nalgorithms updated the values online from a single stream\nof experience. Further details on the experimental setup are\ngiven in the appendix.\nThese experiments demonstrate that the idea of expected\ntraces extends to non-linear function approximation, such as\ndeep neural networks. We consider this to be a rich area of\nfurther investigations. The results presented here are similar\nto earlier results (e.g., Mnih et al. 2015) and are not meant to\ncompete with state-of-the-art performance results, which often depend on replay and much larger amounts of experience\n(e.g., Horgan et al. 2018).\n\n\n**Discussion and Extensions**\n\nWe now discuss various interesting interpretations and relations, and discuss promising extensions.\n\n\n**Predecessor Features**\n\nFor linear value functions the expected trace _z_ ( _s_ ) can be\nexpressed non-recursively as follows:\n\n\n\n�\n\n\n\n_**z**_ ( _s_ ) = E\n\n\n\n_∞_\n� _λ_ [(] _t_ _[n]_ [)] _γt_ [(] _[n]_ [)] **x** _t−n | St_ = _s_\n� _n_ =0\n\n\n\n_,_ (7)\n\n\n\n4Here _s_ denotes observations to the agent, not a full environment\nstate— _s_ is not assumed to be Markovian.\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-1.png)\n\nFigure 4: Prediction errors in the multi-chain. ET( _λ_ ) (orange) consistently outperformed TD( _λ_ ) (blue). Shaded areas depict\nstandard errors across 10 seeds.\n\n\nFigure 5: Comparing value error with linear function approximation a) as function of the number of branches (left), b) as\nfunction of _λ_ (center two plots) and c) as function of _η_ (right). The left three plots show comparisons of TD( _λ_ ) (blue) and ET( _λ_ )\n(orange), showing ET( _λ_ ) attained lower prediction errors. The right plot interpolates between these algorithms via ET( _λ_, _η_ ),\nfrom ET( _λ_ ) = ET( _λ_, 0) to ET( _λ_, 1) = TD( _λ_ ), with _λ_ = 0 _._ 9 (corresponding to a vertical slice indicated in the second plot).\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-3.png)\n\nwhere _γk_ [(] _[n]_ [)] _≡_ [�] _[k]_ _j_ = _k−n_ _[γ][j]_ [. This is interestingly similar to the]\ndefinition of _successor features_ (Barreto et al. 2017):\n\n\n\n�\n\n\n\n_ψ_ ( _s_ ) = E\n\n\n\n_∞_\n� _γt_ [(] +1 _[n][−]_ [1)] **x** _t_ + _n | St_ = _s_\n� _n_ =1\n\n\n\n_._ (8)\n\n\n\nThe summation in (8) is over future features, while in (7)\nwe have a sum over features already observed by the agent.\nWe can thus think of linear expected traces as _predecessor_\n_features_ . A similar connection was made in the tabular setting by Pitis (2018), relating source traces, which aim to\nestimate the source matrix ( _I −_ _γP_ ) _[−]_ [1], to successor representations (Dayan 1993). In a sense, the above generalises\nthis insight. In addition to being interesting in its own right,\nthis connection allows for an intriguing interpretation of _**z**_ ( _s_ )\nas a multidimensional value function. Like with successor\nfeatures, the features **x** _t_ play the role of rewards, discounted\nwith _γ · λ_ rather than _γ_, and with time flowing backwards.\nAlthough the predecessor interpretation only holds in the\nlinear case, it is also of interest as a means to obtain a practical\nimplementation of expected traces with non-linear function\napproximation, for instance applied only to the linear ‘head’\nof a deep neural network. We used this ‘predecessor feature\ntrick’ in our Atari experiments described earlier.\n\n\n**Relation to Model-Based Reinforcement Learning**\n\n\nModel-based reinforcement learning provides an alternative\napproach to efficient credit assignment. The general idea is\nto construct a model that estimates state-transition dynamics,\nand to update the value function based upon hypothetical\n\n\n10002\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-4.png)\n\ntransitions drawn from the model (Sutton 1990), for example\nby prioritised sweeping (Moore and Atkeson 1993; van Seijen\nand Sutton 2013). In practice, model-based approaches have\nproven challenging in environments (such as Atari games)\nwith rich perceptual observations, compared to model-free\napproaches that more directly update the agent’s policy and\npredictions (van Hasselt, Hessel, and Aslanides 2019).\nIn some sense, expected traces also construct a model of\nthe environment—but one that differs in several key regards\nfrom standard state-to-state models used in model-based reinforcement learning. First, expected traces estimate _past_\nquantities rather than _future_ quantities. Backward planning\n(e.g., Chelu, Precup, and van Hasselt 2020) is possible with\nexplicit transition models, but is less common in practice.\nSecond, expected traces estimate the accumulation of _gradi-_\n_ents_ over a multi-step trajectory, rather than trying to learn\nthe full transition dynamics, thereby focusing only on those\naspects that matter for the eventual weight update. Third, expected traces allow credit assignment across these potential\npast trajectories with a single update, without the iterative\ncomputation that is typically required when using a dynamics\nmodel. These differences may be important to side-step some\nof the challenges faced in model-based learning.\n\n\n**Batch Learning and Replay**\n\n\nWe have mainly considered the online learning setting in this\npaper. It is often convenient to learn from batches of data, or\nreplay transitions repeatedly, to enhance data efficiency. A\nnatural extension is replay the experiences sequentially (e.g.\nKapturowski et al. 2018), but perhaps alternatives exist. We\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-1.png)\n\nFigure 6: Performance of Q( _λ_ ) ( _η_ = 1, blue) and QET( _λ_ ) ( _η_ = 0, orange) on Pong and Ms.Pac-Man for various learning rates.\nShaded regions show standard error across 10 random seeds. All results are for _λ_ = 0 _._ 95. Further implementation details and\nhyper-parameters are in the appendix.\n\n\n\nnow discuss one potential extension.\nWe defined a mixed trace _**y**_ _t_ that mixes the instantaneous\nand expected traces. Optionally the expected trace _**z**_ _t_ can\nbe updated towards the mixed trace _**y**_ _t_ as well, instead of\ntowards the instantaneous trace _**e**_ _t_ . Analogously to TD( _λ_ ) we\npropose to then use at least one real step of data:\n\n\n∆ _**θ**_ _t ≡_ _β_ ( _**∇**_ _t_ + _γtλt_ _**y**_ _t−_ 1 _−_ _**zθ**_ ( _St_ )) _[⊤]_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] _,_ (9)\n\n_∂_ _**θ**_\n\n\nwith _**∇**_ _t ≡∇_ **w** _v_ **w** ( _St_ ). This is akin to a forward-view _λ_ return update, with _∇_ **w** _v_ **w** ( _St_ ) in the role of (vector) reward,\nand _**zθ**_ of value, and discounted by _λtγt_, but reversed in time.\nIn other words, this can be considered a sampled Bellman\nequation (Bellman 1957) but backward in time.\nWhen we then choose _η_ = 0, then _**y**_ _t−_ 1 = _z_ _**θ**_ ( _St−_ 1), and\nthen the target in (9) only depends on a single transition.\nInterestingly, that means we can then learn expected traces\nfrom _individual_ transitions, sampled out of temporal order,\nfor instance in batch settings or when using replay.\n\n\n**Application to Other Traces**\n\n\nWe can apply the idea of expected trace to more traces than\nconsidered here. We can for instance consider the characteristic eligibility trace used in REINFORCE (Williams 1992)\nand related policy-gradient algorithms (Sutton et al. 2000).\nAnother appealing application is to the follow-on trace\nor _emphasis_, used in emphatic temporal difference learning\n(Sutton, Mahmood, and White 2016) and related algorithms\n(e.g., Imani, Graves, and White 2018). Emphatic TD was\nproposed to correct an important issue with off-policy learning, which can be unstable and lead to diverging learning\ndynamics. Emphatic TD weights updates according to 1) the\ninherent interest in having accurate predictions in that state\nand, 2) the importance of predictions in that state for updating\n\n\n10003\n\n\n\nother predictions. Emphatic TD uses scalar ‘follow-on’ traces\nto determine the ‘emphasis’ for each update. However, this\nfollow-on trace can have very high, even infinite, variance.\nInstead, we might estimate and use its expectation instead of\nthe instantaneous emphasis. A related idea was explored by\nZhang, Boehmer, and Whiteson (2019) to obtain off-policy\nactor critic algorithms.\n\n\n**Conclusion**\n\n\nWe have proposed a mechanism for efficient credit assignment, using the expectation of an eligibility trace. We have\ndemonstrated this can sometimes speed up credit assignment\ngreatly, and have analyzed concrete algorithms theoretically\nand empirically to increase understanding of the concept.\nExpected traces have several interpretations. First, we can\ninterpret the algorithm as counterfactually updating multiple possible trajectories leading up to the current state. Second, they can be understood as trading off bias and variance,\nwhich can be done smoothly via a unifying _η_ parameter, between standard eligibility traces (low bias, high variance) and\nestimated traces (possibly higher bias, but lower variance).\nFurthermore, with tabular or linear function approximation\nwe can interpret the resulting expected traces as predecessor\nstates or features—object analogous to successor states or features, but time-reversed. Finally, we can interpret the linear\nalgorithm as preconditioning the standard TD update, thereby\npotentially speeding up learning. These interpretations suggest that a variety of complementary ways to potentially\nextend these concepts and algorithms.\nWe have shown expected traces can already be used to\nenhance learning in non-linear settings (i.e., deep reinforcement learning), and in the control setting where we update\nthe policy. Further work is needed to determine the full extent\nof the possibilities of these new algorithms.\n\n\n\n\n**References**\n\n\nAmari, S. I. 1998. Natural gradient works efficiently in\nlearning. _Neural computation_ 10(2): 251–276. ISSN 08997667.\n\n\nBarreto, A.; Dabney, W.; Munos, R.; Hunt, J. J.; Schaul, T.;\nvan Hasselt, H. P.; and Silver, D. 2017. Successor features\nfor transfer in reinforcement learning. In _Advances in neural_\n_information processing systems_, 4055–4065.\n\n\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. _J. Artif. Intell. Res. (JAIR)_ 47:\n253–279.\n\n\nBellman, R. 1957. _Dynamic Programming_ . Princeton University Press.\n\n\nBertsekas, D. P.; and Tsitsiklis, J. N. 1996. _Neuro-dynamic_\n_Programming_ . Athena Scientific, Belmont, MA.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018a. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018b. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBrandfonbrener, D.; and Bruna, J. 2020. Geometric Insights\ninto the Convergence of Non-linear TD Learning. In _Interna-_\n_tional Conference on Learning Representations_ .\n\n\nChelu, V.; Precup, D.; and van Hasselt, H. P. 2020. Forethought and Hindsight in Credit Assignment. In Larochelle,\nH.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H.,\neds., _Advances in Neural Information Processing Systems_,\nvolume 33, 2270–2281.\n\n\nDayan, P. 1992. The convergence of TD( _λ_ ) for general\nlambda. _Machine Learning_ 8: 341–362.\n\n\nDayan, P. 1993. Improving generalization for temporal difference learning: The successor representation. _Neural Com-_\n_putation_ 5(4): 613–624.\n\n\nDuan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel,\nP. 2016. Benchmarking deep reinforcement learning for\ncontinuous control. In _International Conference on Machine_\n_Learning_, 1329–1338.\n\n\nElfwing, S.; Uchibe, E.; and Doya, K. 2018. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_ 107:\n3–11.\n\n\nHennigan, T.; Cai, T.; Norman, T.; and Babuschkin, I. 2020.\nHaiku: Sonnet for JAX. URL http://github.com/deepmind/\ndm-haiku.\n\n\nHessel, M.; Modayil, J.; van Hasselt, H. P.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2018. Rainbow: Combining Improvements in Deep\nReinforcement Learning. _AAAI_ .\n\n\n10004\n\n\n\nHorgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel,\nM.; van Hasselt, H. P.; and Silver, D. 2018. Distributed\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ .\n\n\nImani, E.; Graves, E.; and White, M. 2018. An Off-policy\nPolicy Gradient Theorem Using Emphatic Weightings. In\nBengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; CesaBianchi, N.; and Garnett, R., eds., _Advances in Neural Infor-_\n_mation Processing Systems 31_, 96–106. Curran Associates,\nInc. URL http://papers.nips.cc/paper/7295-an-off-policypolicy-gradient-theorem-using-emphatic-weightings.pdf.\n\n\nKaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1995.\nPlanning and Acting in Partially Observable Stochastic Domains. Unpublished report.\n\n\nKapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and\nDabney, W. 2018. Recurrent experience replay in distributed\nreinforcement learning. In _International conference on learn-_\n_ing representations_ .\n\n\nKingma, D. P.; and Adam, J. B. 2015. A method for stochastic optimization. In _International Conference on Learning_\n_Representation_ .\n\n\nLin, L. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_\n8(3): 293–321.\n\n\nMartens, J. 2016. _Second-order optimization for neural net-_\n_works_ . University of Toronto (Canada).\n\n\nMinsky, M. 1963. Steps Toward Artificial Intelligence.\nIn Feigenbaum, E.; and Feldman, J., eds., _Computers and_\n_Thought_, 406–450. McGraw-Hill, New York.\n\n\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous Methods for Deep Reinforcement Learning. In\n_International Conference on Machine Learning_ .\n\n\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg,\nS.; and Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. _Nature_ 518(7540): 529–533.\n\n\nMoore, A. W.; and Atkeson, C. G. 1993. Prioritized Sweeping: Reinforcement Learning with less Data and less Time.\n_Machine Learning_ 13: 103–130.\n\n\nOllivier, Y. 2018. Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies. _CoRR_\nabs/1805.00869.\n\n\nPeng, J. 1993. _Efficient dynamic programming-based learn-_\n_ing for control_ . Ph.D. thesis, Northeastern University.\n\n\nPeng, J.; and Williams, R. J. 1996. Incremental Multi-step\nQ-learning. _Machine Learning_ 22: 283–290.\n\n\nPitis, S. 2018. Source Traces for Temporal Difference Learning. In McIlraith, S. A.; and Weinberger, K. Q., eds., _Pro-_\n_ceedings of the Thirty-Second AAAI Conference on Artificial_\n_Intelligence_, 3952–3959. AAAI Press.\n\n\n\n\nPohlen, T.; Piot, B.; Hester, T.; Azar, M. G.; Horgan, D.;\nBudden, D.; Barth-Maron, G.; van Hasselt, H. P.; Quan, J.;\nVecerˇ ´ık, M.; Hessel, M.; Munos, R.; and Pietquin, O. 2018.\nObserve and look further: Achieving consistent performance\non Atari. _arXiv preprint arXiv:1805.11593_ .\n\n\nProkhorov, D. V.; and Wunsch, D. C. 1997. Adaptive critic\ndesigns. _IEEE Transactions on Neural Networks_ 8(5): 997–\n1007.\n\n\nPuterman, M. L. 1994. _Markov Decision Processes: Discrete_\n_Stochastic Dynamic Programming_ . John Wiley & Sons, Inc.\nNew York, NY, USA.\n\n\nRiedmiller, M. 2005. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning\nMethod. In Gama, J.; Camacho, R.; Brazdil, P.; Jorge, A.; and\nTorgo, L., eds., _Proceedings of the 16th European Conference_\n_on Machine Learning (ECML’05)_, 317–328. Springer.\n\n\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ . Puerto Rico.\n\n\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering\nthe game of Go with deep neural networks and tree search.\n_Nature_ 529(7587): 484–489.\n\n\nSingh, S. P.; and Sutton, R. S. 1996. Reinforcement Learning\nwith replacing eligibility traces. _Machine Learning_ 22: 123–\n158.\n\n\nSutton, R. S. 1984. _Temporal Credit Assignment in Reinforce-_\n_ment Learning_ . Ph.D. thesis, University of Massachusetts,\nDept. of Comp. and Inf. Sci.\n\n\nSutton, R. S. 1988. Learning to predict by the methods of\ntemporal differences. _Machine learning_ 3(1): 9–44.\n\n\nSutton, R. S. 1990. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Proceedings of the seventh international conference_\n_on machine learning_, 216–224.\n\n\nSutton, R. S.; and Barto, A. G. 2018. _Reinforcement Learning:_\n_An Introduction_ . The MIT press, Cambridge MA.\n\n\nSutton, R. S.; Mahmood, A. R.; and White, M. 2016. An\nEmphatic Approach to the Problem of Off-policy TemporalDifference Learning. _Journal of Machine Learning Research_\n17(73): 1–29.\n\n\nSutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y.\n2000. Policy gradient methods for reinforcement learning\nwith function approximation. _Advances in Neural Informa-_\n_tion Processing Systems 13 (NIPS-00)_ 12: 1057–1063.\n\n\nTesauro, G. 1992. Practical Issues in Temporal Difference\nLearning. In Lippman, D. S.; Moody, J. E.; and Touretzky,\nD. S., eds., _Advances in Neural Information Processing Sys-_\n_tems 4_, 259–266. San Mateo, CA: Morgan Kaufmann.\n\n\nTesauro, G. J. 1994. TD-Gammon, a self-teaching backgammon program, achieves master-level play. _Neural computa-_\n_tion_ 6(2): 215–219.\n\n\n10005\n\n\n\nTsitsiklis, J. N. 1994. Asynchronous stochastic approximation and Q-learning. _Machine Learning_ 16: 185–202.\n\nTsitsiklis, J. N.; and Van Roy, B. 1997. An analysis of\ntemporal-difference learning with function approximation.\n_IEEE Transactions on Automatic Control_ 42(5): 674–690.\n\nvan Hasselt, H. P. 2012. Reinforcement Learning in Continuous State and Action Spaces. In Wiering, M. A.; and\nvan Otterlo, M., eds., _Reinforcement Learning: State of the_\n_Art_, volume 12 of _Adaptation, Learning, and Optimization_,\n207–251. Springer.\n\n\nvan Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver,\nD. 2016. Learning values across many orders of magnitude. In _Advances in Neural Information Processing Systems_\n_29: Annual Conference on Neural Information Processing_\n_Systems 2016, December 5-10, 2016, Barcelona, Spain_, 4287–\n4295.\n\n\nvan Hasselt, H. P.; Guez, A.; and Silver, D. 2016. Deep reinforcement learning with double Q-Learning. In _Proceedings_\n_of the Thirtieth AAAI Conference on Artificial Intelligence_,\n2094–2100.\n\n\nvan Hasselt, H. P.; Hessel, M.; and Aslanides, J. 2019. When\nto use parametric models in reinforcement learning? In _Ad-_\n_vances in Neural Information Processing Systems_, volume 32,\n14322–14333.\n\nvan Hasselt, H. P.; Mahmood, A. R.; and Sutton, R. S. 2014.\nOff-policy TD( _λ_ ) with a true online equivalence. In _Pro-_\n_ceedings of the 30th Conference on Uncertainty in Artificial_\n_Intelligence_, 330–339.\n\nvan Hasselt, H. P.; Quan, J.; Hessel, M.; Xu, Z.; Borsa, D.;\nand Barreto, A. 2019. General non-linear Bellman equations.\n_arXiv preprint arXiv:1907.03687_ .\n\n\nvan Hasselt, H. P.; and Sutton, R. S. 2015. Learning to predict\nindependent of span. _CoRR_ abs/1508.04582.\n\nvan Seijen, H.; and Sutton, R. S. 2013. Planning by Prioritized Sweeping with Small Backups. In _International_\n_Conference on Machine Learning_, 361–369.\n\nvan Seijen, H.; and Sutton, R. S. 2014. True online TD( _λ_ ).\nIn _International Conference on Machine Learning_, 692–700.\n\n\nWang, Z.; de Freitas, N.; Schaul, T.; Hessel, M.; van Hasselt,\nH. P.; and Lanctot, M. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In _International_\n_Conference on Machine Learning_ . New York, NY, USA.\n\nWerbos, P. J. 1990. A menu of designs for reinforcement\nlearning over time. _Neural networks for control_ 67–95.\n\nWhitehead, S. D.; and Ballard, D. H. 1991. Learning to\nperceive and act by trial and error. _Machine Learning_ 7(1):\n45–83.\n\nWilliams, R. J. 1992. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. _Machine_\n_Learning_ 8: 229–256.\n\n\nZhang, S.; Boehmer, W.; and Whiteson, S. 2019. Generalized\noff-policy actor-critic. In _Advances in Neural Information_\n_Processing Systems_, 2001–2011.\n\n\n",
          "ranking": {
            "relevance_score": 0.7493067885948483,
            "citation_score": 0.6302697050551669,
            "recency_score": 0.3906854405837399,
            "final_score": 0.6896372370858013
          },
          "is_open_access": false,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Machine learning approaches to code review have evolved from simple pattern matching to sophisticated neural architectures.",
        "chunk_index": 1
      },
      "summary": "Machine learning approaches to code review have evolved from simple pattern matching to sophisticated neural architectures.",
      "vector_score": 0.6880000000000001,
      "llm_score": 0.86,
      "combined_score": 0.86,
      "source_query": "mock_query_related work"
    },
    {
      "chunk": {
        "chunk_id": "mock_5",
        "paper": {
          "id": "66d76444255be0ac378a0a93ee0379fc721a386f",
          "title": "On Q-learning Convergence for Non-Markov Decision Processes",
          "published": "2018-07-01",
          "authors": [
            "Sultan Javed Majeed",
            "Marcus Hutter"
          ],
          "summary": "Temporal-difference (TD) learning is an attractive, computationally efficient framework for model- free reinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent to learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has only been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state MDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the environment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning when applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We prove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in particular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a necessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.",
          "pdf_url": "https://doi.org/10.24963/ijcai.2018/353",
          "doi": "10.24963/ijcai.2018/353",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "International Joint Conference on Artificial Intelligence",
          "citation_count": 40,
          "bibtex": "@Article{Majeed2018OnQC,\n author = {Sultan Javed Majeed and Marcus Hutter},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {2546-2552},\n title = {On Q-learning Convergence for Non-Markov Decision Processes},\n year = {2018}\n}\n",
          "markdown_text": "[image]\n\n\nNavigation\n\n\n  - Home\n\n  - Conferences\n\n\nFuture Conferences\n\n  Past Conferences\n\n  \n\n  - Proceedings\n\n\n  - IJCAI 2025 Proceedings\n\n  - All Proceedings\n\n\n  - Awards\n\n  - Trustees/officers\n\n\nCurrent trustees\n\n  Trustees Elect\n\n  IJCAI Secretariat\n\n  \n  - IJCAI Sponsorship and Publicity Officers\nIJCAI Team\n\n  \n  - Local Arrangements Chairs\n\n  - Former Trustees serving on the Executive Committee\n\n  - Other Former Officers\n\n\n  - AI Journal\n\n  - About\n\n\nAbout IJCAI\n\n  Contact Information\n\n  \n# **On Q-learning Convergence for Non-Markov** **Decision Processes** **On Q-learning Convergence for Non-Markov** **Decision Processes**\n\n## **Sultan Javed Majeed, Marcus Hutter**\n\n\n[image]\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence\n[Main track. Pages 2546-2552. https://doi.org/10.24963/ijcai.2018/353](https://doi.org/10.24963/ijcai.2018/353)\n[PDF BibTeX](https://www.ijcai.org/proceedings/2018/0353.pdf)\n\n\nTemporal-difference (TD) learning is an attractive, computationally efficient framework for model- free\nreinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent\nto learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has\nonly been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state\nMDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the\nenvironment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning\nwhen applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We\nprove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in\nparticular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a\nnecessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.\nKeywords:\nMachine Learning: Online Learning\nMachine Learning: Reinforcement Learning\nPlanning and Scheduling: Markov Decisions Processes\n\n\nCopyright © 2025,\n\n\n",
          "ranking": {
            "relevance_score": 0.7639998734717994,
            "citation_score": 0.5680913780397937,
            "recency_score": 0.27592885552856466,
            "final_score": 0.6760110725910747
          },
          "is_open_access": true,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Transformer-based models have achieved state-of-the-art results on code understanding benchmarks.",
        "chunk_index": 2
      },
      "summary": "Transformer-based models have achieved state-of-the-art results on code understanding benchmarks.",
      "vector_score": 0.672,
      "llm_score": 0.84,
      "combined_score": 0.84,
      "source_query": "mock_query_related work"
    },
    {
      "chunk": {
        "chunk_id": "mock_6",
        "paper": {
          "id": "e457f7b24e8af833b15c802d31b91bd048b158e8",
          "title": "Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning",
          "published": "2023-01-26",
          "authors": [
            "Brett Daley",
            "Martha White",
            "Chris Amato",
            "Marlos C. Machado"
          ],
          "summary": "Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios to combat the variance of the IS estimator. Unfortunately, once a trace has been fully cut, the effect cannot be reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time. These trajectory-aware methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose a multistep operator that can express both per-decision and trajectory-aware methods. We prove convergence conditions for our operator in the tabular setting, establishing the first guarantees for several existing methods as well as many new ones. Finally, we introduce Recency-Bounded Importance Sampling (RBIS), which leverages trajectory awareness to perform robustly across $\\lambda$-values in an off-policy control task.",
          "pdf_url": "http://arxiv.org/pdf/2301.11321",
          "doi": "10.48550/arXiv.2301.11321",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "International Conference on Machine Learning",
          "citation_count": 3,
          "bibtex": "@Article{Daley2023TrajectoryAwareET,\n author = {Brett Daley and Martha White and Chris Amato and Marlos C. Machado},\n booktitle = {International Conference on Machine Learning},\n pages = {6818-6835},\n title = {Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning},\n year = {2023}\n}\n",
          "markdown_text": "## **Trajectory-Aware Eligibility Traces for** **Off-Policy Reinforcement Learning**\n\n**Brett Daley** [1 2] **Martha White** [1 2 3] **Christopher Amato** [4] **Marlos C. Machado** [1 2 3]\n\n\n\n**Abstract**\n\n\nOff-policy learning from multistep returns is crucial for sample-efficient reinforcement learning,\nbut counteracting off-policy bias without exacerbating variance is challenging. Classically, offpolicy bias is corrected in a _per-decision_ manner:\npast temporal-difference errors are re-weighted by\nthe instantaneous Importance Sampling (IS) ratio\nafter each action via eligibility traces. Many offpolicy algorithms rely on this mechanism, along\nwith differing protocols for _cutting_ the IS ratios\nto combat the variance of the IS estimator. Un\nfortunately, once a trace has been fully cut, the\neffect cannot be reversed. This has led to the\n\ndevelopment of credit-assignment strategies that\naccount for multiple past experiences at a time.\nThese _trajectory-aware_ methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose\na multistep operator that can express both perdecision and trajectory-aware methods. We prove\nconvergence conditions for our operator in the\ntabular setting, establishing the first guarantees\nfor several existing methods as well as many new\nones. Finally, we introduce Recency-Bounded\nImportance Sampling (RBIS), which leverages\ntrajectory awareness to perform robustly across\n_λ_ -values in several off-policy control tasks.\n\n\n**1. Introduction**\n\n\nReinforcement learning concerns an agent interacting with\nits environment through trial and error to maximize its expected cumulative reward. One of the great challenges of re\n\n1Department of Computing Science, University of Alberta,\nEdmonton, AB, Canada [2] Alberta Machine Intelligence Institute\n3Canada CIFAR AI Chair 4Khoury College of Computer Sciences,\nNortheastern University, Boston, MA, USA. Correspondence to:\nBrett Daley _<_ brett.daley@ualberta.ca _>_ .\n\n\n_Proceedings of the 40_ _[th]_ _International Conference on Machine_\n_Learning_, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n\n\n\ninforcement learning is the temporal credit assignment problem (Sutton, 1984): upon receiving a reward, which past actions should be held responsible and, hence, be reinforced?\nBasic temporal-difference (TD) methods assign credit to the\nimmediately taken action (e.g., Watkins, 1989; Rummery &\nNiranjan, 1994), bootstrapping from previous experience to\nlearn long-term dependencies. This process requires a large\nnumber of repetitions to generate effective behaviors from\nrewards, motivating research into _multistep_ return estimation\nin which credit is distributed among multiple past actions\naccording to some eligibility rule (e.g., Sutton, 1988).\n\n\nOne challenge of multistep estimators is that they generally\nhave higher variance than 1-step estimators (Kearns &\nSingh, 2000). This is exacerbated in the off-policy setting,\nwhere environment interaction is conducted according to a\nbehavior policy that differs from the target policy for which\nreturns are being estimated. The discrepancy between the\ntwo policies manifests mathematically as bias in the return\nestimation, which can be detrimental to learning if left\nunaddressed (Precup et al., 2000). Despite these challenges,\noff-policy learning is important for exploration and sample\nefficiency. The canonical bias-correction technique is Importance Sampling (IS; Kahn & Harris, 1951), wherein the\nbias due to the differing policies is eliminated by the product\nof their probability ratios (Precup et al., 2000). Although\nIS theoretically resolves the off-policy bias, it can suffer\nfrom extreme variance that makes it largely impractical.\n\n\nDirectly managing the variance of the IS estimator has been\na fruitful avenue for developing efficient off-policy algorithms. Past work has focused on modifying the individual\nIS ratios to reduce the variance of the full update: e.g., Tree\nBackup (Precup et al., 2000), Q _[π]_ ( _λ_ ) (Harutyunyan et al.,\n2016), Retrace (Munos et al., 2016), ABQ (Mahmood et al.,\n2017), and C-trace (Rowland et al., 2020). All of these\nmethods can be implemented online with _per-decision_ rules\n(Precup et al., 2000) that determine how much to reduce,\nor _cut_, the IS ratio according to the current state-action\npair. The re-weighted TD error is then broadcast to previous experiences using eligibility traces (Barto et al., 1983;\nSutton, 1984). The decisions made by these algorithms are\nMarkov in the sense that each iterative off-policy correction\ndepends on only the current state-action pair. One issue\n\n\n\n1\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\nwith this is that it can lead to suboptimal decisions, since\nfully cutting a trace cannot be reversed later. In contrast, a\n_trajectory-aware_ method can examine an entire sequence\nof past state-action pairs to make globally better decisions\nregarding credit assignment; for example, when a specific\ntransition yields a high IS ratio, a trajectory-aware method\ncan choose to not cut the trace if the product of all previous\nIS ratios remains small.\n\n\nIndeed, some existing off-policy methods already conduct\noffline bias correction in a trajectory-aware manner. Perhaps\nthe simplest example is Truncated IS, where the IS ratio\nproducts are pre-calculated offline and then clipped to\nsome finite value (see Section 4). More recently, Munos\net al. (2016) suggested a recursive variant of Retrace that\nautomatically relaxes the clipping bound when its historical\ntrace magnitude becomes small; the authors conjectured that\nthis could lead to faster learning. No theoretical analysis\nhas been conducted on trajectory-aware algorithms such\nas these; their convergence properties are unknown, and the\nspace of possible algorithms has not yet been fully explored.\n\n\nTo better understand these algorithms, and to support new\ndiscoveries of efficient algorithms, we introduce a unifying\ntheoretical perspective on per-decision and trajectory-aware\noff-policy corrections. We propose a multistep operator that\naccounts for arbitrary dependencies on past experiences,\nsignificantly generalizing the per-decision _R_ operator introduced by Munos et al. (2016). We prove that our operator\nconverges for policy evaluation and control. In the latter\ncase, we remove the assumptions of increasingly greedy\npolicies and pessimistic initialization used by Munos et al.\n(2016), which has implications for per-decision methods.\nFinally, we derive a new method from our theory, RecencyBounded Importance Sampling (RBIS), which performs\nfavorably to other trajectory-aware methods across a wide\nrange of _λ_ -values in an off-policy control task.\n\n\n**2. Preliminaries**\n\n\nWe consider Markov Decision Processes (MDPs) of the\nform ( _S, A, P, R, γ_ ). _S_ and _A_ are finite sets of states and\nactions, respectively. Letting ∆ _X_ denote the set of distributions over a set _X_, then _P_ : _S × A →_ ∆ _S_ is the transition function, _R_ : _S × A →_ R is the reward function, and\n_γ ∈_ [0 _,_ 1) is the discount factor. A policy _π_ : _S →_ ∆ _A_\ndetermines an agent’s probability of selecting a given action\nin each state. A value function _Q_ : _S × A →_ R represents\nthe agent’s estimate of the expected return achievable from\neach state-action pair. For a policy _π_, we define the operator\n\n\n\nerators such as _Pπ_ can hence be interpreted as _n × n_ square\nmatrices that multiply these vectors, with repeated application corresponding to exponentiation: _Pπ_ _[t]_ _[Q]_ [ =] _[ P][π]_ [(] _[P]_ _π_ _[ t][−]_ [1] _Q_ ).\n\n\nIn the _policy evaluation_ setting, we seek to estimate the\nexpected discounted returns for policy _π_, given by _Q_ _[π]_ :=\n� _∞t_ =0 _[γ][t][P]_ _π_ _[ t]_ _[R]_ [. The value function] _[ Q][π]_ [ is the unique fixed]\npoint of the Bellman operator _TπQ_ := _R_ + _γPπQ_, i.e., it\nuniquely solves the Bellman equation _TπQ_ _[π]_ = _Q_ _[π]_ (Bellman, 1966). In the _control_ setting, we seek to estimate the\nexpected returns _Q_ _[∗]_ under the optimal policy _π_ _[∗]_ . _Q_ _[∗]_ is\nthe unique fixed point of the Bellman optimality operator\n( _TQ_ )( _s, a_ ) := max _π_ ( _TπQ_ )( _s, a_ ), i.e., it uniquely solves\n\n=\nthe Bellman optimality equation _TQ_ _[∗]_ _Q_ _[∗]_ . We are particularly interested in the _off-policy_ learning case, where trajectories of the form ( _S_ 0 _, A_ 0) _,_ ( _S_ 1 _, A_ 1) _,_ ( _S_ 2 _, A_ 2) _, . . ._ are\ngenerated by interacting with the MDP using a behavior\npolicy _µ_, where _µ ̸_ = _π_ . We define the TD error for policy _π_\nat time _t_ as\n\n\n_δt_ _[π]_ [:=] _[ R][t]_ [+] _[ γ]_ � _π_ ( _a_ _[′]_ _|St_ +1) _Q_ ( _St_ +1 _, a_ _[′]_ ) _−_ _Q_ ( _St, At_ ) _,_\n\n_a_ _[′]_ _∈A_\n\n\nwhere _Rt_ := _R_ ( _St, At_ ). Let _ρk_ := _[π]_ _µ_ ( [(] _A_ _[A]_ _k_ _[k]_ _|_ _[|]_ _S_ _[S]_ _k_ _[k]_ ) [)] [for brevity.]\n\nMunos et al. (2016) introduced the off-policy operator\n\n\n( _RQ_ )( _s, a_ ) := _Q_ ( _s, a_ ) +\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\nE _µ_\n\n\n\n_∞_ _t_\n� _γ_ _[t]_ �\n\n_t_ =0 � _k_ =1\n\n\n\n_ck_\n\n_k_ =1\n\n\n\n_δt_ _[π]_\n\n�\n\n\n\n_,_ (1)\n\n\n\n( _PπQ_ )( _s, a_ ) := �\n\n_s_ _[′]_ _∈S_\n\n\n\n� _P_ ( _s_ _[′]_ _|s, a_ ) _π_ ( _a_ _[′]_ _|s_ _[′]_ ) _Q_ ( _s_ _[′]_ _, a_ _[′]_ ) _._\n\n_a_ _[′]_ _∈A_\n\n\n\nAs a shorthand, we represent value functions and the reward\nfunction as vectors in R _[n]_, where _n_ = _|S × A|_ . Linear op\n\n\nwhere _ck_ := _c_ ( _Sk, Ak_ ) _∈_ [0 _, ρk_ ]. We refer to the\nproduct [�] _[t]_ _k_ =1 _[c][k]_ [ as the] _[ trace]_ [ for][ (] _[s, a]_ [)][ at time] _[ t]_ [.] If\nany _ck < ρk_, we say that the trace has been (partially)\ncut. If any _ck_ = 0, then we have fully cut it. If\nthe trace is fully cut at _t_ = 1, i.e., _c_ 1 = 0, then\n( _RQ_ )( _s, a_ ) = _Q_ ( _s, a_ ) + E[ _δ_ 0 _[π]_ _[|]_ [ (] _[S]_ [0] _[, A]_ [0][) = (] _[s, a]_ [)] =]\n_R_ ( _s, a_ )+ _γ_ E[ [�] _a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S]_ [1][)] _[Q]_ [(] _[S]_ [1] _[, a][′]_ [)] _[|]_ [(] _[S]_ [0] _[, A]_ [0][) = (] _[s, a]_ [)]][,]\n\nwhich is the standard 1-step bootstrap target like in TD(0)\n(Sutton, 1988). Notice that each _ck_ is Markov, as it depends\nonly on ( _Sk, Ak_ ) and is otherwise independent of the preceding trajectory. In other words, the update for _R_ can be\ncalculated _per decision_ (Precup et al., 2000), permitting an\nefficient online implementation with eligibility traces.\n\n\n**3. Trajectory-Aware Eligibility Traces**\n\n\nWhile per-decision traces are convenient from a computational perspective, they require making choices about how\nmuch to cut the trace without considering the effects of\nprevious choices. This can lead to suboptimal decisions;\nfor example, if the trace is cut by setting _ck_ = 0 at some\ntimestep, then the effect cannot be reversed later. Regardless\nof whatever new experiences are encountered by the agent,\nexperiences before time _k_ will be ineligible for credit assignment, resulting in an opportunity cost. In fact, this exact\n\n\n\n2\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\ndecision trace cutting can lead to excessively small eligibilities, especially when _ϵ_ is close to 0.\n\n\n\n_Figure 1._ The Tightrope Problem. Starting from state _s_ 1, the agent\nmust take a specific sequence of _n_ actions to receive +1 reward.\n\n\nphenomenon is why Watkins’ Q( _λ_ ) (Watkins, 1989) often\nlearns more slowly than Peng’s Q( _λ_ ) (Peng & Williams,\n1996), even though the former avoids off-policy bias (Sutton\n& Barto, 1998; Daley & Amato, 2019; Kozuno et al., 2021).\nThe same effect (but to a lesser extent) impacts Tree Backup\nand Retrace, where _ck ≤_ 1 always in Eq. (1), implying that\nthe traces for past experiences can never increase.\n\n\nWe illustrate this phenomenon in a small, deterministic MDP\nthat we call the Tightrope Problem (see Figure 1). The environment consists of _n_ sequential, non-terminal states with\ntwo actions _a_ 1 _, a_ 2 available. The agent starts in state _s_ 1 and\nadvances from _si_ to _si_ +1 whenever it takes action _a_ 1. If\n_i_ = _n_, then the episode terminates and the agent receives\n+1 reward. Taking action _a_ 2 in any state immediately terminates the episode with no reward. Clearly, the optimal\npolicy is to execute _a_ 1 regardless of the state.\n\n\nNow consider the following off-policy learning scenario.\nSuppose the agent’s behavior policy _µ_ is uniform random,\nbut the target policy _π_ is _ϵ_ -greedy with respect to a value\nfunction _Q_ . For each state _s_, it follows that _π_ ( _a|s_ ) = 1 _−_ _ϵ_\nif _a_ = argmax _a′ Q_ ( _s, a_ _[′]_ ) and _π_ ( _a|s_ ) = _ϵ_ otherwise. We\nassume _ϵ_ is small in the sense that _ϵ <_ [1]\n\n2 [, and that] _[ γ]_ [ = 1][.]\nSuppose now that the agent successfully receives the +1\nreward during an episode, implying that it took action _a_ 1\non every timestep. We can compute the eligibility of the\ninitial state-action pair ( _s_ 1 _, a_ 1) as an expression in the number _k_ of incorrect actions in the greedy policy (i.e., where\nargmax _a′ Q_ ( _s, a_ _[′]_ ) _̸_ = _a_ 1). Letting _λ ∈_ [0 _,_ 1] be a decay\nparameter, the standard IS estimator (which does not cut\ntraces when _λ_ = 1) provides an eligibility of\n\n\n\nThis issue stems from the fact that Retrace is not _aware_ of\n\nits past eligibilities, and continues to decay them even when\nthey already form an underestimate compared to IS. This\nissue is not unique to Retrace, and affects other per-decision\nmethods like Tree Backup. Instead, a _trajectory-aware_\nmethod that can actively adapt its trace-cutting behavior\nbased on the magnitude of past eligibilities would be better.\n\n\nOne way to obtain a trajectory-aware method is to compute\nthe exact IS product in Eq. (2), and then make adjustments\nto it to achieve certain properties (e.g., convergence and variance reduction). For example, Truncated IS (see Section 4)\nsimply imposes a fixed bound on the IS estimator:\n\n\n_λ_ _[n]_ min�1 _,_ [2(1 _−_ _ϵ_ )] _[n][−][k]_ (2 _ϵ_ ) _[k]_ [�] _._ (3)\n\n\nIgnoring _λ_, Truncated IS reduces the eligibility only when it\nexceeds a pre-specified threshold, effectively avoiding trace\ncuts when the true IS estimate is small. In Section 6, we\npropose an algorithm, RBIS, which achieves a similar effect\nusing a recursive, time-decaying threshold.\n\n\nAs this example demonstrates, it can be advantageous to\nconsider the agent’s past experiences to produce better decisions regarding credit assignment. One of our principal\ncontributions is the proposal and analysis of an off-policy\noperator _M_ that encompasses this possibility. Let _Ft_ :=\n( _S_ 0 _, A_ 0) _,_ ( _S_ 1 _, A_ 1) _, . . .,_ ( _St, At_ ). We define _M_ such that\n\n\n( _MQ_ )( _s, a_ ) := _Q_ ( _s, a_ ) +\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\nE _µ_\n\n\n\n� _γ_ _[t]_ _βtδt_ _[π]_\n\n\n_t_ =0\n\n\n\n_,_ (4)\n\n\n\n_k_\n= _λ_ _[n]_ [2(1 _−_ _ϵ_ )] _[n][−][k]_ (2 _ϵ_ ) _[k]_ _._ (2)\n�\n\n\n\nwhere _βt_ := _β_ ( _Ft_ ) is a trace that generally depends on\nthe history _Ft_ . We define _β_ 0 := 1 to ensure that the first\nTD error, _δ_ 0 _[π]_ [, is applied. In Section][ 5][, we characterize the]\nvalues of _βt_ for _t ≥_ 1 that lead to convergence.\n\n\nThe major analytical challenge of _M_ —and its main\nnovelty—is the complex dependence on the sequence _Ft_ .\nThis makes the operator difficult to analyze mathematically,\nas the terms in the series 1+ _γβ_ 1+ _γ_ [2] _β_ 2+ _· · ·_ generally share\nno common factors that would allow a recursive formula for\n\neligibility traces. Some off-policy methods, however, cannot\nbe described by factored traces, and therefore removing this\nassumption is necessary to understand existing algorithms\n(see Section 4), while also paving the way for new creditassignment methods. In the special case where _βt_ does\nfactor into Markov coefficients, i.e., _βt_ = [�] _[t]_ _k_ =1 _[c][k]_ [, then]\nEq. (4) reduces to Eq. (1), taking us back to the per-decision\nsetting studied by Munos et al. (2016). _M, therefore, unifies_\n_per-decision and trajectory-aware methods._\n\n\n\n_λ_ [1] _[ −]_ _[ϵ]_\n� 1 _/_ 2\n\n\n\n_n−k_\n_ϵ_\n_λ_\n� � 1 _/_ 2\n\n\n\nThis value can be greater than 1 when _k ≪_ _n_, which suggests that the agent’s behavior should be heavily reinforced\nwhen the greedy policy agrees closely with the optimal policy; however, a per-decision method like Retrace, which cuts\ntraces without considering the full trajectory (see Section 4),\nultimately assigns a much lower eligibility:\n\n\n\n_n−k_\n_ϵ_\n�� � _λ_ min�1 _,_ 1 _/_ 2\n\n\n\n� _λ_ min�1 _,_ [1] 1 _[ −]_ _/_ 2 _[ϵ]_\n\n\n\n1 _/_ 2\n\n\n\n_k_\n= _λ_ _[n]_ (2 _ϵ_ ) _[k]_ _._\n��\n\n\n\nThe eligibility now decays monotonically for every suboptimal action in the greedy policy, illuminating how per\n\n\n3\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\n**4. Unifying Off-Policy Algorithms**\n\n\nThe operator _M_ is a strict generalization of the previous\noperator considered for trace-based methods, allowing us to\nexpress existing algorithms in this form. We provide a nonexhaustive list of examples below with the corresponding\n_βt_ used in _M_ . For brevity, let Π _t_ := [�] _[t]_ _k_ =1 _[ρ][k]_ [.]\n\n\n**Importance Sampling:** _βt_ = _λ_ _[t]_ Π _t_ (Kahn & Harris, 1951).\nThe standard approach for correcting off-policy bias. Although it is the only unbiased estimator in this list (if _λ_ = 1),\nit suffers from high variance, making it difficult to utilize.\n\n\n**Q** _[π]_ **(** _λ_ **):** _βt_ = _λ_ _[t]_ (Harutyunyan et al., 2016). A straightforward algorithm that decays the TD errors by a fixed constant.\nThe algorithm does not require explicitly knowing _µ_, which\nis desirable, but can diverge if _π_ and _µ_ differ too much\n(Harutyunyan et al., 2016, Theorem 1).\n\n\n**Tree Backup:** _βt_ = [�] _[t]_ _k_ =1 _[λπ]_ [(] _[A][k][|][S][k]_ [)][ (][Precup et al.][,][ 2000][).]\nA method that automatically cuts traces according to the\nproduct of probabilities under _π_, which forms a conservative\nlower bound on the IS estimate. Tree Backup converges for\nany behavior policy _µ_, but it is not efficient since traces are\ncut excessively—especially in the on-policy case.\n\n\n**Retrace:** _βt_ = [�] _[t]_ _k_ =1 _[λ]_ [ min(1] _[, ρ][k]_ [)][ (][Munos et al.][,][ 2016][).]\nA convergent algorithm for arbitrary policies _π_ and _µ_ that\nremains efficient in the on-policy case because it does not cut\ntraces (if _λ_ = 1); however, the fact that _βt_ never increases\ncan cause the trace products to decay too quickly in practice\n(Mahmood et al., 2017; Rowland et al., 2020).\n\n\nAll of the above can be analyzed using a per-decision operator. The next two, on the other hand, have weightings based\non the entire trajectory. We use the theory for our general\n_M_ operator to prove properties about these methods.\n\n\n**Recursive Retrace:** _βt_ = _λ_ min(1 _, βt−_ 1 _ρt_ ) (Munos et al.,\n2016). A modification to Retrace conjectured to lead to\nfaster learning. It clips large products of ratios, rather than\nindividual ratios. Its convergence for control is an open\nquestion, which we solve in Section 5.\n\n\n**Truncated Importance Sampling:** _βt_ = _λ_ _[t]_ min(1 _,_ Π _t_ )\n(Ionides, 2008). A simple but effective method to combat\nthe variance of IS. Variations of this algorithm have been applied in the reinforcement learning literature (e.g., Uchibe &\nDoya, 2004; Wawrzynski & Pacut´, 2007; Wawrzynski´, 2009;\nWang et al., 2017), but, to our knowledge, its convergence\nin an MDP setting has not been studied. In Section 5.3, we\nshow that it can diverge in at least one off-policy problem.\n\n\n**5. Convergence Analysis**\n\n\nIn this section, we study the convergence properties of the\n_M_ operator for policy evaluation and control. It will be\nconvenient to re-express Eq. (4) in vector notation for our\n\n\n\n_Bt_ is a linear operator and hence can be represented as a\nmatrix in R _[n][×][n]_, the elements of which are nonnegative.\nEach element of _Bt_, row-indexed by ( _s, a_ ) and columnindexed by ( _s_ _[′]_ _, a_ _[′]_ ), has the form\n\n\n_Bt_ (( _s, a_ ) _,_ ( _s_ _[′]_ _, a_ _[′]_ ))=Pr _µ_ [((] _[S][t][, A][t]_ [)=(] _[s][′][, a][′]_ [)] _[|]_ [(] _[S]_ [0] _[, A]_ [0][)=(] _[s, a]_ [))]\n\n\n_′_ _′_\n_×_ E _µ_ � _βt_ �� ( _S_ 0 _, A_ 0)=( _s, a_ ) _,_ ( _St, At_ )=( _s_ _, a_ )� _._ (8)\n\n\nWe justify this form in Appendix A. Note that _B_ 0 = _I_, the\nidentity matrix, because of our earlier definition of _β_ 0 := 1.\nIn the following sections, all inequalities involving vectors\nor matrices should be interpreted element wise. We let\n_∥X∥_ := _∥X∥∞_ for a matrix (or vector) _X_, which corresponds to the maximum absolute row sum of _X_ . We also\ndefine **1** _∈_ R _[n]_ to be the vector of ones, such that _X_ **1** gives\nthe row sums of _X_ .\n\n\n**5.1. Convergence for Policy Evaluation**\n\n\nWe start in the off-policy policy evaluation setting. Specifically, our goal is to prove that the repeated application of\nthe _M_ operator to an arbitrarily initialized vector _Q ∈_ R _[n]_\n\nconverges to _Q_ _[π]_ .\n\n\n**Condition 5.1.** _βt ≤_ _βt−_ 1 _ρt, ∀Ft, ∀_ _t ≥_ 1 _._\n\n\n**Theorem 5.2.** _If Condition 5.1 holds, then M is a con-_\n_traction mapping with Q_ _[π]_ _as its unique fixed point. Conse-_\n_quently,_ lim _i→∞_ _M_ _[i]_ _Q_ = _Q_ _[π]_ _, ∀_ _Q ∈_ R _[n]_ _._\n\n\n_Proof._ In Lemma B.1 (Appendix B.1), we show that _Q_ _[π]_ is\na fixed point of _M_ and that\n\n\n_MQ −_ _Q_ _[π]_ = _Z_ ( _Q −_ _Q_ _[π]_ ) _,_ (9)\n\n\nwhere _Z_ := [�] _[∞]_ _t_ =1 _[γ][t]_ [(] _[B][t][−]_ [1] _[P][π][ −]_ _[B][t]_ [)][. In Lemma][ B.2][ (Ap-]\npendix B.2), we also show that _Z ≥_ 0 and _Z_ **1** _≤_ _γ_ using the\nassumption that _βt ≤_ _βt−_ 1 _ρt_, _∀Ft_, _∀_ _t ≥_ 1 (Condition 5.1).\nConsequently, _Z_ ( _Q −_ _Q_ _[π]_ ) is a vector whose components\n\n\n\nanalysis. To do this, let us first bring the expectation inside\nthe sum, by linearity of expectation:\n\n\n( _MQ_ )( _s, a_ ) = _Q_ ( _s, a_ ) +\n\n\n_∞_\n� _γ_ _[t]_ E _µ_ � _βtδt_ _[π]_ �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _._ (5)\n\n\n_t_ =0\n\n\nTo write Eq. (5) in vector form, we define an operator _Bt_\nsuch that, for an arbitrary vector _X_ in R _[n]_,\n\n\n( _BtX_ )( _s, a_ ):= E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _,_ (6)\n\n\nallowing us to express the _M_ operator as\n\n\n\n_MQ_ = _Q_ +\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _Q_ ) _._ (7)\n\n\n_t_ =0\n\n\n\n4\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\neach comprise a nonnegative-weighted combination of the\ncomponents of _Q −_ _Q_ _[π]_, where the weights add up to at\nmost _γ_ . This means _∥MQ −_ _Q_ _[π]_ _∥≤_ _γ∥Q −_ _Q_ _[π]_ _∥_, and _M_\nis a contraction mapping. Its fixed point, _Q_ _[π]_, must therefore\nbe unique by the Banach fixed-point theorem, implying that\nlim _i→∞_ _M_ _[i]_ _Q_ = _Q_ _[π]_ for every _Q ∈_ R _[n]_ when _γ <_ 1.\n\n\nGiven that the ratio _βt_\n_βt−_ 1 [is bounded by] _[ ρ][t]_ [ (Condition][ 5.1][),]\nthe _M_ operator converges to _Q_ _[π]_ . Intuitively, we can think\nof this ratio as the _effective_ per-decision factor at time _t_ ;\nconvergence is guaranteed whenever this factor is no greater\nthan _ρt_, analogous to the convergence result for the _R_ operator (Munos et al., 2016, Theorem 1). Our theorem implies\nthe existence of a space of convergent trajectory-aware\nalgorithms, because each trace _βt_ can be chosen arbitrarily\nso long as it always satisfies the bound on this ratio.\n\n\n**5.2. Convergence for Control**\n\n\nWe now consider the more challenging setting of control.\nGiven sequences of target policies ( _πi_ ) _i≥_ 0 and behavior\npolicies ( _µi_ ) _i≥_ 0, we aim to show that the sequence of value\nfunctions ( _Qi_ ) _i≥_ 0 given by _Qi_ +1 := _MiQi_ converges to\n_Q_ _[∗]_ . Here, _Mi_ is the _M_ operator defined for _πi_ and _µi_ .\n\n\nCompared to the convergence proof of the _R_ operator\n(Munos et al., 2016, Theorem 2), the main novelty of our\nproof is the fact that the traces under _M_ are not Markov.\nConsequently, we require new techniques to establish\nbounds on _Q −_ _Q_ _[∗]_, since Eq. (4) is not representable as\nan infinite geometric series and so the summation does not\nhave a closed-form expression. We additionally relax two\nassumptions in the previous work, on initialization of the\nvalue function and on increasing greediness of the policy.\nWe require only that the target policies become _greedy in_\n_the limit_ . We say that a sequence of policies is greedy in the\nlimit if _TπiQi →_ _TQi_ as _i →∞_ . We discuss the significance of these relaxations to the assumptions in Section 5.4.\n\n\nFirst, let _Ci_ := [�] _[∞]_ _t_ =0 _[γ][t][B][t]_ [ for the policies] _[ π][i]_ [ and] _[ µ][i]_ [, and]\nwrite the _M_ operator at iteration _i_ as\n\n\n_MiQ_ = _Q_ + _Ci_ ( _TπiQ −_ _Q_ ) _._ (10)\n\n\nWe now present our convergence theorem for control.\n\n\n**Theorem 5.3.** _Consider a sequence of target policies_\n( _πi_ ) _i≥_ 0 _and a sequence of arbitrary behavior policies_\n( _µi_ ) _i≥_ 0 _. Let Q_ 0 _be an arbitrary vector in_ R _[n]_ _and define_\n_the sequence Qi_ +1 := _MiQi, where Mi is the operator_\n_defined by Eq._ (10) _. Assume that_ ( _πi_ ) _i≥_ 0 _is greedy in the_\n_limit, and let ϵi ≥_ 0 _be the smallest constant such that_\n_TπiQi ≥_ _TQi −_ _ϵi∥Qi∥_ **1** _. If Condition 5.1 holds for all i,_\n_then_\n\n\n_ϵi_\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥_ + (11)\n1 _−_ _γ_ _[∥][Q][i][∥][,]_\n\n\n_and, consequently,_ lim\n_i→∞_ _[Q][i]_ [ =] _[ Q][∗][.]_\n\n\n\n_Proof (sketch; full proof in Appendix B.3)._ We define matrices _Zi_ and _Zi_ _[∗]_ [, which correspond to] _[ Z]_ [ in Eq. (][9][) for]\ntarget policies _πi_ and _π_ _[∗]_, respectively, and behavior policy\n_µi_ . We then derive the inequalities\n\n\n_Zi_ _[∗]_ [(] _[Q][i]_ _[−][Q][∗]_ [)] _[−][ϵ][i][∥][Q][i][∥][C][i]_ **[1]** _[ ≤M][i][Q][i]_ _[−][Q][∗]_ _[≤]_ _[Z][i]_ [(] _[Q][i]_ _[−][Q][∗]_ [)] _[,]_\n\n\nwhich together imply Eq. (11). Thus, _Mi_ is nearly a contraction mapping with _Q_ _[∗]_ as its unique fixed point, excepting\nthe influence of the _O_ ( _∥Qi∥_ ) term. However, the greedyin-the-limit target policies guarantee that _ϵi →_ 0. Showing that _∥Qi∥_ remains finite completes the proof because\n_∥Qi −_ _Q_ _[∗]_ _∥→_ 0 must follow.\n\n\nThe convergence criteria for _βt_ (Condition 5.1) is the same\nfor both policy evaluation and control. In fact, the only\nadditional assumption we need for control is the greedy-inthe-limit target policies. Crucially, the proof allows arbitrary\nbehavior policies and an arbitrary value function initialization _Q_ 0, which we further discuss in Section 5.4.\n\n\n**5.3. Examples of Convergence and Divergence**\n\n\nThe generality of the _M_ operator means that it provides\nconvergence guarantees for a number of credit-assignment\nmethods that we did not discuss in Section 4. These include\n\nvariable or past-dependent _λ_ -values (e.g., Watkins, 1989;\nSingh & Sutton, 1996; Yu et al., 2018). All of these can be\nrepresented in a common form and shown to satisfy Condition 5.1; convergence for policy evaluation and control for\nthe instantiated trajectory-aware operator follows as a corollary, since Condition 5.1 is sufficient to apply Theorems 5.2\nand 5.3.\n\n\n**Proposition 5.4.** _Any traces expressible in the form_\n_βt_ = [�] _[t]_ _k_ =1 _[λ]_ [(] _[F][k]_ [)] _[ρ][k][,][ λ]_ [(] _[F][k]_ [)] _[ ∈]_ [[0] _[,]_ [ 1]] _[, satisfy Condition][ 5.1][.]_\n\n\n_Proof. βt_ = _βt−_ 1 _λ_ ( _Ft_ ) _ρt ≤_ _βt−_ 1 _ρt_ .\n\n\nIn Section 4, we also discussed two existing trajectory-aware\nmethods whose convergence is unknown. We show that\nRecursive Retrace satisfies our required condition.\n\n\n**Proposition 5.5.** _Recursive Retrace satisfies Condition 5.1._\n\n\n_Proof._ For Recursive Retrace, _βt_ = _ctβt−_ 1, where _ct_ =\n_λ_ min� _βt_ 1 _−_ 1 _[, ρ][t]_ � (Munos et al., 2016, Eq. 9). This means\n\n\n\n1\n_βt_ = _λ_ min _, ρt_\n� _βt−_ 1\n\n\n\n_βt−_ 1\n�\n\n\n\n= _λ_ min (1 _, βt−_ 1 _ρt_ )\n\n\n_≤_ _βt−_ 1 _ρt,_ (12)\n\n\nwhich is the bound required by Condition 5.1.\n\n\nUnfortunately, the traces for Truncated IS do not always\nsatisfy the required bound.\n\n\n**Proposition 5.6.** _Truncated IS may violate Condition 5.1._\n\n\n\n5\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\n_Proof._ We show this by providing a counterexample. Recall\nthat Truncated IS has _βt_ = _λ_ _[t]_ min(1 _,_ Π _t_ ). Assume a trajectory _Ft_ such that Π _t−_ 1 = 2 and [1] 2 _[< ρ][t][ < λ]_ [. (It is straight-]\n\nforward to define an MDP, behavior policy, and target policy\nto create such a trajectory.) Because _ρt >_ 1 _/_ Π _t−_ 1, then\nΠ _t_ = Π _t−_ 1 _ρt >_ 1. Thus, _βt_ = _λ_ _[t]_ and _βt−_ 1 = _λ_ _[t][−]_ [1], and\nCondition 5.1 is violated because _βt_\n_βt−_ 1 [=] _[ λ][ ̸≤]_ _[ρ][t]_ [.]\n\n\nBecause Theorems 5.2 and 5.3 cannot be applied, the precise\nconditions under which Truncated IS converges remains an\nopen problem. We do know Condition 5.1 is sufficient for\nconvergence, but it is unlikely to be strictly necessary. This\nis because our proofs of Theorems 5.2 and 5.3 use this\nassumption to guarantee that the matrix _Z_ in Eq. (9) has\nnonnegative elements, making it straightforward to show\nthat its row sums are sufficiently bounded to guarantee that\n_M_ is a contraction mapping. However, _M_ could remain a\ncontraction mapping even when _Z_ has negative elements,\nso long as _∥Z∥_ _<_ 1. It could theoretically be the case for\nTruncated IS that _Z_ occasionally contains negative elements\nbut _∥Z∥_ is still bounded enough to permit convergence.\n\n\nNevertheless, we are able to find at least one off-policy\nproblem for which this is not true, implying that certain\ninitializations of the value function could ultimately cause\nTruncated IS to diverge.\n\n\n**Counterexample 5.7** (Off-Policy Truncated IS) **.** _Consider_\n_Truncated IS with λ_ = 1 _, so βt_ = min(1 _,_ Π _t_ ) _. Assume_\n_the MDP has one state and two actions: S_ = _{s} and_\n_A_ = _{a_ 1 _, a_ 2 _}, the behavior policy µ is uniform random,_\n_and π selects a_ 1 _with probability p ∈_ (0 _,_ 1) _and selects a_ 2\n_otherwise. When p_ = 0 _._ 6 _and γ_ = 0 _._ 94 _, then ∥Z∥_ _>_ 1 _._\n\n\nMany choices of _p_ and _γ_ make _∥Z∥_ _>_ 1, but we discuss\nspecific ones for the counterexample in Appendix C.1.\n\n\nCompared to the per-decision case, where Munos et al.\n(2016) showed that arbitrary trace cuts always produce a\nconvergent algorithm, this result is surprising. Why would\nthe analogous result—in which we ensure that _βt ≤_ Π _t_ for\nall timesteps—not hold here? After all, clipping _βt_ such\nthat it never exceeds the IS estimate Π _t_ would be expected\nto simply incur bias in the return estimation. For some insight, assume the following expectations are conditioned on\n( _S_ 0 _, A_ 0) = ( _s, a_ ), and observe that Eq. (4) is equivalent to\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n_∞_\n�\n� _t_ =1\n\n\n\n_._\n\n�\n\n\n\n�\n\n\n\nE _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_\n\n\n_t_ =0\n\n\n\n+ E _µ_\n\n\n\n� _γ_ _[t]_ ( _βt−_ 1 _ρt −_ _βt_ ) _Q_ ( _St, At_ )\n\n\n_t_ =1\n\n\n\n_._\n\n\n\nWe show the derivation in Appendix A. The first term is\na (partially) bias-corrected estimate of the discounted return. The second term is a weighted combination of valuefunction bootstraps, whose weights are nonnegative when\nCondition 5.1 is met. If the condition is violated on any\ntimestep, then we may actually be subtracting bootstraps\nfrom the return estimate, which does not seem sensible.\n\n\n\nWe believe this is related to the root cause of divergence\nin Counterexample 5.7; however, it remains open whether\nCondition 5.1 is necessary or merely sufficient.\n\n\nAs our next counterexample example will demonstrate, this\neffect can even cause divergence in _on-policy_ settings.\n\n\n**Counterexample 5.8** (On-Policy Binary Traces) **.** _Assume_\n_the MDP has one state and two actions: S_ = _{s} and_\n_A_ = _{a_ 1 _, a_ 2 _}. Define a trajectory-aware method such that_\n_βt_ = 1 _if At_ = _a_ 1 _and βt_ = 0 _if At_ = _a_ 2 _(without loss of_\n_generality). Assume π and µ are uniform random. When_\n_γ ≥_ [2] 3 _[, then][ ∥][Z][∥≥]_ [1] _[.]_\n\n\nWe provide details in Appendix C.2. Even though _βt ≤_\nΠ _t_ = 1 always, we are able to produce a non-contraction.\nThe method either fully cuts a trace or does not cut it at all,\nproducing backups that consist of a sparse sum of on-policy\nTD errors. It is therefore surprising that divergence occurs.\nFor the same reason we described above, the non-Markov\nnature of the trace appears to sometimes cause adverse bootstrapping effects; in this instance, the ability to examine each\ntrajectory allows the method to strategically de-emphasize\ncertain state-action pairs, ultimately producing a detrimental effect on learning. Notice that Condition 5.1 is indeed\nviolated in this case because there is always some chance\nthat _βt_ = 1 after _βt−_ 1 = 0. If we add the restriction that\n_βt−_ 1 = 0 = _⇒_ _βt_ = 0, i.e., we permanently cut the traces,\nthen convergence is reestablished by Theorem 5.2.\n\n\n**5.4. Discussion**\n\n\nIn this section, we summarize our main theoretical\ncontributions and their significance. We focused on\ncharacterizing the contraction properties of the _M_ operator,\nboth for policy evaluation and control, in the tabular setting.\nThese results parallel those for the _R_ operator underlying\nRetrace, where _M_ is a strict generalization of _R_ . These\nresults indicate that using fixed-point updates, like dynamic\nprogramming and temporal difference learning updates,\nmay have divergence issues. It does not, however, imply\nother algorithms, such as gradient-based algorithms, cannot\nfind these fixed points. We show the fixed points still exist\nand are unbiased, but that algorithms based on iterating\nwith the _M_ operator might diverge.\n\n\n**Removal of the Markov assumption.** Removing the\nMarkov (per-decision) assumption of the _R_ operator\n(Munos et al., 2016) to enable trajectory-aware eligibility\ntraces was our primary goal. When the trace factors are\nMarkov, the operator _Bt_ is independent of _t_, allowing the\nsum [�] _[∞]_ _t_ =0 _[γ][t][B][t]_ [ to be reduced to][ �] _[∞]_ _t_ =0 [(] _[γP][cµ]_ [)] _[t]_ [ for a linear]\noperator _Pcµ_ . The resulting geometric series can then be\nevaluated analytically, as was done by Munos et al. (2016).\nIn our proofs, we avoided the Markov assumption by directly analyzing the infinite summation, which generally\n\n\n\n6\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\ndoes not have a closed-form expression. Our work is the\nfirst to do this, establishing the first convergence guarantees\nfor general trajectory-aware methods.\n\n\n**Arbitrary initialization of the value function.** We permit\nany initialization of _Q_ 0 in the control setting. In contrast,\nMunos et al. (2016) made the assumption that _Tπ_ 0 _Q_ 0 _−_\n_Q_ 0 _≥_ 0 in order to produce a lower bound on _RiQi −_ _Q_ _[∗]_,\naccomplished in practice by a pessimistic initialization of\nthe value function: _Q_ 0( _s, a_ ) = _−∥R∥_ _/_ (1 _−_ _γ_ ), _∀_ ( _s, a_ ) _∈_\n_S × A_ . Since _R_ is a special case of our operator _M_ where\neach trace _βt_ factors into Markov coefficients, we deduce as\na corollary that Retrace and all other algorithms described by\n_R_ do not require pessimistic initialization for convergence.\n\n\n**Greedy-in-the-limit policies.** Our requirement of greedyin-the-limit target policies in Theorem 5.3 is less restrictive\nthan the increasingly greedy policies proposed by Munos\net al. (2016). We need only lim _i→∞_ _TπiQi_ = _TQi_, and\nwe do not force the sequence of target policies to satisfy\n_Pπi_ +1 _Qi_ +1 _≥_ _PπiQi_ +1. This implies that the agent may\ntarget non-greedy policies for any finite period of time, as\nlong as the policies do eventually become arbitrarily close\nto the greedy policy. As a corollary, increasingly greedy\npolicies are not necessary for the optimal convergence of\nRetrace and other per-decision methods.\n\n\n**6. Recency-Bounded Importance Sampling**\n\n\nTheorem 5.2 guarantees convergence to _Q_ _[π]_ whenever Condition 5.1 holds, but we do not expect that all choices of\ncoefficients that satisfy this condition will perform well\nin practice. At one extreme, if _βt ≤_ [�] _[t]_ _k_ =1 _[λ]_ [ min(1] _[, ρ][k]_ [)]\nfor every _Ft_, then we have a method that cuts coefficients\nmore aggressively than Retrace does; it seems unlikely that\nsuch a method would learn faster than Retrace, or other\nper-decision methods. At the other extreme, when _βt_ = Π _t_\nfor every _Ft_, we recover the standard IS estimator, which\nsuffers from high variance and is often ineffectual. We therefore know that it is possible to have a method that preserves\ntraces _too much_, to the point of being detrimental. Thus,\nit is important to maintain some minimum efficiency by\navoiding unnecessary cuts, yet equally important to control\nthe overall variance of the traces.\n\n\nIntuitively, we want something that falls between Retrace\nand IS in terms of trace cutting, in order to quickly backpropagate credit while still managing the variance. We further\nhypothesize that effective trajectory-aware methods will\nfirst compute _βt−_ 1 _ρt_ —i.e., the maximum trace permitted\nby Condition 5.1—and then apply some transformation that\nlimits its magnitude to reduce variance. This ensures that\ntraces are cut only as needed.\n\n\nWe propose one method, Recency-Bounded Importance\nSampling (RBIS), which achieves this by cutting the traces\n\n\n\nonly when they exceed an exponentially decaying threshold.\nSpecifically, we define\n\n\n_βt_ = min( _λ_ _[t]_ _, βt−_ 1 _ρt_ ) _._ (RBIS)\n\n\nIt is easy to see that RBIS always converges, by construction.\n\n\n**Proposition 6.1.** _RBIS satisfies Condition 5.1._\n\n\n_Proof. βt_ = min( _λ_ _[t]_ _, βt−_ 1 _ρt_ ) _≤_ _βt−_ 1 _ρt_ .\n\n\nFor further insight, we unroll the recursion to obtain\n\n\nmin( _λ_ _[t]_ _, βt−_ 1 _ρt_ )\n\n= min( _λ_ _[t]_ _,_ min( _λ_ _[t][−]_ [1] _, βt−_ 2 _ρt−_ 1) _ρt_ )\n\n\n_· · ·_\n\n\n= min( _λ_ _[t]_ _, λ_ _[t][−]_ [1] _ρt, λ_ _[t][−]_ [2] _ρt−_ 1 _ρt, . . .,_ Π _t_ ) _._ (13)\n\n\nRBIS effectively takes the minimum of all past, discounted\n_n_ -step IS estimates. This reveals another property of RBIS:\nits traces are never less than those of Retrace, because\n\n\n\nSince the inequality is true for all _j_, it is not possible for\nRetrace’s traces to exceed any of the arguments to the min\nfunction in Eq. (13). We have achieved exactly what we\nwanted earlier: a method that falls somewhere between\n\nRetrace and IS in regard to trace cutting. This does not automatically mean that RBIS will outperform Retrace, though,\nsince preserving the magnitude of the trace _βt_ too much\ncan lead to high variance. However, we do expect RBIS to\nperform well in decision-making problems in which a few\ncritical actions largely determine the long-term outcome\nof an episode. In such scenarios, the agent’s bottleneck to\nlearning is its ability to assign meaningful credit to these\ncritical actions over a potentially long time horizon.\n\n\nIn order to test this empirically, we construct an environment called the Bifurcated Gridworld (see Figure 2). This\n5 _×_ 5 deterministic gridworld has walls arranged such that\ntwo unequal-length paths from the start (S) to the goal (G)\nare available. The agent may move up, down, left, or right;\ntaking any of these actions in the goal yields a reward of\n+1 and terminates the episode. The problem is discounted\n( _γ_ = 0 _._ 9) to encourage the agent to learn the shorter path.\nImportantly, the action taken at the bifurcation (B) solely\ndetermines which path the agent follows, and quickly assigning credit to this state is paramount to learning the task.\n\n\nWe compare RBIS against Retrace, Truncated IS, and\nRecursive Retrace when learning this task from off-policy\n\n\n\n_t_\n� _λ_ min(1 _, ρk_ ) _≤_\n\n\n_k_ =1\n\n\n\n_t_\n� min( _λ, ρk_ )\n\n\n_k_ =1\n\n\n\n_t_\n_≤_ _λ_ _[t][−][j]_ � _ρk, ∀_ _j ∈{_ 0 _,_ 1 _, . . ., t}._\n\n_k_ = _t−j_ +1\n\n\n\n7\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\ndata. Both behavior and target policies were _ϵ_ -greedy with\nrespect to the value function _Q_ . The target policy used\n_ϵ_ = 0 _._ 1. The behavior policy used a piecewise schedule:\n_ϵ_ = 1 for the first 5 episodes and then _ϵ_ = 0 _._ 2 afterwards.\nThe agents learned from online TD updates with eligibility\ntraces (see Appendix D for pseudocode). The policies\nwere updated only at the end of each episode, and then\nthe discounted return obtained by a near-greedy policy\n( _ϵ_ = 0 _._ 05) was evaluated. The area under the curve (AUC)\nof each resulting learning curve was calculated, with the\nhighest AUC achieved over a grid search of stepsizes being\nplotted for each _λ_ -value in Figure 2. We averaged the\nresults over 1,000 independent trials and indicate the 95%\nconfidence interval by the shaded regions. In Appendix E,\nwe repeated the experiment for three more gridworld\ntopologies; the obtained results are qualitatively similar to\nFigure 2. Our experiment code is available online. [1]\n\n\nWe make several observations regarding the results in Figure 2. First, the peak performance obtained by RBIS is\nsignificantly higher than that of the other three methods.\nThis is notable because both Truncated IS and Recursive\n\nRetrace are also trajectory aware, indicating that different\nimplementations of trajectory awareness are beneficial to\nvarying degrees. In particular, the preservation of long-term\neligibilities is not sufficient on its own to guarantee strong\nperformance in general, as it appears that _when_ and _how_\n_much_ the traces are cut are important considerations as well.\nThe role of _λ_ as a decay hyperparameter is evidently critical for all methods to achieve their maximum performance,\nsince _λ_ = 1 never leads to the fastest learning. In fact,\n_λ →_ 1 is especially catastrophic for Truncated IS, which we\nbelieve is related to the divergence issue identified in Section 5.3. Finally, Retrace degrades less for larger _λ_, likely\nbecause it cuts traces more. It would be interesting to develop a trajectory-aware method that obtains the robustness\nof RBIS but also accounts for larger _λ_ -values.\n\n\n**7. Conclusion**\n\n\nIn this work, we extended theory for per-decision eligibility\ntraces to trajectory-aware traces. This extension allows\nus to consider a broader family of algorithms, with more\nflexibility in obtaining off-policy corrections. Specifically,\nwe introduced the _M_ operator as a generalization of the _R_\noperator, and a sufficient condition to ensure convergence\nunder _M_ . Using our general result, we established the\nfirst convergence guarantee for an existing trajectory-aware\nmethod, Recursive Retrace, in the control setting. We also\nshowed that Truncated IS may violate our condition and\nprovided a counterexample showing that it can diverge.\n\n\n[1https://github.com/brett-daley/](https://github.com/brett-daley/trajectory-aware-etraces)\n[trajectory-aware-etraces](https://github.com/brett-daley/trajectory-aware-etraces)\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n|||||**G**|\n||||||\n|**S**||**B**|||\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-7-0.png)\n\n_Figure 2._ The Bifurcated Gridworld environment. The choice\nmade at B greatly impacts the discounted return ultimately earned.\nWe plot the AUC obtained by four off-policy methods across the\n_λ_ -spectrum. The dashed horizontal lines mark the highest AUC\nachieved by each method.\n\n\nWe also proposed a new trajectory-aware method, RBIS,\nthat demonstrates one instance of how trajectory awareness\ncan be utilized for faster learning in off-policy control tasks.\nRBIS is able to outperform the other trajectory-aware methods that we tested in the Bifurcated Gridworld, suggesting\nthat it possesses at least one unique property that is beneficial for long-term, off-policy credit assignment. It would\nbe interesting to search for additional beneficial properties\nin future work, in order to better characterize off-policy\nmethods that reliably lead to efficient and stable learning in\nchallenging reinforcement learning environments.\n\n\nThis work focused on convergence _in expectation_ ; a natural next step is to extend this result to the stochastic algorithms used in practice. Previous results for TD learning\nrely primarily on the properties of the expected update, with\nadditional conditions on the noise in the update and appropriately annealed stepsizes (see Bertsekas & Tsitsiklis, 1996,\nSection 4.3). Similar analysis should be applicable, given\nthat we know the expected update with _M_ is a contraction\nmapping when Condition 5.1 is met.\n\n\nAn important next step is extending these methods and results to function approximation. Incorporating these traces\ninto deep reinforcement learning methods that rely on experience replay (Lin, 1992) should be straightforward. Multistep returns can be computed offline in the replay memory,\nand then randomly sampled in minibatches to train the neural network. Using a TD learning update, though, can suffer from convergence issues under function approximation\nand off-policy learning; this has been previously resolved\nby developing gradient-based updates (Sutton et al., 2009;\nTouati et al., 2018). An important next step is to develop a\ngradient-based trajectory-aware algorithm. The contraction\nproperties of the operator still impact the quality of the solution, as has been shown to be the case for other off-policy\napproaches (Patterson et al., 2022). The insights in this\nwork, therefore, may provide insights on how to get quality\nsolutions with gradient-based approaches.\n\n\n\n8\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\n**References**\n\n\nBarto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve difficult learning control\nproblems. _IEEE Transactions on Systems, Man, and Cyber-_\n_netics_, pp. 834–846, 1983.\n\n\nBellman, R. Dynamic programming. _Science_, 153(3731):\n34–37, 1966.\n\n\nBertsekas, D. P. and Tsitsiklis, J. N. _Neuro-Dynamic Program-_\n_ming_ . Athena Scientific, 1996.\n\n\nDaley, B. and Amato, C. Reconciling _λ_ -returns with experience replay. In _Advances in Neural Information Processing_\n_Systems_, pp. 1133–1142, 2019.\n\n\nHarutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos,\nR. Q( _λ_ ) with off-policy corrections. In _International Confer-_\n_ence on Algorithmic Learning Theory_, pp. 305–320, 2016.\n\n\nIonides, E. L. Truncated importance sampling. _Journal of Com-_\n_putational and Graphical Statistics_, 17(2):295–311, 2008.\n\n\nKahn, H. and Harris, T. E. Estimation of particle transmission\nby random sampling. _National Bureau of Standards: Applied_\n_Mathematics Series_, 12:27–30, 1951.\n\n\nKearns, M. J. and Singh, S. P. Bias-variance error bounds for\ntemporal difference updates. In _Conference on Learning_\n_Theory_, pp. 142–147, 2000.\n\n\nKozuno, T., Tang, Y., Rowland, M., Munos, R., Kapturowski,\nS., Dabney, W., Valko, M., and Abel, D. Revisiting Peng’s\nQ( _λ_ ) for modern reinforcement learning. _arXiv:2103.00107_,\n\n2021.\n\n\nLin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and reaching. _Machine Learning_, 8:\n293–321, 1992.\n\n\nMahmood, A. R., Yu, H., and Sutton, R. S. Multi-step\noff-policy learning without importance sampling ratios.\n_arXiv:1702.03006_, 2017.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare,\nM. G. Safe and efficient off-policy reinforcement learning. In\n_Advances in Neural Information Processing Systems_, 2016.\n\n\nPatterson, A., White, A., and White, M. A generalized projected Bellman error for off-policy value estimation in reinforcement learning. _Journal of Machine Learning Research_,\n\n2022.\n\n\nPeng, J. and Williams, R. J. Incremental multi-step Q-Learning.\n_Machine Learning_, 22:226–232, 1996.\n\n\nPrecup, D., Sutton, R. S., and Singh, S. Eligibility traces for\noff-policy policy evaluation. In _International Conference on_\n_Machine Learning_, pp. 759–766, 2000.\n\n\n\nRowland, M., Dabney, W., and Munos, R. Adaptive tradeoffs in off-policy learning. In _International Conference on_\n_Artificial Intelligence and Statistics_, pp. 34–44, 2020.\n\n\nRummery, G. A. and Niranjan, M. On-line Q-Learning using connectionist systems. Technical report, Cambridge\nUniversity, 1994.\n\n\nSingh, S. P. and Sutton, R. S. Reinforcement learning with\nreplacing eligibility traces. _Machine Learning_, 22:123–158,\n\n1996.\n\n\nSutton, R. S. _Temporal Credit Assignment in Reinforcement_\n_Learning_ . PhD thesis, University of Massachusetts Amherst,\n\n1984.\n\n\nSutton, R. S. Learning to predict by the methods of temporal\ndifferences. _Machine Learning_, 3(1):9–44, 1988.\n\n\nSutton, R. S. and Barto, A. G. _Reinforcement Learning: An_\n_Introduction_ . MIT Press, 1st edition, 1998.\n\n\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver,\nD., Szepesvari, C., and Wiewiora, E. Fast gradient-descent´\nmethods for temporal-difference learning with linear function approximation. In _International Conference on Machine_\n_Learning_, pp. 993–1000, 2009.\n\n\nTouati, A., Bacon, P.-L., Precup, D., and Vincent, P. Convergent\nTree Backup and Retrace with function approximation. In\n_International Conference on Machine Learning_, pp. 4955–\n4964, 2018.\n\n\nUchibe, E. and Doya, K. Competitive-cooperative-concurrent\nreinforcement learning with importance sampling. In _Inter-_\n_national Conference on Simulation of Adaptive Behavior:_\n_From Animals and Animats_, pp. 287–296, 2004.\n\n\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R.,\nKavukcuoglu, K., and de Freitas, N. Sample efficient actorcritic with experience replay. In _International Conference_\n_on Learning Representations_, 2017.\n\n\nWatkins, C. J. C. H. _Learning from Delayed Rewards_ . PhD\nthesis, King’s College, Cambridge, 1989.\n\n\nWawrzynski, P. Real-time reinforcement learning by sequential´\nactor-critics and experience replay. _Neural Networks_, 22\n(10):1484–1497, 2009.\n\n\nWawrzynski, P. and Pacut, A. Truncated importance sampling´\nfor reinforcement learning with experience replay. _Interna-_\n_tional Multiconference on Computer Science and Informa-_\n_tion Technology_, pp. 305–315, 2007.\n\n\nYu, H., Mahmood, A. R., and Sutton, R. S. On generalized Bellman equations and temporal-difference learning. _Journal of_\n_Machine Learning Research_, 19(1):1864–1912, 2018.\n\n\n\n9\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n**A.** _M_ **Operator Details**\n\n\nIn Section 5, we defined a linear operator _Bt_, where\n\n\n( _BtX_ )( _s, a_ ) = E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _,_ (6)\n\n\nsuch that the expected-value version of our _M_ operator,\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n( _MQ_ )( _s, a_ ) = _Q_ ( _s, a_ ) + E _µ_\n\n\n\n� _γ_ _[t]_ _βtδt_ _[π]_\n\n\n_t_ =0\n\n\n\n(4)\n\n\n\n= _Q_ ( _s, a_ ) +\n\n\nis element-wise equivalent to the vector version,\n\n\n\n_∞_\n� _γ_ _[t]_ E _µ_ � _βtδt_ _[π]_ �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _,_ (5)\n\n\n_t_ =0\n\n\n\n_MQ_ = _Q_ +\n\n\nWe claimed that each element of _Bt_ must have the form\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _Q_ ) _._ (7)\n\n\n_t_ =0\n\n\n\n_Bt_ (( _s, a_ ) _,_ ( _s_ _[′]_ _, a_ _[′]_ )) = Pr _µ_ [((] _[S][t][, A][t]_ [) = (] _[s][′][, a][′]_ [)] _[ |]_ [ (] _[S]_ [0] _[, A]_ [0][) = (] _[s, a]_ [))] _[ ×]_ [ E] _[µ]_ � _βt_ �� ( _S_ 0 _, A_ 0) = ( _s, a_ ) _,_ ( _St, At_ ) = ( _s′, a′_ )� _,_ (8)\n\n\nwith ( _s, a_ ) as the row index and ( _s_ _[′]_ _, a_ _[′]_ ) as the column index. This is because multiplying this matrix _Bt_ with a vector _X_\nresults in the same operation as the weighted expected value in Eq. (5):\n\n\n\n�\n\n\n\n�\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n� _Bt_ (( _s, a_ ) _,_ ( _s_ _[′]_ _, a_ _[′]_ )) _X_ ( _s_ _[′]_ _, a_ _[′]_ ) = E _µ_\n\n_s_ _[′]_ _,a_ _[′]_\n\n\n\nE _µ_ � _βt_ �� ( _S_ 0 _, A_ 0) = ( _s, a_ ) _,_ ( _St, At_ )� _· X_ ( _St, At_ )\n\n\n\n= E _µ_ E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ ) _,_ ( _St, At_ )� [�] ( _S_ 0 _, A_ 0) = ( _s, a_ )\n\n� ��� �\n\n\n\n= E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _._ (14)\n\n\nSo, when _X_ is the expected TD error _TπQ −_ _Q_, Eq. (7) becomes Eq. (5) exactly.\n\n\n_M_ is a contraction mapping whenever _βt ≤_ _βt−_ 1 _ρt_ for all _t_ (Condition 5.1), which Theorem 5.2 establishes. As we discussed\nin Section 5.3, violating this condition can sometimes cause _M_ to no longer contract, even with on-policy updates. We can see\none plausible reason for this by refactoring the definition of _M_ . Let _qt_ := _Q_ ( _St, At_ ) and _vt_ := [�] _a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [)] _[Q]_ [(] _[S][t][, a][′]_ [)][,]\n\nso _δt_ _[π]_ [=] _[ R][t]_ [+] _[ γv][t]_ [+1] _[−]_ _[q][t]_ [. Further, assume the following expectations are conditioned on][ (] _[S]_ [0] _[, A]_ [0][) = (] _[s, a]_ [)][. Eq. (][4][) is]\nequivalent to\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n�\n\n\n\n( _MQ_ )( _s, a_ ) = _q_ 0 + E _µ_\n\n\n\n� _γ_ _[t]_ _βt_ ( _Rt_ + _γvt_ +1 _−_ _qt_ )\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _βtqt_\n\n\n_t_ =0\n\n\n\n_∞_\n�\n\n\n\n�\n\n\n\n_∞_\n� _γ_ _[t]_ _βt−_ 1 _vt −_\n\n\n_t_ =1\n\n\n\n= _q_ 0 + E _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_ +\n\n\n_t_ =0\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n_∞_\n�\n� _t_ =0\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n�\n\n�\n\n\n\n_∞_\n� _γ_ _[t]_ _βtqt_\n\n\n_t_ =1\n\n\n\n_∞_\n�\n\n\n\n�\n\n\n\n_∞_\n� _γ_ _[t]_ _βt−_ 1 _vt −_\n\n\n_t_ =1\n\n\n\n_∞_\n�\n\n\n\n= E _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_ +\n\n\n_t_ =0\n\n\n\n�\n\n�\n\n\n\n_∞_\n�\n� _t_ =1\n\n\n_∞_\n�\n� _t_ =1\n\n\n\n� _γ_ _[t]_ ( _βt−_ 1 _vt −_ _βtqt_ )\n\n\n_t_ =1\n\n\n\n= E _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_\n\n\n_t_ =0\n\n\n\n+ E _µ_\n\n\n\n= E _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_\n\n\n_t_ =0\n\n\n\n+ E _µ_\n\n\n\n� _γ_ _[t]_ ( _βt−_ 1 _ρt −_ _βt_ ) _qt_\n\n\n_t_ =1\n\n\n\n_,_ (15)\n\n\n\nand we discussed in Section 5.3 that these two terms represent a biased return estimate and an infinite sum of weighted\nvalue-function bootstraps, respectively. In particular, this can be problematic if _βt > βt−_ 1 _ρt_ because the corresponding\nbootstrap’s weight becomes negative, causing it to get subtracted from the return estimate.\n\n\n10\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n**B. Additional Proofs**\n\n\n**B.1. Proof of Lemma B.1**\n\n\n**Lemma B.1.** _Q_ _[π]_ _is a fixed point of M; the difference between MQ and Q_ _[π]_ _is given by_\n\n\n_MQ −_ _Q_ _[π]_ = _Z_ ( _Q −_ _Q_ _[π]_ ) _,_ (16)\n\n\n_where Z_ := [�] _[∞]_ _t_ =1 _[γ][t]_ [(] _[B][t][−]_ [1] _[P][π][ −]_ _[B][t]_ [)] _[.]_\n\n\n_Proof._ It is evident from Eq. (7) that _Q_ _[π]_ is a fixed point of _M_ because _TπQ_ _[π]_ _−_ _Q_ _[π]_ = 0, and so _MQ_ _[π]_ = _Q_ _[π]_ . Therefore,\n\n\n_MQ −_ _Q_ _[π]_ = _MQ −MQ_ _[π]_\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ_ _[π]_ _−_ _Q_ _[π]_ )\n\n\n_t_ =0\n\n\n\n= _Q_ +\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _Q_ ) _−_ _Q_ _[π]_ _−_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _Q −_ _Q_ _[π]_ )\n\n\n_t_ =0\n\n\n\n= _Q −_ _Q_ _[π]_ +\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _TπQ_ _[π]_ ) _−_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _Q −_ _Q_ _[π]_ )\n\n\n_t_ =1\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _Q −_ _Q_ _[π]_ )\n\n\n_t_ =1\n\n\n\n=\n\n\n=\n\n\n=\n\n\n=\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _TπQ_ _[π]_ ) _−_\n\n\n_t_ =0\n\n\n_∞_\n� _γ_ _[t]_ [+1] _BtPπ_ ( _Q −_ _Q_ _[π]_ ) _−_\n\n\n_t_ =0\n\n\n\n_∞_\n�\n� _t_ =1\n\n\n\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπ −_ _Bt_ )\n\n\n_t_ =1\n\n\n\n( _Q −_ _Q_ _[π]_ )\n\n\n\n_∞_\n� _γ_ _[t]_ [+1] _BtPπ −_\n� _t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_\n\n\n_t_ =1\n\n\n\n�\n\n\n\n( _Q −_ _Q_ _[π]_ )\n\n\n\n�\n\n\n\n= _Z_ ( _Q −_ _Q_ _[π]_ ) _,_\n\n\nwhich is the desired result.\n\n\n**B.2. Proof of Lemma B.2**\n\n\n**Lemma B.2.** _If Condition 5.1 holds, then Z has nonnegative elements and its row sums obey Z_ **1** _≤_ _γ._\n\n\n_Proof._ Define the linear operator _Dt_ := _Bt−_ 1 _Pπ −_ _Bt_ and notice that _Z_ = [�] _[∞]_ _t_ =1 _[γ][t][D][t]_ [. We will show that] _[ D][t]_ [ comprises]\nonly nonnegative elements, and therefore so does _Z_ . For any _X ∈_ R _[n]_, observe that\n\n\n\n( _DtX_ )( _s, a_ ) = E _µ_\n\n�\n\n\n\n_βt−_ 1 �\n\n_St∈S_\n\n\n\n� _P_ ( _St|Ft−_ 1) _π_ ( _At|St_ ) _X_ ( _St, At_ )\n\n_At∈A_\n\n\n\n_−_ E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ )�\n\n\n\n= E _µ_\n\n�\n\n\n\n_βt−_ 1 �\n\n_St∈S_\n\n\n\n� _P_ ( _St|Ft−_ 1) _π_ ( _At|St_ ) _X_ ( _St, At_ )\n\n_At∈A_\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n�� _St∈S_\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n_−_ E _µ_\n\n\n\n_St∈S_\n\n\n\n� _P_ ( _St|Ft−_ 1) _µ_ ( _At|St_ ) _βtX_ ( _St, At_ )\n\n_At∈A_\n\n\n\n�� _St∈S_\n\n\n\n_At∈A_\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n= E _µ_\n\n\n\n_P_ ( _St|Ft−_ 1) �\n_St∈S_ _At_\n\n\n\n� _π_ ( _At|St_ ) _βt−_ 1 _−_ _µ_ ( _At|St_ ) _βt_ � _X_ ( _St, At_ )\n\n\n\n_._ (17)\n\n\n\nSince we assumed that _βt ≤_ _βt−_ 1 _ρt_ in Condition 5.1, we have _π_ ( _At|St_ ) _βt−_ 1 _−_ _µ_ ( _At|St_ ) _βt ≥_ 0, which implies that _Dt ≥_ 0.\nFurthermore, this holds for all _t ≥_ 1, so _Z ≥_ 0 follows immediately.\n\n\n11\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\nTo complete the proof, we show that the row sums of _Z_ are bounded by _γ_ . Recall that _Pπ_ **1** = **1** . Hence,\n\n\n\n_Z_ **1** =\n\n\n=\n\n\n=\n\n\n\n_∞_\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπ −_ _Bt_ ) **1**\n\n\n_t_ =1\n\n\n_∞_\n� _γ_ _[t]_ ( _Bt−_ 1 **1** _−_ _Bt_ **1** )\n\n\n_t_ =1\n\n\n\n_∞_\n� _γ_ _[t]_ [+1] _Bt_ **1** _−_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ **1**\n\n\n_t_ =1\n\n\n\n_∞_\n\n= _γ_ **1** + � _γ_ _[t]_ [+1] _Bt_ **1** _−_\n\n\n_t_ =1\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ **1**\n\n\n_t_ =1\n\n\n\n= _γ_ **1** _−_ (1 _−_ _γ_ )\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ **1**\n\n\n_t_ =1\n\n\n\n_≤_ _γ_ **1** _,_ (18)\n\n\nbecause _Bt ≥_ 0, _∀_ _t ≥_ 1.\n\n\n**B.3. Proof of Theorem 5.3**\n\n\n**Theorem 5.3.** _Consider a sequence of target policies_ ( _πi_ ) _i≥_ 0 _and a sequence of arbitrary behavior policies_ ( _µi_ ) _i≥_ 0 _. Let_\n_Q_ 0 _be an arbitrary vector in_ R _[n]_ _and define the sequence Qi_ +1 := _MiQi, where Mi is the operator defined by Eq._ (10) _._\n_Assume that_ ( _πi_ ) _i≥_ 0 _is greedy in the limit, and let ϵi ≥_ 0 _be the smallest constant such that TπiQi ≥_ _TQi −_ _ϵi∥Qi∥_ **1** _. If_\n_Condition 5.1 holds for all i, then_\n\n\n_ϵi_\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥_ + (11)\n1 _−_ _γ_ _[∥][Q][i][∥][,]_\n\n\n_and, consequently,_ lim\n_i→∞_ _[Q][i]_ [ =] _[ Q][∗][.]_\n\n\n_Proof._ We first derive the following upper bound:\n\n\n_TπiQi −_ _TQ_ _[∗]_ = _γPπiQi −_ _γ_ max _PπQ_ _[∗]_ _≤_ _γPπi_ ( _Qi −_ _Q_ _[∗]_ ) _._ (19)\n_π_\n\n\nFrom Eq. (10) and because _Ci_ has nonnegative entries, we can deduce that\n\n\n_MiQi −_ _Q_ _[∗]_ = ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _Ci_ ( _TπiQi −_ _Q_ _[∗]_ ) (20)\n\n= ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _Ci_ ( _TπiQi −_ _TQ_ _[∗]_ )\n\n_≤_ ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _γCiPπi_ ( _Qi −_ _Q_ _[∗]_ )\n\n= _Zi_ ( _Qi −_ _Q_ _[∗]_ ) _,_ (21)\n\n\nwhere _Zi_ := _I −_ _Ci_ ( _I −_ _γPπi_ ). Notice that _Zi_ is analogous to the matrix _Z_ in Eq. (9) because, for policies _πi_ and _µi_,\n\n\n\n_I −_ _Ci_ ( _I −_ _γPπi_ ) = _I_ +\n\n\n= _I_ +\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _γPπi −_ _I_ )\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ [+1] _BtPπi −_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_\n\n\n_t_ =1\n\n\n\n=\n\n\n=\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt−_ 1 _Pπi −_\n\n\n_t_ =1\n\n\n\n_∞_\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπi −_ _Bt_ ) _._ (22)\n\n\n_t_ =1\n\n\n12\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\nNext, we derive the following lower bound:\n\n\n_TQi −_ _TQ_ _[∗]_ _≥_ _Tπ∗_ _Qi −_ _TQ_ _[∗]_ = _γPπ∗_ ( _Qi −_ _Q_ _[∗]_ ) _._ (23)\n\n\nAdditionally, for each policy _πi_, there exists some _ϵi ≥_ 0 such that _TπiQi ≥_ _TQi −_ _ϵi∥Qi∥_ **1** (recall that we defined _ϵi_ to be\nas small as possible). Starting again from Eq. (20), and noting that the elements of _Ci_ are nonnegative, we obtain\n\n\n_MiQi −_ _Q_ _[∗]_ _≥_ ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _Ci_ ( _TQi −_ _Q_ _[∗]_ ) _−_ _ϵi∥Qi∥Ci_ **1**\n\n\n= ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _Ci_ ( _TQi −_ _TQ_ _[∗]_ ) _−_ _ϵi∥Qi∥Ci_ **1**\n\n\n_≥_ ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _γCiPπ∗_ ( _Qi −_ _Q_ _[∗]_ ) _−_ _ϵi∥Qi∥Ci_ **1**\n\n= _Zi_ _[∗]_ [(] _[Q][i]_ _[−]_ _[Q][∗]_ [)] _[ −]_ _[ϵ][i][∥][Q][i][∥][C][i]_ **[1]** _[,]_ (24)\n\n\nwhere we have defined _Zi_ _[∗]_ [:=] _[ I][ −]_ _[C][i]_ [(] _[I][ −]_ _[γP][π][∗]_ [)][. By Lemma][ B.2][, since we assumed Condition][ 5.1][ holds, both] _[ Z][i]_ [ and] _[ Z]_ _i_ _[∗]_\nhave nonnegative elements and their row sums are bounded by _γ_ . Therefore, when _MiQi −_ _Q_ _[∗]_ _≥_ 0, Eq. (21) implies\n\n\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥,_ (25)\n\n\nbecause element-wise inequality for nonnegative matrices implies the inequality holds also for their norms. When\n_MiQi −_ _Q_ _[∗]_ _≤_ 0, we must use Eq. (24) and multiply both sides by _−_ 1 to get nonnegative matrices, giving\n\n\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥_ + _ϵi∥Qi∥∥Ci∥_\n\n\n_ϵi_\n_≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥_ + (26)\n1 _−_ _γ_ _[∥][Q][i][∥][,]_\n\n\nbecause _∥Ci∥≤_ [�] _[∞]_ _t_ =0 _[γ][t][∥][P][π]_ _i_ _[∥][t]_ [ = (1] _[ −]_ _[γ]_ [)] _[−]_ [1][. Since Eq. (][26][) is looser than Eq. (][25][), its bound holds in the worst case. It]\nremains to show that this bound implies convergence to _Q_ _[∗]_ . Observe that\n\n\n_ϵi_ _ϵi_\n_γ∥Qi −_ _Q_ _[∗]_ _∥_ +\n1 _−_ _γ_ _[∥][Q][i][∥≤]_ _[γ][∥][Q][i][ −]_ _[Q][∗][∥]_ [+] 1 _−_ _γ_ [(] _[∥][Q][i][ −]_ _[Q][∗][∥]_ [+] _[ ∥][Q][∗][∥]_ [)]\n\n\n\n_ϵi_\n= _γ_ +\n� 1 _−_ _γ_\n\n\n\n_ϵi_\n_∥Qi −_ _Q_ _[∗]_ _∥_ + (27)\n� 1 _−_ _γ_ _[∥][Q][∗][∥][.]_\n\n\n\nOur assumption of greedy-in-the-limit policies tells us that _ϵi →_ 0 as _i →∞_ ; thus, there must exist some iteration _i_ _[∗]_ such\nthat _ϵi ≤_ [1] 2 [(1] _[ −]_ _[γ]_ [)][2][,] _[ ∀]_ _[i][ ≥]_ _[i][∗]_ [. Therefore, for] _[ i][ ≥]_ _[i][∗]_ [,]\n\n\n\n\n_[γ]_ _ϵi_\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ [1 +] _∥Qi −_ _Q_ _[∗]_ _∥_ + (28)\n\n2 1 _−_ _γ_ _[∥][Q][∗][∥][.]_\n\n\n\nIf _γ <_ 1, then 2 [1] [(1 +] _[ γ]_ [)] _[ <]_ [ 1][, and since] _[ ∥][Q][∗][∥]_ [is finite, we conclude that] _[ ∥][Q][i][ −]_ _[Q][∗][∥→]_ [0][ as] _[ i][ →∞]_ [.]\n\n\n**C. Examples of Divergence**\n\n\n**C.1. Counterexample 5.7: Off-Policy Truncated IS**\n\n\nOur definitions of _π_ and _µ_ give us\n\n\n\n_p_ 1 _−_ _p_\n_Pπ_ = � _p_ 1 _−_ _p_\n\n\n\n� _,_ _Pµ_ = 2 [1]\n\n\n\n1 1\n\n_._ (29)\n\n1 1\n� �\n\n\n\nRecall that we assumed _λ_ = 1. We define the following constant, using the definition of _βt_ for Truncated IS:\n\n\n_βt_ [(1)] := E� _βt_ �� ( _St, At_ ) = ( _s, a_ 1)�\n\n\n\n� _Ft_ Pr _µ_ ( _Ft |_ ( _St, At_ ) = ( _s, a_ 1)) _·_ min �1 _,_ [Pr] Pr _[π]_ _µ_ ( [(] _F_ _[F]_ _t_ _[t]_ ) [)]\n\n\n\n=\n�\n\n\n\nPr _µ_ ( _Ft_ )\n\n\n\n�\n\n\n\n\n_[p]_\n_F_ � _t−_ 1 Pr _µ_ ( _Ft−_ 1) min �1 _,_ Pr [Pr] _µ_ _[π]_ ( [(] _F_ _[F]_ _t_ _[t]_ _−_ _[−]_ 1 [1] ) [)] _·_ _[ ·]_ [1] 2\n\n\n\n=\n�\n\n\n\nPr _µ_ ( _Ft−_ 1) _·_ [1] 2\n\n\n\n2\n\n\n\n(30)\n�\n\n\n\n13\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n= � min (Pr _µ_ ( _Ft−_ 1) _,_ 2 _p ·_ Pr _π_ ( _Ft−_ 1))\n\n_Ft−_ 1\n\n\n\n1\n\n= _F_ � _t−_ 1 min � 2 _[t][−]_ [1] _[,]_ [ 2] _[p][ ·]_ [ Pr] _[π]_ [(] _[F][t][−]_ [1][)] � _._ (31)\n\n\n\nEq. (30) is justified because the conditional probability of a trajectory ending in action _a_ 1 is just the probability of _Ft−_ 1\nunder _µ_, due to the 1-state (memoryless) MDP. We can simplify _βt_ [(1)] further by using the binomial theorem to calculate\n_t−_ 1\nPr _π_ ( _Ft−_ 1) = _p_ _[k]_ (1 _−_ _p_ ) _[t][−]_ [1] _[−][k]_, where _k ∈_ [0 _, t −_ 1] is the number of times _a_ 1 is taken in _Ft−_ 1. There are � _k_ � trajectories\nwith this same probability. Therefore,\n\n\n\n1\n\n_βt_ [(1)] = _F_ � _t−_ 1 min � 2 _[t][−]_ [1] _[,]_ [ 2] _[p][ ·]_ [ Pr] _[π]_ [(] _[F][t][−]_ [1][)] � =\n\n\n\n_t−_ 1\n�\n\n\n_k_ =0\n\n\n\n_t −_ 1 1\n� _k_ � min � 2 _[t][−]_ [1] _[,]_ [ 2] _[p][ ·][ p][k]_ [(1] _[ −]_ _[p]_ [)] _[t][−]_ [1] _[−][k]_ � _._ (32)\n\n\n\nLikewise, we can compute _βt_ [(2)] by swapping _p_ and 1 _−_ _p_ above. Let _⊙_ denote element-wise multiplication. Using the fact\nthat _Pµ_ _[t]_ [=] _[ P][µ]_ [,] _[ ∀]_ _[t][ ≥]_ [1][, it follows that]\n\n\n\n_._ (33)\n\n�\n\n\n\n_βt_ [(1)] _βt_ [(2)]\n� _βt_ [(1)] _βt_ [(2)]\n\n\n\n_Bt_ = _Pµ_ _[t]_ _[⊙]_\n\n\n\n_βt_ [(1)] _βt_ [(2)]\n� _βt_ [(1)] _βt_ [(2)] �\n\n\n\n= [1]\n\n2\n\n\n\nUsing a computer program to calculate _Z_, assuming that _p_ = 0 _._ 6 and _γ_ = 0 _._ 94, we obtain\n\n\n\n_Z_ =\n\n\n\n_∞_\n\n0 _._ 704 _−_ 0 _._ 436\n\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπ −_ _Bt_ ) _≈_ 0 _._ 704 _−_ 0 _._ 436 _._ (34)\n\n_t_ =1 � �\n\n\n\nTherefore, _∥Z∥≈_ 1 _._ 14, which is not a contraction, and the norm continues to increase for _p >_ 0 _._ 6 or _γ >_ 0 _._ 94.\n\n\n**C.2. Counterexample 5.8: On-Policy Binary Traces**\n\n\nThe policy _π_ is uniform random, so we have\n\n\n\n_._ (35)\n�\n\n\n\n_Pπ_ = [1] 2\n\n\n\n1 1\n\n1 1\n�\n\n\n\nLet _⊙_ denote element-wise multiplication. Because _βt_ = 1 only when the trajectory _Ft_ terminates in ( _s, a_ 1) and _βt_ = 0\notherwise, and since _Pπ_ _[t]_ [=] _[ P][π]_ [,] _[ ∀]_ _[t][ ≥]_ [1][, we also have]\n\n\n\n1 0\n_Bt_ = _Pπ_ _[t]_ _[⊙]_ 1 0\n�\n\n\n\n= [1]\n� 2\n\n\n\n_._ (36)\n�\n\n\n\n2\n\n\n\n1 0\n\n1 0\n�\n\n\n\nUsing a computer program to calculate _Z_, assuming that _γ_ = 3 [2] [, we obtain]\n\n\n\n3\n\n\n\n_._ (37)\n�\n\n\n\n_Z_ =\n\n\n\n_∞_\n�\n\n\n\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπ −_ _Bt_ ) = 3 [1]\n\n_t_ =1\n\n\n\n_−_ 1 2\n\n_−_ 1 2\n�\n\n\n\nTherefore, _∥Z∥_ = 1, which is not a contraction, and the norm continues to increase for _γ >_ [2] 3 [.]\n\n\n**D. Implementation of Trajectory-Aware Eligibility Traces**\n\n\nThe implementation of trajectory-aware methods is closely related to that of backward-view TD( _λ_ ) in the tabular setting\n(see, e.g., Sutton & Barto, 1998, Chapter 7.3). On each timestep, an environment interaction is conducted according to the\nbehavior policy _µ_ . Then, the eligibilities for previously visited state-action pairs are modified, the eligibility for the current\nstate-action pair is incremented, and the current TD error is applied to all state-action pairs in proportion to their eligibilities.\nThe only difference in the trajectory-aware case is that the eligibilities are not modified by simply multiplying a constant\ndecay factor _γλ_ .\n\n\n14\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\nArbitrary, trajectory-dependent traces _β_ ( _Ft_ ), as studied in our theoretical results, can be complicated to implement. This\nstems from the fact that the timestep _t_ in the _M_ operator is defined _relative_ to when the updated state-action pair was\ntaken. In other words, each state-action pair ( _Sk, Ak_ ) “disagrees” on the start of the current trajectory, generating its update\nfrom the unique sub-trajectory ( _Sk, Ak_ ) _, . . .,_ ( _St, At_ ). Implementing coefficients of this form would be possible using the\ngeneral update\n_Q_ ( _Sk, Ak_ ) _←_ _Q_ ( _Sk, Ak_ ) + _αγ_ _[t][−][k]_ _β_ (( _Sk, Ak_ ) _, . . .,_ ( _St, At_ )) _δt_ _[π][,]_ (38)\n\n\nwhere _α ∈_ (0 _,_ 1] is the stepsize, but this would require repeatedly slicing the list of visited state-action pairs\n( _S_ 0 _, A_ 0) _, . . .,_ ( _St, At_ ). While this is certainly feasible, it does not easily accommodate vectorization or parallelization.\n\n\nFortunately, this level of generality is rarely needed in practice, and specific optimizations can be made depending on the\nfunctional form of _β_ . For example, Truncated IS defines _β_ to be a pure function of the IS estimate Π _t_, which is useful\nbecause per-decision eligibility traces can be used to efficiently generate the IS estimates for every state-action pair visited\nduring the episode. We demonstrate how this can be done in pseudocode (see Algorithm 1).\n\n\nRecursive methods like Recursive Retrace and RBIS, where _βt_ explicitly depends on _βt−_ 1, require only two minor\nchanges compared to Algorithm 1 for their implementations. These changes, which we highlight in blue for RBIS in\nAlgorithm 2, correspond to the fact that the dynamic array _Y_ is now used to store the previous trace _βt−_ 1 rather than the\nprevious IS estimate Π _t−_ 1 at each timestep. The computational requirements for the methods remain nearly identical. The\nimplementation for Recursive Retrace easily follows by changing line 10 of Algorithm 2 to\n\n\n_Y_ ( _k_ ) _←_ _λ_ min(1 _, Y_ ( _k_ ) _· ρt_ ) _._ (39)\n\n\n**E. Additional Experiment Details and Results**\n\n\nWe conducted a grid search to find the best stepsize _α_ for every _λ_ -value for the four off-policy methods we evaluated\nin the Bifurcated Gridworld (Section 6). Using a training set of 1,000 trials, we searched over _λ ∈{_ 0 _,_ 0 _._ 1 _, . . .,_ 1 _}_ and\n_α ∈{_ 0 _._ 1 _,_ 0 _._ 3 _,_ 0 _._ 5 _,_ 0 _._ 7 _,_ 0 _._ 9 _}_, for a total of 55 hyperparameter combinations. At the start of each trial, the initial value\nfunction _Q_ was sampled from a zero-mean Gaussian distribution with standard deviation _σ_ = 0 _._ 01. We trained each agent\nfor 3,000 timesteps, allowing extra time to complete the final episode. We then generated learning curves by plotting the\n100-episode moving average of these returns as a function of the number of timesteps and calculated their AUCs. In Table 1,\nwe report the stepsize _α_ that led to the highest average AUC for each _λ_ -value. Then, using a separate test set of 1,000 trials\nto avoid bias in the search results, these _α_ -values were used to generate the learning curves in Figure 3. The AUCs for these\nlearning curves were finally used in the creation of the _λ_ -sweep plot (Figure 2).\n\n\nIn Figure 4, we repeated this procedure for three additional, more complex gridworld topologies. Like the Bifurcated\nGridworld, these environments feature one or more bifurcations that make fast credit assignment imperative, as well as\nadditional challenges such as multiple goal cells. As before, the agent starts in S and receives +1 reward for taking any\naction in a goal, terminating the episode. The results are qualitatively similar to Figure 2; RBIS outperforms the other three\nmethods by a significant margin over the left-hand portion of the _λ_ -spectrum, and performs similarly to Retrace as _λ →_ 1.\n\n\n_Table 1._ The best stepsizes found by our grid search in the Bifurcated Gridworld.\n\n\n_λ_ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n\n\nRetrace 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.7 0.7 0.5\n\nTruncated IS 0.9 0.9 0.9 0.9 0.9 0.9 0.7 0.5 0.5 0.5 0.3\n\nRecursive Retrace 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.7 0.5 0.5\n\nRBIS 0.9 0.9 0.9 0.9 0.9 0.7 0.7 0.7 0.7 0.7 0.5\n\n\n15\n\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-6.png)\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-4.png)\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-5.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-7.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-8.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-10.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-11.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-17.png)\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-9.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-12.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-18.png)\n\n\n\n_Figure 3._ Learning curves for the _λ_ -values we tested in the Bifurcated Gridworld environment. The dashed black line indicates the\noptimal discounted return for this problem.\n\n16\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n|Col1|Col2|G|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||**S**||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||**G**||||\n||||||||\n||||||||\n|**S**|||||||\n\n\n|G|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|**G**||||||\n|**G**||||||\n|||||||\n|||**S**||||\n|||||||\n\n\n\n_Figure 4. λ_ -sweeps conducted on three additional gridworld topologies. The experiment procedure was identical to that used in the\ncreation of Figure 2.\n\n\n17\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-16-3.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-16-7.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-16-11.png)\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n**Algorithm 1** Truncated Importance Sampling\n\n1: **Input:** value function _Q_, stepsize _α ∈_ (0 _,_ 1]\n\n\n2: **for** each episode **do**\n\n\n3: Reset environment and observe state _S_ 0\n\n4: Reset dynamic array _Y_\n\n5: **repeat** _{_ for _t_ = 0 _,_ 1 _,_ 2 _, . . . }_\n\n6: Take action _At ∼_ _µ_ ( _·|St_ ), receive reward _Rt_, and observe next state _St_ +1\n7: _ρt_ = _[π]_ _µ_ ( [(] _A_ _[A]_ _t_ _[t]_ _|_ _[|]_ _S_ _[S]_ _t_ _[t]_ ) [)]\n\n\n\n_a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [+1][)] _[Q]_ [(] _[S][t]_ [+1] _[, a][′]_ [)] else\n\n\n\n8: _δt_ =\n\n\n\n\n\n\n\n\n_Rt −_ _Q_ ( _St, At_ ) if _St_ +1 is terminal\n\n _Rt −_ _Q_ ( _St, At_ ) + _γ_ [�] _a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [+1][)] _[Q]_ [(] _[S][t]_ [+1] _[, a][′]_ [)] else\n\n\n\n_Rt −_ _Q_ ( _St, At_ ) + _γ_ [�]\n\n\n\n9: **for** _k_ = 0 _, . . ., t −_ 1 **do**\n\n10: _Y_ ( _k_ ) _←_ _Y_ ( _k_ ) _· ρt_\n\n11: **end for**\n\n\n12: _Y_ ( _t_ ) _←_ 1\n\n\n13: **for** _k_ = 0 _, . . ., t_ **do**\n\n14: _z ←_ ( _γλ_ ) _[t][−][k]_ min(1 _, Y_ ( _k_ ))\n\n15: _Q_ ( _Sk, Ak_ ) _←_ _Q_ ( _Sk, Ak_ ) + _αzδt_\n\n16: **end for**\n\n\n17: **until** _St_ +1 is terminal\n\n\n18: **end for**\n\n\n**Algorithm 2** Recency-Bounded Importance Sampling (RBIS)\n\n1: **Input:** value function _Q_, stepsize _α ∈_ (0 _,_ 1]\n\n\n2: **for** each episode **do**\n\n\n3: Reset environment and observe state _S_ 0\n\n4: Reset dynamic array _Y_\n\n5: **repeat** _{_ for _t_ = 0 _,_ 1 _,_ 2 _, . . . }_\n\n6: Take action _At ∼_ _µ_ ( _·|St_ ), receive reward _Rt_, and observe next state _St_ +1\n7: _ρt_ = _[π]_ _µ_ ( [(] _A_ _[A]_ _t_ _[t]_ _|_ _[|]_ _S_ _[S]_ _t_ _[t]_ ) [)]\n\n\n\n_a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [+1][)] _[Q]_ [(] _[S][t]_ [+1] _[, a][′]_ [)] else\n\n\n\n8: _δt_ =\n\n\n\n\n\n\n\n\n_Rt −_ _Q_ ( _St, At_ ) if _St_ +1 is terminal\n\n _Rt −_ _Q_ ( _St, At_ ) + _γ_ [�] _a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [+1][)] _[Q]_ [(] _[S][t]_ [+1] _[, a][′]_ [)] else\n\n\n\n_Rt −_ _Q_ ( _St, At_ ) + _γ_ [�]\n\n\n\n9: **for** _k_ = 0 _, . . ., t −_ 1 **do**\n\n10: _Y_ ( _k_ ) _←_ min( _λ_ _[t][−][k]_ _, Y_ ( _k_ ) _· ρt_ )\n\n\n11: **end for**\n\n\n12: _Y_ ( _t_ ) _←_ 1\n\n\n13: **for** _k_ = 0 _, . . ., t_ **do**\n\n14: _z ←_ _γ_ _[t][−][k]_ _Y_ ( _k_ )\n\n15: _Q_ ( _Sk, Ak_ ) _←_ _Q_ ( _Sk, Ak_ ) + _αzδt_\n\n16: **end for**\n\n\n17: **until** _St_ +1 is terminal\n\n\n18: **end for**\n\n\n\n18\n\n\n",
          "ranking": {
            "relevance_score": 0.7753779146649299,
            "citation_score": 0.3439655172413793,
            "recency_score": 0.6093829095338229,
            "final_score": 0.672495934667109
          },
          "is_open_access": true,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Traditional program analysis techniques provide formal guarantees but often face scalability challenges.",
        "chunk_index": 3
      },
      "summary": "Traditional program analysis techniques provide formal guarantees but often face scalability challenges.",
      "vector_score": 0.656,
      "llm_score": 0.82,
      "combined_score": 0.82,
      "source_query": "mock_query_related work"
    }
  ],
  "Methods": [
    {
      "chunk": {
        "chunk_id": "mock_7",
        "paper": {
          "id": "user_2404.15822v1",
          "title": "Recursive Backwards Q-Learning in Deterministic Environments",
          "published": "2024-04-24",
          "authors": [
            "Jan Diekhoff",
            "Jorn Fischer"
          ],
          "summary": "Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.",
          "pdf_url": "",
          "doi": "10.48550/arXiv.2404.15822",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "arXiv.org",
          "citation_count": 0,
          "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n",
          "markdown_text": "## RECURSIVE BACKWARDS Q-LEARNING IN DETERMINISTIC ENVIRONMENTS\n\n\n\n**Jan Diekhoff**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: jan.diekhoff@web.de\n\n\n\n**Jörn Fischer**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: j.fischer@hs-mannheim.de\n\n\n\n**ABSTRACT**\n\n\nReinforcement learning is a popular method of finding optimal solutions to complex problems.\nAlgorithms like Q-learning excel at learning to solve stochastic problems without a model of their\nenvironment. However, they take longer to solve deterministic problems than is necessary. Q-learning\ncan be improved to better solve deterministic problems by introducing such a model-based approach.\nThis paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds\na model of the environment. After reaching a terminal state, it recursively propagates its value\nbackwards through this model. This lets each state be evaluated to its optimal value without a lengthy\nlearning process. In the example of finding the shortest path through a maze, this agent greatly\noutperforms a regular Q-learning agent.\n\n\n_**Keywords**_ Q-learning _·_ deterministic _·_ recursive _·_ reinforcement learning\n\n\n**1** **Introduction**\n\n\nMachine learning and reinforcement learning are increasingly popular and important fields in the modern age. There are\nproblems that reinforcement learning agents can learn to solve more efficiently and consistently than any human when\ngiven enough time to practice. However, modern approaches like Q-learning run into issues when facing certain types\nof problems. Their approach to solving problems in combination with not using a model of the environment causes\nthem to take longer than is necessary to learn to solve problems that are deterministic in nature. By working without\nmodel of the environment, information that is available and help the learning process is ignored.\n\n\nThis paper introduces an adapted Q-learning agent called the _recursive backwards Q-Learning (RBQL) agent_ . It solves\nthese types of problems by building a model of its environment as it explores and recursively applying the Q-value\nupdate rule to find an optimal policy much quicker than a regular Q-learning agent. This agent is shown to work with\nthe example of finding the fastest path through a maze. Its results are compared to the results of a regular Q-learning\nagent.\n\n\n**2** **Reinforcement Learning**\n\n\nReinforcement learning is one of the main fields of machine learning. It is commonly used for optimizing solutions to\nproblems. At its most fundamental level, a reinforcement learning method is an implementation of an agent for solving\na Markov decision process [1] by interacting with an environment. Markov decision processes describe problems as\na set of states _S_, a set of actions _A_ and a set of rewards _R_ . For every time step _t_, the agent chooses an action _a ∈_ _A_\nand receives a new state _s ∈_ _S_ and a reward _r ∈_ _R_ for the action [2]. Rewards may be positive or negative, depending\non the outcome of the action, to encourage or discourage taking that action in the future [3]. The process of the agent\ninteracting with the environment is called an episode which ends when a terminal state is reached which resets the\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nenvironment and agent to their original configuration for the start of a new episode [3]. For the purposes of this paper,\nonly finite Markov decision processes are considered, meaning the environment has at least one terminal state.\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-1-0.png)\n\n\n_St_ +1\n\n\nFigure 1: Basic agent-environment relationship in a Markov decision process. The agent chooses an action _At_ and the\nenvironment returns a new state _St_ +1 and a reward _Rt_ +1. The dotted line represents the transition from step _t_ to step\n_t_ + 1 [3].\n\n\nReinforcement learning agents learn an optimal strategy for a given Markov decision process by estimating the value of\neither being in a state or taking a certain action in a certain state. They do this through a value function or action-value\nfunction respectively. The aim of the agent is to maximize the reward they receive in an episode [3]. To achieve this,\nvalue estimations do not only consider the immediate action the agent takes but also consider all future states and actions\nthat may occur when taking the original action. Agents follow so-called policies according to which they choose which\nactions to take. Through gaining knowledge, they continuously adapt this policy in order to eventually reach an optimal\npolicy - a policy which chooses the optimal action at every step. To explore, agents have to balance between exploration\nand exploitation [3]. Exploration is the act of following suboptimal actions to attempt to find an even better policy. On\nthe other hand, exploitation is following the actions that will yield the currently highest estimated value. An agent that\nonly exploits acts _greedily_ . To ensure continual exploration so that all actions get updated given enough time, agents\ncan choose policies that are mostly greedy but choose to explore sometimes [2]. To this end, an approach like _ϵ_ -greedy\nmay be used. Here, _ϵ_ is the probability of choosing a random action and 1 _−_ _ϵ_ is the probability of acting greedily.\n\n\nA widely used modern approach to RL is temporal difference learning [4], more specifically Q-learning [2]. Q-learning\nworks with the Q-learning update formula to update its policies:\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + _α ·_\n\n[ _Rt_ +1 + _γ ·_ max _Q_ ( _St_ +1 _, a_ ) _−_ _Q_ ( _St, At_ )] (1)\n_a_\n\n\n_Q_ ( _St, At_ ) is the estimated value for any given state-action pair. The equation shows how it is updated after taking\naction _At_ from state _St_ . _Rt_ +1 represents the reward gained, max _Q_ ( _St_ +1 _, a_ ) is the value estimation of the best action\n_a_\n_a ∈_ _At_ +1 that can be taken from _St_ +1 according to the current policy, the state resulting from action _At_ . _α_ is a step-size\nparameter, also known as the _learning rate_ . Its value lies between 0 and 1 and it determines how importantly the agent\nvalues new information against the current estimate it already has. A value of 0 completely ignores new information\nwhile a value of 1 completely overrides the preexisting value estimate. _γ_ is the discount factor, weighing future rewards\nless than immediate ones. It also lies between 0 and 1, where 1 weighs the best future action equally to the current one\nand 0 does not consider it at all.\n\n\n**3** **Recursive Backwards Q-Learning**\n\n\n**3.1** **Idea**\n\n\nQ-learning agents are very widespread in modern reinforcement learning. Working free of a model allows them to\nbe generally applicable to many problems. However, some Markov decision processes take longer to solve than is\nnecessary because the agent ignores readily available information. This is noticeable in deterministic, episodic tasks\nwhere a positive reward is only given when reaching a terminal state. Before this state is reached for the first time, the\nagent appears to be moving entirely at random. Looking at figure 2, the issue becomes apparent. Even when following\nthe optimal path at every step, it still takes multiple episodes for the reward of the terminal state to propagate back to the\nstarting state. In fact, the optimal paths value estimation gets worse before it gets better. If every step has a reward of\n\n\n2\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_−_ 1, values along the optimal path get worse if they do not lead to a state that has already been reached by the terminal\nstate’s positive reward as it travels backwards.\n\n\nIn this paper, grid worlds [3] are used as an example Markov decision process for the agent to solve. Grid worlds are a\ntwo-dimensional grid in which every tile represents a state and the actions are limited to walking up, down, left or right.\nGrid worlds are useful in that they are very simple to understand and to display, they have a limited set of actions and\ntheir set of states can be as small or large as is desired. Additionally, showing the value or optimal policy for each state\nis as easy as writing a number or drawing an arrow on the corresponding tile. Actions that would place the agent off of\nthe grid simply return the state the agent is already in, but may still give a reward. Special tiles can also be defined, such\nas walls that act like the grid edge or pits that are terminal fail states because the agent cannot leave them once it has\nfallen in. Every grid world tile gives a reward of _−_ 1 to punish taking unnecessary actions in favor of taking the fastest\npath to the goal.\n\n\n_Q_ greedy policy\nw.r.t. _Q_\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -0.8\n\n\n-1\n\n\n-1\n\n\n-1 0.52\n\n\n-1\n\n\n-1\n\n\n-1 1.99\n\n\n-1\n\n\n-1\n\n\n-1 3.27\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 0.78\n\n\n-1\n\n\n-1\n\n\n-1 3.15\n\n\n-1\n\n\n-1\n\n\n-1 4.96\n\n\n-1\n\n\n-1\n\n\n-1 6.17\n\n\n-1\n\n\n-1\n\n\n-1 6.93\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 4.5\n\n\n-1\n\n\n-1\n\n\n-1 7.25\n\n\n-1\n\n\n-1\n\n\n-1 8.63\n\n\n-1\n\n\n-1\n\n\n-1 9.32\n\n\n-1\n\n\n-1\n\n\n-1 9.66\n\n\n-1\n\n\n-1\n\n\n-1 9.83\n\n\n-1\n\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n\nep. 0\n\n\nep. 1\n\n\nep. 2\n\n\nep. 3\n\n\nep. 4\n\n\nep. 5\n\n\nep. 6\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -1.6\n\n\n-1\n\n\n-1\n\n\n-1 -1.66\n\n\n-1\n\n\n-1\n\n\n-1 -1.1\n\n\n-1\n\n\n-1\n\n\n-1 -0.15\n\n\n-1\n\n\n\nFigure 2: Q-learning in a one-dimensional grid world. All Q-values are initialized as _−_ 1. Actions that lead to the\nterminal state reward 10. All other actions reward -1. The discount rate _γ_ is set to 0 _._ 9. The learning rate _α_ is set to 0 _._ 5.\nThe value of _ϵ_ is irrelevant as the only action the agent takes is _→_ .\n\n\nFigure 2 is a very simple grid world and it still takes six episodes to reach an optimal policy, even when taking the\noptimal action at every step. This problem will only grow worse and add noticeably more episodes of training for grid\nworlds that are not as trivial to solve, or even more complex tasks with more variables to consider. As stated, the issue\nis that the agent has no source of direction until it has randomly stumbled across the terminal state, its only source of\npositive rewards. The larger the state space, the longer it is blindly searching.\n\n\nReinforcement learning agents that work with a model of their environment are known as _model-based_ reinforcement\nlearning agents. They can either work with a preexisting model or, more commonly, build their own. The way they\nconstruct their models is important as having perfect knowledge of an environment is neither feasible nor sensible. In\nthe case of a grid world it is no problem, but imagining a more complex scenario like a self-driving car makes this fact\napparent. When trying to drive from one city to another, knowing every centimeter of the road with every possible place\nother cars might be on the route is resource intensive and unnecessary. Instead, an agent should attempt to simplify its\nmodel as much as possible. Instead of every bit of road, long stretches going straight can be clumped together. Similar\nsituations like a car in front slowing down can be treated the same wherever they occur.\n\n\nThe purpose of this paper is to introduce and evaluate a new type of model-based agent called the RBQL agent. The\nRBQL agent solves deterministic, episodic tasks that positively reward only the terminal state more efficiently than a\nregular Q-learning agent. It functions by building a model of its environment through exploration. When it reaches a\nterminal state, it recursively travels backwards through all previously explored states, applying a modified Q-learning\nupdate rule, the RBQL update rule. By setting the learning rate _α_ to 1, equation (1) can be simplified as such:\n\n\n3\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + 1 _·_ [ _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )]\n= _Q_ ( _St, At_ ) + _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )\n= _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n\n(2)\n\n\n\nAs can be seen in formula (2), the Q-value now exclusively depends on the reward and the discounted reward of the\nbest neighbor. Because the algorithm applies this formula starting with what is guaranteed to be the highest value of the\nenvironment and working its way away from it, the best possible neighbor for any given state is always the previously\nevaluated state.\n\n\nEvaluating all states at the end of the episode is reminiscent of dynamic programming [5] or Monte Carlo methods [3]\nand is a point of critique for those approaches. However, as will be shown in chapter 4, this evaluation method is so\neffective in RBQL that evaluating all known states in one go is still cost effective. RBQL also differs in comparison\nto dynamic programming and Monte Carlo in a few major ways. In contrast with dynamic programming, it does not\nstart out with a perfect model but has to build its own. It also propagates its reward throughout all states much more\nquickly and it uses an action-value function, not a state-value function. In contrast with Monte Carlo, it does not use\nexploring starts to guarantee exploration. It also does not only update the values that were seen in an episode. Instead,\nto facilitate exploration, it always prioritizes visiting unexplored actions, only following the greedy path when there\nare none. Because this mode of exploration still results in unexplored actions, the _ϵ_ -greedy approach is adapted for\nRBQL. Instead of exploring steps, the agent has exploration episodes. _ϵ_ serves the same purpose as before, marking\nthe probability of taking an exploration episode while 1 _−_ _ϵ_ is the probability of taking an exploitation episode. In an\nexploration episode, the agent randomly chooses an unexplored action anywhere in its model, navigates the model to\nput itself in a position to take that action and then continues to explore until it finds a known path again or the episode\nends.\n\n\nIn this paper, finding an optimal path through a randomly generated grid world maze is used as an example task for\nRBQL to solve. It is also used to compare the performance of RBQL to Q-learning.\n\n\n**3.2** **Implementation**\n\n\nTo implement RBQL [1], the Godot game engine v. 3.5 [2] was used. Godot is a free, open source engine used mainly for\nvideo game development. Its main language is GDScript, an internal language that is very similar in syntax to Python,\nthough it also supports C, C++, C# and VisualScript. Because Python is very popular for machine learning development,\nthe implementation is written in GDScript so that it is easily readable for interested parties. Godot uses a hierarchical\nstructure of objects called _nodes_ . In the implementation, there are two main nodes: the agent and the environment.\n\n\n**3.2.1** **Environment**\n\n\nThe environment is of the type `TileMap` [3] – a class designed for creating maps in grid-based environments like grid\nworlds. Before starting the first episode, the environment generates a maze given a width _w_ and a height _h_ using a\nrecursive backtracking algorithm [6]. The starting point for the agent is always (0 _,_ 0) and the goal it attempts to reach –\nthe only terminal state – is ( _w −_ 1 _, h −_ 1). To ensure that the agent has the ability to improve even after finding the goal\nin the first episode, a maze with multiple paths is needed. Because a maze generated with recursive backtracking only\nhas one path to the terminal state, a number of alternate paths are generated by taking _w · h/_ 4 random positions and a\ndirection for each position. If the position has a wall in that direction, it is removed. If not, nothing happens.\n\n\nThe environment has a function `step(state,action)` that serves as the only way for the agent to interact with it.\nThe possible moves are `UP`, `DOWN`, `LEFT` and `RIGHT` . The state is described as a coordinate of the current position. In\nGodot, the class `Vector2(x,y)` [4] is used for this purpose. `step()` checks if taking the given action from the given\nstate results in hitting a wall or not. If not, the agent moves to a new position. There are three different rewards: _−_ 1 for\nany normal tile, _−_ 5 for hitting a wall and 10 for reaching the terminal state. _−_ 1 is awarded at every step to discourage\nagents from taking unnecessary steps. Walls give _−_ 5 to quickly teach the agent to ignore them. After taking an action,\n\n\n1 The source code can be downloaded at `[https://github.com/JanDiekhoff/BackwardsLearner](https://github.com/JanDiekhoff/BackwardsLearner)`\n2 Godot v. 3.5 can be downloaded at `[https://godotengine.org/download/archive/3.5-stable/](https://godotengine.org/download/archive/3.5-stable/)`\n3 `[https://docs.godotengine.org/en/3.5/classes/class_tilemap.html](https://docs.godotengine.org/en/3.5/classes/class_tilemap.html)`\n4 `[https://docs.godotengine.org/en/3.5/classes/class_vector2.html](https://docs.godotengine.org/en/3.5/classes/class_vector2.html)`\n\n\n4\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nthe new state and reward are returned to the agent, as well as a notification if the episode has ended or not and if the\nagent has hit a wall or not.\n\n\nThe `TileMap` has a tile for each combination of having or not having a wall in each of the four directions, totaling 2 [4] or\n16 total possible tiles. Another option would be to just have a floor tile and a wall tile. However, that would make a\nmaze with an equivalent wall layout much larger, leading to a larger state set and longer solving times. To determine if a\nwall is in a certain direction, the id of each tile from 0 to 15 acts as a four-bit flag. Each direction is assigned one of the\nbits ( `UP` = 0, `RIGHT` = 1, `DOWN` = 2 and `LEFT` = 3). If the flag is set, there is a wall in the corresponding direction. The\nid for an L-shaped tile for example would be 2 [2] + 2 [3] = 12 as `DOWN` and `LEFT` have walls. The process for determining\nif the agent can move in a given direction _d_ from a position _p_ is ( _¬idp_ ) & (2 _[d]_ ), where _idp_ is the id of the tile at _p_ .\n\n\n**3.2.2** **RBQL Agent**\n\n\nThe RBQL agent is represented by a `Sprite` [5] object – a 2D image – so it can be observed while solving a maze. During\nits runtime, the agent keeps track of a few key things:\n\n\n    - A model of the environment ( `explored_map` )\n\n\n    - A list of rewards for each state-action pair ( `rewards` )\n\n\n    - The last reward received ( `reward` )\n\n\n    - A list of steps taken per episode ( `steps_taken` )\n\n\n    - The Q-table ( `qtable` )\n\n\n    - The current state ( `current_state` )\n\n\n    - The previous state ( `old_state` )\n\n\n    - The last taken action ( `action` )\n\n\nThe model of the environment starts out as an empty dictionary. Every time a new state is discovered, an entry for that state is made and initialized as an empty array. When an action is taken from this state, the resulting new state is entered into the previous state’s array at the index of the taken action’s designated number\n( `explored_map[old_state][action] = current_state` ). When hitting a wall, the “new” state is the same as the\nstate from which the action was taken. Similarly, when an action is taken, the resulting reward is saved in the rewards\nlist ( `rewards[old_state][action] = reward` ). Because the agent uses state-action values, not state values, the\ntiles are treated like nodes in a directed graph. Going from tile A to tile B might result in a different reward than when\ngoing from B to A, so when the agent learns the reward of going from A to B, it does not also learn the reward of going\nfrom B to A.\n\n\nBeing a Q-learner makes it simpler to generalize the agent for other tasks, but it causes a lot of exploratory steps and\nexploratory episodes to only explore one position at a time. If an exploration episode chooses an unexplored state-action\npair that results in hitting a wall, the exploration episode immediately ends with little information gained. To alleviate\nthis problem, the agent takes exploratory “look-ahead” steps. After entering a tile, it takes a step in every direction but\nonly saves the result if it hits a wall. This guarantees that exploratory episodes always take new paths and not just hit a\nwall and continue on the best known path.\n\n\nThe agent also keeps track of a list of the actions it has taken – except for when hitting a wall – for the case that it\nreaches a dead end, or rather a state with no unexplored neighbors. In this case, the agent would normally follow the\noptimal path until it finds a new unexplored path or reaches the terminal state. However, if the path the agent is on has\nnot been explored before it has not yet been evaluated and there is no optimal path to follow. In this case, the agent\nbacktracks by taking the opposite action of the most recent in the list, then removes it from the list, until an unexplored\ntile or an evaluated path to follow is found.\n\n\nFinally, when the terminal state is reached, the Q-table is updated with the rewards saved in `rewards` according to the\nRBQL update rule.\n```\n             qtable[state][action] =\n\n```\n\n`rewards[state][action] + discount_rate` _·_\n\n```\n             qtable[explored_map[state][action]].max()\n\n```\n\nTo do this, a copy of `explored_map` is inverted to be able to traverse it in reverse. This is then done with a breadth-first\nsearch algorithm, starting at the terminal state, and the Q-value is calculated for each state. Breadth-first search is chosen\n\n\n5 `[https://docs.godotengine.org/en/3.5/classes/class_sprite.html](https://docs.godotengine.org/en/3.5/classes/class_sprite.html)`\n\n\n5\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nover a depth-first search algorithm so that each state must only be visited once as the value is directly proportional to\nthe distance from the terminal state. With breadth-first search, each state gets the highest possible value on its first visit\nbecause it is visited from its highest possible valued neighbor.\n\n\nWhen all known states have been evaluated, a new episode begins. After the first episode, episodes are chosen to be\neither exploratory or exploitative, similar to how an _ϵ_ -greedy policy may choose exploratory actions. In an exploitative\nepisode, the agent simply follows the best path it knows, choosing at random if two states are equally good, but still\nalways exploring unknown states directly adjacent to the path above all else. In an exploratory episode, a random state\nwith an unexplored neighbor is chosen. The agent navigates to this state with the help of the A* search algorithm [7]\nand follow the unexplored path from there until it finds a known state again. This exploratory excursion may only find\none new state or it may find a vastly superior path to what was known before. _ϵ_ is decreased after every episode as\nfollows:\n_ϵ_ = `min_epsilon + (max_epsilon - min_epsilon)`\n\n\n_· e_ [(] _[−]_ `[decay_rate]` _[ ·]_ `[ current_episode]` [)]\n\n\nwhere `min_epsilon`, `max_epsilon` and `decay_rate` can be any value within a range of [0 _,_ 1] and `current_episode`\nis the number of the current episode starting with 0. Once every state is explored, the agent is guaranteed to have found\nthe optimal path, or paths, through the maze. In its entirety, the algorithm can be expressed like this:\n\n\n**Algorithm 1** Backwards Q-Learning Algorithm\n\nSet exploration_episode to false\n**while** true **do**\n\n**if** exploration_episode **then**\n\nFind unexplored path\nTravel to unexplored path\n**end if**\n**while** episode is not over **do**\n\n**if** current position has an unexplored neighbor **then**\n\nVisit unexplored neighbor\nUpdate model\nSave reward\n\n**if** no wall hit **then**\n\nSave action in action queue\n**end if**\n**else if** there is an optimal path to follow **then**\n\nVisit best neighbor\n**end if**\n**while** current pos. has no unexplored neighbor **do**\n\nBacktrack\n\n**end while**\n\n**end while**\nCreate state queue with breadth-first search\n**for** state in queue **do**\n\nApply RBQL formula\n**end for**\nSet exploration_episode to random() _<_ = _ϵ_\nApply decay to _ϵ_\n**end while**\n\n\n**3.2.3** **Q-learning agent**\n\n\nA standard Q-learning agent has been implemented in Godot as well to compare the performance of the RBQL agent to.\nThis agent is comparatively simple:\n\n\n**4** **Tests and Results**\n\n\nTo compare the performance of the two agents, three sets of tests have been done for different maze sizes: 5 _×_ 5, 10 _×_ 10\nand 15 _×_ 15. All variables have been set to common values. The decay rate is set somewhat high to account for the\nrelatively low episode amount:\n\n\n6\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**Algorithm 2** Q-Learning Algorithm\n\n\n**while** true **do**\n\n**if** random() _<_ = _ϵ_ **then**\n\nChoose random action\n\n**else**\n\nChoose greedy action\n**end if**\n\nTake action\n\nReceive new state and reward\nUpdate Q-table for old state and action\n**if** terminal state reached **then**\n\nStart new episode\n**end if**\nApply decay to _ϵ_\n**end while**\n\n\n    - _γ_ = 0 _._ 9\n\n\n    - _α_ = 0 _._ 1 (RBQL has _α_ = 1 as explained in equation (2))\n\n\n    - `min_epsilon` = 0 _._ 01\n\n\n    - `max_epsilon` = 1\n\n\n    - `decay_rate` = _−_ 0 _._ 01\n\n\nFor every maze size, each agent is given the same set of 50 randomly generated mazes. Each agent is given 25 episodes\nper maze to train. These values are chosen to offer a reasonably large sample size without requiring an enormous\namount of time to compute. Agents are compared by the number of steps taken per episode, with less steps taken being\na more desirable outcome. The step counter is increased every time `step()` is called, including the look-ahead steps of\nthe RBQL. For a sense of perspective, the best possible solution to any square maze of size _s_ [2] is 2 _s −_ 2. Assuming a\nmaze with no walls, the shortest distance between two points _A_ and _B_ can be expressed as their Manhattan distance\n_|AX −_ _BX_ _|_ + _|AY −_ _BY |_ [8]. In the corners of a square, it holds that _AX_ = _AY_ and _BX_ = _BY_, so the distance can\nbe simplified as 2 _· |A −_ _B|_ . Setting _A_ = 0 and _B_ = _s −_ 1, this further simplifies to 2 _s −_ 2. This means that while\nthe amount of states (and thereby state-action pairs) increases quadratically, the best possible solution only increases\nlinearly. This in turn means that the amount of states that are not on the optimal path that the agent has to evaluate will\noften increase drastically with the size of the maze.\n\n\nLooking at the results, a few things can be observed. First of all, the average number of steps the RBQL agent takes\nis consistently lower than the Q-learning agent in all three maze sizes. It also has much less variation in step counts,\nwhich can be seen when looking at the areas of lighter hue. The light red areas are much more sporadic and spike\nfurther away from the average. The green areas stick much closer together. If the highest two step counts per episode\nwere not removed, RBQL would also have a few small spikes. These spikes would represent exploratory episodes\nwhere a new path is explored, resulting in a higher step count. In cases where the line is flat for a long period of time, it\ncan be assumed that the optimal solution is found. This can be seen in all three figures, where both the average and\nthe min/max range become a straight line close to the minimum. Important to note is that every maze has a different\noptimal solution, hence why the average sits above the blue line which denotes the lowest possible step count in any\nmaze of this size. It can also be observed that none of the lines ever go below this boundary, as is to be expected.\n\n\nSecond, even when removing the highest two step counts per episode, many of the Q-learning agent’s step counts are so\nlarge that scaling the graphs to fit them makes the RBQL agent’s data and the lower boundary difficult to see in the\ngraphs for the larger mazes. The highest step count values that have not been cut are 858 steps in figure 3, 7,585 in\nfigure 4 and 21,147 in figure 5, while the highest in total are 3,716 steps in figure 3, 20,553 in figure 4 and 26,315 in\nfigure 5.\n\n\nThird, it is interesting to see how the differences in average step counts evolve with the grid size. Table 1 shows this\ndifference in the first and last episode. The difference between the average step counts in the last episode especially is\nstriking, as it is close to doubling from each size to the next. Further, looking at the improvement of each agent as seen\nin table 2, one can see that the factor by which RBQL improves massively increases the bigger the maze becomes while\nthe Q-learner only slightly improves its performance in comparison. Additionally, most of the improvement of RBQL is\ndone in the first two episodes, while the Q-learner has a more gradual learning curve.\n\n\n7\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**1** _**,**_ **000**\n\n\n**900**\n\n\n**800**\n\n\n**700**\n\n\n**600**\n\n\n**500**\n\n\n**400**\n\n\n**300**\n\n\n**200**\n\n\n**100**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 3: Number of steps taken to find the goal in a randomly generated grid world maze of size 5 _×_ 5. The blue line is\nthe minimum step threshold for any maze of this size. The light red area shows the range of Q-learning agent’s highest\nand lowest step count, excluding the highest and lowest two. The red line shows the average performance. Similarly, the\nlight green area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest\ntwo, and the green line shows the average performance.\n\n\n8\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-7-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**8** _**,**_ **000**\n\n\n**6** _**,**_ **000**\n\n\n**5** _**,**_ **000**\n\n\n**4** _**,**_ **000**\n\n\n**3** _**,**_ **000**\n\n\n**2** _**,**_ **000**\n\n\n**1** _**,**_ **000**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 4: Number of steps taken to find the goal in a randomly generated grid world maze of size 10 _×_ 10. The light\nred area shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two.\nThe red shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n9\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-8-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 5: Number of steps taken to find the goal in a randomly generated grid world maze of size 15 _×_ 15. The light red\narea shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two. The\nred line shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n10\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-9-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nTable 1: Difference in average step counts of the Q-learner and RBQL. The difference expresses how many times more\nsteps the Q-learner took compared to RBQL.\n\n\nGrid size Q-learner steps RBQL steps Difference\n**Episode 0**\n5 _×_ 5 278.06 191.84 1.45\n\n10 _×_ 10 3,308.46 843.52 3.92\n15 _×_ 15 7,180.98 1,965 3.65\n**Episode 24**\n5 _×_ 5 49.14 9.62 5.11\n\n10 _×_ 10 281.44 23.68 11.89\n\n15 _×_ 15 778.68 35.96 21.65\n\n\nLastly, the RBQL agent seems to find an optimal policy at around episode 4 for the 5 _×_ 5, episode 6 for the 10 _×_ 10 and\nepisode 10 for the 15 _×_ 15 grid. As the previous figures show, the Q-learning agent does not come close to similarly\nlow step counts and therefore does not reach an optimal policy at all with the same amount of training.\n\n\nTable 2: Difference in average step counts of the Q-learner and RBQL. Improvement shows the factor by which the\namount of steps is reduced from episode 0 to 24.\n\n\nGrid size Steps in episode 0 Steps in episode 24 Improvement\n**Q-learning agent**\n5 _×_ 5 278.06 49.14 5.66\n\n10 _×_ 10 3,308.46 281.44 11.76\n15 _×_ 15 7,180.98 778.68 9.22\n**RBQL agent**\n5 _×_ 5 191.84 9.62 19.94\n\n10 _×_ 10 843.52 23.68 35.62\n\n15 _×_ 15 1,965 35.96 90.76\n\n\nTo further show RBQL’s efficiency, it has also been tested under the same parameters in a grid of size 50 _×_ 50. The\nresults can be seen in figure 6. This test is done to demonstrate that even such a large maze can be explored by RBQL.\nAs with the previous examples, by far the largest policy improvement still happens in the first episode. With mazes of\nsuch a large size, a lot more spikes in step counts are seen in later episodes because there are more states to explore. The\ndifference in average step counts goes from 20,811.08 in episode 0 to 344.9 in episode 24, an improvement by a factor\nof 60.34. This is worse than the improvement in the 15 _×_ 15 mazes, but still almost double that of the 10 _×_ 10 mazes.\n\n\n**5** **Discussion**\n\n\nThis chapter explores the practicality of using this algorithm to solve other Markov decision processes. It discusses\nwhich parts of the implementation are and are not specific to the problem of fastest path through a maze, which\nimprovements can be made to make it more applicable for other problems and showcases further points for research in\nthis field. The constraints given in this paper are that the agent will attempt to solve deterministic, episodic tasks with a\nsingle terminal state as its only source of positive rewards. This chapter also discusses which of these constraints can be\ndismissed.\n\n\nThere are a few parts of the implementation as presented in chapter 3.2 that are only applicable to this specific problem.\nThis is not necessarily a bad thing, as the purpose of the RBQL agent is to utilize knowledge of its environment. As a\nresult of this, the only parts that cannot be directly adapted for other problems are the way the agent builds its model.\nIn the grid world maze, it can assume that every state has the same actions it can take and has a neighboring state in\neach direction (though it may sometimes be itself). Further, every action always has an opposite action, going up can\nalways be undone by going down for example. These assumptions allow it to easily build a model of the grid world\nand influence its policy in how it further explores it. They allow the agent to take steps in each direction to check for\nwalls and they allow the agent to backtrack when it is stuck in a dead end. These assumptions cannot be guaranteed\nfor other Markov decision processes or even for grid worlds with more complex behavior like a wind tunnel that if\nwalked through also pushes the agent one tile in the direction the wind is traveling. The way in which the agent builds a\n\n\n11\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**3**\n\n\n**2** _**.**_ **8**\n\n\n**2** _**.**_ **6**\n\n\n**2** _**.**_ **4**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 6: Number of steps taken to find the goal in a randomly generated grid world maze of size 50 _×_ 50. The light\ngreen area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest two,\nand the green line shows the average performance.\n\n\n12\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-11-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nmodel has to either be designed for each environment individually or it has to be abstracted so that it is more broadly\napplicable. Finding such an approach to model building is one area of improvement for RBQL. Importantly though,\nnone of these assumptions are required for the agent to function. Backtracking, opposite steps and the same actions for\nevery state simply make the implementation easier and more efficient. As long as no path of a directed graph would\ncause the agent to be stuck with no way to reach a terminal state, it can be explored and evaluated.\n\n\nAnother improvement to the way the implementation builds its model is to simplify it as far as possible. As the amount\nof states directly influences how long a problem takes to solve, RBQL will become more efficient the more it can\nremove unnecessary states. Currently, every position has its own state. If the agent could detect “hallways” – tiles with\nparallel walls – they could be removed without problem in favor of directly connecting the two tiles at either side of the\nhallway – only the negative rewards for the length of the hallway would have to be implemented into the model. Further,\nif there is a non-forking path that leads into a dead end, the entire path could be treated as a wall and ignored entirely.\nThis would leave only the starting state, terminal state, turns and forking paths to evaluate. Both of these additions leave\nthe key part of the algorithm, traversing the model backwards and applying the RBQL update formula, untouched.\n\n\nRBQL can be easily adapted to include multiple terminal states with the same or different rewards and this is already\nsupported by the implementation. There are two possible ways to do this. First is to create an imaginary state that\nall terminal states lead into from which the backtracking always starts. Second is to remember all terminal states and\nbacktrack from each of them. The first option is much more efficient as each state still only gets evaluated once while\nthe second version avoids having to tamper with the model.\n\n\nFinally, RBQL could be adapted to work in non-deterministic environments. To reiterate, deterministic means that a\nstate-action pair always yields the same state-reward pair. If the agent could, while building its model, also estimate the\ntransition probabilities of a state-action pair to a new state, RBQL could still be used to evaluate the states. The RBQL\nupdate rule can be generalized to\n\n\n\n_Q_ ( _St, At_ ) _←_ �\n\n_s∈St_ +1\n\n\n\n( _Rs_ + _γ_ max _Q_ ( _s, a_ )) _· p_ (3)\n_a_\n� �\n\n\n\nwhere _St_ +1 is the set of possible states when taking _At_ from _St_, _p_ is the probability of reaching _s_ when taking _At_ from\n_St_ and _Rs_ is the reward of reaching _s_ . In a deterministic environment, _St_ +1 only consists of one state with _p_ = 1,\nnegating these additions. Whether RBQL would be as effective in non-deterministic environments as in deterministic\nenvironments is something to be explored in further studies.\n\n\nThe only constraint on the algorithm that cannot easily be circumvented is its episodic nature. Because the agent relies\non a terminal state from which to propagate the rewards backwards from, a continuous task implementation seems\nimpossible to implement.\n\n\n**6** **Conclusion**\n\n\nThis paper has introduced recursive backwards Q-learning, a model-based reinforcement learning algorithm that\nevaluates all known state-action pairs of the model at the end of each episode with the Q-learning update rule. It has\nalso shown how recursive backwards Q-learning relates to, adapts and improves on them. This paper has presented\nan implementation of recursive backwards Q-learning in the Godot game engine to test its performance. Through\nmultiple tests, it has been shown to be superior in finding the shortest path through a randomly generated grid world\nmaze. It has been argued that this algorithm could be adapted to solve other deterministic, episodic tasks more quickly\nthan Q-learning. Further, it has given avenues for further research in adapting recursive backwards Q-learning for\nnon-deterministic problems.\n\n\n**References**\n\n\n[1] Richard Bellman, “A markovian decision process,” _Journal of Mathematics and Mechanics_, vol. 6, no. 5, pp. 679–\n684, 1957. [Online]. Available: `[http://www.jstor.org/stable/24900506](http://www.jstor.org/stable/24900506)` .\n\n[2] Christopher John Cornish Hellaby Watkins, “Learning from delayed rewards,” 1989.\n\n[3] Richard S Sutton and Andrew G Barto, _Reinforcement learning: An introduction_ . MIT press, 2018.\n\n[4] Richard S. Sutton, “Learning to predict by the methods of temporal differences,” _Machine Learning_, vol. 3, no. 1,\npp. 9–44, 1988. DOI: `[10.1007/bf00115009](https://doi.org/10.1007/bf00115009)` .\n\n[5] Richard Bellman, “Dynamic programming,” _Princeton, USA: Princeton University Press_, vol. 1, no. 2, p. 3, 1957.\n\n[6] Peter Gabrovšek, “Analysis of maze generating algorithms,” _IPSI Transactions on Internet Research_, vol. 15,\nno. 1, pp. 23–30, 2019.\n\n\n13\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n[7] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael, “A formal basis for the heuristic determination of minimum\ncost paths,” _IEEE Transactions on Systems Science and Cybernetics_, vol. 4, no. 2, pp. 100–107, 1968. DOI:\n`[10.1109/TSSC.1968.300136](https://doi.org/10.1109/TSSC.1968.300136)` .\n\n[8] Eugene F Krause, “Taxicab geometry,” _The Mathematics Teacher_, vol. 66, no. 8, pp. 695–706, 1973.\n\n\n14\n\n\n",
          "ranking": null,
          "is_open_access": false,
          "user_provided": true,
          "pdf_path": "output/literature/user_2404.15822v1/user_2404.15822v1.pdf"
        },
        "chunk_text": "We employ a two-stage pipeline: first extracting code features using AST parsing, then applying transformer encoders for semantic understanding.",
        "chunk_index": 0
      },
      "summary": "We employ a two-stage pipeline: first extracting code features using AST parsing, then applying transformer encoders for semantic understanding.",
      "vector_score": 0.7280000000000001,
      "llm_score": 0.91,
      "combined_score": 0.91,
      "source_query": "mock_query_methods"
    },
    {
      "chunk": {
        "chunk_id": "mock_8",
        "paper": {
          "id": "acda55ebdf39c6634e89a9730ff7d963471f2b0a",
          "title": "Expected Eligibility Traces",
          "published": "2020-07-03",
          "authors": [
            "H. V. Hasselt",
            "Sephora Madjiheurem",
            "Matteo Hessel",
            "David Silver",
            "André Barreto",
            "Diana Borsa"
          ],
          "summary": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state.\nIn this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that some- times substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
          "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17200/17007",
          "doi": "10.1609/aaai.v35i11.17200",
          "fields_of_study": [
            "Computer Science",
            "Mathematics"
          ],
          "venue": "AAAI Conference on Artificial Intelligence",
          "citation_count": 41,
          "bibtex": "@Article{Hasselt2020ExpectedET,\n author = {H. V. Hasselt and Sephora Madjiheurem and Matteo Hessel and David Silver and André Barreto and Diana Borsa},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Expected Eligibility Traces},\n volume = {abs/2007.01839},\n year = {2020}\n}\n",
          "markdown_text": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)\n\n# **Expected Eligibility Traces**\n\n\n**Hado van Hasselt** [1] **, Sephora Madjiheurem** [2] **, Matteo Hessel** [1]\n\n**David Silver** [1] **, Andr´e Barreto** [1] **, Diana Borsa** [1]\n\n1 DeepMind\n2 University College London, UK\n\n\n\n**Abstract**\n\n\nThe question of how to determine which states and actions\nare responsible for a certain outcome is known as the _credit_\n_assignment problem_ and remains a central research question\nin reinforcement learning and artificial intelligence. _Eligibil-_\n_ity traces_ enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not\nto counterfactual sequences that could also have led to the\ncurrent state. In this work, we introduce _expected eligibility_\n_traces_ . Expected traces allow, with a single update, to update\nstates and actions that could have preceded the current state,\neven if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous)\ntraces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide\na way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which\nensures that the resulting algorithm is a strict generalisation\nof TD( _λ_ ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.\n\n\n**Motivation and Summary**\n\n\nAppropriate credit assignment has long been a major research\ntopic in artificial intelligence (Minsky 1963). To make effective decisions and understand the world, we need to accurately associate events, like rewards or penalties, to relevant\nearlier decisions or situations. This is important both for learning accurate predictions, and for making good decisions.\n_Temporal credit assignment_ can be achieved with repeated\ntemporal-difference (TD) updates (Sutton 1988). One-step\nTD updates propagate information slowly: when a surprising value is observed, the state immediately preceding it is\nupdated, but no earlier states or decisions are updated. _Multi-_\n_step_ updates (Sutton 1988; Sutton and Barto 2018) propagate\ninformation faster over longer temporal spans, speeding up\ncredit assignment and learning. Multi-step updates can be\nimplemented online using _eligibility traces_ (Sutton 1988),\nwithout incurring significant additional computational expense, even if the time spans are long; these algorithms have\ncomputation that is independent of the temporal span of the\npredictions (van Hasselt and Sutton 2015).\n\n\nCopyright c _⃝_ 2021, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\nMDP True value TD(0) TD(λ) ET(λ)\n\n\nFigure 1: A comparison of TD(0), TD( _λ_ ), and the new\nexpected-trace algorithm ET( _λ_ ) (with _λ_ = 0 _._ 9). The MDP\nis illustrated on the left. Each episode, the agent moves randomly down and right from the top left to the bottom right,\nwhere any action terminates the episode. Reward on termination are +1 with probability 0.2, and zero otherwise—all\nother rewards are zero. We plot the value estimates after the\nfirst positive reward, which occurred in episode 5. We see\na) TD(0) only updated the last state, b) TD( _λ_ ) updated the\ntrajectory in this episode, and c) ET( _λ_ ) additionally updated\ntrajectories from earlier (unrewarding) episodes.\n\n\nTraces provide temporal credit assignment, but do not assign credit _counterfactually_ to states or actions that _could_\nhave led to the current state, but did not do so this time.\nCredit will eventually trickle backwards over the course of\nmultiple visits, but this can take many iterations. As an example, suppose we collect a key to open a door, which leads\nto an unexpected reward. Using standard one-step TD learning, we would update the state in which the door opened.\nUsing eligibility traces, we would also update the preceding\ntrajectory, including the acquisition of the key. But we would\nnot update other sequences that _could_ have led to the reward,\nsuch as collecting a spare key or finding a different entrance.\nThe problem of credit assignment to counterfactual states\nmay be addressed by learning a model, and using the model\nto propagate credit (cf. Sutton 1990; Moore and Atkeson\n1993; Chelu, Precup, and van Hasselt 2020); however, it\nhas often proven challenging to construct and use models\neffectively in complex environments (cf. van Hasselt, Hessel,\nand Aslanides 2019).\nWe introduce a new approach to counterfactual credit assignment, based on the concept of _expected eligibility traces_ .\nWe present a family of algorithms, which we call ET( _λ_ ), that\nuse expected traces to update their predictions. We analyse\nthe nature of these expected traces, and illustrate their benefits empirically in several settings—see Figure 1 for a first\n\n\n\n9997\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-1.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-3.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-4.png)\n\n\nillustration. We introduce a bootstrapping mechanism that\nprovides a spectrum of algorithms between standard eligibility traces and expected eligibility traces, and also discuss\nways to apply these ideas with deep neural networks. Finally,\nwe discuss possible extensions and connections to related\nideas such as successor features.\n\n\n**Background**\nSequential decision problems can be modelled as Markov\ndecision processes [1] (MDP) ( _S, A, p_ ) (Puterman 1994), with\nstate space _S_, action space _A_, and a joint transition and\nreward distribution _p_ ( _r, s_ _[′]_ _|s, a_ ). An agent selects actions according to its policy _π_, such that _At ∼_ _π_ ( _·|St_ ) where _π_ ( _a|s_ )\ndenotes the probability of selecting _a_ in _s_, and observes random rewards and states generated according to the MDP, resulting in trajectories _τt_ : _T_ = _{St, At, Rt_ +1 _, St_ +1 _, . . ., ST }_ .\nA central goal is to predict _returns_ of future discounted rewards (Sutton and Barto 2018)\n\n\n_Gt ≡_ _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _Rt_ +2 + _γt_ +1 _γt_ +2 _Rt_ +3 + _. . ._\n\n\n\n=\n\n\n\n_T_\n� _γt_ [(] +1 _[i][−]_ [1)] _[R][t]_ [+] _[i][,]_\n\n\n_i_ =1\n\n\n\nwhere _T_ is for instance the time the current episode terminates or _T_ = _∞_, and where _γt ∈_ [0 _,_ 1] is a (possibly constant) discount factor and _γt_ [(] _[n]_ [)] = [�] _[n]_ _k_ =0 _[−]_ [1] _[γ][t]_ [+] _[k]_ [, and] _[ γ]_ _t_ [(0)] = 1.\nThe value _vπ_ ( _s_ ) = E [ _Gt|St_ = _s, π_ ] of state _s_ is the expected return for a policy _π_ . Rather than writing the return as\na random variable _Gt_, it will be convenient to instead write it\nas an explicit function _G_ ( _τ_ ) of the random trajectory _τ_ . Note\nthat _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ).\nWe approximate the value with a function _v_ **w** ( _s_ ) _≈_ _vπ_ ( _s_ ).\nThis can for instance be a table—with a single separate entry\n_w_ [ _s_ ] for each state—a linear function of some input features,\nor a non-linear function such as a neural network with parameters **w** . The goal is to iteratively update **w** with\n\n\n**w** _t_ +1 = **w** _t_ + ∆ **w** _t_\n\n\nsuch that _v_ **w** approaches the true _vπ_ . Perhaps the simplest\nalgorithm to do so is the Monte Carlo (MC) algorithm\n\n\n∆ **w** _t ≡_ _α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._\n\n\nMonte Carlo is effective, but has high variance, which can\nlead to slow learning. TD learning (Sutton 1988; Sutton and\nBarto 2018) instead replaces the return with the current estimate of its expectation _v_ ( _St_ +1) _≈_ _G_ ( _τt_ +1: _T_ ), yielding\n\n\n∆ **w** _t ≡_ _αδt∇_ **w** _v_ **w** ( _St_ ) _,_ (1)\nwhere _δt ≡_ _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ ) _,_\n\n\nwhere _δt_ is called the temporal-difference (TD) error. We\ncan interpolate between these extremes, for instance with\n_λ_ -returns which smoothly mix values and sampled returns:\n\n\n_G_ _[λ]_ ( _τt_ : _T_ ) = _Rt_ +1+ _γt_ +1�(1 _−λ_ ) _v_ **w** ( _St_ +1)+ _λG_ _[λ]_ ( _τt_ +1: _T_ )� _._\n\n\n‘Forward view’ algorithms, like the MC algorithm, use returns\nthat depend on future trajectories and need to wait until the\n\n\n1The ideas in this paper extend naturally to POMDPs (cf. **?** ).\n\n\n\nend of an episode to construct their updates, which can take a\nlong time. Conversely, ‘backward view’ algorithms rely only\non past experiences and can update their predictions online,\nduring an episode. Such algorithms build an _eligibility trace_\n(Sutton 1988; Sutton and Barto 2018). An example is TD( _λ_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ _t,_ with _**e**_ _t_ = _γtλ_ _**e**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ ) _,_\n\n\nwhere _**e**_ _t_ is an accumulating eligibility trace. This trace can\nbe viewed as a function _**e**_ _t ≡_ _**e**_ ( _τ_ 0: _t_ ) of the trajectory of past\ntransitions. The TD update in (1) is known as TD(0), because\nit corresponds to using _λ_ = 0. TD( _λ_ = 1) corresponds to an\nonline implementation of the MC algorithm. Other variants\nexist, using other kinds of traces, and equivalences have been\nshown between these algorithms and their forward views that\nuse _λ_ -returns: these backward-view algorithms converge to\nthe same solution as the corresponding forward view, and can\nin some cases yield equivalent weight updates (Sutton 1988;\nvan Seijen and Sutton 2014; van Hasselt and Sutton 2015).\n\n\n**Expected Traces**\n\n\nThe main idea of this paper is to use the concept of an _ex-_\n_pected eligibility trace_, defined as\n\n\n_**z**_ ( _s_ ) _≡_ E [ _**e**_ _t | St_ = _s_ ] _,_\n\n\nwhere the expectation is over the agent’s policy and the MDP\ndynamics. We introduce a concrete family of algorithms,\nwhich we call ET( _λ_ ) and ET( _λ_, _η_ ), that learn expected traces\nand use them in value updates. We analyse these algorithms\ntheoretically, describe specific instances, and discuss computational and algorithmic properties.\n\n\n**ET(** _λ_ **)**\n\n\nWe propose to learn approximations _**zθ**_ ( _s_ ) _≈_ _**z**_ ( _s_ ), with parameters _**θ**_ _∈_ R _[d]_ (e.g., the weights of a neural network). One\nway to learn _**zθ**_ is by updating it toward the instantaneous\ntrace _**e**_ _t_, by minimizing an empirical loss _L_ ( _**e**_ _t,_ _**zθ**_ ( _St_ )). For\ninstance, _L_ could be a component-wise squared loss, optimized with stochastic gradient descent:\n\n\n_**θ**_ _t_ +1 = _**θ**_ _t_ + ∆ _**θ**_ _t,_ where (2)\n\n\n1\n\n∆ _**θ**_ _t_ = _−β_ _[∂]_\n\n_∂_ _**θ**_ 2 [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))] _[⊤]_ [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))]\n\n= _β_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] ( _**e**_ _t −_ _**zθ**_ ( _St_ )) _,_ (3)\n\n_∂_ _**θ**_\n\n\nwhere _[∂z]_ _**[θ]**_ _∂_ [(] _**θ**_ _[S][t]_ [)] is a _|_ _**θ**_ _| × |_ _**e**_ _|_ Jacobian [2] and _β_ is a step size.\n\nThe idea is then to use _**zθ**_ ( _s_ ) _≈_ E [ _**e**_ _t | St_ = _s_ ] in place\nof _**e**_ _t_ in the value update, which becomes\n\n\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (4)\n\n\nWe call this ET( _λ_ ). Below, we prove that this update can\nbe unbiased and can have lower variance than TD( _λ_ ). Algorithm 1 shows pseudo-code for a concrete instance of ET( _λ_ ).\n\n\n2The Jacobian-vector product can efficiently be computed (e.g.,\nvia auto-differentiation) with computational requirements that are\ncomparable to the computation of the loss.\n\n\n\n9998\n\n\n\n\n**Algorithm 1** ET( _λ_ )\n\n\n1: initialise **w**, _**θ**_\n2: **for** _M_ episodes **do**\n3: initialise _**e**_ = **0**\n\n4: observe initial state _S_\n5: **repeat** for each step in episode _m_\n6: generate _R_ and _S_ _[′]_\n\n7: _δ ←_ _R_ + _γv_ **w** ( _S_ _[′]_ ) _−_ _v_ **w** ( _S_ )\n8: _**e**_ _←_ _γλ_ _**e**_ + _∇_ **w** _v_ **w** ( _S_ )\n\n9: _**θ**_ _←_ _**θ**_ + _β_ _[∂]_ _**[z]**_ _∂_ _**[θ]**_ _**θ**_ [(] _[S]_ [)] ( _**e**_ _−_ _**zθ**_ ( _S_ ))\n\n10: **w** _←_ **w** + _αδ_ _**zθ**_ ( _S_ )\n11: **until** _S_ is terminal\n\n12: **end for**\n\n13: **Return w**\n\n\n**Interpretation and ET(** _λ, η_ **)**\n\nWe can interpret TD(0) as taking the MC update and replacing the return from the subsequent state, which is a function\nof the future trajectory, with a state-based estimate of its expectation: _v_ **w** ( _St_ +1) _≈_ E [ _G_ ( _τt_ +1: _T_ ) _|St_ +1 ]. This becomes\nmost clear when juxtaposing the updates:\n\n\n_α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _,_ (MC)\n_α_ ( _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._ (TD)\n\n\nTD( _λ_ ) also uses a function of a trajectory: the trace _**e**_ _t_ . We\npropose replacing this as well with a function of state: the\nexpected trace _**zθ**_ ( _St_ ) _≈_ E [ _**e**_ ( _τ_ 0: _t_ ) _|St_ ]. Again juxtaposing:\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ ( _τ_ 0: _t_ ) _,_ (TD( _λ_ ))\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (ET( _λ_ ))\n\n\nWe can interpolate smoothly between MC and TD(0) via\n_λ_ . This is often useful to trade off variance of the return with\npotential bias of the value estimate. For instance, we might\nnot have access to the true state _s_, and might instead have to\nrely on features **x** ( _s_ ). Then we cannot always represent or\nlearn the true values _v_ ( _s_ )—for instance different states may\nbe aliased (Whitehead and Ballard 1991).\nSimilarly, when moving from TD( _λ_ ) to ET( _λ_ ) we replaced\na trajectory-based trace with a state-based estimate. This\nmight induce bias and, again, we can smoothly interpolate by\nusing a recursively defined mixture trace _**y**_ _t_, as defined as [3]\n\n\n_**y**_ _t_ = (1 _−_ _η_ ) _**zθ**_ ( _St_ ) + _η_ � _γtλ_ _**y**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ )� _._ (5)\n\n\nThis recursive usage of the estimates _**zθ**_ ( _s_ ) at previous states\nis analogous to bootstrapping on future state values when\nusing a _λ_ -return, with the important difference that the arrow\nof time is opposite. This means we do not first have to convert\nthis into a backward view: the quantity can already be computed from past experience directly. We call the algorithm\nthat uses this mixture trace ET( _λ_, _η_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**y**_ ( _St_ ) _._ (ET( _λ_, _η_ ))\n\n\n3While _**y**_ _t_ depends on both _η_ and _λ_ we leave this dependence\nimplicit, as is conventional for traces.\n\n\n\nNote that if _η_ = 1 then _**y**_ _t_ = _**e**_ _t_ equals the instantaneous\ntrace: ET( _λ_, 1) is equivalent to TD( _λ_ ). If _η_ = 0 then _**y**_ _t_ = _**z**_ _t_\nequals the expected trace; the algorithm introduced earlier\nas ET( _λ_ ) is equivalent to ET( _λ_, 0). By setting _η ∈_ (0 _,_ 1), we\ncan smoothly interpolate between these extremes.\n\n\n**Theoretical Analysis**\n\nWe now analyse the new ET algorithms theoretically. First\nwe show that if we use _**z**_ ( _s_ ) directly and _s_ is Markov then the\nupdate has the same expectation as TD( _λ_ ) (though possibly\nwith lower variance), and therefore also inherits the same\nfixed point and convergence properties.\n\n\n**Lemma 1.** _If s is Markov, then_\n\n\nE [ _δt_ _**e**_ _t | St_ = _s_ ] = E [ _δt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] _._\n\n\n_Proof._ In Appendix .\n\n\n**Proposition 1.** _Let_ _**e**_ _t be any trace vector, updated in any_\n_way. Let_ _**z**_ ( _s_ ) = E [ _**e**_ _t | St_ = _s_ ] _. Consider the ET(λ) algo-_\n_rithm_ ∆ **w** _t_ = _αtδt_ _**z**_ ( _St_ ) _. For all Markov states s the expec-_\n_tation of this update is equal to the expected update under_\n_instantaneous trace_ _**e**_ _t, and its variance is lower or equal:_\n\n\nE [ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] = E [ _αtδt_ _**e**_ _t|St_ = _s_ ] _and_\nV[ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] _≤_ V[ _αtδt_ _**e**_ _t|St_ = _s_ ] _,_\n\n\n_where the second inequality holds component-wise for the_\n_update vector, and is strict when_ V[ _**e**_ _t|St_ ] _>_ 0 _._\n\n\n_Proof._ We have\n\n\nE [ _αtδt_ _**e**_ _t | St_ = _s_ ]\n= E [ _αtδt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] (Lemma 1)\n= E [ _αtδt | St_ = _s_ ] _**z**_ ( _s_ )\n= E [ _αtδt_ _**z**_ ( _St_ ) _| St_ = _s_ ] _._ (6)\n\n\nDenote the _i_ -th component of _**z**_ ( _St_ ) by _zt,i_ and the _i_ -th\ncomponent of _**e**_ _t_ by _et,i_ . Then, we also have\n\n\nE � ( _αtδtzt,i_ ) [2] _|St_ = _s_ � = E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � _zt,i_ [2]\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E [ _et,i|St_ = _s_ ] [2]\n\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ ��E � _e_ [2] _t,i_ _[|][S][t]_ [=] _[ s]_ � _−_ V[ _et,i|St_ = _s_ ]�\n\n_≤_ E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E � _e_ [2] _t,i_ _[|][ S][t]_ [=] _[ s]_ �\n\n= E � ( _αtδtet,i_ ) [2] _| St_ = _s_ � _,_\n\n\nwhere the last step used the fact that _s_ is Markov, and the inequality is strict when V[ _et,i|St_ ] _>_ 0. Since the expectations\nare equal, as shown in (6), the conclusion follows.\n\n\n**Interpretation** Proposition 1 is a strong result: it holds for\nany trace update, including accumulating traces (Sutton 1984,\n1988), replacing traces (Singh and Sutton 1996), dutch traces\n(van Seijen and Sutton 2014; van Hasselt, Mahmood, and\nSutton 2014; van Hasselt and Sutton 2015), and future traces\nthat may be discovered. It implies convergence of ET( _λ_ )\nunder the same conditions as TD( _λ_ ) (Dayan 1992; Peng 1993;\n\n\n\n9999\n\n\n\n\nTsitsiklis 1994) with lower variance when V[ _**e**_ _t|St_ ] _>_ 0,\nwhich is the common case.\nNext, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a\nlearned approximation _**z**_ _t_ ( _s_ ) _≈_ _**z**_ ( _s_ ) that relies solely on\nobserved experience.\n\n**Proposition 2.** _Let_ _**e**_ _t an instantaneous trace vector. Then_\n1 _nt_ ( _s_ )\n_let_ _**z**_ _t_ ( _s_ ) _be the empirical mean_ _**z**_ _t_ ( _s_ ) = _nt_ ( _s_ ) � _i_ _**e**_ _t_ _[s]_ _i_ _[,]_\n_where t_ _[s]_ _i_ _[denotes past times when we have been in state]_\n_s, that is St_ _[s]_ _i_ [=] _[ s][, and][ n][t]_ [(] _[s]_ [)] _[ is the number of visits to][ s]_\n_in the first t steps. Consider the expected trace algorithm_\n**w** _t_ +1 = **w** _t_ + _αtδt_ _**z**_ _t_ ( _St_ ) _. If St is Markov, the expectation of_\n_this update is equal to the expected update with instantaneous_\n_traces_ _**e**_ _t, while attaining a potentially lower variance:_\n\n\nE [ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] = E [ _αtδt_ _**e**_ _t | St_ ] _and_\nV[ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] _≤_ V[ _αtδt_ _**e**_ _t | St_ ] _,_\n\n\n_where the second inequality holds component-wise. The in-_\n_equality is strict when_ V[ _**e**_ _t | St_ ] _>_ 0 _._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** Proposition 2 mirrors Proposition 1 but, importantly, covers the case where we estimate the expected\ntraces from data, rather than relying on exact estimates. This\nmeans the benefits extend to this pure learning setting. Again,\nthe result holds for any trace update. The inequality is typically strict when the path leading to state _St_ = _s_ is stochastic\n(due to environment or policy).\nNext we consider what happens if we do not have Markov\nstates and instead have to rely on, possibly non-Markovian,\nfeatures **x** ( _s_ ). We then have to pick a function class and for\nthe purpose of this analysis we consider linear expected traces\n_**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) and values _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), as convergence for non-linear values can not always be assured even\nfor standard TD( _λ_ ) (Tsitsiklis and Van Roy 1997), without\nadditional assumptions (e.g., Ollivier 2018; Brandfonbrener\nand Bruna 2020).\n\n**Proposition 3.** _When using approximations z_ **Θ** ( _s_ ) = **Θx** ( _s_ )\n_and v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ) _then, if_ (1 _−_ _η_ ) **Θ** + _η_ I _is non-singular,_\n_ET(λ, η) has the same fixed point as TD(λη)._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** This result implies that linear ET( _λ_, _η_ ) converges under similar conditions as linear TD( _λ_ _[′]_ ) for _λ_ _[′]_ = _λ·η_ .\nIn particular, when **Θ** is non-singular, using the approximation _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) in ET( _λ_, 0) = ET( _λ_ ) implies convergence to the fixed point of TD(0).\nThough ET( _λ_, _η_ ) and TD( _λη_ ) have the same fixed point,\nthe algorithms are not equivalent. In general, their updates\nare not the same. Linear approximations are more general\nthan tabular functions (which are linear functions of a indicator vector for the current state), and we have already seen\nin Figure 1 that ET( _λ_ ) behaves quite differently from both\nTD(0) and TD( _λ_ ), and we have seen its variance can be lower\nin Propositions 1 and 2. Interestingly, **Θ** resembles a preconditioner that speeds up the linear semi-gradient TD update,\n\n\n10000\n\n\n\nepisode 5\n1st reward\n\n\n\nepisode 12\n2nd reward\n\n\n\nepisode 100\n20 rewards\n\n\n\nepisode 1K\n~200 rewards\n\n\n\nepisode 10K\n~2K rewards\n\n\n\nFigure 2: In the same setting as Figure 1, we show later value\nestimates after more rewards have been observed. TD(0)\nlearns slowly but steadily, TD( _λ_ ) learns faster but with higher\nvariance, and ET( _λ_ ) learns both fast and stable.\n\n\nsimilar to how second-order optimisation algorithms (Amari\n1998; Martens 2016) precondition the gradient updates.\n\n\n**Empirical Analysis**\n\nFrom the insights above, we expect that ET( _λ_ ) yields lower\nprediction errors because it has lower variance and aggregates information across episodes better. In this section we\nempirically investigate expected traces in several experiments.\nWhenever we refer to ET( _λ_ ), this is equivalent to ET( _λ_, 0).\n\n\n\n**An Open World**\n\nFirst consider the grid world depicted in Figure 1. The agent\nrandomly moves right or down (excluding moves that would\nhit a wall), starting from the top-left corner. Any action in the\nbottom-right corner terminates the episode with +1 reward\nwith probability 0 _._ 2, and 0 otherwise. All other rewards are 0.\nFigure 1 shows value estimates after the first positive reward, which occurred in the fifth episode. TD(0) updated a\nsingle state, TD( _λ_ ) updated earlier states in that episode, and\nET( _λ_ ) additionally updated states from previous episodes.\nFigure 2 additionally shows value estimates after the\nsecond reward (which occurred in episode 12), and after\nroughly 20, 200, and 2000 rewards (or 100, 1000, and 10 _,_ 000\nepisodes, respectively). ET( _λ_ ) converged faster than TD(0),\nwhich propagated information slowly, and faster than TD( _λ_ ),\nwhich exhibited higher variance. All step sizes decayed as\n_α_ = _β_ = ~~�~~ 1 _/k_, where _k_ is the current episode number.\n\n\n**A Multi-Chain**\n\nWe now consider the multi-chain shown in Figure 3. We\nfirst compare TD( _λ_ ) and ET( _λ_ ) with tabular values on various variants of the multi-chain, corresponding to _m ∈_\n_{_ 1 _,_ 2 _,_ 4 _,_ 8 _, ...,_ 128 _}_ parallel chains of length _n_ = 4. The leftmost plot in Figure 4 shows the average root mean squared\nerror (RMSE) of the value predictions after 1024 episodes.\nWe ran 10 seeds for each combination of step size 1 _/t_ _[d]_ with\n_d ∈{_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 1 _}_ and _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ .\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-4-0.png)\n\neither is +1 with probability 0 _._ 9 or _−_ 1 with probability 0 _._ 1.\n\n\nThe left plot in Figure 4 shows value errors for different\n_m_, minimized over _d_ and _λ_ . The prediction error of TD( _λ_ )\n(blue) grew quickly with the number of parallel chains. ET( _λ_ )\n(orange) scaled better, because it updates values in multiple\nchains (from past episodes) upon receiving a surprising reward (e.g., _−_ 1) on termination. The other three plots in Figure\n4 show value error as a function of _λ_ for a subset of problems\ncorresponding to _m ∈{_ 8 _,_ 32 _,_ 128 _}_ . The dependence on _λ_\ndiffers across algorithms and problem instances, but ET( _λ_ )\nconsistently achieved lower error than TD( _λ_ ), especially with\nhigh _λ_ . Further analysis, including on step-size sensitivity, is\nincluded in the appendix.\nNext, we encode each state with a feature vector **x** ( _s_ )\ncontaining a binary indicator vector of the branch, a binary\nindicator of the progress along the chain, a bias that always\nequals one, and two binary features indicating when we are in\nthe start (white) or bottleneck (orange) state. We extend the\nlengths of the chains to _n_ = 16. Both TD( _λ_ ) and ET( _λ_ ) use\na linear value function _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), and ET( _λ_ ) uses a\nlinear expected trace _z_ **Θ** ( _s_ ) = **Θx** ( _s_ ). All updates use the\nsame constant step size _α_ . The left plot in Figure 5 shows the\naverage root mean squared value error after 1024 episodes\n(averaged over 10 seeds). For each point the best constant\nstep size _α ∈{_ 0 _._ 01 _,_ 0 _._ 03 _,_ 0 _._ 1 _}_ (shared across all updates)\nand _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ is selected. ET( _λ_ ) (orange)\nattained lower errors across all values of _m_ (left plot), and\nfor all _λ_ (center two plots, for two specific _m_ ). The right plot\nshows results for smooth interpolations via _η_, for _λ_ = 0 _._ 9\nand _m_ = 16. The full expected trace ( _η_ = 0) performed well\nhere, we expect in other settings the additional flexibility of\n_η_ could be beneficial.\n\n\n**Expected Traces in Deep Reinforcement Learning**\n\n(Deep) neural networks are a common choice of function\nclass in reinforcement learning (e.g., Werbos 1990; Tesauro\n1992, 1994; Bertsekas and Tsitsiklis 1996; Prokhorov and\nWunsch 1997; Riedmiller 2005; van Hasselt 2012; Mnih\net al. 2015; van Hasselt, Guez, and Silver 2016; Wang et al.\n2016; Silver et al. 2016; Duan et al. 2016; Hessel et al. 2018).\nEligibility traces are not very commonly combined with deep\nnetworks (but see Tesauro 1992; Elfwing, Uchibe, and Doya\n2018), perhaps in part because of the popularity of experience\n\n\n10001\n\n\n\nreplay (Lin 1992; Mnih et al. 2015; Horgan et al. 2018).\nPerhaps the simplest way to extend expected traces to deep\nneural networks is to first separate the value function into\na representation **x** ( _s_ ) and a value _v_ ( **w** _,_ _**ξ**_ )( _s_ ) = **w** _[⊤]_ **x** _**ξ**_ ( _s_ ),\nwhere **x** _**ξ**_ is some (non-linear) function of the observations\n_s_ . [4] We can then apply the same expected trace algorithm as\nused in the previous sections by learning a separate linear\nfunction _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) using the representation which is\nlearned by backpropagating the value updates:\n\n\n**w** _t_ +1 = **w** _t_ + _αδ_ _**z**_ **Θ** ( _St_ ) _,_\n\n_**ξ**_ _t_ +1 = _**ξ**_ _t_ + _αδ_ _**e**_ _**[ξ]**_ _t_ _[,]_\n\nwhere _**e**_ _**[ξ]**_ _t_ [=] _[ γ][t][λ]_ _**[e][ξ]**_ _t−_ 1 [+] _[ ∇]_ _**[ξ]**_ _[v]_ [(] **[w]** _[,]_ _**[ξ]**_ [)][(] _[S][t]_ [)] _[,]_\n\n_**e**_ **[w]** _t_ [=] _[ γ][t][λ]_ _**[e]**_ **[w]** _t−_ 1 [+] _[ ∇]_ **[w]** _[v]_ ( **w** _,_ _**ξ**_ ) [(] _[S][t]_ [)] _[,]_\n\n\nand then updating **Θ** to minimise component-wise squared\ndifferences between _**e**_ **[w]** _t_ [and] _**[ z]**_ **[Θ]** _t_ [(] _[S][t]_ [)][, as in (2) and (3).]\nInteresting challenges appear outside the fully linear case.\nFirst, the representation will itself be updated and will have\nits own trace _**e**_ _**[ξ]**_ _t_ [. Second, in the control case we optimise]\nbehaviour: the policy will change. Both these properties of\nthe non-linear control setting imply that the expected traces\nmust track a non-stationary target. We found that being able to\ntrack this rather quickly improved performance: the expected\ntrace parameters **Θ** in the following experiment were updated\nwith a relatively high step size of _β_ = 0 _._ 1.\nWe tested this idea on two canonical Atari games: Pong and\nMs. Pac-Man. The results in Figure 6 show that the expected\ntraces helped speed up learning compared to the baseline\nwhich uses accumulating traces, for various step sizes. Unlike\nmost prior work on this domain, which often relies on replay\n(Mnih et al. 2015; Schaul et al. 2016; Horgan et al. 2018)\nor parallel streams of experience (Mnih et al. 2016), these\nalgorithms updated the values online from a single stream\nof experience. Further details on the experimental setup are\ngiven in the appendix.\nThese experiments demonstrate that the idea of expected\ntraces extends to non-linear function approximation, such as\ndeep neural networks. We consider this to be a rich area of\nfurther investigations. The results presented here are similar\nto earlier results (e.g., Mnih et al. 2015) and are not meant to\ncompete with state-of-the-art performance results, which often depend on replay and much larger amounts of experience\n(e.g., Horgan et al. 2018).\n\n\n**Discussion and Extensions**\n\nWe now discuss various interesting interpretations and relations, and discuss promising extensions.\n\n\n**Predecessor Features**\n\nFor linear value functions the expected trace _z_ ( _s_ ) can be\nexpressed non-recursively as follows:\n\n\n\n�\n\n\n\n_**z**_ ( _s_ ) = E\n\n\n\n_∞_\n� _λ_ [(] _t_ _[n]_ [)] _γt_ [(] _[n]_ [)] **x** _t−n | St_ = _s_\n� _n_ =0\n\n\n\n_,_ (7)\n\n\n\n4Here _s_ denotes observations to the agent, not a full environment\nstate— _s_ is not assumed to be Markovian.\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-1.png)\n\nFigure 4: Prediction errors in the multi-chain. ET( _λ_ ) (orange) consistently outperformed TD( _λ_ ) (blue). Shaded areas depict\nstandard errors across 10 seeds.\n\n\nFigure 5: Comparing value error with linear function approximation a) as function of the number of branches (left), b) as\nfunction of _λ_ (center two plots) and c) as function of _η_ (right). The left three plots show comparisons of TD( _λ_ ) (blue) and ET( _λ_ )\n(orange), showing ET( _λ_ ) attained lower prediction errors. The right plot interpolates between these algorithms via ET( _λ_, _η_ ),\nfrom ET( _λ_ ) = ET( _λ_, 0) to ET( _λ_, 1) = TD( _λ_ ), with _λ_ = 0 _._ 9 (corresponding to a vertical slice indicated in the second plot).\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-3.png)\n\nwhere _γk_ [(] _[n]_ [)] _≡_ [�] _[k]_ _j_ = _k−n_ _[γ][j]_ [. This is interestingly similar to the]\ndefinition of _successor features_ (Barreto et al. 2017):\n\n\n\n�\n\n\n\n_ψ_ ( _s_ ) = E\n\n\n\n_∞_\n� _γt_ [(] +1 _[n][−]_ [1)] **x** _t_ + _n | St_ = _s_\n� _n_ =1\n\n\n\n_._ (8)\n\n\n\nThe summation in (8) is over future features, while in (7)\nwe have a sum over features already observed by the agent.\nWe can thus think of linear expected traces as _predecessor_\n_features_ . A similar connection was made in the tabular setting by Pitis (2018), relating source traces, which aim to\nestimate the source matrix ( _I −_ _γP_ ) _[−]_ [1], to successor representations (Dayan 1993). In a sense, the above generalises\nthis insight. In addition to being interesting in its own right,\nthis connection allows for an intriguing interpretation of _**z**_ ( _s_ )\nas a multidimensional value function. Like with successor\nfeatures, the features **x** _t_ play the role of rewards, discounted\nwith _γ · λ_ rather than _γ_, and with time flowing backwards.\nAlthough the predecessor interpretation only holds in the\nlinear case, it is also of interest as a means to obtain a practical\nimplementation of expected traces with non-linear function\napproximation, for instance applied only to the linear ‘head’\nof a deep neural network. We used this ‘predecessor feature\ntrick’ in our Atari experiments described earlier.\n\n\n**Relation to Model-Based Reinforcement Learning**\n\n\nModel-based reinforcement learning provides an alternative\napproach to efficient credit assignment. The general idea is\nto construct a model that estimates state-transition dynamics,\nand to update the value function based upon hypothetical\n\n\n10002\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-4.png)\n\ntransitions drawn from the model (Sutton 1990), for example\nby prioritised sweeping (Moore and Atkeson 1993; van Seijen\nand Sutton 2013). In practice, model-based approaches have\nproven challenging in environments (such as Atari games)\nwith rich perceptual observations, compared to model-free\napproaches that more directly update the agent’s policy and\npredictions (van Hasselt, Hessel, and Aslanides 2019).\nIn some sense, expected traces also construct a model of\nthe environment—but one that differs in several key regards\nfrom standard state-to-state models used in model-based reinforcement learning. First, expected traces estimate _past_\nquantities rather than _future_ quantities. Backward planning\n(e.g., Chelu, Precup, and van Hasselt 2020) is possible with\nexplicit transition models, but is less common in practice.\nSecond, expected traces estimate the accumulation of _gradi-_\n_ents_ over a multi-step trajectory, rather than trying to learn\nthe full transition dynamics, thereby focusing only on those\naspects that matter for the eventual weight update. Third, expected traces allow credit assignment across these potential\npast trajectories with a single update, without the iterative\ncomputation that is typically required when using a dynamics\nmodel. These differences may be important to side-step some\nof the challenges faced in model-based learning.\n\n\n**Batch Learning and Replay**\n\n\nWe have mainly considered the online learning setting in this\npaper. It is often convenient to learn from batches of data, or\nreplay transitions repeatedly, to enhance data efficiency. A\nnatural extension is replay the experiences sequentially (e.g.\nKapturowski et al. 2018), but perhaps alternatives exist. We\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-1.png)\n\nFigure 6: Performance of Q( _λ_ ) ( _η_ = 1, blue) and QET( _λ_ ) ( _η_ = 0, orange) on Pong and Ms.Pac-Man for various learning rates.\nShaded regions show standard error across 10 random seeds. All results are for _λ_ = 0 _._ 95. Further implementation details and\nhyper-parameters are in the appendix.\n\n\n\nnow discuss one potential extension.\nWe defined a mixed trace _**y**_ _t_ that mixes the instantaneous\nand expected traces. Optionally the expected trace _**z**_ _t_ can\nbe updated towards the mixed trace _**y**_ _t_ as well, instead of\ntowards the instantaneous trace _**e**_ _t_ . Analogously to TD( _λ_ ) we\npropose to then use at least one real step of data:\n\n\n∆ _**θ**_ _t ≡_ _β_ ( _**∇**_ _t_ + _γtλt_ _**y**_ _t−_ 1 _−_ _**zθ**_ ( _St_ )) _[⊤]_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] _,_ (9)\n\n_∂_ _**θ**_\n\n\nwith _**∇**_ _t ≡∇_ **w** _v_ **w** ( _St_ ). This is akin to a forward-view _λ_ return update, with _∇_ **w** _v_ **w** ( _St_ ) in the role of (vector) reward,\nand _**zθ**_ of value, and discounted by _λtγt_, but reversed in time.\nIn other words, this can be considered a sampled Bellman\nequation (Bellman 1957) but backward in time.\nWhen we then choose _η_ = 0, then _**y**_ _t−_ 1 = _z_ _**θ**_ ( _St−_ 1), and\nthen the target in (9) only depends on a single transition.\nInterestingly, that means we can then learn expected traces\nfrom _individual_ transitions, sampled out of temporal order,\nfor instance in batch settings or when using replay.\n\n\n**Application to Other Traces**\n\n\nWe can apply the idea of expected trace to more traces than\nconsidered here. We can for instance consider the characteristic eligibility trace used in REINFORCE (Williams 1992)\nand related policy-gradient algorithms (Sutton et al. 2000).\nAnother appealing application is to the follow-on trace\nor _emphasis_, used in emphatic temporal difference learning\n(Sutton, Mahmood, and White 2016) and related algorithms\n(e.g., Imani, Graves, and White 2018). Emphatic TD was\nproposed to correct an important issue with off-policy learning, which can be unstable and lead to diverging learning\ndynamics. Emphatic TD weights updates according to 1) the\ninherent interest in having accurate predictions in that state\nand, 2) the importance of predictions in that state for updating\n\n\n10003\n\n\n\nother predictions. Emphatic TD uses scalar ‘follow-on’ traces\nto determine the ‘emphasis’ for each update. However, this\nfollow-on trace can have very high, even infinite, variance.\nInstead, we might estimate and use its expectation instead of\nthe instantaneous emphasis. A related idea was explored by\nZhang, Boehmer, and Whiteson (2019) to obtain off-policy\nactor critic algorithms.\n\n\n**Conclusion**\n\n\nWe have proposed a mechanism for efficient credit assignment, using the expectation of an eligibility trace. We have\ndemonstrated this can sometimes speed up credit assignment\ngreatly, and have analyzed concrete algorithms theoretically\nand empirically to increase understanding of the concept.\nExpected traces have several interpretations. First, we can\ninterpret the algorithm as counterfactually updating multiple possible trajectories leading up to the current state. Second, they can be understood as trading off bias and variance,\nwhich can be done smoothly via a unifying _η_ parameter, between standard eligibility traces (low bias, high variance) and\nestimated traces (possibly higher bias, but lower variance).\nFurthermore, with tabular or linear function approximation\nwe can interpret the resulting expected traces as predecessor\nstates or features—object analogous to successor states or features, but time-reversed. Finally, we can interpret the linear\nalgorithm as preconditioning the standard TD update, thereby\npotentially speeding up learning. These interpretations suggest that a variety of complementary ways to potentially\nextend these concepts and algorithms.\nWe have shown expected traces can already be used to\nenhance learning in non-linear settings (i.e., deep reinforcement learning), and in the control setting where we update\nthe policy. Further work is needed to determine the full extent\nof the possibilities of these new algorithms.\n\n\n\n\n**References**\n\n\nAmari, S. I. 1998. Natural gradient works efficiently in\nlearning. _Neural computation_ 10(2): 251–276. ISSN 08997667.\n\n\nBarreto, A.; Dabney, W.; Munos, R.; Hunt, J. J.; Schaul, T.;\nvan Hasselt, H. P.; and Silver, D. 2017. Successor features\nfor transfer in reinforcement learning. In _Advances in neural_\n_information processing systems_, 4055–4065.\n\n\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. _J. Artif. Intell. Res. (JAIR)_ 47:\n253–279.\n\n\nBellman, R. 1957. _Dynamic Programming_ . Princeton University Press.\n\n\nBertsekas, D. P.; and Tsitsiklis, J. N. 1996. _Neuro-dynamic_\n_Programming_ . Athena Scientific, Belmont, MA.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018a. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018b. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBrandfonbrener, D.; and Bruna, J. 2020. Geometric Insights\ninto the Convergence of Non-linear TD Learning. In _Interna-_\n_tional Conference on Learning Representations_ .\n\n\nChelu, V.; Precup, D.; and van Hasselt, H. P. 2020. Forethought and Hindsight in Credit Assignment. In Larochelle,\nH.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H.,\neds., _Advances in Neural Information Processing Systems_,\nvolume 33, 2270–2281.\n\n\nDayan, P. 1992. The convergence of TD( _λ_ ) for general\nlambda. _Machine Learning_ 8: 341–362.\n\n\nDayan, P. 1993. Improving generalization for temporal difference learning: The successor representation. _Neural Com-_\n_putation_ 5(4): 613–624.\n\n\nDuan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel,\nP. 2016. Benchmarking deep reinforcement learning for\ncontinuous control. In _International Conference on Machine_\n_Learning_, 1329–1338.\n\n\nElfwing, S.; Uchibe, E.; and Doya, K. 2018. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_ 107:\n3–11.\n\n\nHennigan, T.; Cai, T.; Norman, T.; and Babuschkin, I. 2020.\nHaiku: Sonnet for JAX. URL http://github.com/deepmind/\ndm-haiku.\n\n\nHessel, M.; Modayil, J.; van Hasselt, H. P.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2018. Rainbow: Combining Improvements in Deep\nReinforcement Learning. _AAAI_ .\n\n\n10004\n\n\n\nHorgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel,\nM.; van Hasselt, H. P.; and Silver, D. 2018. Distributed\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ .\n\n\nImani, E.; Graves, E.; and White, M. 2018. An Off-policy\nPolicy Gradient Theorem Using Emphatic Weightings. In\nBengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; CesaBianchi, N.; and Garnett, R., eds., _Advances in Neural Infor-_\n_mation Processing Systems 31_, 96–106. Curran Associates,\nInc. URL http://papers.nips.cc/paper/7295-an-off-policypolicy-gradient-theorem-using-emphatic-weightings.pdf.\n\n\nKaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1995.\nPlanning and Acting in Partially Observable Stochastic Domains. Unpublished report.\n\n\nKapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and\nDabney, W. 2018. Recurrent experience replay in distributed\nreinforcement learning. In _International conference on learn-_\n_ing representations_ .\n\n\nKingma, D. P.; and Adam, J. B. 2015. A method for stochastic optimization. In _International Conference on Learning_\n_Representation_ .\n\n\nLin, L. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_\n8(3): 293–321.\n\n\nMartens, J. 2016. _Second-order optimization for neural net-_\n_works_ . University of Toronto (Canada).\n\n\nMinsky, M. 1963. Steps Toward Artificial Intelligence.\nIn Feigenbaum, E.; and Feldman, J., eds., _Computers and_\n_Thought_, 406–450. McGraw-Hill, New York.\n\n\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous Methods for Deep Reinforcement Learning. In\n_International Conference on Machine Learning_ .\n\n\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg,\nS.; and Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. _Nature_ 518(7540): 529–533.\n\n\nMoore, A. W.; and Atkeson, C. G. 1993. Prioritized Sweeping: Reinforcement Learning with less Data and less Time.\n_Machine Learning_ 13: 103–130.\n\n\nOllivier, Y. 2018. Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies. _CoRR_\nabs/1805.00869.\n\n\nPeng, J. 1993. _Efficient dynamic programming-based learn-_\n_ing for control_ . Ph.D. thesis, Northeastern University.\n\n\nPeng, J.; and Williams, R. J. 1996. Incremental Multi-step\nQ-learning. _Machine Learning_ 22: 283–290.\n\n\nPitis, S. 2018. Source Traces for Temporal Difference Learning. In McIlraith, S. A.; and Weinberger, K. Q., eds., _Pro-_\n_ceedings of the Thirty-Second AAAI Conference on Artificial_\n_Intelligence_, 3952–3959. AAAI Press.\n\n\n\n\nPohlen, T.; Piot, B.; Hester, T.; Azar, M. G.; Horgan, D.;\nBudden, D.; Barth-Maron, G.; van Hasselt, H. P.; Quan, J.;\nVecerˇ ´ık, M.; Hessel, M.; Munos, R.; and Pietquin, O. 2018.\nObserve and look further: Achieving consistent performance\non Atari. _arXiv preprint arXiv:1805.11593_ .\n\n\nProkhorov, D. V.; and Wunsch, D. C. 1997. Adaptive critic\ndesigns. _IEEE Transactions on Neural Networks_ 8(5): 997–\n1007.\n\n\nPuterman, M. L. 1994. _Markov Decision Processes: Discrete_\n_Stochastic Dynamic Programming_ . John Wiley & Sons, Inc.\nNew York, NY, USA.\n\n\nRiedmiller, M. 2005. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning\nMethod. In Gama, J.; Camacho, R.; Brazdil, P.; Jorge, A.; and\nTorgo, L., eds., _Proceedings of the 16th European Conference_\n_on Machine Learning (ECML’05)_, 317–328. Springer.\n\n\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ . Puerto Rico.\n\n\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering\nthe game of Go with deep neural networks and tree search.\n_Nature_ 529(7587): 484–489.\n\n\nSingh, S. P.; and Sutton, R. S. 1996. Reinforcement Learning\nwith replacing eligibility traces. _Machine Learning_ 22: 123–\n158.\n\n\nSutton, R. S. 1984. _Temporal Credit Assignment in Reinforce-_\n_ment Learning_ . Ph.D. thesis, University of Massachusetts,\nDept. of Comp. and Inf. Sci.\n\n\nSutton, R. S. 1988. Learning to predict by the methods of\ntemporal differences. _Machine learning_ 3(1): 9–44.\n\n\nSutton, R. S. 1990. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Proceedings of the seventh international conference_\n_on machine learning_, 216–224.\n\n\nSutton, R. S.; and Barto, A. G. 2018. _Reinforcement Learning:_\n_An Introduction_ . The MIT press, Cambridge MA.\n\n\nSutton, R. S.; Mahmood, A. R.; and White, M. 2016. An\nEmphatic Approach to the Problem of Off-policy TemporalDifference Learning. _Journal of Machine Learning Research_\n17(73): 1–29.\n\n\nSutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y.\n2000. Policy gradient methods for reinforcement learning\nwith function approximation. _Advances in Neural Informa-_\n_tion Processing Systems 13 (NIPS-00)_ 12: 1057–1063.\n\n\nTesauro, G. 1992. Practical Issues in Temporal Difference\nLearning. In Lippman, D. S.; Moody, J. E.; and Touretzky,\nD. S., eds., _Advances in Neural Information Processing Sys-_\n_tems 4_, 259–266. San Mateo, CA: Morgan Kaufmann.\n\n\nTesauro, G. J. 1994. TD-Gammon, a self-teaching backgammon program, achieves master-level play. _Neural computa-_\n_tion_ 6(2): 215–219.\n\n\n10005\n\n\n\nTsitsiklis, J. N. 1994. Asynchronous stochastic approximation and Q-learning. _Machine Learning_ 16: 185–202.\n\nTsitsiklis, J. N.; and Van Roy, B. 1997. An analysis of\ntemporal-difference learning with function approximation.\n_IEEE Transactions on Automatic Control_ 42(5): 674–690.\n\nvan Hasselt, H. P. 2012. Reinforcement Learning in Continuous State and Action Spaces. In Wiering, M. A.; and\nvan Otterlo, M., eds., _Reinforcement Learning: State of the_\n_Art_, volume 12 of _Adaptation, Learning, and Optimization_,\n207–251. Springer.\n\n\nvan Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver,\nD. 2016. Learning values across many orders of magnitude. In _Advances in Neural Information Processing Systems_\n_29: Annual Conference on Neural Information Processing_\n_Systems 2016, December 5-10, 2016, Barcelona, Spain_, 4287–\n4295.\n\n\nvan Hasselt, H. P.; Guez, A.; and Silver, D. 2016. Deep reinforcement learning with double Q-Learning. In _Proceedings_\n_of the Thirtieth AAAI Conference on Artificial Intelligence_,\n2094–2100.\n\n\nvan Hasselt, H. P.; Hessel, M.; and Aslanides, J. 2019. When\nto use parametric models in reinforcement learning? In _Ad-_\n_vances in Neural Information Processing Systems_, volume 32,\n14322–14333.\n\nvan Hasselt, H. P.; Mahmood, A. R.; and Sutton, R. S. 2014.\nOff-policy TD( _λ_ ) with a true online equivalence. In _Pro-_\n_ceedings of the 30th Conference on Uncertainty in Artificial_\n_Intelligence_, 330–339.\n\nvan Hasselt, H. P.; Quan, J.; Hessel, M.; Xu, Z.; Borsa, D.;\nand Barreto, A. 2019. General non-linear Bellman equations.\n_arXiv preprint arXiv:1907.03687_ .\n\n\nvan Hasselt, H. P.; and Sutton, R. S. 2015. Learning to predict\nindependent of span. _CoRR_ abs/1508.04582.\n\nvan Seijen, H.; and Sutton, R. S. 2013. Planning by Prioritized Sweeping with Small Backups. In _International_\n_Conference on Machine Learning_, 361–369.\n\nvan Seijen, H.; and Sutton, R. S. 2014. True online TD( _λ_ ).\nIn _International Conference on Machine Learning_, 692–700.\n\n\nWang, Z.; de Freitas, N.; Schaul, T.; Hessel, M.; van Hasselt,\nH. P.; and Lanctot, M. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In _International_\n_Conference on Machine Learning_ . New York, NY, USA.\n\nWerbos, P. J. 1990. A menu of designs for reinforcement\nlearning over time. _Neural networks for control_ 67–95.\n\nWhitehead, S. D.; and Ballard, D. H. 1991. Learning to\nperceive and act by trial and error. _Machine Learning_ 7(1):\n45–83.\n\nWilliams, R. J. 1992. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. _Machine_\n_Learning_ 8: 229–256.\n\n\nZhang, S.; Boehmer, W.; and Whiteson, S. 2019. Generalized\noff-policy actor-critic. In _Advances in Neural Information_\n_Processing Systems_, 2001–2011.\n\n\n",
          "ranking": {
            "relevance_score": 0.7493067885948483,
            "citation_score": 0.6302697050551669,
            "recency_score": 0.3906854405837399,
            "final_score": 0.6896372370858013
          },
          "is_open_access": false,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "The model architecture consists of a shared encoder with task-specific heads for different analysis objectives.",
        "chunk_index": 1
      },
      "summary": "The model architecture consists of a shared encoder with task-specific heads for different analysis objectives.",
      "vector_score": 0.7040000000000001,
      "llm_score": 0.88,
      "combined_score": 0.88,
      "source_query": "mock_query_methods"
    },
    {
      "chunk": {
        "chunk_id": "mock_9",
        "paper": {
          "id": "66d76444255be0ac378a0a93ee0379fc721a386f",
          "title": "On Q-learning Convergence for Non-Markov Decision Processes",
          "published": "2018-07-01",
          "authors": [
            "Sultan Javed Majeed",
            "Marcus Hutter"
          ],
          "summary": "Temporal-difference (TD) learning is an attractive, computationally efficient framework for model- free reinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent to learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has only been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state MDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the environment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning when applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We prove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in particular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a necessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.",
          "pdf_url": "https://doi.org/10.24963/ijcai.2018/353",
          "doi": "10.24963/ijcai.2018/353",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "International Joint Conference on Artificial Intelligence",
          "citation_count": 40,
          "bibtex": "@Article{Majeed2018OnQC,\n author = {Sultan Javed Majeed and Marcus Hutter},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {2546-2552},\n title = {On Q-learning Convergence for Non-Markov Decision Processes},\n year = {2018}\n}\n",
          "markdown_text": "[image]\n\n\nNavigation\n\n\n  - Home\n\n  - Conferences\n\n\nFuture Conferences\n\n  Past Conferences\n\n  \n\n  - Proceedings\n\n\n  - IJCAI 2025 Proceedings\n\n  - All Proceedings\n\n\n  - Awards\n\n  - Trustees/officers\n\n\nCurrent trustees\n\n  Trustees Elect\n\n  IJCAI Secretariat\n\n  \n  - IJCAI Sponsorship and Publicity Officers\nIJCAI Team\n\n  \n  - Local Arrangements Chairs\n\n  - Former Trustees serving on the Executive Committee\n\n  - Other Former Officers\n\n\n  - AI Journal\n\n  - About\n\n\nAbout IJCAI\n\n  Contact Information\n\n  \n# **On Q-learning Convergence for Non-Markov** **Decision Processes** **On Q-learning Convergence for Non-Markov** **Decision Processes**\n\n## **Sultan Javed Majeed, Marcus Hutter**\n\n\n[image]\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence\n[Main track. Pages 2546-2552. https://doi.org/10.24963/ijcai.2018/353](https://doi.org/10.24963/ijcai.2018/353)\n[PDF BibTeX](https://www.ijcai.org/proceedings/2018/0353.pdf)\n\n\nTemporal-difference (TD) learning is an attractive, computationally efficient framework for model- free\nreinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent\nto learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has\nonly been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state\nMDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the\nenvironment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning\nwhen applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We\nprove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in\nparticular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a\nnecessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.\nKeywords:\nMachine Learning: Online Learning\nMachine Learning: Reinforcement Learning\nPlanning and Scheduling: Markov Decisions Processes\n\n\nCopyright © 2025,\n\n\n",
          "ranking": {
            "relevance_score": 0.7639998734717994,
            "citation_score": 0.5680913780397937,
            "recency_score": 0.27592885552856466,
            "final_score": 0.6760110725910747
          },
          "is_open_access": true,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Training data is augmented using code transformation techniques while preserving semantic equivalence.",
        "chunk_index": 2
      },
      "summary": "Training data is augmented using code transformation techniques while preserving semantic equivalence.",
      "vector_score": 0.68,
      "llm_score": 0.85,
      "combined_score": 0.85,
      "source_query": "mock_query_methods"
    }
  ],
  "Results": [
    {
      "chunk": {
        "chunk_id": "mock_10",
        "paper": {
          "id": "user_2404.15822v1",
          "title": "Recursive Backwards Q-Learning in Deterministic Environments",
          "published": "2024-04-24",
          "authors": [
            "Jan Diekhoff",
            "Jorn Fischer"
          ],
          "summary": "Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.",
          "pdf_url": "",
          "doi": "10.48550/arXiv.2404.15822",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "arXiv.org",
          "citation_count": 0,
          "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n",
          "markdown_text": "## RECURSIVE BACKWARDS Q-LEARNING IN DETERMINISTIC ENVIRONMENTS\n\n\n\n**Jan Diekhoff**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: jan.diekhoff@web.de\n\n\n\n**Jörn Fischer**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: j.fischer@hs-mannheim.de\n\n\n\n**ABSTRACT**\n\n\nReinforcement learning is a popular method of finding optimal solutions to complex problems.\nAlgorithms like Q-learning excel at learning to solve stochastic problems without a model of their\nenvironment. However, they take longer to solve deterministic problems than is necessary. Q-learning\ncan be improved to better solve deterministic problems by introducing such a model-based approach.\nThis paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds\na model of the environment. After reaching a terminal state, it recursively propagates its value\nbackwards through this model. This lets each state be evaluated to its optimal value without a lengthy\nlearning process. In the example of finding the shortest path through a maze, this agent greatly\noutperforms a regular Q-learning agent.\n\n\n_**Keywords**_ Q-learning _·_ deterministic _·_ recursive _·_ reinforcement learning\n\n\n**1** **Introduction**\n\n\nMachine learning and reinforcement learning are increasingly popular and important fields in the modern age. There are\nproblems that reinforcement learning agents can learn to solve more efficiently and consistently than any human when\ngiven enough time to practice. However, modern approaches like Q-learning run into issues when facing certain types\nof problems. Their approach to solving problems in combination with not using a model of the environment causes\nthem to take longer than is necessary to learn to solve problems that are deterministic in nature. By working without\nmodel of the environment, information that is available and help the learning process is ignored.\n\n\nThis paper introduces an adapted Q-learning agent called the _recursive backwards Q-Learning (RBQL) agent_ . It solves\nthese types of problems by building a model of its environment as it explores and recursively applying the Q-value\nupdate rule to find an optimal policy much quicker than a regular Q-learning agent. This agent is shown to work with\nthe example of finding the fastest path through a maze. Its results are compared to the results of a regular Q-learning\nagent.\n\n\n**2** **Reinforcement Learning**\n\n\nReinforcement learning is one of the main fields of machine learning. It is commonly used for optimizing solutions to\nproblems. At its most fundamental level, a reinforcement learning method is an implementation of an agent for solving\na Markov decision process [1] by interacting with an environment. Markov decision processes describe problems as\na set of states _S_, a set of actions _A_ and a set of rewards _R_ . For every time step _t_, the agent chooses an action _a ∈_ _A_\nand receives a new state _s ∈_ _S_ and a reward _r ∈_ _R_ for the action [2]. Rewards may be positive or negative, depending\non the outcome of the action, to encourage or discourage taking that action in the future [3]. The process of the agent\ninteracting with the environment is called an episode which ends when a terminal state is reached which resets the\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nenvironment and agent to their original configuration for the start of a new episode [3]. For the purposes of this paper,\nonly finite Markov decision processes are considered, meaning the environment has at least one terminal state.\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-1-0.png)\n\n\n_St_ +1\n\n\nFigure 1: Basic agent-environment relationship in a Markov decision process. The agent chooses an action _At_ and the\nenvironment returns a new state _St_ +1 and a reward _Rt_ +1. The dotted line represents the transition from step _t_ to step\n_t_ + 1 [3].\n\n\nReinforcement learning agents learn an optimal strategy for a given Markov decision process by estimating the value of\neither being in a state or taking a certain action in a certain state. They do this through a value function or action-value\nfunction respectively. The aim of the agent is to maximize the reward they receive in an episode [3]. To achieve this,\nvalue estimations do not only consider the immediate action the agent takes but also consider all future states and actions\nthat may occur when taking the original action. Agents follow so-called policies according to which they choose which\nactions to take. Through gaining knowledge, they continuously adapt this policy in order to eventually reach an optimal\npolicy - a policy which chooses the optimal action at every step. To explore, agents have to balance between exploration\nand exploitation [3]. Exploration is the act of following suboptimal actions to attempt to find an even better policy. On\nthe other hand, exploitation is following the actions that will yield the currently highest estimated value. An agent that\nonly exploits acts _greedily_ . To ensure continual exploration so that all actions get updated given enough time, agents\ncan choose policies that are mostly greedy but choose to explore sometimes [2]. To this end, an approach like _ϵ_ -greedy\nmay be used. Here, _ϵ_ is the probability of choosing a random action and 1 _−_ _ϵ_ is the probability of acting greedily.\n\n\nA widely used modern approach to RL is temporal difference learning [4], more specifically Q-learning [2]. Q-learning\nworks with the Q-learning update formula to update its policies:\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + _α ·_\n\n[ _Rt_ +1 + _γ ·_ max _Q_ ( _St_ +1 _, a_ ) _−_ _Q_ ( _St, At_ )] (1)\n_a_\n\n\n_Q_ ( _St, At_ ) is the estimated value for any given state-action pair. The equation shows how it is updated after taking\naction _At_ from state _St_ . _Rt_ +1 represents the reward gained, max _Q_ ( _St_ +1 _, a_ ) is the value estimation of the best action\n_a_\n_a ∈_ _At_ +1 that can be taken from _St_ +1 according to the current policy, the state resulting from action _At_ . _α_ is a step-size\nparameter, also known as the _learning rate_ . Its value lies between 0 and 1 and it determines how importantly the agent\nvalues new information against the current estimate it already has. A value of 0 completely ignores new information\nwhile a value of 1 completely overrides the preexisting value estimate. _γ_ is the discount factor, weighing future rewards\nless than immediate ones. It also lies between 0 and 1, where 1 weighs the best future action equally to the current one\nand 0 does not consider it at all.\n\n\n**3** **Recursive Backwards Q-Learning**\n\n\n**3.1** **Idea**\n\n\nQ-learning agents are very widespread in modern reinforcement learning. Working free of a model allows them to\nbe generally applicable to many problems. However, some Markov decision processes take longer to solve than is\nnecessary because the agent ignores readily available information. This is noticeable in deterministic, episodic tasks\nwhere a positive reward is only given when reaching a terminal state. Before this state is reached for the first time, the\nagent appears to be moving entirely at random. Looking at figure 2, the issue becomes apparent. Even when following\nthe optimal path at every step, it still takes multiple episodes for the reward of the terminal state to propagate back to the\nstarting state. In fact, the optimal paths value estimation gets worse before it gets better. If every step has a reward of\n\n\n2\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_−_ 1, values along the optimal path get worse if they do not lead to a state that has already been reached by the terminal\nstate’s positive reward as it travels backwards.\n\n\nIn this paper, grid worlds [3] are used as an example Markov decision process for the agent to solve. Grid worlds are a\ntwo-dimensional grid in which every tile represents a state and the actions are limited to walking up, down, left or right.\nGrid worlds are useful in that they are very simple to understand and to display, they have a limited set of actions and\ntheir set of states can be as small or large as is desired. Additionally, showing the value or optimal policy for each state\nis as easy as writing a number or drawing an arrow on the corresponding tile. Actions that would place the agent off of\nthe grid simply return the state the agent is already in, but may still give a reward. Special tiles can also be defined, such\nas walls that act like the grid edge or pits that are terminal fail states because the agent cannot leave them once it has\nfallen in. Every grid world tile gives a reward of _−_ 1 to punish taking unnecessary actions in favor of taking the fastest\npath to the goal.\n\n\n_Q_ greedy policy\nw.r.t. _Q_\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -0.8\n\n\n-1\n\n\n-1\n\n\n-1 0.52\n\n\n-1\n\n\n-1\n\n\n-1 1.99\n\n\n-1\n\n\n-1\n\n\n-1 3.27\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 0.78\n\n\n-1\n\n\n-1\n\n\n-1 3.15\n\n\n-1\n\n\n-1\n\n\n-1 4.96\n\n\n-1\n\n\n-1\n\n\n-1 6.17\n\n\n-1\n\n\n-1\n\n\n-1 6.93\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 4.5\n\n\n-1\n\n\n-1\n\n\n-1 7.25\n\n\n-1\n\n\n-1\n\n\n-1 8.63\n\n\n-1\n\n\n-1\n\n\n-1 9.32\n\n\n-1\n\n\n-1\n\n\n-1 9.66\n\n\n-1\n\n\n-1\n\n\n-1 9.83\n\n\n-1\n\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n\nep. 0\n\n\nep. 1\n\n\nep. 2\n\n\nep. 3\n\n\nep. 4\n\n\nep. 5\n\n\nep. 6\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -1.6\n\n\n-1\n\n\n-1\n\n\n-1 -1.66\n\n\n-1\n\n\n-1\n\n\n-1 -1.1\n\n\n-1\n\n\n-1\n\n\n-1 -0.15\n\n\n-1\n\n\n\nFigure 2: Q-learning in a one-dimensional grid world. All Q-values are initialized as _−_ 1. Actions that lead to the\nterminal state reward 10. All other actions reward -1. The discount rate _γ_ is set to 0 _._ 9. The learning rate _α_ is set to 0 _._ 5.\nThe value of _ϵ_ is irrelevant as the only action the agent takes is _→_ .\n\n\nFigure 2 is a very simple grid world and it still takes six episodes to reach an optimal policy, even when taking the\noptimal action at every step. This problem will only grow worse and add noticeably more episodes of training for grid\nworlds that are not as trivial to solve, or even more complex tasks with more variables to consider. As stated, the issue\nis that the agent has no source of direction until it has randomly stumbled across the terminal state, its only source of\npositive rewards. The larger the state space, the longer it is blindly searching.\n\n\nReinforcement learning agents that work with a model of their environment are known as _model-based_ reinforcement\nlearning agents. They can either work with a preexisting model or, more commonly, build their own. The way they\nconstruct their models is important as having perfect knowledge of an environment is neither feasible nor sensible. In\nthe case of a grid world it is no problem, but imagining a more complex scenario like a self-driving car makes this fact\napparent. When trying to drive from one city to another, knowing every centimeter of the road with every possible place\nother cars might be on the route is resource intensive and unnecessary. Instead, an agent should attempt to simplify its\nmodel as much as possible. Instead of every bit of road, long stretches going straight can be clumped together. Similar\nsituations like a car in front slowing down can be treated the same wherever they occur.\n\n\nThe purpose of this paper is to introduce and evaluate a new type of model-based agent called the RBQL agent. The\nRBQL agent solves deterministic, episodic tasks that positively reward only the terminal state more efficiently than a\nregular Q-learning agent. It functions by building a model of its environment through exploration. When it reaches a\nterminal state, it recursively travels backwards through all previously explored states, applying a modified Q-learning\nupdate rule, the RBQL update rule. By setting the learning rate _α_ to 1, equation (1) can be simplified as such:\n\n\n3\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + 1 _·_ [ _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )]\n= _Q_ ( _St, At_ ) + _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )\n= _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n\n(2)\n\n\n\nAs can be seen in formula (2), the Q-value now exclusively depends on the reward and the discounted reward of the\nbest neighbor. Because the algorithm applies this formula starting with what is guaranteed to be the highest value of the\nenvironment and working its way away from it, the best possible neighbor for any given state is always the previously\nevaluated state.\n\n\nEvaluating all states at the end of the episode is reminiscent of dynamic programming [5] or Monte Carlo methods [3]\nand is a point of critique for those approaches. However, as will be shown in chapter 4, this evaluation method is so\neffective in RBQL that evaluating all known states in one go is still cost effective. RBQL also differs in comparison\nto dynamic programming and Monte Carlo in a few major ways. In contrast with dynamic programming, it does not\nstart out with a perfect model but has to build its own. It also propagates its reward throughout all states much more\nquickly and it uses an action-value function, not a state-value function. In contrast with Monte Carlo, it does not use\nexploring starts to guarantee exploration. It also does not only update the values that were seen in an episode. Instead,\nto facilitate exploration, it always prioritizes visiting unexplored actions, only following the greedy path when there\nare none. Because this mode of exploration still results in unexplored actions, the _ϵ_ -greedy approach is adapted for\nRBQL. Instead of exploring steps, the agent has exploration episodes. _ϵ_ serves the same purpose as before, marking\nthe probability of taking an exploration episode while 1 _−_ _ϵ_ is the probability of taking an exploitation episode. In an\nexploration episode, the agent randomly chooses an unexplored action anywhere in its model, navigates the model to\nput itself in a position to take that action and then continues to explore until it finds a known path again or the episode\nends.\n\n\nIn this paper, finding an optimal path through a randomly generated grid world maze is used as an example task for\nRBQL to solve. It is also used to compare the performance of RBQL to Q-learning.\n\n\n**3.2** **Implementation**\n\n\nTo implement RBQL [1], the Godot game engine v. 3.5 [2] was used. Godot is a free, open source engine used mainly for\nvideo game development. Its main language is GDScript, an internal language that is very similar in syntax to Python,\nthough it also supports C, C++, C# and VisualScript. Because Python is very popular for machine learning development,\nthe implementation is written in GDScript so that it is easily readable for interested parties. Godot uses a hierarchical\nstructure of objects called _nodes_ . In the implementation, there are two main nodes: the agent and the environment.\n\n\n**3.2.1** **Environment**\n\n\nThe environment is of the type `TileMap` [3] – a class designed for creating maps in grid-based environments like grid\nworlds. Before starting the first episode, the environment generates a maze given a width _w_ and a height _h_ using a\nrecursive backtracking algorithm [6]. The starting point for the agent is always (0 _,_ 0) and the goal it attempts to reach –\nthe only terminal state – is ( _w −_ 1 _, h −_ 1). To ensure that the agent has the ability to improve even after finding the goal\nin the first episode, a maze with multiple paths is needed. Because a maze generated with recursive backtracking only\nhas one path to the terminal state, a number of alternate paths are generated by taking _w · h/_ 4 random positions and a\ndirection for each position. If the position has a wall in that direction, it is removed. If not, nothing happens.\n\n\nThe environment has a function `step(state,action)` that serves as the only way for the agent to interact with it.\nThe possible moves are `UP`, `DOWN`, `LEFT` and `RIGHT` . The state is described as a coordinate of the current position. In\nGodot, the class `Vector2(x,y)` [4] is used for this purpose. `step()` checks if taking the given action from the given\nstate results in hitting a wall or not. If not, the agent moves to a new position. There are three different rewards: _−_ 1 for\nany normal tile, _−_ 5 for hitting a wall and 10 for reaching the terminal state. _−_ 1 is awarded at every step to discourage\nagents from taking unnecessary steps. Walls give _−_ 5 to quickly teach the agent to ignore them. After taking an action,\n\n\n1 The source code can be downloaded at `[https://github.com/JanDiekhoff/BackwardsLearner](https://github.com/JanDiekhoff/BackwardsLearner)`\n2 Godot v. 3.5 can be downloaded at `[https://godotengine.org/download/archive/3.5-stable/](https://godotengine.org/download/archive/3.5-stable/)`\n3 `[https://docs.godotengine.org/en/3.5/classes/class_tilemap.html](https://docs.godotengine.org/en/3.5/classes/class_tilemap.html)`\n4 `[https://docs.godotengine.org/en/3.5/classes/class_vector2.html](https://docs.godotengine.org/en/3.5/classes/class_vector2.html)`\n\n\n4\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nthe new state and reward are returned to the agent, as well as a notification if the episode has ended or not and if the\nagent has hit a wall or not.\n\n\nThe `TileMap` has a tile for each combination of having or not having a wall in each of the four directions, totaling 2 [4] or\n16 total possible tiles. Another option would be to just have a floor tile and a wall tile. However, that would make a\nmaze with an equivalent wall layout much larger, leading to a larger state set and longer solving times. To determine if a\nwall is in a certain direction, the id of each tile from 0 to 15 acts as a four-bit flag. Each direction is assigned one of the\nbits ( `UP` = 0, `RIGHT` = 1, `DOWN` = 2 and `LEFT` = 3). If the flag is set, there is a wall in the corresponding direction. The\nid for an L-shaped tile for example would be 2 [2] + 2 [3] = 12 as `DOWN` and `LEFT` have walls. The process for determining\nif the agent can move in a given direction _d_ from a position _p_ is ( _¬idp_ ) & (2 _[d]_ ), where _idp_ is the id of the tile at _p_ .\n\n\n**3.2.2** **RBQL Agent**\n\n\nThe RBQL agent is represented by a `Sprite` [5] object – a 2D image – so it can be observed while solving a maze. During\nits runtime, the agent keeps track of a few key things:\n\n\n    - A model of the environment ( `explored_map` )\n\n\n    - A list of rewards for each state-action pair ( `rewards` )\n\n\n    - The last reward received ( `reward` )\n\n\n    - A list of steps taken per episode ( `steps_taken` )\n\n\n    - The Q-table ( `qtable` )\n\n\n    - The current state ( `current_state` )\n\n\n    - The previous state ( `old_state` )\n\n\n    - The last taken action ( `action` )\n\n\nThe model of the environment starts out as an empty dictionary. Every time a new state is discovered, an entry for that state is made and initialized as an empty array. When an action is taken from this state, the resulting new state is entered into the previous state’s array at the index of the taken action’s designated number\n( `explored_map[old_state][action] = current_state` ). When hitting a wall, the “new” state is the same as the\nstate from which the action was taken. Similarly, when an action is taken, the resulting reward is saved in the rewards\nlist ( `rewards[old_state][action] = reward` ). Because the agent uses state-action values, not state values, the\ntiles are treated like nodes in a directed graph. Going from tile A to tile B might result in a different reward than when\ngoing from B to A, so when the agent learns the reward of going from A to B, it does not also learn the reward of going\nfrom B to A.\n\n\nBeing a Q-learner makes it simpler to generalize the agent for other tasks, but it causes a lot of exploratory steps and\nexploratory episodes to only explore one position at a time. If an exploration episode chooses an unexplored state-action\npair that results in hitting a wall, the exploration episode immediately ends with little information gained. To alleviate\nthis problem, the agent takes exploratory “look-ahead” steps. After entering a tile, it takes a step in every direction but\nonly saves the result if it hits a wall. This guarantees that exploratory episodes always take new paths and not just hit a\nwall and continue on the best known path.\n\n\nThe agent also keeps track of a list of the actions it has taken – except for when hitting a wall – for the case that it\nreaches a dead end, or rather a state with no unexplored neighbors. In this case, the agent would normally follow the\noptimal path until it finds a new unexplored path or reaches the terminal state. However, if the path the agent is on has\nnot been explored before it has not yet been evaluated and there is no optimal path to follow. In this case, the agent\nbacktracks by taking the opposite action of the most recent in the list, then removes it from the list, until an unexplored\ntile or an evaluated path to follow is found.\n\n\nFinally, when the terminal state is reached, the Q-table is updated with the rewards saved in `rewards` according to the\nRBQL update rule.\n```\n             qtable[state][action] =\n\n```\n\n`rewards[state][action] + discount_rate` _·_\n\n```\n             qtable[explored_map[state][action]].max()\n\n```\n\nTo do this, a copy of `explored_map` is inverted to be able to traverse it in reverse. This is then done with a breadth-first\nsearch algorithm, starting at the terminal state, and the Q-value is calculated for each state. Breadth-first search is chosen\n\n\n5 `[https://docs.godotengine.org/en/3.5/classes/class_sprite.html](https://docs.godotengine.org/en/3.5/classes/class_sprite.html)`\n\n\n5\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nover a depth-first search algorithm so that each state must only be visited once as the value is directly proportional to\nthe distance from the terminal state. With breadth-first search, each state gets the highest possible value on its first visit\nbecause it is visited from its highest possible valued neighbor.\n\n\nWhen all known states have been evaluated, a new episode begins. After the first episode, episodes are chosen to be\neither exploratory or exploitative, similar to how an _ϵ_ -greedy policy may choose exploratory actions. In an exploitative\nepisode, the agent simply follows the best path it knows, choosing at random if two states are equally good, but still\nalways exploring unknown states directly adjacent to the path above all else. In an exploratory episode, a random state\nwith an unexplored neighbor is chosen. The agent navigates to this state with the help of the A* search algorithm [7]\nand follow the unexplored path from there until it finds a known state again. This exploratory excursion may only find\none new state or it may find a vastly superior path to what was known before. _ϵ_ is decreased after every episode as\nfollows:\n_ϵ_ = `min_epsilon + (max_epsilon - min_epsilon)`\n\n\n_· e_ [(] _[−]_ `[decay_rate]` _[ ·]_ `[ current_episode]` [)]\n\n\nwhere `min_epsilon`, `max_epsilon` and `decay_rate` can be any value within a range of [0 _,_ 1] and `current_episode`\nis the number of the current episode starting with 0. Once every state is explored, the agent is guaranteed to have found\nthe optimal path, or paths, through the maze. In its entirety, the algorithm can be expressed like this:\n\n\n**Algorithm 1** Backwards Q-Learning Algorithm\n\nSet exploration_episode to false\n**while** true **do**\n\n**if** exploration_episode **then**\n\nFind unexplored path\nTravel to unexplored path\n**end if**\n**while** episode is not over **do**\n\n**if** current position has an unexplored neighbor **then**\n\nVisit unexplored neighbor\nUpdate model\nSave reward\n\n**if** no wall hit **then**\n\nSave action in action queue\n**end if**\n**else if** there is an optimal path to follow **then**\n\nVisit best neighbor\n**end if**\n**while** current pos. has no unexplored neighbor **do**\n\nBacktrack\n\n**end while**\n\n**end while**\nCreate state queue with breadth-first search\n**for** state in queue **do**\n\nApply RBQL formula\n**end for**\nSet exploration_episode to random() _<_ = _ϵ_\nApply decay to _ϵ_\n**end while**\n\n\n**3.2.3** **Q-learning agent**\n\n\nA standard Q-learning agent has been implemented in Godot as well to compare the performance of the RBQL agent to.\nThis agent is comparatively simple:\n\n\n**4** **Tests and Results**\n\n\nTo compare the performance of the two agents, three sets of tests have been done for different maze sizes: 5 _×_ 5, 10 _×_ 10\nand 15 _×_ 15. All variables have been set to common values. The decay rate is set somewhat high to account for the\nrelatively low episode amount:\n\n\n6\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**Algorithm 2** Q-Learning Algorithm\n\n\n**while** true **do**\n\n**if** random() _<_ = _ϵ_ **then**\n\nChoose random action\n\n**else**\n\nChoose greedy action\n**end if**\n\nTake action\n\nReceive new state and reward\nUpdate Q-table for old state and action\n**if** terminal state reached **then**\n\nStart new episode\n**end if**\nApply decay to _ϵ_\n**end while**\n\n\n    - _γ_ = 0 _._ 9\n\n\n    - _α_ = 0 _._ 1 (RBQL has _α_ = 1 as explained in equation (2))\n\n\n    - `min_epsilon` = 0 _._ 01\n\n\n    - `max_epsilon` = 1\n\n\n    - `decay_rate` = _−_ 0 _._ 01\n\n\nFor every maze size, each agent is given the same set of 50 randomly generated mazes. Each agent is given 25 episodes\nper maze to train. These values are chosen to offer a reasonably large sample size without requiring an enormous\namount of time to compute. Agents are compared by the number of steps taken per episode, with less steps taken being\na more desirable outcome. The step counter is increased every time `step()` is called, including the look-ahead steps of\nthe RBQL. For a sense of perspective, the best possible solution to any square maze of size _s_ [2] is 2 _s −_ 2. Assuming a\nmaze with no walls, the shortest distance between two points _A_ and _B_ can be expressed as their Manhattan distance\n_|AX −_ _BX_ _|_ + _|AY −_ _BY |_ [8]. In the corners of a square, it holds that _AX_ = _AY_ and _BX_ = _BY_, so the distance can\nbe simplified as 2 _· |A −_ _B|_ . Setting _A_ = 0 and _B_ = _s −_ 1, this further simplifies to 2 _s −_ 2. This means that while\nthe amount of states (and thereby state-action pairs) increases quadratically, the best possible solution only increases\nlinearly. This in turn means that the amount of states that are not on the optimal path that the agent has to evaluate will\noften increase drastically with the size of the maze.\n\n\nLooking at the results, a few things can be observed. First of all, the average number of steps the RBQL agent takes\nis consistently lower than the Q-learning agent in all three maze sizes. It also has much less variation in step counts,\nwhich can be seen when looking at the areas of lighter hue. The light red areas are much more sporadic and spike\nfurther away from the average. The green areas stick much closer together. If the highest two step counts per episode\nwere not removed, RBQL would also have a few small spikes. These spikes would represent exploratory episodes\nwhere a new path is explored, resulting in a higher step count. In cases where the line is flat for a long period of time, it\ncan be assumed that the optimal solution is found. This can be seen in all three figures, where both the average and\nthe min/max range become a straight line close to the minimum. Important to note is that every maze has a different\noptimal solution, hence why the average sits above the blue line which denotes the lowest possible step count in any\nmaze of this size. It can also be observed that none of the lines ever go below this boundary, as is to be expected.\n\n\nSecond, even when removing the highest two step counts per episode, many of the Q-learning agent’s step counts are so\nlarge that scaling the graphs to fit them makes the RBQL agent’s data and the lower boundary difficult to see in the\ngraphs for the larger mazes. The highest step count values that have not been cut are 858 steps in figure 3, 7,585 in\nfigure 4 and 21,147 in figure 5, while the highest in total are 3,716 steps in figure 3, 20,553 in figure 4 and 26,315 in\nfigure 5.\n\n\nThird, it is interesting to see how the differences in average step counts evolve with the grid size. Table 1 shows this\ndifference in the first and last episode. The difference between the average step counts in the last episode especially is\nstriking, as it is close to doubling from each size to the next. Further, looking at the improvement of each agent as seen\nin table 2, one can see that the factor by which RBQL improves massively increases the bigger the maze becomes while\nthe Q-learner only slightly improves its performance in comparison. Additionally, most of the improvement of RBQL is\ndone in the first two episodes, while the Q-learner has a more gradual learning curve.\n\n\n7\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**1** _**,**_ **000**\n\n\n**900**\n\n\n**800**\n\n\n**700**\n\n\n**600**\n\n\n**500**\n\n\n**400**\n\n\n**300**\n\n\n**200**\n\n\n**100**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 3: Number of steps taken to find the goal in a randomly generated grid world maze of size 5 _×_ 5. The blue line is\nthe minimum step threshold for any maze of this size. The light red area shows the range of Q-learning agent’s highest\nand lowest step count, excluding the highest and lowest two. The red line shows the average performance. Similarly, the\nlight green area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest\ntwo, and the green line shows the average performance.\n\n\n8\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-7-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**8** _**,**_ **000**\n\n\n**6** _**,**_ **000**\n\n\n**5** _**,**_ **000**\n\n\n**4** _**,**_ **000**\n\n\n**3** _**,**_ **000**\n\n\n**2** _**,**_ **000**\n\n\n**1** _**,**_ **000**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 4: Number of steps taken to find the goal in a randomly generated grid world maze of size 10 _×_ 10. The light\nred area shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two.\nThe red shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n9\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-8-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 5: Number of steps taken to find the goal in a randomly generated grid world maze of size 15 _×_ 15. The light red\narea shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two. The\nred line shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n10\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-9-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nTable 1: Difference in average step counts of the Q-learner and RBQL. The difference expresses how many times more\nsteps the Q-learner took compared to RBQL.\n\n\nGrid size Q-learner steps RBQL steps Difference\n**Episode 0**\n5 _×_ 5 278.06 191.84 1.45\n\n10 _×_ 10 3,308.46 843.52 3.92\n15 _×_ 15 7,180.98 1,965 3.65\n**Episode 24**\n5 _×_ 5 49.14 9.62 5.11\n\n10 _×_ 10 281.44 23.68 11.89\n\n15 _×_ 15 778.68 35.96 21.65\n\n\nLastly, the RBQL agent seems to find an optimal policy at around episode 4 for the 5 _×_ 5, episode 6 for the 10 _×_ 10 and\nepisode 10 for the 15 _×_ 15 grid. As the previous figures show, the Q-learning agent does not come close to similarly\nlow step counts and therefore does not reach an optimal policy at all with the same amount of training.\n\n\nTable 2: Difference in average step counts of the Q-learner and RBQL. Improvement shows the factor by which the\namount of steps is reduced from episode 0 to 24.\n\n\nGrid size Steps in episode 0 Steps in episode 24 Improvement\n**Q-learning agent**\n5 _×_ 5 278.06 49.14 5.66\n\n10 _×_ 10 3,308.46 281.44 11.76\n15 _×_ 15 7,180.98 778.68 9.22\n**RBQL agent**\n5 _×_ 5 191.84 9.62 19.94\n\n10 _×_ 10 843.52 23.68 35.62\n\n15 _×_ 15 1,965 35.96 90.76\n\n\nTo further show RBQL’s efficiency, it has also been tested under the same parameters in a grid of size 50 _×_ 50. The\nresults can be seen in figure 6. This test is done to demonstrate that even such a large maze can be explored by RBQL.\nAs with the previous examples, by far the largest policy improvement still happens in the first episode. With mazes of\nsuch a large size, a lot more spikes in step counts are seen in later episodes because there are more states to explore. The\ndifference in average step counts goes from 20,811.08 in episode 0 to 344.9 in episode 24, an improvement by a factor\nof 60.34. This is worse than the improvement in the 15 _×_ 15 mazes, but still almost double that of the 10 _×_ 10 mazes.\n\n\n**5** **Discussion**\n\n\nThis chapter explores the practicality of using this algorithm to solve other Markov decision processes. It discusses\nwhich parts of the implementation are and are not specific to the problem of fastest path through a maze, which\nimprovements can be made to make it more applicable for other problems and showcases further points for research in\nthis field. The constraints given in this paper are that the agent will attempt to solve deterministic, episodic tasks with a\nsingle terminal state as its only source of positive rewards. This chapter also discusses which of these constraints can be\ndismissed.\n\n\nThere are a few parts of the implementation as presented in chapter 3.2 that are only applicable to this specific problem.\nThis is not necessarily a bad thing, as the purpose of the RBQL agent is to utilize knowledge of its environment. As a\nresult of this, the only parts that cannot be directly adapted for other problems are the way the agent builds its model.\nIn the grid world maze, it can assume that every state has the same actions it can take and has a neighboring state in\neach direction (though it may sometimes be itself). Further, every action always has an opposite action, going up can\nalways be undone by going down for example. These assumptions allow it to easily build a model of the grid world\nand influence its policy in how it further explores it. They allow the agent to take steps in each direction to check for\nwalls and they allow the agent to backtrack when it is stuck in a dead end. These assumptions cannot be guaranteed\nfor other Markov decision processes or even for grid worlds with more complex behavior like a wind tunnel that if\nwalked through also pushes the agent one tile in the direction the wind is traveling. The way in which the agent builds a\n\n\n11\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**3**\n\n\n**2** _**.**_ **8**\n\n\n**2** _**.**_ **6**\n\n\n**2** _**.**_ **4**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 6: Number of steps taken to find the goal in a randomly generated grid world maze of size 50 _×_ 50. The light\ngreen area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest two,\nand the green line shows the average performance.\n\n\n12\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-11-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nmodel has to either be designed for each environment individually or it has to be abstracted so that it is more broadly\napplicable. Finding such an approach to model building is one area of improvement for RBQL. Importantly though,\nnone of these assumptions are required for the agent to function. Backtracking, opposite steps and the same actions for\nevery state simply make the implementation easier and more efficient. As long as no path of a directed graph would\ncause the agent to be stuck with no way to reach a terminal state, it can be explored and evaluated.\n\n\nAnother improvement to the way the implementation builds its model is to simplify it as far as possible. As the amount\nof states directly influences how long a problem takes to solve, RBQL will become more efficient the more it can\nremove unnecessary states. Currently, every position has its own state. If the agent could detect “hallways” – tiles with\nparallel walls – they could be removed without problem in favor of directly connecting the two tiles at either side of the\nhallway – only the negative rewards for the length of the hallway would have to be implemented into the model. Further,\nif there is a non-forking path that leads into a dead end, the entire path could be treated as a wall and ignored entirely.\nThis would leave only the starting state, terminal state, turns and forking paths to evaluate. Both of these additions leave\nthe key part of the algorithm, traversing the model backwards and applying the RBQL update formula, untouched.\n\n\nRBQL can be easily adapted to include multiple terminal states with the same or different rewards and this is already\nsupported by the implementation. There are two possible ways to do this. First is to create an imaginary state that\nall terminal states lead into from which the backtracking always starts. Second is to remember all terminal states and\nbacktrack from each of them. The first option is much more efficient as each state still only gets evaluated once while\nthe second version avoids having to tamper with the model.\n\n\nFinally, RBQL could be adapted to work in non-deterministic environments. To reiterate, deterministic means that a\nstate-action pair always yields the same state-reward pair. If the agent could, while building its model, also estimate the\ntransition probabilities of a state-action pair to a new state, RBQL could still be used to evaluate the states. The RBQL\nupdate rule can be generalized to\n\n\n\n_Q_ ( _St, At_ ) _←_ �\n\n_s∈St_ +1\n\n\n\n( _Rs_ + _γ_ max _Q_ ( _s, a_ )) _· p_ (3)\n_a_\n� �\n\n\n\nwhere _St_ +1 is the set of possible states when taking _At_ from _St_, _p_ is the probability of reaching _s_ when taking _At_ from\n_St_ and _Rs_ is the reward of reaching _s_ . In a deterministic environment, _St_ +1 only consists of one state with _p_ = 1,\nnegating these additions. Whether RBQL would be as effective in non-deterministic environments as in deterministic\nenvironments is something to be explored in further studies.\n\n\nThe only constraint on the algorithm that cannot easily be circumvented is its episodic nature. Because the agent relies\non a terminal state from which to propagate the rewards backwards from, a continuous task implementation seems\nimpossible to implement.\n\n\n**6** **Conclusion**\n\n\nThis paper has introduced recursive backwards Q-learning, a model-based reinforcement learning algorithm that\nevaluates all known state-action pairs of the model at the end of each episode with the Q-learning update rule. It has\nalso shown how recursive backwards Q-learning relates to, adapts and improves on them. This paper has presented\nan implementation of recursive backwards Q-learning in the Godot game engine to test its performance. Through\nmultiple tests, it has been shown to be superior in finding the shortest path through a randomly generated grid world\nmaze. It has been argued that this algorithm could be adapted to solve other deterministic, episodic tasks more quickly\nthan Q-learning. Further, it has given avenues for further research in adapting recursive backwards Q-learning for\nnon-deterministic problems.\n\n\n**References**\n\n\n[1] Richard Bellman, “A markovian decision process,” _Journal of Mathematics and Mechanics_, vol. 6, no. 5, pp. 679–\n684, 1957. [Online]. Available: `[http://www.jstor.org/stable/24900506](http://www.jstor.org/stable/24900506)` .\n\n[2] Christopher John Cornish Hellaby Watkins, “Learning from delayed rewards,” 1989.\n\n[3] Richard S Sutton and Andrew G Barto, _Reinforcement learning: An introduction_ . MIT press, 2018.\n\n[4] Richard S. Sutton, “Learning to predict by the methods of temporal differences,” _Machine Learning_, vol. 3, no. 1,\npp. 9–44, 1988. DOI: `[10.1007/bf00115009](https://doi.org/10.1007/bf00115009)` .\n\n[5] Richard Bellman, “Dynamic programming,” _Princeton, USA: Princeton University Press_, vol. 1, no. 2, p. 3, 1957.\n\n[6] Peter Gabrovšek, “Analysis of maze generating algorithms,” _IPSI Transactions on Internet Research_, vol. 15,\nno. 1, pp. 23–30, 2019.\n\n\n13\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n[7] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael, “A formal basis for the heuristic determination of minimum\ncost paths,” _IEEE Transactions on Systems Science and Cybernetics_, vol. 4, no. 2, pp. 100–107, 1968. DOI:\n`[10.1109/TSSC.1968.300136](https://doi.org/10.1109/TSSC.1968.300136)` .\n\n[8] Eugene F Krause, “Taxicab geometry,” _The Mathematics Teacher_, vol. 66, no. 8, pp. 695–706, 1973.\n\n\n14\n\n\n",
          "ranking": null,
          "is_open_access": false,
          "user_provided": true,
          "pdf_path": "output/literature/user_2404.15822v1/user_2404.15822v1.pdf"
        },
        "chunk_text": "Our approach achieves 94.2% precision and 89.7% recall on the benchmark dataset, outperforming baseline methods.",
        "chunk_index": 0
      },
      "summary": "Our approach achieves 94.2% precision and 89.7% recall on the benchmark dataset, outperforming baseline methods.",
      "vector_score": 0.7440000000000001,
      "llm_score": 0.93,
      "combined_score": 0.93,
      "source_query": "mock_query_results"
    },
    {
      "chunk": {
        "chunk_id": "mock_11",
        "paper": {
          "id": "acda55ebdf39c6634e89a9730ff7d963471f2b0a",
          "title": "Expected Eligibility Traces",
          "published": "2020-07-03",
          "authors": [
            "H. V. Hasselt",
            "Sephora Madjiheurem",
            "Matteo Hessel",
            "David Silver",
            "André Barreto",
            "Diana Borsa"
          ],
          "summary": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state.\nIn this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that some- times substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
          "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17200/17007",
          "doi": "10.1609/aaai.v35i11.17200",
          "fields_of_study": [
            "Computer Science",
            "Mathematics"
          ],
          "venue": "AAAI Conference on Artificial Intelligence",
          "citation_count": 41,
          "bibtex": "@Article{Hasselt2020ExpectedET,\n author = {H. V. Hasselt and Sephora Madjiheurem and Matteo Hessel and David Silver and André Barreto and Diana Borsa},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Expected Eligibility Traces},\n volume = {abs/2007.01839},\n year = {2020}\n}\n",
          "markdown_text": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)\n\n# **Expected Eligibility Traces**\n\n\n**Hado van Hasselt** [1] **, Sephora Madjiheurem** [2] **, Matteo Hessel** [1]\n\n**David Silver** [1] **, Andr´e Barreto** [1] **, Diana Borsa** [1]\n\n1 DeepMind\n2 University College London, UK\n\n\n\n**Abstract**\n\n\nThe question of how to determine which states and actions\nare responsible for a certain outcome is known as the _credit_\n_assignment problem_ and remains a central research question\nin reinforcement learning and artificial intelligence. _Eligibil-_\n_ity traces_ enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not\nto counterfactual sequences that could also have led to the\ncurrent state. In this work, we introduce _expected eligibility_\n_traces_ . Expected traces allow, with a single update, to update\nstates and actions that could have preceded the current state,\neven if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous)\ntraces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide\na way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which\nensures that the resulting algorithm is a strict generalisation\nof TD( _λ_ ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.\n\n\n**Motivation and Summary**\n\n\nAppropriate credit assignment has long been a major research\ntopic in artificial intelligence (Minsky 1963). To make effective decisions and understand the world, we need to accurately associate events, like rewards or penalties, to relevant\nearlier decisions or situations. This is important both for learning accurate predictions, and for making good decisions.\n_Temporal credit assignment_ can be achieved with repeated\ntemporal-difference (TD) updates (Sutton 1988). One-step\nTD updates propagate information slowly: when a surprising value is observed, the state immediately preceding it is\nupdated, but no earlier states or decisions are updated. _Multi-_\n_step_ updates (Sutton 1988; Sutton and Barto 2018) propagate\ninformation faster over longer temporal spans, speeding up\ncredit assignment and learning. Multi-step updates can be\nimplemented online using _eligibility traces_ (Sutton 1988),\nwithout incurring significant additional computational expense, even if the time spans are long; these algorithms have\ncomputation that is independent of the temporal span of the\npredictions (van Hasselt and Sutton 2015).\n\n\nCopyright c _⃝_ 2021, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\nMDP True value TD(0) TD(λ) ET(λ)\n\n\nFigure 1: A comparison of TD(0), TD( _λ_ ), and the new\nexpected-trace algorithm ET( _λ_ ) (with _λ_ = 0 _._ 9). The MDP\nis illustrated on the left. Each episode, the agent moves randomly down and right from the top left to the bottom right,\nwhere any action terminates the episode. Reward on termination are +1 with probability 0.2, and zero otherwise—all\nother rewards are zero. We plot the value estimates after the\nfirst positive reward, which occurred in episode 5. We see\na) TD(0) only updated the last state, b) TD( _λ_ ) updated the\ntrajectory in this episode, and c) ET( _λ_ ) additionally updated\ntrajectories from earlier (unrewarding) episodes.\n\n\nTraces provide temporal credit assignment, but do not assign credit _counterfactually_ to states or actions that _could_\nhave led to the current state, but did not do so this time.\nCredit will eventually trickle backwards over the course of\nmultiple visits, but this can take many iterations. As an example, suppose we collect a key to open a door, which leads\nto an unexpected reward. Using standard one-step TD learning, we would update the state in which the door opened.\nUsing eligibility traces, we would also update the preceding\ntrajectory, including the acquisition of the key. But we would\nnot update other sequences that _could_ have led to the reward,\nsuch as collecting a spare key or finding a different entrance.\nThe problem of credit assignment to counterfactual states\nmay be addressed by learning a model, and using the model\nto propagate credit (cf. Sutton 1990; Moore and Atkeson\n1993; Chelu, Precup, and van Hasselt 2020); however, it\nhas often proven challenging to construct and use models\neffectively in complex environments (cf. van Hasselt, Hessel,\nand Aslanides 2019).\nWe introduce a new approach to counterfactual credit assignment, based on the concept of _expected eligibility traces_ .\nWe present a family of algorithms, which we call ET( _λ_ ), that\nuse expected traces to update their predictions. We analyse\nthe nature of these expected traces, and illustrate their benefits empirically in several settings—see Figure 1 for a first\n\n\n\n9997\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-1.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-3.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-4.png)\n\n\nillustration. We introduce a bootstrapping mechanism that\nprovides a spectrum of algorithms between standard eligibility traces and expected eligibility traces, and also discuss\nways to apply these ideas with deep neural networks. Finally,\nwe discuss possible extensions and connections to related\nideas such as successor features.\n\n\n**Background**\nSequential decision problems can be modelled as Markov\ndecision processes [1] (MDP) ( _S, A, p_ ) (Puterman 1994), with\nstate space _S_, action space _A_, and a joint transition and\nreward distribution _p_ ( _r, s_ _[′]_ _|s, a_ ). An agent selects actions according to its policy _π_, such that _At ∼_ _π_ ( _·|St_ ) where _π_ ( _a|s_ )\ndenotes the probability of selecting _a_ in _s_, and observes random rewards and states generated according to the MDP, resulting in trajectories _τt_ : _T_ = _{St, At, Rt_ +1 _, St_ +1 _, . . ., ST }_ .\nA central goal is to predict _returns_ of future discounted rewards (Sutton and Barto 2018)\n\n\n_Gt ≡_ _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _Rt_ +2 + _γt_ +1 _γt_ +2 _Rt_ +3 + _. . ._\n\n\n\n=\n\n\n\n_T_\n� _γt_ [(] +1 _[i][−]_ [1)] _[R][t]_ [+] _[i][,]_\n\n\n_i_ =1\n\n\n\nwhere _T_ is for instance the time the current episode terminates or _T_ = _∞_, and where _γt ∈_ [0 _,_ 1] is a (possibly constant) discount factor and _γt_ [(] _[n]_ [)] = [�] _[n]_ _k_ =0 _[−]_ [1] _[γ][t]_ [+] _[k]_ [, and] _[ γ]_ _t_ [(0)] = 1.\nThe value _vπ_ ( _s_ ) = E [ _Gt|St_ = _s, π_ ] of state _s_ is the expected return for a policy _π_ . Rather than writing the return as\na random variable _Gt_, it will be convenient to instead write it\nas an explicit function _G_ ( _τ_ ) of the random trajectory _τ_ . Note\nthat _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ).\nWe approximate the value with a function _v_ **w** ( _s_ ) _≈_ _vπ_ ( _s_ ).\nThis can for instance be a table—with a single separate entry\n_w_ [ _s_ ] for each state—a linear function of some input features,\nor a non-linear function such as a neural network with parameters **w** . The goal is to iteratively update **w** with\n\n\n**w** _t_ +1 = **w** _t_ + ∆ **w** _t_\n\n\nsuch that _v_ **w** approaches the true _vπ_ . Perhaps the simplest\nalgorithm to do so is the Monte Carlo (MC) algorithm\n\n\n∆ **w** _t ≡_ _α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._\n\n\nMonte Carlo is effective, but has high variance, which can\nlead to slow learning. TD learning (Sutton 1988; Sutton and\nBarto 2018) instead replaces the return with the current estimate of its expectation _v_ ( _St_ +1) _≈_ _G_ ( _τt_ +1: _T_ ), yielding\n\n\n∆ **w** _t ≡_ _αδt∇_ **w** _v_ **w** ( _St_ ) _,_ (1)\nwhere _δt ≡_ _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ ) _,_\n\n\nwhere _δt_ is called the temporal-difference (TD) error. We\ncan interpolate between these extremes, for instance with\n_λ_ -returns which smoothly mix values and sampled returns:\n\n\n_G_ _[λ]_ ( _τt_ : _T_ ) = _Rt_ +1+ _γt_ +1�(1 _−λ_ ) _v_ **w** ( _St_ +1)+ _λG_ _[λ]_ ( _τt_ +1: _T_ )� _._\n\n\n‘Forward view’ algorithms, like the MC algorithm, use returns\nthat depend on future trajectories and need to wait until the\n\n\n1The ideas in this paper extend naturally to POMDPs (cf. **?** ).\n\n\n\nend of an episode to construct their updates, which can take a\nlong time. Conversely, ‘backward view’ algorithms rely only\non past experiences and can update their predictions online,\nduring an episode. Such algorithms build an _eligibility trace_\n(Sutton 1988; Sutton and Barto 2018). An example is TD( _λ_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ _t,_ with _**e**_ _t_ = _γtλ_ _**e**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ ) _,_\n\n\nwhere _**e**_ _t_ is an accumulating eligibility trace. This trace can\nbe viewed as a function _**e**_ _t ≡_ _**e**_ ( _τ_ 0: _t_ ) of the trajectory of past\ntransitions. The TD update in (1) is known as TD(0), because\nit corresponds to using _λ_ = 0. TD( _λ_ = 1) corresponds to an\nonline implementation of the MC algorithm. Other variants\nexist, using other kinds of traces, and equivalences have been\nshown between these algorithms and their forward views that\nuse _λ_ -returns: these backward-view algorithms converge to\nthe same solution as the corresponding forward view, and can\nin some cases yield equivalent weight updates (Sutton 1988;\nvan Seijen and Sutton 2014; van Hasselt and Sutton 2015).\n\n\n**Expected Traces**\n\n\nThe main idea of this paper is to use the concept of an _ex-_\n_pected eligibility trace_, defined as\n\n\n_**z**_ ( _s_ ) _≡_ E [ _**e**_ _t | St_ = _s_ ] _,_\n\n\nwhere the expectation is over the agent’s policy and the MDP\ndynamics. We introduce a concrete family of algorithms,\nwhich we call ET( _λ_ ) and ET( _λ_, _η_ ), that learn expected traces\nand use them in value updates. We analyse these algorithms\ntheoretically, describe specific instances, and discuss computational and algorithmic properties.\n\n\n**ET(** _λ_ **)**\n\n\nWe propose to learn approximations _**zθ**_ ( _s_ ) _≈_ _**z**_ ( _s_ ), with parameters _**θ**_ _∈_ R _[d]_ (e.g., the weights of a neural network). One\nway to learn _**zθ**_ is by updating it toward the instantaneous\ntrace _**e**_ _t_, by minimizing an empirical loss _L_ ( _**e**_ _t,_ _**zθ**_ ( _St_ )). For\ninstance, _L_ could be a component-wise squared loss, optimized with stochastic gradient descent:\n\n\n_**θ**_ _t_ +1 = _**θ**_ _t_ + ∆ _**θ**_ _t,_ where (2)\n\n\n1\n\n∆ _**θ**_ _t_ = _−β_ _[∂]_\n\n_∂_ _**θ**_ 2 [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))] _[⊤]_ [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))]\n\n= _β_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] ( _**e**_ _t −_ _**zθ**_ ( _St_ )) _,_ (3)\n\n_∂_ _**θ**_\n\n\nwhere _[∂z]_ _**[θ]**_ _∂_ [(] _**θ**_ _[S][t]_ [)] is a _|_ _**θ**_ _| × |_ _**e**_ _|_ Jacobian [2] and _β_ is a step size.\n\nThe idea is then to use _**zθ**_ ( _s_ ) _≈_ E [ _**e**_ _t | St_ = _s_ ] in place\nof _**e**_ _t_ in the value update, which becomes\n\n\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (4)\n\n\nWe call this ET( _λ_ ). Below, we prove that this update can\nbe unbiased and can have lower variance than TD( _λ_ ). Algorithm 1 shows pseudo-code for a concrete instance of ET( _λ_ ).\n\n\n2The Jacobian-vector product can efficiently be computed (e.g.,\nvia auto-differentiation) with computational requirements that are\ncomparable to the computation of the loss.\n\n\n\n9998\n\n\n\n\n**Algorithm 1** ET( _λ_ )\n\n\n1: initialise **w**, _**θ**_\n2: **for** _M_ episodes **do**\n3: initialise _**e**_ = **0**\n\n4: observe initial state _S_\n5: **repeat** for each step in episode _m_\n6: generate _R_ and _S_ _[′]_\n\n7: _δ ←_ _R_ + _γv_ **w** ( _S_ _[′]_ ) _−_ _v_ **w** ( _S_ )\n8: _**e**_ _←_ _γλ_ _**e**_ + _∇_ **w** _v_ **w** ( _S_ )\n\n9: _**θ**_ _←_ _**θ**_ + _β_ _[∂]_ _**[z]**_ _∂_ _**[θ]**_ _**θ**_ [(] _[S]_ [)] ( _**e**_ _−_ _**zθ**_ ( _S_ ))\n\n10: **w** _←_ **w** + _αδ_ _**zθ**_ ( _S_ )\n11: **until** _S_ is terminal\n\n12: **end for**\n\n13: **Return w**\n\n\n**Interpretation and ET(** _λ, η_ **)**\n\nWe can interpret TD(0) as taking the MC update and replacing the return from the subsequent state, which is a function\nof the future trajectory, with a state-based estimate of its expectation: _v_ **w** ( _St_ +1) _≈_ E [ _G_ ( _τt_ +1: _T_ ) _|St_ +1 ]. This becomes\nmost clear when juxtaposing the updates:\n\n\n_α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _,_ (MC)\n_α_ ( _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._ (TD)\n\n\nTD( _λ_ ) also uses a function of a trajectory: the trace _**e**_ _t_ . We\npropose replacing this as well with a function of state: the\nexpected trace _**zθ**_ ( _St_ ) _≈_ E [ _**e**_ ( _τ_ 0: _t_ ) _|St_ ]. Again juxtaposing:\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ ( _τ_ 0: _t_ ) _,_ (TD( _λ_ ))\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (ET( _λ_ ))\n\n\nWe can interpolate smoothly between MC and TD(0) via\n_λ_ . This is often useful to trade off variance of the return with\npotential bias of the value estimate. For instance, we might\nnot have access to the true state _s_, and might instead have to\nrely on features **x** ( _s_ ). Then we cannot always represent or\nlearn the true values _v_ ( _s_ )—for instance different states may\nbe aliased (Whitehead and Ballard 1991).\nSimilarly, when moving from TD( _λ_ ) to ET( _λ_ ) we replaced\na trajectory-based trace with a state-based estimate. This\nmight induce bias and, again, we can smoothly interpolate by\nusing a recursively defined mixture trace _**y**_ _t_, as defined as [3]\n\n\n_**y**_ _t_ = (1 _−_ _η_ ) _**zθ**_ ( _St_ ) + _η_ � _γtλ_ _**y**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ )� _._ (5)\n\n\nThis recursive usage of the estimates _**zθ**_ ( _s_ ) at previous states\nis analogous to bootstrapping on future state values when\nusing a _λ_ -return, with the important difference that the arrow\nof time is opposite. This means we do not first have to convert\nthis into a backward view: the quantity can already be computed from past experience directly. We call the algorithm\nthat uses this mixture trace ET( _λ_, _η_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**y**_ ( _St_ ) _._ (ET( _λ_, _η_ ))\n\n\n3While _**y**_ _t_ depends on both _η_ and _λ_ we leave this dependence\nimplicit, as is conventional for traces.\n\n\n\nNote that if _η_ = 1 then _**y**_ _t_ = _**e**_ _t_ equals the instantaneous\ntrace: ET( _λ_, 1) is equivalent to TD( _λ_ ). If _η_ = 0 then _**y**_ _t_ = _**z**_ _t_\nequals the expected trace; the algorithm introduced earlier\nas ET( _λ_ ) is equivalent to ET( _λ_, 0). By setting _η ∈_ (0 _,_ 1), we\ncan smoothly interpolate between these extremes.\n\n\n**Theoretical Analysis**\n\nWe now analyse the new ET algorithms theoretically. First\nwe show that if we use _**z**_ ( _s_ ) directly and _s_ is Markov then the\nupdate has the same expectation as TD( _λ_ ) (though possibly\nwith lower variance), and therefore also inherits the same\nfixed point and convergence properties.\n\n\n**Lemma 1.** _If s is Markov, then_\n\n\nE [ _δt_ _**e**_ _t | St_ = _s_ ] = E [ _δt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] _._\n\n\n_Proof._ In Appendix .\n\n\n**Proposition 1.** _Let_ _**e**_ _t be any trace vector, updated in any_\n_way. Let_ _**z**_ ( _s_ ) = E [ _**e**_ _t | St_ = _s_ ] _. Consider the ET(λ) algo-_\n_rithm_ ∆ **w** _t_ = _αtδt_ _**z**_ ( _St_ ) _. For all Markov states s the expec-_\n_tation of this update is equal to the expected update under_\n_instantaneous trace_ _**e**_ _t, and its variance is lower or equal:_\n\n\nE [ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] = E [ _αtδt_ _**e**_ _t|St_ = _s_ ] _and_\nV[ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] _≤_ V[ _αtδt_ _**e**_ _t|St_ = _s_ ] _,_\n\n\n_where the second inequality holds component-wise for the_\n_update vector, and is strict when_ V[ _**e**_ _t|St_ ] _>_ 0 _._\n\n\n_Proof._ We have\n\n\nE [ _αtδt_ _**e**_ _t | St_ = _s_ ]\n= E [ _αtδt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] (Lemma 1)\n= E [ _αtδt | St_ = _s_ ] _**z**_ ( _s_ )\n= E [ _αtδt_ _**z**_ ( _St_ ) _| St_ = _s_ ] _._ (6)\n\n\nDenote the _i_ -th component of _**z**_ ( _St_ ) by _zt,i_ and the _i_ -th\ncomponent of _**e**_ _t_ by _et,i_ . Then, we also have\n\n\nE � ( _αtδtzt,i_ ) [2] _|St_ = _s_ � = E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � _zt,i_ [2]\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E [ _et,i|St_ = _s_ ] [2]\n\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ ��E � _e_ [2] _t,i_ _[|][S][t]_ [=] _[ s]_ � _−_ V[ _et,i|St_ = _s_ ]�\n\n_≤_ E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E � _e_ [2] _t,i_ _[|][ S][t]_ [=] _[ s]_ �\n\n= E � ( _αtδtet,i_ ) [2] _| St_ = _s_ � _,_\n\n\nwhere the last step used the fact that _s_ is Markov, and the inequality is strict when V[ _et,i|St_ ] _>_ 0. Since the expectations\nare equal, as shown in (6), the conclusion follows.\n\n\n**Interpretation** Proposition 1 is a strong result: it holds for\nany trace update, including accumulating traces (Sutton 1984,\n1988), replacing traces (Singh and Sutton 1996), dutch traces\n(van Seijen and Sutton 2014; van Hasselt, Mahmood, and\nSutton 2014; van Hasselt and Sutton 2015), and future traces\nthat may be discovered. It implies convergence of ET( _λ_ )\nunder the same conditions as TD( _λ_ ) (Dayan 1992; Peng 1993;\n\n\n\n9999\n\n\n\n\nTsitsiklis 1994) with lower variance when V[ _**e**_ _t|St_ ] _>_ 0,\nwhich is the common case.\nNext, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a\nlearned approximation _**z**_ _t_ ( _s_ ) _≈_ _**z**_ ( _s_ ) that relies solely on\nobserved experience.\n\n**Proposition 2.** _Let_ _**e**_ _t an instantaneous trace vector. Then_\n1 _nt_ ( _s_ )\n_let_ _**z**_ _t_ ( _s_ ) _be the empirical mean_ _**z**_ _t_ ( _s_ ) = _nt_ ( _s_ ) � _i_ _**e**_ _t_ _[s]_ _i_ _[,]_\n_where t_ _[s]_ _i_ _[denotes past times when we have been in state]_\n_s, that is St_ _[s]_ _i_ [=] _[ s][, and][ n][t]_ [(] _[s]_ [)] _[ is the number of visits to][ s]_\n_in the first t steps. Consider the expected trace algorithm_\n**w** _t_ +1 = **w** _t_ + _αtδt_ _**z**_ _t_ ( _St_ ) _. If St is Markov, the expectation of_\n_this update is equal to the expected update with instantaneous_\n_traces_ _**e**_ _t, while attaining a potentially lower variance:_\n\n\nE [ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] = E [ _αtδt_ _**e**_ _t | St_ ] _and_\nV[ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] _≤_ V[ _αtδt_ _**e**_ _t | St_ ] _,_\n\n\n_where the second inequality holds component-wise. The in-_\n_equality is strict when_ V[ _**e**_ _t | St_ ] _>_ 0 _._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** Proposition 2 mirrors Proposition 1 but, importantly, covers the case where we estimate the expected\ntraces from data, rather than relying on exact estimates. This\nmeans the benefits extend to this pure learning setting. Again,\nthe result holds for any trace update. The inequality is typically strict when the path leading to state _St_ = _s_ is stochastic\n(due to environment or policy).\nNext we consider what happens if we do not have Markov\nstates and instead have to rely on, possibly non-Markovian,\nfeatures **x** ( _s_ ). We then have to pick a function class and for\nthe purpose of this analysis we consider linear expected traces\n_**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) and values _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), as convergence for non-linear values can not always be assured even\nfor standard TD( _λ_ ) (Tsitsiklis and Van Roy 1997), without\nadditional assumptions (e.g., Ollivier 2018; Brandfonbrener\nand Bruna 2020).\n\n**Proposition 3.** _When using approximations z_ **Θ** ( _s_ ) = **Θx** ( _s_ )\n_and v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ) _then, if_ (1 _−_ _η_ ) **Θ** + _η_ I _is non-singular,_\n_ET(λ, η) has the same fixed point as TD(λη)._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** This result implies that linear ET( _λ_, _η_ ) converges under similar conditions as linear TD( _λ_ _[′]_ ) for _λ_ _[′]_ = _λ·η_ .\nIn particular, when **Θ** is non-singular, using the approximation _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) in ET( _λ_, 0) = ET( _λ_ ) implies convergence to the fixed point of TD(0).\nThough ET( _λ_, _η_ ) and TD( _λη_ ) have the same fixed point,\nthe algorithms are not equivalent. In general, their updates\nare not the same. Linear approximations are more general\nthan tabular functions (which are linear functions of a indicator vector for the current state), and we have already seen\nin Figure 1 that ET( _λ_ ) behaves quite differently from both\nTD(0) and TD( _λ_ ), and we have seen its variance can be lower\nin Propositions 1 and 2. Interestingly, **Θ** resembles a preconditioner that speeds up the linear semi-gradient TD update,\n\n\n10000\n\n\n\nepisode 5\n1st reward\n\n\n\nepisode 12\n2nd reward\n\n\n\nepisode 100\n20 rewards\n\n\n\nepisode 1K\n~200 rewards\n\n\n\nepisode 10K\n~2K rewards\n\n\n\nFigure 2: In the same setting as Figure 1, we show later value\nestimates after more rewards have been observed. TD(0)\nlearns slowly but steadily, TD( _λ_ ) learns faster but with higher\nvariance, and ET( _λ_ ) learns both fast and stable.\n\n\nsimilar to how second-order optimisation algorithms (Amari\n1998; Martens 2016) precondition the gradient updates.\n\n\n**Empirical Analysis**\n\nFrom the insights above, we expect that ET( _λ_ ) yields lower\nprediction errors because it has lower variance and aggregates information across episodes better. In this section we\nempirically investigate expected traces in several experiments.\nWhenever we refer to ET( _λ_ ), this is equivalent to ET( _λ_, 0).\n\n\n\n**An Open World**\n\nFirst consider the grid world depicted in Figure 1. The agent\nrandomly moves right or down (excluding moves that would\nhit a wall), starting from the top-left corner. Any action in the\nbottom-right corner terminates the episode with +1 reward\nwith probability 0 _._ 2, and 0 otherwise. All other rewards are 0.\nFigure 1 shows value estimates after the first positive reward, which occurred in the fifth episode. TD(0) updated a\nsingle state, TD( _λ_ ) updated earlier states in that episode, and\nET( _λ_ ) additionally updated states from previous episodes.\nFigure 2 additionally shows value estimates after the\nsecond reward (which occurred in episode 12), and after\nroughly 20, 200, and 2000 rewards (or 100, 1000, and 10 _,_ 000\nepisodes, respectively). ET( _λ_ ) converged faster than TD(0),\nwhich propagated information slowly, and faster than TD( _λ_ ),\nwhich exhibited higher variance. All step sizes decayed as\n_α_ = _β_ = ~~�~~ 1 _/k_, where _k_ is the current episode number.\n\n\n**A Multi-Chain**\n\nWe now consider the multi-chain shown in Figure 3. We\nfirst compare TD( _λ_ ) and ET( _λ_ ) with tabular values on various variants of the multi-chain, corresponding to _m ∈_\n_{_ 1 _,_ 2 _,_ 4 _,_ 8 _, ...,_ 128 _}_ parallel chains of length _n_ = 4. The leftmost plot in Figure 4 shows the average root mean squared\nerror (RMSE) of the value predictions after 1024 episodes.\nWe ran 10 seeds for each combination of step size 1 _/t_ _[d]_ with\n_d ∈{_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 1 _}_ and _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ .\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-4-0.png)\n\neither is +1 with probability 0 _._ 9 or _−_ 1 with probability 0 _._ 1.\n\n\nThe left plot in Figure 4 shows value errors for different\n_m_, minimized over _d_ and _λ_ . The prediction error of TD( _λ_ )\n(blue) grew quickly with the number of parallel chains. ET( _λ_ )\n(orange) scaled better, because it updates values in multiple\nchains (from past episodes) upon receiving a surprising reward (e.g., _−_ 1) on termination. The other three plots in Figure\n4 show value error as a function of _λ_ for a subset of problems\ncorresponding to _m ∈{_ 8 _,_ 32 _,_ 128 _}_ . The dependence on _λ_\ndiffers across algorithms and problem instances, but ET( _λ_ )\nconsistently achieved lower error than TD( _λ_ ), especially with\nhigh _λ_ . Further analysis, including on step-size sensitivity, is\nincluded in the appendix.\nNext, we encode each state with a feature vector **x** ( _s_ )\ncontaining a binary indicator vector of the branch, a binary\nindicator of the progress along the chain, a bias that always\nequals one, and two binary features indicating when we are in\nthe start (white) or bottleneck (orange) state. We extend the\nlengths of the chains to _n_ = 16. Both TD( _λ_ ) and ET( _λ_ ) use\na linear value function _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), and ET( _λ_ ) uses a\nlinear expected trace _z_ **Θ** ( _s_ ) = **Θx** ( _s_ ). All updates use the\nsame constant step size _α_ . The left plot in Figure 5 shows the\naverage root mean squared value error after 1024 episodes\n(averaged over 10 seeds). For each point the best constant\nstep size _α ∈{_ 0 _._ 01 _,_ 0 _._ 03 _,_ 0 _._ 1 _}_ (shared across all updates)\nand _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ is selected. ET( _λ_ ) (orange)\nattained lower errors across all values of _m_ (left plot), and\nfor all _λ_ (center two plots, for two specific _m_ ). The right plot\nshows results for smooth interpolations via _η_, for _λ_ = 0 _._ 9\nand _m_ = 16. The full expected trace ( _η_ = 0) performed well\nhere, we expect in other settings the additional flexibility of\n_η_ could be beneficial.\n\n\n**Expected Traces in Deep Reinforcement Learning**\n\n(Deep) neural networks are a common choice of function\nclass in reinforcement learning (e.g., Werbos 1990; Tesauro\n1992, 1994; Bertsekas and Tsitsiklis 1996; Prokhorov and\nWunsch 1997; Riedmiller 2005; van Hasselt 2012; Mnih\net al. 2015; van Hasselt, Guez, and Silver 2016; Wang et al.\n2016; Silver et al. 2016; Duan et al. 2016; Hessel et al. 2018).\nEligibility traces are not very commonly combined with deep\nnetworks (but see Tesauro 1992; Elfwing, Uchibe, and Doya\n2018), perhaps in part because of the popularity of experience\n\n\n10001\n\n\n\nreplay (Lin 1992; Mnih et al. 2015; Horgan et al. 2018).\nPerhaps the simplest way to extend expected traces to deep\nneural networks is to first separate the value function into\na representation **x** ( _s_ ) and a value _v_ ( **w** _,_ _**ξ**_ )( _s_ ) = **w** _[⊤]_ **x** _**ξ**_ ( _s_ ),\nwhere **x** _**ξ**_ is some (non-linear) function of the observations\n_s_ . [4] We can then apply the same expected trace algorithm as\nused in the previous sections by learning a separate linear\nfunction _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) using the representation which is\nlearned by backpropagating the value updates:\n\n\n**w** _t_ +1 = **w** _t_ + _αδ_ _**z**_ **Θ** ( _St_ ) _,_\n\n_**ξ**_ _t_ +1 = _**ξ**_ _t_ + _αδ_ _**e**_ _**[ξ]**_ _t_ _[,]_\n\nwhere _**e**_ _**[ξ]**_ _t_ [=] _[ γ][t][λ]_ _**[e][ξ]**_ _t−_ 1 [+] _[ ∇]_ _**[ξ]**_ _[v]_ [(] **[w]** _[,]_ _**[ξ]**_ [)][(] _[S][t]_ [)] _[,]_\n\n_**e**_ **[w]** _t_ [=] _[ γ][t][λ]_ _**[e]**_ **[w]** _t−_ 1 [+] _[ ∇]_ **[w]** _[v]_ ( **w** _,_ _**ξ**_ ) [(] _[S][t]_ [)] _[,]_\n\n\nand then updating **Θ** to minimise component-wise squared\ndifferences between _**e**_ **[w]** _t_ [and] _**[ z]**_ **[Θ]** _t_ [(] _[S][t]_ [)][, as in (2) and (3).]\nInteresting challenges appear outside the fully linear case.\nFirst, the representation will itself be updated and will have\nits own trace _**e**_ _**[ξ]**_ _t_ [. Second, in the control case we optimise]\nbehaviour: the policy will change. Both these properties of\nthe non-linear control setting imply that the expected traces\nmust track a non-stationary target. We found that being able to\ntrack this rather quickly improved performance: the expected\ntrace parameters **Θ** in the following experiment were updated\nwith a relatively high step size of _β_ = 0 _._ 1.\nWe tested this idea on two canonical Atari games: Pong and\nMs. Pac-Man. The results in Figure 6 show that the expected\ntraces helped speed up learning compared to the baseline\nwhich uses accumulating traces, for various step sizes. Unlike\nmost prior work on this domain, which often relies on replay\n(Mnih et al. 2015; Schaul et al. 2016; Horgan et al. 2018)\nor parallel streams of experience (Mnih et al. 2016), these\nalgorithms updated the values online from a single stream\nof experience. Further details on the experimental setup are\ngiven in the appendix.\nThese experiments demonstrate that the idea of expected\ntraces extends to non-linear function approximation, such as\ndeep neural networks. We consider this to be a rich area of\nfurther investigations. The results presented here are similar\nto earlier results (e.g., Mnih et al. 2015) and are not meant to\ncompete with state-of-the-art performance results, which often depend on replay and much larger amounts of experience\n(e.g., Horgan et al. 2018).\n\n\n**Discussion and Extensions**\n\nWe now discuss various interesting interpretations and relations, and discuss promising extensions.\n\n\n**Predecessor Features**\n\nFor linear value functions the expected trace _z_ ( _s_ ) can be\nexpressed non-recursively as follows:\n\n\n\n�\n\n\n\n_**z**_ ( _s_ ) = E\n\n\n\n_∞_\n� _λ_ [(] _t_ _[n]_ [)] _γt_ [(] _[n]_ [)] **x** _t−n | St_ = _s_\n� _n_ =0\n\n\n\n_,_ (7)\n\n\n\n4Here _s_ denotes observations to the agent, not a full environment\nstate— _s_ is not assumed to be Markovian.\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-1.png)\n\nFigure 4: Prediction errors in the multi-chain. ET( _λ_ ) (orange) consistently outperformed TD( _λ_ ) (blue). Shaded areas depict\nstandard errors across 10 seeds.\n\n\nFigure 5: Comparing value error with linear function approximation a) as function of the number of branches (left), b) as\nfunction of _λ_ (center two plots) and c) as function of _η_ (right). The left three plots show comparisons of TD( _λ_ ) (blue) and ET( _λ_ )\n(orange), showing ET( _λ_ ) attained lower prediction errors. The right plot interpolates between these algorithms via ET( _λ_, _η_ ),\nfrom ET( _λ_ ) = ET( _λ_, 0) to ET( _λ_, 1) = TD( _λ_ ), with _λ_ = 0 _._ 9 (corresponding to a vertical slice indicated in the second plot).\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-3.png)\n\nwhere _γk_ [(] _[n]_ [)] _≡_ [�] _[k]_ _j_ = _k−n_ _[γ][j]_ [. This is interestingly similar to the]\ndefinition of _successor features_ (Barreto et al. 2017):\n\n\n\n�\n\n\n\n_ψ_ ( _s_ ) = E\n\n\n\n_∞_\n� _γt_ [(] +1 _[n][−]_ [1)] **x** _t_ + _n | St_ = _s_\n� _n_ =1\n\n\n\n_._ (8)\n\n\n\nThe summation in (8) is over future features, while in (7)\nwe have a sum over features already observed by the agent.\nWe can thus think of linear expected traces as _predecessor_\n_features_ . A similar connection was made in the tabular setting by Pitis (2018), relating source traces, which aim to\nestimate the source matrix ( _I −_ _γP_ ) _[−]_ [1], to successor representations (Dayan 1993). In a sense, the above generalises\nthis insight. In addition to being interesting in its own right,\nthis connection allows for an intriguing interpretation of _**z**_ ( _s_ )\nas a multidimensional value function. Like with successor\nfeatures, the features **x** _t_ play the role of rewards, discounted\nwith _γ · λ_ rather than _γ_, and with time flowing backwards.\nAlthough the predecessor interpretation only holds in the\nlinear case, it is also of interest as a means to obtain a practical\nimplementation of expected traces with non-linear function\napproximation, for instance applied only to the linear ‘head’\nof a deep neural network. We used this ‘predecessor feature\ntrick’ in our Atari experiments described earlier.\n\n\n**Relation to Model-Based Reinforcement Learning**\n\n\nModel-based reinforcement learning provides an alternative\napproach to efficient credit assignment. The general idea is\nto construct a model that estimates state-transition dynamics,\nand to update the value function based upon hypothetical\n\n\n10002\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-4.png)\n\ntransitions drawn from the model (Sutton 1990), for example\nby prioritised sweeping (Moore and Atkeson 1993; van Seijen\nand Sutton 2013). In practice, model-based approaches have\nproven challenging in environments (such as Atari games)\nwith rich perceptual observations, compared to model-free\napproaches that more directly update the agent’s policy and\npredictions (van Hasselt, Hessel, and Aslanides 2019).\nIn some sense, expected traces also construct a model of\nthe environment—but one that differs in several key regards\nfrom standard state-to-state models used in model-based reinforcement learning. First, expected traces estimate _past_\nquantities rather than _future_ quantities. Backward planning\n(e.g., Chelu, Precup, and van Hasselt 2020) is possible with\nexplicit transition models, but is less common in practice.\nSecond, expected traces estimate the accumulation of _gradi-_\n_ents_ over a multi-step trajectory, rather than trying to learn\nthe full transition dynamics, thereby focusing only on those\naspects that matter for the eventual weight update. Third, expected traces allow credit assignment across these potential\npast trajectories with a single update, without the iterative\ncomputation that is typically required when using a dynamics\nmodel. These differences may be important to side-step some\nof the challenges faced in model-based learning.\n\n\n**Batch Learning and Replay**\n\n\nWe have mainly considered the online learning setting in this\npaper. It is often convenient to learn from batches of data, or\nreplay transitions repeatedly, to enhance data efficiency. A\nnatural extension is replay the experiences sequentially (e.g.\nKapturowski et al. 2018), but perhaps alternatives exist. We\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-1.png)\n\nFigure 6: Performance of Q( _λ_ ) ( _η_ = 1, blue) and QET( _λ_ ) ( _η_ = 0, orange) on Pong and Ms.Pac-Man for various learning rates.\nShaded regions show standard error across 10 random seeds. All results are for _λ_ = 0 _._ 95. Further implementation details and\nhyper-parameters are in the appendix.\n\n\n\nnow discuss one potential extension.\nWe defined a mixed trace _**y**_ _t_ that mixes the instantaneous\nand expected traces. Optionally the expected trace _**z**_ _t_ can\nbe updated towards the mixed trace _**y**_ _t_ as well, instead of\ntowards the instantaneous trace _**e**_ _t_ . Analogously to TD( _λ_ ) we\npropose to then use at least one real step of data:\n\n\n∆ _**θ**_ _t ≡_ _β_ ( _**∇**_ _t_ + _γtλt_ _**y**_ _t−_ 1 _−_ _**zθ**_ ( _St_ )) _[⊤]_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] _,_ (9)\n\n_∂_ _**θ**_\n\n\nwith _**∇**_ _t ≡∇_ **w** _v_ **w** ( _St_ ). This is akin to a forward-view _λ_ return update, with _∇_ **w** _v_ **w** ( _St_ ) in the role of (vector) reward,\nand _**zθ**_ of value, and discounted by _λtγt_, but reversed in time.\nIn other words, this can be considered a sampled Bellman\nequation (Bellman 1957) but backward in time.\nWhen we then choose _η_ = 0, then _**y**_ _t−_ 1 = _z_ _**θ**_ ( _St−_ 1), and\nthen the target in (9) only depends on a single transition.\nInterestingly, that means we can then learn expected traces\nfrom _individual_ transitions, sampled out of temporal order,\nfor instance in batch settings or when using replay.\n\n\n**Application to Other Traces**\n\n\nWe can apply the idea of expected trace to more traces than\nconsidered here. We can for instance consider the characteristic eligibility trace used in REINFORCE (Williams 1992)\nand related policy-gradient algorithms (Sutton et al. 2000).\nAnother appealing application is to the follow-on trace\nor _emphasis_, used in emphatic temporal difference learning\n(Sutton, Mahmood, and White 2016) and related algorithms\n(e.g., Imani, Graves, and White 2018). Emphatic TD was\nproposed to correct an important issue with off-policy learning, which can be unstable and lead to diverging learning\ndynamics. Emphatic TD weights updates according to 1) the\ninherent interest in having accurate predictions in that state\nand, 2) the importance of predictions in that state for updating\n\n\n10003\n\n\n\nother predictions. Emphatic TD uses scalar ‘follow-on’ traces\nto determine the ‘emphasis’ for each update. However, this\nfollow-on trace can have very high, even infinite, variance.\nInstead, we might estimate and use its expectation instead of\nthe instantaneous emphasis. A related idea was explored by\nZhang, Boehmer, and Whiteson (2019) to obtain off-policy\nactor critic algorithms.\n\n\n**Conclusion**\n\n\nWe have proposed a mechanism for efficient credit assignment, using the expectation of an eligibility trace. We have\ndemonstrated this can sometimes speed up credit assignment\ngreatly, and have analyzed concrete algorithms theoretically\nand empirically to increase understanding of the concept.\nExpected traces have several interpretations. First, we can\ninterpret the algorithm as counterfactually updating multiple possible trajectories leading up to the current state. Second, they can be understood as trading off bias and variance,\nwhich can be done smoothly via a unifying _η_ parameter, between standard eligibility traces (low bias, high variance) and\nestimated traces (possibly higher bias, but lower variance).\nFurthermore, with tabular or linear function approximation\nwe can interpret the resulting expected traces as predecessor\nstates or features—object analogous to successor states or features, but time-reversed. Finally, we can interpret the linear\nalgorithm as preconditioning the standard TD update, thereby\npotentially speeding up learning. These interpretations suggest that a variety of complementary ways to potentially\nextend these concepts and algorithms.\nWe have shown expected traces can already be used to\nenhance learning in non-linear settings (i.e., deep reinforcement learning), and in the control setting where we update\nthe policy. Further work is needed to determine the full extent\nof the possibilities of these new algorithms.\n\n\n\n\n**References**\n\n\nAmari, S. I. 1998. Natural gradient works efficiently in\nlearning. _Neural computation_ 10(2): 251–276. ISSN 08997667.\n\n\nBarreto, A.; Dabney, W.; Munos, R.; Hunt, J. J.; Schaul, T.;\nvan Hasselt, H. P.; and Silver, D. 2017. Successor features\nfor transfer in reinforcement learning. In _Advances in neural_\n_information processing systems_, 4055–4065.\n\n\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. _J. Artif. Intell. Res. (JAIR)_ 47:\n253–279.\n\n\nBellman, R. 1957. _Dynamic Programming_ . Princeton University Press.\n\n\nBertsekas, D. P.; and Tsitsiklis, J. N. 1996. _Neuro-dynamic_\n_Programming_ . Athena Scientific, Belmont, MA.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018a. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018b. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBrandfonbrener, D.; and Bruna, J. 2020. Geometric Insights\ninto the Convergence of Non-linear TD Learning. In _Interna-_\n_tional Conference on Learning Representations_ .\n\n\nChelu, V.; Precup, D.; and van Hasselt, H. P. 2020. Forethought and Hindsight in Credit Assignment. In Larochelle,\nH.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H.,\neds., _Advances in Neural Information Processing Systems_,\nvolume 33, 2270–2281.\n\n\nDayan, P. 1992. The convergence of TD( _λ_ ) for general\nlambda. _Machine Learning_ 8: 341–362.\n\n\nDayan, P. 1993. Improving generalization for temporal difference learning: The successor representation. _Neural Com-_\n_putation_ 5(4): 613–624.\n\n\nDuan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel,\nP. 2016. Benchmarking deep reinforcement learning for\ncontinuous control. In _International Conference on Machine_\n_Learning_, 1329–1338.\n\n\nElfwing, S.; Uchibe, E.; and Doya, K. 2018. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_ 107:\n3–11.\n\n\nHennigan, T.; Cai, T.; Norman, T.; and Babuschkin, I. 2020.\nHaiku: Sonnet for JAX. URL http://github.com/deepmind/\ndm-haiku.\n\n\nHessel, M.; Modayil, J.; van Hasselt, H. P.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2018. Rainbow: Combining Improvements in Deep\nReinforcement Learning. _AAAI_ .\n\n\n10004\n\n\n\nHorgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel,\nM.; van Hasselt, H. P.; and Silver, D. 2018. Distributed\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ .\n\n\nImani, E.; Graves, E.; and White, M. 2018. An Off-policy\nPolicy Gradient Theorem Using Emphatic Weightings. In\nBengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; CesaBianchi, N.; and Garnett, R., eds., _Advances in Neural Infor-_\n_mation Processing Systems 31_, 96–106. Curran Associates,\nInc. URL http://papers.nips.cc/paper/7295-an-off-policypolicy-gradient-theorem-using-emphatic-weightings.pdf.\n\n\nKaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1995.\nPlanning and Acting in Partially Observable Stochastic Domains. Unpublished report.\n\n\nKapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and\nDabney, W. 2018. Recurrent experience replay in distributed\nreinforcement learning. In _International conference on learn-_\n_ing representations_ .\n\n\nKingma, D. P.; and Adam, J. B. 2015. A method for stochastic optimization. In _International Conference on Learning_\n_Representation_ .\n\n\nLin, L. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_\n8(3): 293–321.\n\n\nMartens, J. 2016. _Second-order optimization for neural net-_\n_works_ . University of Toronto (Canada).\n\n\nMinsky, M. 1963. Steps Toward Artificial Intelligence.\nIn Feigenbaum, E.; and Feldman, J., eds., _Computers and_\n_Thought_, 406–450. McGraw-Hill, New York.\n\n\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous Methods for Deep Reinforcement Learning. In\n_International Conference on Machine Learning_ .\n\n\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg,\nS.; and Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. _Nature_ 518(7540): 529–533.\n\n\nMoore, A. W.; and Atkeson, C. G. 1993. Prioritized Sweeping: Reinforcement Learning with less Data and less Time.\n_Machine Learning_ 13: 103–130.\n\n\nOllivier, Y. 2018. Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies. _CoRR_\nabs/1805.00869.\n\n\nPeng, J. 1993. _Efficient dynamic programming-based learn-_\n_ing for control_ . Ph.D. thesis, Northeastern University.\n\n\nPeng, J.; and Williams, R. J. 1996. Incremental Multi-step\nQ-learning. _Machine Learning_ 22: 283–290.\n\n\nPitis, S. 2018. Source Traces for Temporal Difference Learning. In McIlraith, S. A.; and Weinberger, K. Q., eds., _Pro-_\n_ceedings of the Thirty-Second AAAI Conference on Artificial_\n_Intelligence_, 3952–3959. AAAI Press.\n\n\n\n\nPohlen, T.; Piot, B.; Hester, T.; Azar, M. G.; Horgan, D.;\nBudden, D.; Barth-Maron, G.; van Hasselt, H. P.; Quan, J.;\nVecerˇ ´ık, M.; Hessel, M.; Munos, R.; and Pietquin, O. 2018.\nObserve and look further: Achieving consistent performance\non Atari. _arXiv preprint arXiv:1805.11593_ .\n\n\nProkhorov, D. V.; and Wunsch, D. C. 1997. Adaptive critic\ndesigns. _IEEE Transactions on Neural Networks_ 8(5): 997–\n1007.\n\n\nPuterman, M. L. 1994. _Markov Decision Processes: Discrete_\n_Stochastic Dynamic Programming_ . John Wiley & Sons, Inc.\nNew York, NY, USA.\n\n\nRiedmiller, M. 2005. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning\nMethod. In Gama, J.; Camacho, R.; Brazdil, P.; Jorge, A.; and\nTorgo, L., eds., _Proceedings of the 16th European Conference_\n_on Machine Learning (ECML’05)_, 317–328. Springer.\n\n\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ . Puerto Rico.\n\n\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering\nthe game of Go with deep neural networks and tree search.\n_Nature_ 529(7587): 484–489.\n\n\nSingh, S. P.; and Sutton, R. S. 1996. Reinforcement Learning\nwith replacing eligibility traces. _Machine Learning_ 22: 123–\n158.\n\n\nSutton, R. S. 1984. _Temporal Credit Assignment in Reinforce-_\n_ment Learning_ . Ph.D. thesis, University of Massachusetts,\nDept. of Comp. and Inf. Sci.\n\n\nSutton, R. S. 1988. Learning to predict by the methods of\ntemporal differences. _Machine learning_ 3(1): 9–44.\n\n\nSutton, R. S. 1990. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Proceedings of the seventh international conference_\n_on machine learning_, 216–224.\n\n\nSutton, R. S.; and Barto, A. G. 2018. _Reinforcement Learning:_\n_An Introduction_ . The MIT press, Cambridge MA.\n\n\nSutton, R. S.; Mahmood, A. R.; and White, M. 2016. An\nEmphatic Approach to the Problem of Off-policy TemporalDifference Learning. _Journal of Machine Learning Research_\n17(73): 1–29.\n\n\nSutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y.\n2000. Policy gradient methods for reinforcement learning\nwith function approximation. _Advances in Neural Informa-_\n_tion Processing Systems 13 (NIPS-00)_ 12: 1057–1063.\n\n\nTesauro, G. 1992. Practical Issues in Temporal Difference\nLearning. In Lippman, D. S.; Moody, J. E.; and Touretzky,\nD. S., eds., _Advances in Neural Information Processing Sys-_\n_tems 4_, 259–266. San Mateo, CA: Morgan Kaufmann.\n\n\nTesauro, G. J. 1994. TD-Gammon, a self-teaching backgammon program, achieves master-level play. _Neural computa-_\n_tion_ 6(2): 215–219.\n\n\n10005\n\n\n\nTsitsiklis, J. N. 1994. Asynchronous stochastic approximation and Q-learning. _Machine Learning_ 16: 185–202.\n\nTsitsiklis, J. N.; and Van Roy, B. 1997. An analysis of\ntemporal-difference learning with function approximation.\n_IEEE Transactions on Automatic Control_ 42(5): 674–690.\n\nvan Hasselt, H. P. 2012. Reinforcement Learning in Continuous State and Action Spaces. In Wiering, M. A.; and\nvan Otterlo, M., eds., _Reinforcement Learning: State of the_\n_Art_, volume 12 of _Adaptation, Learning, and Optimization_,\n207–251. Springer.\n\n\nvan Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver,\nD. 2016. Learning values across many orders of magnitude. In _Advances in Neural Information Processing Systems_\n_29: Annual Conference on Neural Information Processing_\n_Systems 2016, December 5-10, 2016, Barcelona, Spain_, 4287–\n4295.\n\n\nvan Hasselt, H. P.; Guez, A.; and Silver, D. 2016. Deep reinforcement learning with double Q-Learning. In _Proceedings_\n_of the Thirtieth AAAI Conference on Artificial Intelligence_,\n2094–2100.\n\n\nvan Hasselt, H. P.; Hessel, M.; and Aslanides, J. 2019. When\nto use parametric models in reinforcement learning? In _Ad-_\n_vances in Neural Information Processing Systems_, volume 32,\n14322–14333.\n\nvan Hasselt, H. P.; Mahmood, A. R.; and Sutton, R. S. 2014.\nOff-policy TD( _λ_ ) with a true online equivalence. In _Pro-_\n_ceedings of the 30th Conference on Uncertainty in Artificial_\n_Intelligence_, 330–339.\n\nvan Hasselt, H. P.; Quan, J.; Hessel, M.; Xu, Z.; Borsa, D.;\nand Barreto, A. 2019. General non-linear Bellman equations.\n_arXiv preprint arXiv:1907.03687_ .\n\n\nvan Hasselt, H. P.; and Sutton, R. S. 2015. Learning to predict\nindependent of span. _CoRR_ abs/1508.04582.\n\nvan Seijen, H.; and Sutton, R. S. 2013. Planning by Prioritized Sweeping with Small Backups. In _International_\n_Conference on Machine Learning_, 361–369.\n\nvan Seijen, H.; and Sutton, R. S. 2014. True online TD( _λ_ ).\nIn _International Conference on Machine Learning_, 692–700.\n\n\nWang, Z.; de Freitas, N.; Schaul, T.; Hessel, M.; van Hasselt,\nH. P.; and Lanctot, M. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In _International_\n_Conference on Machine Learning_ . New York, NY, USA.\n\nWerbos, P. J. 1990. A menu of designs for reinforcement\nlearning over time. _Neural networks for control_ 67–95.\n\nWhitehead, S. D.; and Ballard, D. H. 1991. Learning to\nperceive and act by trial and error. _Machine Learning_ 7(1):\n45–83.\n\nWilliams, R. J. 1992. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. _Machine_\n_Learning_ 8: 229–256.\n\n\nZhang, S.; Boehmer, W.; and Whiteson, S. 2019. Generalized\noff-policy actor-critic. In _Advances in Neural Information_\n_Processing Systems_, 2001–2011.\n\n\n",
          "ranking": {
            "relevance_score": 0.7493067885948483,
            "citation_score": 0.6302697050551669,
            "recency_score": 0.3906854405837399,
            "final_score": 0.6896372370858013
          },
          "is_open_access": false,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Ablation studies confirm the importance of the multi-task learning objective for generalization.",
        "chunk_index": 1
      },
      "summary": "Ablation studies confirm the importance of the multi-task learning objective for generalization.",
      "vector_score": 0.6960000000000001,
      "llm_score": 0.87,
      "combined_score": 0.87,
      "source_query": "mock_query_results"
    },
    {
      "chunk": {
        "chunk_id": "mock_12",
        "paper": {
          "id": "66d76444255be0ac378a0a93ee0379fc721a386f",
          "title": "On Q-learning Convergence for Non-Markov Decision Processes",
          "published": "2018-07-01",
          "authors": [
            "Sultan Javed Majeed",
            "Marcus Hutter"
          ],
          "summary": "Temporal-difference (TD) learning is an attractive, computationally efficient framework for model- free reinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent to learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has only been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state MDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the environment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning when applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We prove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in particular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a necessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.",
          "pdf_url": "https://doi.org/10.24963/ijcai.2018/353",
          "doi": "10.24963/ijcai.2018/353",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "International Joint Conference on Artificial Intelligence",
          "citation_count": 40,
          "bibtex": "@Article{Majeed2018OnQC,\n author = {Sultan Javed Majeed and Marcus Hutter},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {2546-2552},\n title = {On Q-learning Convergence for Non-Markov Decision Processes},\n year = {2018}\n}\n",
          "markdown_text": "[image]\n\n\nNavigation\n\n\n  - Home\n\n  - Conferences\n\n\nFuture Conferences\n\n  Past Conferences\n\n  \n\n  - Proceedings\n\n\n  - IJCAI 2025 Proceedings\n\n  - All Proceedings\n\n\n  - Awards\n\n  - Trustees/officers\n\n\nCurrent trustees\n\n  Trustees Elect\n\n  IJCAI Secretariat\n\n  \n  - IJCAI Sponsorship and Publicity Officers\nIJCAI Team\n\n  \n  - Local Arrangements Chairs\n\n  - Former Trustees serving on the Executive Committee\n\n  - Other Former Officers\n\n\n  - AI Journal\n\n  - About\n\n\nAbout IJCAI\n\n  Contact Information\n\n  \n# **On Q-learning Convergence for Non-Markov** **Decision Processes** **On Q-learning Convergence for Non-Markov** **Decision Processes**\n\n## **Sultan Javed Majeed, Marcus Hutter**\n\n\n[image]\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence\n[Main track. Pages 2546-2552. https://doi.org/10.24963/ijcai.2018/353](https://doi.org/10.24963/ijcai.2018/353)\n[PDF BibTeX](https://www.ijcai.org/proceedings/2018/0353.pdf)\n\n\nTemporal-difference (TD) learning is an attractive, computationally efficient framework for model- free\nreinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent\nto learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has\nonly been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state\nMDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the\nenvironment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning\nwhen applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We\nprove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in\nparticular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a\nnecessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.\nKeywords:\nMachine Learning: Online Learning\nMachine Learning: Reinforcement Learning\nPlanning and Scheduling: Markov Decisions Processes\n\n\nCopyright © 2025,\n\n\n",
          "ranking": {
            "relevance_score": 0.7639998734717994,
            "citation_score": 0.5680913780397937,
            "recency_score": 0.27592885552856466,
            "final_score": 0.6760110725910747
          },
          "is_open_access": true,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "User studies indicate high acceptance rates for the generated suggestions among professional developers.",
        "chunk_index": 2
      },
      "summary": "User studies indicate high acceptance rates for the generated suggestions among professional developers.",
      "vector_score": 0.672,
      "llm_score": 0.84,
      "combined_score": 0.84,
      "source_query": "mock_query_results"
    }
  ],
  "Discussion": [
    {
      "chunk": {
        "chunk_id": "mock_13",
        "paper": {
          "id": "user_2404.15822v1",
          "title": "Recursive Backwards Q-Learning in Deterministic Environments",
          "published": "2024-04-24",
          "authors": [
            "Jan Diekhoff",
            "Jorn Fischer"
          ],
          "summary": "Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.",
          "pdf_url": "",
          "doi": "10.48550/arXiv.2404.15822",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "arXiv.org",
          "citation_count": 0,
          "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n",
          "markdown_text": "## RECURSIVE BACKWARDS Q-LEARNING IN DETERMINISTIC ENVIRONMENTS\n\n\n\n**Jan Diekhoff**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: jan.diekhoff@web.de\n\n\n\n**Jörn Fischer**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: j.fischer@hs-mannheim.de\n\n\n\n**ABSTRACT**\n\n\nReinforcement learning is a popular method of finding optimal solutions to complex problems.\nAlgorithms like Q-learning excel at learning to solve stochastic problems without a model of their\nenvironment. However, they take longer to solve deterministic problems than is necessary. Q-learning\ncan be improved to better solve deterministic problems by introducing such a model-based approach.\nThis paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds\na model of the environment. After reaching a terminal state, it recursively propagates its value\nbackwards through this model. This lets each state be evaluated to its optimal value without a lengthy\nlearning process. In the example of finding the shortest path through a maze, this agent greatly\noutperforms a regular Q-learning agent.\n\n\n_**Keywords**_ Q-learning _·_ deterministic _·_ recursive _·_ reinforcement learning\n\n\n**1** **Introduction**\n\n\nMachine learning and reinforcement learning are increasingly popular and important fields in the modern age. There are\nproblems that reinforcement learning agents can learn to solve more efficiently and consistently than any human when\ngiven enough time to practice. However, modern approaches like Q-learning run into issues when facing certain types\nof problems. Their approach to solving problems in combination with not using a model of the environment causes\nthem to take longer than is necessary to learn to solve problems that are deterministic in nature. By working without\nmodel of the environment, information that is available and help the learning process is ignored.\n\n\nThis paper introduces an adapted Q-learning agent called the _recursive backwards Q-Learning (RBQL) agent_ . It solves\nthese types of problems by building a model of its environment as it explores and recursively applying the Q-value\nupdate rule to find an optimal policy much quicker than a regular Q-learning agent. This agent is shown to work with\nthe example of finding the fastest path through a maze. Its results are compared to the results of a regular Q-learning\nagent.\n\n\n**2** **Reinforcement Learning**\n\n\nReinforcement learning is one of the main fields of machine learning. It is commonly used for optimizing solutions to\nproblems. At its most fundamental level, a reinforcement learning method is an implementation of an agent for solving\na Markov decision process [1] by interacting with an environment. Markov decision processes describe problems as\na set of states _S_, a set of actions _A_ and a set of rewards _R_ . For every time step _t_, the agent chooses an action _a ∈_ _A_\nand receives a new state _s ∈_ _S_ and a reward _r ∈_ _R_ for the action [2]. Rewards may be positive or negative, depending\non the outcome of the action, to encourage or discourage taking that action in the future [3]. The process of the agent\ninteracting with the environment is called an episode which ends when a terminal state is reached which resets the\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nenvironment and agent to their original configuration for the start of a new episode [3]. For the purposes of this paper,\nonly finite Markov decision processes are considered, meaning the environment has at least one terminal state.\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-1-0.png)\n\n\n_St_ +1\n\n\nFigure 1: Basic agent-environment relationship in a Markov decision process. The agent chooses an action _At_ and the\nenvironment returns a new state _St_ +1 and a reward _Rt_ +1. The dotted line represents the transition from step _t_ to step\n_t_ + 1 [3].\n\n\nReinforcement learning agents learn an optimal strategy for a given Markov decision process by estimating the value of\neither being in a state or taking a certain action in a certain state. They do this through a value function or action-value\nfunction respectively. The aim of the agent is to maximize the reward they receive in an episode [3]. To achieve this,\nvalue estimations do not only consider the immediate action the agent takes but also consider all future states and actions\nthat may occur when taking the original action. Agents follow so-called policies according to which they choose which\nactions to take. Through gaining knowledge, they continuously adapt this policy in order to eventually reach an optimal\npolicy - a policy which chooses the optimal action at every step. To explore, agents have to balance between exploration\nand exploitation [3]. Exploration is the act of following suboptimal actions to attempt to find an even better policy. On\nthe other hand, exploitation is following the actions that will yield the currently highest estimated value. An agent that\nonly exploits acts _greedily_ . To ensure continual exploration so that all actions get updated given enough time, agents\ncan choose policies that are mostly greedy but choose to explore sometimes [2]. To this end, an approach like _ϵ_ -greedy\nmay be used. Here, _ϵ_ is the probability of choosing a random action and 1 _−_ _ϵ_ is the probability of acting greedily.\n\n\nA widely used modern approach to RL is temporal difference learning [4], more specifically Q-learning [2]. Q-learning\nworks with the Q-learning update formula to update its policies:\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + _α ·_\n\n[ _Rt_ +1 + _γ ·_ max _Q_ ( _St_ +1 _, a_ ) _−_ _Q_ ( _St, At_ )] (1)\n_a_\n\n\n_Q_ ( _St, At_ ) is the estimated value for any given state-action pair. The equation shows how it is updated after taking\naction _At_ from state _St_ . _Rt_ +1 represents the reward gained, max _Q_ ( _St_ +1 _, a_ ) is the value estimation of the best action\n_a_\n_a ∈_ _At_ +1 that can be taken from _St_ +1 according to the current policy, the state resulting from action _At_ . _α_ is a step-size\nparameter, also known as the _learning rate_ . Its value lies between 0 and 1 and it determines how importantly the agent\nvalues new information against the current estimate it already has. A value of 0 completely ignores new information\nwhile a value of 1 completely overrides the preexisting value estimate. _γ_ is the discount factor, weighing future rewards\nless than immediate ones. It also lies between 0 and 1, where 1 weighs the best future action equally to the current one\nand 0 does not consider it at all.\n\n\n**3** **Recursive Backwards Q-Learning**\n\n\n**3.1** **Idea**\n\n\nQ-learning agents are very widespread in modern reinforcement learning. Working free of a model allows them to\nbe generally applicable to many problems. However, some Markov decision processes take longer to solve than is\nnecessary because the agent ignores readily available information. This is noticeable in deterministic, episodic tasks\nwhere a positive reward is only given when reaching a terminal state. Before this state is reached for the first time, the\nagent appears to be moving entirely at random. Looking at figure 2, the issue becomes apparent. Even when following\nthe optimal path at every step, it still takes multiple episodes for the reward of the terminal state to propagate back to the\nstarting state. In fact, the optimal paths value estimation gets worse before it gets better. If every step has a reward of\n\n\n2\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_−_ 1, values along the optimal path get worse if they do not lead to a state that has already been reached by the terminal\nstate’s positive reward as it travels backwards.\n\n\nIn this paper, grid worlds [3] are used as an example Markov decision process for the agent to solve. Grid worlds are a\ntwo-dimensional grid in which every tile represents a state and the actions are limited to walking up, down, left or right.\nGrid worlds are useful in that they are very simple to understand and to display, they have a limited set of actions and\ntheir set of states can be as small or large as is desired. Additionally, showing the value or optimal policy for each state\nis as easy as writing a number or drawing an arrow on the corresponding tile. Actions that would place the agent off of\nthe grid simply return the state the agent is already in, but may still give a reward. Special tiles can also be defined, such\nas walls that act like the grid edge or pits that are terminal fail states because the agent cannot leave them once it has\nfallen in. Every grid world tile gives a reward of _−_ 1 to punish taking unnecessary actions in favor of taking the fastest\npath to the goal.\n\n\n_Q_ greedy policy\nw.r.t. _Q_\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -0.8\n\n\n-1\n\n\n-1\n\n\n-1 0.52\n\n\n-1\n\n\n-1\n\n\n-1 1.99\n\n\n-1\n\n\n-1\n\n\n-1 3.27\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 0.78\n\n\n-1\n\n\n-1\n\n\n-1 3.15\n\n\n-1\n\n\n-1\n\n\n-1 4.96\n\n\n-1\n\n\n-1\n\n\n-1 6.17\n\n\n-1\n\n\n-1\n\n\n-1 6.93\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 4.5\n\n\n-1\n\n\n-1\n\n\n-1 7.25\n\n\n-1\n\n\n-1\n\n\n-1 8.63\n\n\n-1\n\n\n-1\n\n\n-1 9.32\n\n\n-1\n\n\n-1\n\n\n-1 9.66\n\n\n-1\n\n\n-1\n\n\n-1 9.83\n\n\n-1\n\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n\nep. 0\n\n\nep. 1\n\n\nep. 2\n\n\nep. 3\n\n\nep. 4\n\n\nep. 5\n\n\nep. 6\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -1.6\n\n\n-1\n\n\n-1\n\n\n-1 -1.66\n\n\n-1\n\n\n-1\n\n\n-1 -1.1\n\n\n-1\n\n\n-1\n\n\n-1 -0.15\n\n\n-1\n\n\n\nFigure 2: Q-learning in a one-dimensional grid world. All Q-values are initialized as _−_ 1. Actions that lead to the\nterminal state reward 10. All other actions reward -1. The discount rate _γ_ is set to 0 _._ 9. The learning rate _α_ is set to 0 _._ 5.\nThe value of _ϵ_ is irrelevant as the only action the agent takes is _→_ .\n\n\nFigure 2 is a very simple grid world and it still takes six episodes to reach an optimal policy, even when taking the\noptimal action at every step. This problem will only grow worse and add noticeably more episodes of training for grid\nworlds that are not as trivial to solve, or even more complex tasks with more variables to consider. As stated, the issue\nis that the agent has no source of direction until it has randomly stumbled across the terminal state, its only source of\npositive rewards. The larger the state space, the longer it is blindly searching.\n\n\nReinforcement learning agents that work with a model of their environment are known as _model-based_ reinforcement\nlearning agents. They can either work with a preexisting model or, more commonly, build their own. The way they\nconstruct their models is important as having perfect knowledge of an environment is neither feasible nor sensible. In\nthe case of a grid world it is no problem, but imagining a more complex scenario like a self-driving car makes this fact\napparent. When trying to drive from one city to another, knowing every centimeter of the road with every possible place\nother cars might be on the route is resource intensive and unnecessary. Instead, an agent should attempt to simplify its\nmodel as much as possible. Instead of every bit of road, long stretches going straight can be clumped together. Similar\nsituations like a car in front slowing down can be treated the same wherever they occur.\n\n\nThe purpose of this paper is to introduce and evaluate a new type of model-based agent called the RBQL agent. The\nRBQL agent solves deterministic, episodic tasks that positively reward only the terminal state more efficiently than a\nregular Q-learning agent. It functions by building a model of its environment through exploration. When it reaches a\nterminal state, it recursively travels backwards through all previously explored states, applying a modified Q-learning\nupdate rule, the RBQL update rule. By setting the learning rate _α_ to 1, equation (1) can be simplified as such:\n\n\n3\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + 1 _·_ [ _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )]\n= _Q_ ( _St, At_ ) + _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )\n= _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n\n(2)\n\n\n\nAs can be seen in formula (2), the Q-value now exclusively depends on the reward and the discounted reward of the\nbest neighbor. Because the algorithm applies this formula starting with what is guaranteed to be the highest value of the\nenvironment and working its way away from it, the best possible neighbor for any given state is always the previously\nevaluated state.\n\n\nEvaluating all states at the end of the episode is reminiscent of dynamic programming [5] or Monte Carlo methods [3]\nand is a point of critique for those approaches. However, as will be shown in chapter 4, this evaluation method is so\neffective in RBQL that evaluating all known states in one go is still cost effective. RBQL also differs in comparison\nto dynamic programming and Monte Carlo in a few major ways. In contrast with dynamic programming, it does not\nstart out with a perfect model but has to build its own. It also propagates its reward throughout all states much more\nquickly and it uses an action-value function, not a state-value function. In contrast with Monte Carlo, it does not use\nexploring starts to guarantee exploration. It also does not only update the values that were seen in an episode. Instead,\nto facilitate exploration, it always prioritizes visiting unexplored actions, only following the greedy path when there\nare none. Because this mode of exploration still results in unexplored actions, the _ϵ_ -greedy approach is adapted for\nRBQL. Instead of exploring steps, the agent has exploration episodes. _ϵ_ serves the same purpose as before, marking\nthe probability of taking an exploration episode while 1 _−_ _ϵ_ is the probability of taking an exploitation episode. In an\nexploration episode, the agent randomly chooses an unexplored action anywhere in its model, navigates the model to\nput itself in a position to take that action and then continues to explore until it finds a known path again or the episode\nends.\n\n\nIn this paper, finding an optimal path through a randomly generated grid world maze is used as an example task for\nRBQL to solve. It is also used to compare the performance of RBQL to Q-learning.\n\n\n**3.2** **Implementation**\n\n\nTo implement RBQL [1], the Godot game engine v. 3.5 [2] was used. Godot is a free, open source engine used mainly for\nvideo game development. Its main language is GDScript, an internal language that is very similar in syntax to Python,\nthough it also supports C, C++, C# and VisualScript. Because Python is very popular for machine learning development,\nthe implementation is written in GDScript so that it is easily readable for interested parties. Godot uses a hierarchical\nstructure of objects called _nodes_ . In the implementation, there are two main nodes: the agent and the environment.\n\n\n**3.2.1** **Environment**\n\n\nThe environment is of the type `TileMap` [3] – a class designed for creating maps in grid-based environments like grid\nworlds. Before starting the first episode, the environment generates a maze given a width _w_ and a height _h_ using a\nrecursive backtracking algorithm [6]. The starting point for the agent is always (0 _,_ 0) and the goal it attempts to reach –\nthe only terminal state – is ( _w −_ 1 _, h −_ 1). To ensure that the agent has the ability to improve even after finding the goal\nin the first episode, a maze with multiple paths is needed. Because a maze generated with recursive backtracking only\nhas one path to the terminal state, a number of alternate paths are generated by taking _w · h/_ 4 random positions and a\ndirection for each position. If the position has a wall in that direction, it is removed. If not, nothing happens.\n\n\nThe environment has a function `step(state,action)` that serves as the only way for the agent to interact with it.\nThe possible moves are `UP`, `DOWN`, `LEFT` and `RIGHT` . The state is described as a coordinate of the current position. In\nGodot, the class `Vector2(x,y)` [4] is used for this purpose. `step()` checks if taking the given action from the given\nstate results in hitting a wall or not. If not, the agent moves to a new position. There are three different rewards: _−_ 1 for\nany normal tile, _−_ 5 for hitting a wall and 10 for reaching the terminal state. _−_ 1 is awarded at every step to discourage\nagents from taking unnecessary steps. Walls give _−_ 5 to quickly teach the agent to ignore them. After taking an action,\n\n\n1 The source code can be downloaded at `[https://github.com/JanDiekhoff/BackwardsLearner](https://github.com/JanDiekhoff/BackwardsLearner)`\n2 Godot v. 3.5 can be downloaded at `[https://godotengine.org/download/archive/3.5-stable/](https://godotengine.org/download/archive/3.5-stable/)`\n3 `[https://docs.godotengine.org/en/3.5/classes/class_tilemap.html](https://docs.godotengine.org/en/3.5/classes/class_tilemap.html)`\n4 `[https://docs.godotengine.org/en/3.5/classes/class_vector2.html](https://docs.godotengine.org/en/3.5/classes/class_vector2.html)`\n\n\n4\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nthe new state and reward are returned to the agent, as well as a notification if the episode has ended or not and if the\nagent has hit a wall or not.\n\n\nThe `TileMap` has a tile for each combination of having or not having a wall in each of the four directions, totaling 2 [4] or\n16 total possible tiles. Another option would be to just have a floor tile and a wall tile. However, that would make a\nmaze with an equivalent wall layout much larger, leading to a larger state set and longer solving times. To determine if a\nwall is in a certain direction, the id of each tile from 0 to 15 acts as a four-bit flag. Each direction is assigned one of the\nbits ( `UP` = 0, `RIGHT` = 1, `DOWN` = 2 and `LEFT` = 3). If the flag is set, there is a wall in the corresponding direction. The\nid for an L-shaped tile for example would be 2 [2] + 2 [3] = 12 as `DOWN` and `LEFT` have walls. The process for determining\nif the agent can move in a given direction _d_ from a position _p_ is ( _¬idp_ ) & (2 _[d]_ ), where _idp_ is the id of the tile at _p_ .\n\n\n**3.2.2** **RBQL Agent**\n\n\nThe RBQL agent is represented by a `Sprite` [5] object – a 2D image – so it can be observed while solving a maze. During\nits runtime, the agent keeps track of a few key things:\n\n\n    - A model of the environment ( `explored_map` )\n\n\n    - A list of rewards for each state-action pair ( `rewards` )\n\n\n    - The last reward received ( `reward` )\n\n\n    - A list of steps taken per episode ( `steps_taken` )\n\n\n    - The Q-table ( `qtable` )\n\n\n    - The current state ( `current_state` )\n\n\n    - The previous state ( `old_state` )\n\n\n    - The last taken action ( `action` )\n\n\nThe model of the environment starts out as an empty dictionary. Every time a new state is discovered, an entry for that state is made and initialized as an empty array. When an action is taken from this state, the resulting new state is entered into the previous state’s array at the index of the taken action’s designated number\n( `explored_map[old_state][action] = current_state` ). When hitting a wall, the “new” state is the same as the\nstate from which the action was taken. Similarly, when an action is taken, the resulting reward is saved in the rewards\nlist ( `rewards[old_state][action] = reward` ). Because the agent uses state-action values, not state values, the\ntiles are treated like nodes in a directed graph. Going from tile A to tile B might result in a different reward than when\ngoing from B to A, so when the agent learns the reward of going from A to B, it does not also learn the reward of going\nfrom B to A.\n\n\nBeing a Q-learner makes it simpler to generalize the agent for other tasks, but it causes a lot of exploratory steps and\nexploratory episodes to only explore one position at a time. If an exploration episode chooses an unexplored state-action\npair that results in hitting a wall, the exploration episode immediately ends with little information gained. To alleviate\nthis problem, the agent takes exploratory “look-ahead” steps. After entering a tile, it takes a step in every direction but\nonly saves the result if it hits a wall. This guarantees that exploratory episodes always take new paths and not just hit a\nwall and continue on the best known path.\n\n\nThe agent also keeps track of a list of the actions it has taken – except for when hitting a wall – for the case that it\nreaches a dead end, or rather a state with no unexplored neighbors. In this case, the agent would normally follow the\noptimal path until it finds a new unexplored path or reaches the terminal state. However, if the path the agent is on has\nnot been explored before it has not yet been evaluated and there is no optimal path to follow. In this case, the agent\nbacktracks by taking the opposite action of the most recent in the list, then removes it from the list, until an unexplored\ntile or an evaluated path to follow is found.\n\n\nFinally, when the terminal state is reached, the Q-table is updated with the rewards saved in `rewards` according to the\nRBQL update rule.\n```\n             qtable[state][action] =\n\n```\n\n`rewards[state][action] + discount_rate` _·_\n\n```\n             qtable[explored_map[state][action]].max()\n\n```\n\nTo do this, a copy of `explored_map` is inverted to be able to traverse it in reverse. This is then done with a breadth-first\nsearch algorithm, starting at the terminal state, and the Q-value is calculated for each state. Breadth-first search is chosen\n\n\n5 `[https://docs.godotengine.org/en/3.5/classes/class_sprite.html](https://docs.godotengine.org/en/3.5/classes/class_sprite.html)`\n\n\n5\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nover a depth-first search algorithm so that each state must only be visited once as the value is directly proportional to\nthe distance from the terminal state. With breadth-first search, each state gets the highest possible value on its first visit\nbecause it is visited from its highest possible valued neighbor.\n\n\nWhen all known states have been evaluated, a new episode begins. After the first episode, episodes are chosen to be\neither exploratory or exploitative, similar to how an _ϵ_ -greedy policy may choose exploratory actions. In an exploitative\nepisode, the agent simply follows the best path it knows, choosing at random if two states are equally good, but still\nalways exploring unknown states directly adjacent to the path above all else. In an exploratory episode, a random state\nwith an unexplored neighbor is chosen. The agent navigates to this state with the help of the A* search algorithm [7]\nand follow the unexplored path from there until it finds a known state again. This exploratory excursion may only find\none new state or it may find a vastly superior path to what was known before. _ϵ_ is decreased after every episode as\nfollows:\n_ϵ_ = `min_epsilon + (max_epsilon - min_epsilon)`\n\n\n_· e_ [(] _[−]_ `[decay_rate]` _[ ·]_ `[ current_episode]` [)]\n\n\nwhere `min_epsilon`, `max_epsilon` and `decay_rate` can be any value within a range of [0 _,_ 1] and `current_episode`\nis the number of the current episode starting with 0. Once every state is explored, the agent is guaranteed to have found\nthe optimal path, or paths, through the maze. In its entirety, the algorithm can be expressed like this:\n\n\n**Algorithm 1** Backwards Q-Learning Algorithm\n\nSet exploration_episode to false\n**while** true **do**\n\n**if** exploration_episode **then**\n\nFind unexplored path\nTravel to unexplored path\n**end if**\n**while** episode is not over **do**\n\n**if** current position has an unexplored neighbor **then**\n\nVisit unexplored neighbor\nUpdate model\nSave reward\n\n**if** no wall hit **then**\n\nSave action in action queue\n**end if**\n**else if** there is an optimal path to follow **then**\n\nVisit best neighbor\n**end if**\n**while** current pos. has no unexplored neighbor **do**\n\nBacktrack\n\n**end while**\n\n**end while**\nCreate state queue with breadth-first search\n**for** state in queue **do**\n\nApply RBQL formula\n**end for**\nSet exploration_episode to random() _<_ = _ϵ_\nApply decay to _ϵ_\n**end while**\n\n\n**3.2.3** **Q-learning agent**\n\n\nA standard Q-learning agent has been implemented in Godot as well to compare the performance of the RBQL agent to.\nThis agent is comparatively simple:\n\n\n**4** **Tests and Results**\n\n\nTo compare the performance of the two agents, three sets of tests have been done for different maze sizes: 5 _×_ 5, 10 _×_ 10\nand 15 _×_ 15. All variables have been set to common values. The decay rate is set somewhat high to account for the\nrelatively low episode amount:\n\n\n6\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**Algorithm 2** Q-Learning Algorithm\n\n\n**while** true **do**\n\n**if** random() _<_ = _ϵ_ **then**\n\nChoose random action\n\n**else**\n\nChoose greedy action\n**end if**\n\nTake action\n\nReceive new state and reward\nUpdate Q-table for old state and action\n**if** terminal state reached **then**\n\nStart new episode\n**end if**\nApply decay to _ϵ_\n**end while**\n\n\n    - _γ_ = 0 _._ 9\n\n\n    - _α_ = 0 _._ 1 (RBQL has _α_ = 1 as explained in equation (2))\n\n\n    - `min_epsilon` = 0 _._ 01\n\n\n    - `max_epsilon` = 1\n\n\n    - `decay_rate` = _−_ 0 _._ 01\n\n\nFor every maze size, each agent is given the same set of 50 randomly generated mazes. Each agent is given 25 episodes\nper maze to train. These values are chosen to offer a reasonably large sample size without requiring an enormous\namount of time to compute. Agents are compared by the number of steps taken per episode, with less steps taken being\na more desirable outcome. The step counter is increased every time `step()` is called, including the look-ahead steps of\nthe RBQL. For a sense of perspective, the best possible solution to any square maze of size _s_ [2] is 2 _s −_ 2. Assuming a\nmaze with no walls, the shortest distance between two points _A_ and _B_ can be expressed as their Manhattan distance\n_|AX −_ _BX_ _|_ + _|AY −_ _BY |_ [8]. In the corners of a square, it holds that _AX_ = _AY_ and _BX_ = _BY_, so the distance can\nbe simplified as 2 _· |A −_ _B|_ . Setting _A_ = 0 and _B_ = _s −_ 1, this further simplifies to 2 _s −_ 2. This means that while\nthe amount of states (and thereby state-action pairs) increases quadratically, the best possible solution only increases\nlinearly. This in turn means that the amount of states that are not on the optimal path that the agent has to evaluate will\noften increase drastically with the size of the maze.\n\n\nLooking at the results, a few things can be observed. First of all, the average number of steps the RBQL agent takes\nis consistently lower than the Q-learning agent in all three maze sizes. It also has much less variation in step counts,\nwhich can be seen when looking at the areas of lighter hue. The light red areas are much more sporadic and spike\nfurther away from the average. The green areas stick much closer together. If the highest two step counts per episode\nwere not removed, RBQL would also have a few small spikes. These spikes would represent exploratory episodes\nwhere a new path is explored, resulting in a higher step count. In cases where the line is flat for a long period of time, it\ncan be assumed that the optimal solution is found. This can be seen in all three figures, where both the average and\nthe min/max range become a straight line close to the minimum. Important to note is that every maze has a different\noptimal solution, hence why the average sits above the blue line which denotes the lowest possible step count in any\nmaze of this size. It can also be observed that none of the lines ever go below this boundary, as is to be expected.\n\n\nSecond, even when removing the highest two step counts per episode, many of the Q-learning agent’s step counts are so\nlarge that scaling the graphs to fit them makes the RBQL agent’s data and the lower boundary difficult to see in the\ngraphs for the larger mazes. The highest step count values that have not been cut are 858 steps in figure 3, 7,585 in\nfigure 4 and 21,147 in figure 5, while the highest in total are 3,716 steps in figure 3, 20,553 in figure 4 and 26,315 in\nfigure 5.\n\n\nThird, it is interesting to see how the differences in average step counts evolve with the grid size. Table 1 shows this\ndifference in the first and last episode. The difference between the average step counts in the last episode especially is\nstriking, as it is close to doubling from each size to the next. Further, looking at the improvement of each agent as seen\nin table 2, one can see that the factor by which RBQL improves massively increases the bigger the maze becomes while\nthe Q-learner only slightly improves its performance in comparison. Additionally, most of the improvement of RBQL is\ndone in the first two episodes, while the Q-learner has a more gradual learning curve.\n\n\n7\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**1** _**,**_ **000**\n\n\n**900**\n\n\n**800**\n\n\n**700**\n\n\n**600**\n\n\n**500**\n\n\n**400**\n\n\n**300**\n\n\n**200**\n\n\n**100**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 3: Number of steps taken to find the goal in a randomly generated grid world maze of size 5 _×_ 5. The blue line is\nthe minimum step threshold for any maze of this size. The light red area shows the range of Q-learning agent’s highest\nand lowest step count, excluding the highest and lowest two. The red line shows the average performance. Similarly, the\nlight green area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest\ntwo, and the green line shows the average performance.\n\n\n8\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-7-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**8** _**,**_ **000**\n\n\n**6** _**,**_ **000**\n\n\n**5** _**,**_ **000**\n\n\n**4** _**,**_ **000**\n\n\n**3** _**,**_ **000**\n\n\n**2** _**,**_ **000**\n\n\n**1** _**,**_ **000**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 4: Number of steps taken to find the goal in a randomly generated grid world maze of size 10 _×_ 10. The light\nred area shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two.\nThe red shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n9\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-8-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 5: Number of steps taken to find the goal in a randomly generated grid world maze of size 15 _×_ 15. The light red\narea shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two. The\nred line shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n10\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-9-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nTable 1: Difference in average step counts of the Q-learner and RBQL. The difference expresses how many times more\nsteps the Q-learner took compared to RBQL.\n\n\nGrid size Q-learner steps RBQL steps Difference\n**Episode 0**\n5 _×_ 5 278.06 191.84 1.45\n\n10 _×_ 10 3,308.46 843.52 3.92\n15 _×_ 15 7,180.98 1,965 3.65\n**Episode 24**\n5 _×_ 5 49.14 9.62 5.11\n\n10 _×_ 10 281.44 23.68 11.89\n\n15 _×_ 15 778.68 35.96 21.65\n\n\nLastly, the RBQL agent seems to find an optimal policy at around episode 4 for the 5 _×_ 5, episode 6 for the 10 _×_ 10 and\nepisode 10 for the 15 _×_ 15 grid. As the previous figures show, the Q-learning agent does not come close to similarly\nlow step counts and therefore does not reach an optimal policy at all with the same amount of training.\n\n\nTable 2: Difference in average step counts of the Q-learner and RBQL. Improvement shows the factor by which the\namount of steps is reduced from episode 0 to 24.\n\n\nGrid size Steps in episode 0 Steps in episode 24 Improvement\n**Q-learning agent**\n5 _×_ 5 278.06 49.14 5.66\n\n10 _×_ 10 3,308.46 281.44 11.76\n15 _×_ 15 7,180.98 778.68 9.22\n**RBQL agent**\n5 _×_ 5 191.84 9.62 19.94\n\n10 _×_ 10 843.52 23.68 35.62\n\n15 _×_ 15 1,965 35.96 90.76\n\n\nTo further show RBQL’s efficiency, it has also been tested under the same parameters in a grid of size 50 _×_ 50. The\nresults can be seen in figure 6. This test is done to demonstrate that even such a large maze can be explored by RBQL.\nAs with the previous examples, by far the largest policy improvement still happens in the first episode. With mazes of\nsuch a large size, a lot more spikes in step counts are seen in later episodes because there are more states to explore. The\ndifference in average step counts goes from 20,811.08 in episode 0 to 344.9 in episode 24, an improvement by a factor\nof 60.34. This is worse than the improvement in the 15 _×_ 15 mazes, but still almost double that of the 10 _×_ 10 mazes.\n\n\n**5** **Discussion**\n\n\nThis chapter explores the practicality of using this algorithm to solve other Markov decision processes. It discusses\nwhich parts of the implementation are and are not specific to the problem of fastest path through a maze, which\nimprovements can be made to make it more applicable for other problems and showcases further points for research in\nthis field. The constraints given in this paper are that the agent will attempt to solve deterministic, episodic tasks with a\nsingle terminal state as its only source of positive rewards. This chapter also discusses which of these constraints can be\ndismissed.\n\n\nThere are a few parts of the implementation as presented in chapter 3.2 that are only applicable to this specific problem.\nThis is not necessarily a bad thing, as the purpose of the RBQL agent is to utilize knowledge of its environment. As a\nresult of this, the only parts that cannot be directly adapted for other problems are the way the agent builds its model.\nIn the grid world maze, it can assume that every state has the same actions it can take and has a neighboring state in\neach direction (though it may sometimes be itself). Further, every action always has an opposite action, going up can\nalways be undone by going down for example. These assumptions allow it to easily build a model of the grid world\nand influence its policy in how it further explores it. They allow the agent to take steps in each direction to check for\nwalls and they allow the agent to backtrack when it is stuck in a dead end. These assumptions cannot be guaranteed\nfor other Markov decision processes or even for grid worlds with more complex behavior like a wind tunnel that if\nwalked through also pushes the agent one tile in the direction the wind is traveling. The way in which the agent builds a\n\n\n11\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**3**\n\n\n**2** _**.**_ **8**\n\n\n**2** _**.**_ **6**\n\n\n**2** _**.**_ **4**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 6: Number of steps taken to find the goal in a randomly generated grid world maze of size 50 _×_ 50. The light\ngreen area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest two,\nand the green line shows the average performance.\n\n\n12\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-11-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nmodel has to either be designed for each environment individually or it has to be abstracted so that it is more broadly\napplicable. Finding such an approach to model building is one area of improvement for RBQL. Importantly though,\nnone of these assumptions are required for the agent to function. Backtracking, opposite steps and the same actions for\nevery state simply make the implementation easier and more efficient. As long as no path of a directed graph would\ncause the agent to be stuck with no way to reach a terminal state, it can be explored and evaluated.\n\n\nAnother improvement to the way the implementation builds its model is to simplify it as far as possible. As the amount\nof states directly influences how long a problem takes to solve, RBQL will become more efficient the more it can\nremove unnecessary states. Currently, every position has its own state. If the agent could detect “hallways” – tiles with\nparallel walls – they could be removed without problem in favor of directly connecting the two tiles at either side of the\nhallway – only the negative rewards for the length of the hallway would have to be implemented into the model. Further,\nif there is a non-forking path that leads into a dead end, the entire path could be treated as a wall and ignored entirely.\nThis would leave only the starting state, terminal state, turns and forking paths to evaluate. Both of these additions leave\nthe key part of the algorithm, traversing the model backwards and applying the RBQL update formula, untouched.\n\n\nRBQL can be easily adapted to include multiple terminal states with the same or different rewards and this is already\nsupported by the implementation. There are two possible ways to do this. First is to create an imaginary state that\nall terminal states lead into from which the backtracking always starts. Second is to remember all terminal states and\nbacktrack from each of them. The first option is much more efficient as each state still only gets evaluated once while\nthe second version avoids having to tamper with the model.\n\n\nFinally, RBQL could be adapted to work in non-deterministic environments. To reiterate, deterministic means that a\nstate-action pair always yields the same state-reward pair. If the agent could, while building its model, also estimate the\ntransition probabilities of a state-action pair to a new state, RBQL could still be used to evaluate the states. The RBQL\nupdate rule can be generalized to\n\n\n\n_Q_ ( _St, At_ ) _←_ �\n\n_s∈St_ +1\n\n\n\n( _Rs_ + _γ_ max _Q_ ( _s, a_ )) _· p_ (3)\n_a_\n� �\n\n\n\nwhere _St_ +1 is the set of possible states when taking _At_ from _St_, _p_ is the probability of reaching _s_ when taking _At_ from\n_St_ and _Rs_ is the reward of reaching _s_ . In a deterministic environment, _St_ +1 only consists of one state with _p_ = 1,\nnegating these additions. Whether RBQL would be as effective in non-deterministic environments as in deterministic\nenvironments is something to be explored in further studies.\n\n\nThe only constraint on the algorithm that cannot easily be circumvented is its episodic nature. Because the agent relies\non a terminal state from which to propagate the rewards backwards from, a continuous task implementation seems\nimpossible to implement.\n\n\n**6** **Conclusion**\n\n\nThis paper has introduced recursive backwards Q-learning, a model-based reinforcement learning algorithm that\nevaluates all known state-action pairs of the model at the end of each episode with the Q-learning update rule. It has\nalso shown how recursive backwards Q-learning relates to, adapts and improves on them. This paper has presented\nan implementation of recursive backwards Q-learning in the Godot game engine to test its performance. Through\nmultiple tests, it has been shown to be superior in finding the shortest path through a randomly generated grid world\nmaze. It has been argued that this algorithm could be adapted to solve other deterministic, episodic tasks more quickly\nthan Q-learning. Further, it has given avenues for further research in adapting recursive backwards Q-learning for\nnon-deterministic problems.\n\n\n**References**\n\n\n[1] Richard Bellman, “A markovian decision process,” _Journal of Mathematics and Mechanics_, vol. 6, no. 5, pp. 679–\n684, 1957. [Online]. Available: `[http://www.jstor.org/stable/24900506](http://www.jstor.org/stable/24900506)` .\n\n[2] Christopher John Cornish Hellaby Watkins, “Learning from delayed rewards,” 1989.\n\n[3] Richard S Sutton and Andrew G Barto, _Reinforcement learning: An introduction_ . MIT press, 2018.\n\n[4] Richard S. Sutton, “Learning to predict by the methods of temporal differences,” _Machine Learning_, vol. 3, no. 1,\npp. 9–44, 1988. DOI: `[10.1007/bf00115009](https://doi.org/10.1007/bf00115009)` .\n\n[5] Richard Bellman, “Dynamic programming,” _Princeton, USA: Princeton University Press_, vol. 1, no. 2, p. 3, 1957.\n\n[6] Peter Gabrovšek, “Analysis of maze generating algorithms,” _IPSI Transactions on Internet Research_, vol. 15,\nno. 1, pp. 23–30, 2019.\n\n\n13\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n[7] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael, “A formal basis for the heuristic determination of minimum\ncost paths,” _IEEE Transactions on Systems Science and Cybernetics_, vol. 4, no. 2, pp. 100–107, 1968. DOI:\n`[10.1109/TSSC.1968.300136](https://doi.org/10.1109/TSSC.1968.300136)` .\n\n[8] Eugene F Krause, “Taxicab geometry,” _The Mathematics Teacher_, vol. 66, no. 8, pp. 695–706, 1973.\n\n\n14\n\n\n",
          "ranking": null,
          "is_open_access": false,
          "user_provided": true,
          "pdf_path": "output/literature/user_2404.15822v1/user_2404.15822v1.pdf"
        },
        "chunk_text": "The strong performance on unseen repositories suggests good generalization capabilities of the learned representations.",
        "chunk_index": 0
      },
      "summary": "The strong performance on unseen repositories suggests good generalization capabilities of the learned representations.",
      "vector_score": 0.6880000000000001,
      "llm_score": 0.86,
      "combined_score": 0.86,
      "source_query": "mock_query_discussion"
    },
    {
      "chunk": {
        "chunk_id": "mock_14",
        "paper": {
          "id": "acda55ebdf39c6634e89a9730ff7d963471f2b0a",
          "title": "Expected Eligibility Traces",
          "published": "2020-07-03",
          "authors": [
            "H. V. Hasselt",
            "Sephora Madjiheurem",
            "Matteo Hessel",
            "David Silver",
            "André Barreto",
            "Diana Borsa"
          ],
          "summary": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state.\nIn this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that some- times substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
          "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17200/17007",
          "doi": "10.1609/aaai.v35i11.17200",
          "fields_of_study": [
            "Computer Science",
            "Mathematics"
          ],
          "venue": "AAAI Conference on Artificial Intelligence",
          "citation_count": 41,
          "bibtex": "@Article{Hasselt2020ExpectedET,\n author = {H. V. Hasselt and Sephora Madjiheurem and Matteo Hessel and David Silver and André Barreto and Diana Borsa},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Expected Eligibility Traces},\n volume = {abs/2007.01839},\n year = {2020}\n}\n",
          "markdown_text": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)\n\n# **Expected Eligibility Traces**\n\n\n**Hado van Hasselt** [1] **, Sephora Madjiheurem** [2] **, Matteo Hessel** [1]\n\n**David Silver** [1] **, Andr´e Barreto** [1] **, Diana Borsa** [1]\n\n1 DeepMind\n2 University College London, UK\n\n\n\n**Abstract**\n\n\nThe question of how to determine which states and actions\nare responsible for a certain outcome is known as the _credit_\n_assignment problem_ and remains a central research question\nin reinforcement learning and artificial intelligence. _Eligibil-_\n_ity traces_ enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not\nto counterfactual sequences that could also have led to the\ncurrent state. In this work, we introduce _expected eligibility_\n_traces_ . Expected traces allow, with a single update, to update\nstates and actions that could have preceded the current state,\neven if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous)\ntraces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide\na way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which\nensures that the resulting algorithm is a strict generalisation\nof TD( _λ_ ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.\n\n\n**Motivation and Summary**\n\n\nAppropriate credit assignment has long been a major research\ntopic in artificial intelligence (Minsky 1963). To make effective decisions and understand the world, we need to accurately associate events, like rewards or penalties, to relevant\nearlier decisions or situations. This is important both for learning accurate predictions, and for making good decisions.\n_Temporal credit assignment_ can be achieved with repeated\ntemporal-difference (TD) updates (Sutton 1988). One-step\nTD updates propagate information slowly: when a surprising value is observed, the state immediately preceding it is\nupdated, but no earlier states or decisions are updated. _Multi-_\n_step_ updates (Sutton 1988; Sutton and Barto 2018) propagate\ninformation faster over longer temporal spans, speeding up\ncredit assignment and learning. Multi-step updates can be\nimplemented online using _eligibility traces_ (Sutton 1988),\nwithout incurring significant additional computational expense, even if the time spans are long; these algorithms have\ncomputation that is independent of the temporal span of the\npredictions (van Hasselt and Sutton 2015).\n\n\nCopyright c _⃝_ 2021, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\nMDP True value TD(0) TD(λ) ET(λ)\n\n\nFigure 1: A comparison of TD(0), TD( _λ_ ), and the new\nexpected-trace algorithm ET( _λ_ ) (with _λ_ = 0 _._ 9). The MDP\nis illustrated on the left. Each episode, the agent moves randomly down and right from the top left to the bottom right,\nwhere any action terminates the episode. Reward on termination are +1 with probability 0.2, and zero otherwise—all\nother rewards are zero. We plot the value estimates after the\nfirst positive reward, which occurred in episode 5. We see\na) TD(0) only updated the last state, b) TD( _λ_ ) updated the\ntrajectory in this episode, and c) ET( _λ_ ) additionally updated\ntrajectories from earlier (unrewarding) episodes.\n\n\nTraces provide temporal credit assignment, but do not assign credit _counterfactually_ to states or actions that _could_\nhave led to the current state, but did not do so this time.\nCredit will eventually trickle backwards over the course of\nmultiple visits, but this can take many iterations. As an example, suppose we collect a key to open a door, which leads\nto an unexpected reward. Using standard one-step TD learning, we would update the state in which the door opened.\nUsing eligibility traces, we would also update the preceding\ntrajectory, including the acquisition of the key. But we would\nnot update other sequences that _could_ have led to the reward,\nsuch as collecting a spare key or finding a different entrance.\nThe problem of credit assignment to counterfactual states\nmay be addressed by learning a model, and using the model\nto propagate credit (cf. Sutton 1990; Moore and Atkeson\n1993; Chelu, Precup, and van Hasselt 2020); however, it\nhas often proven challenging to construct and use models\neffectively in complex environments (cf. van Hasselt, Hessel,\nand Aslanides 2019).\nWe introduce a new approach to counterfactual credit assignment, based on the concept of _expected eligibility traces_ .\nWe present a family of algorithms, which we call ET( _λ_ ), that\nuse expected traces to update their predictions. We analyse\nthe nature of these expected traces, and illustrate their benefits empirically in several settings—see Figure 1 for a first\n\n\n\n9997\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-1.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-3.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-4.png)\n\n\nillustration. We introduce a bootstrapping mechanism that\nprovides a spectrum of algorithms between standard eligibility traces and expected eligibility traces, and also discuss\nways to apply these ideas with deep neural networks. Finally,\nwe discuss possible extensions and connections to related\nideas such as successor features.\n\n\n**Background**\nSequential decision problems can be modelled as Markov\ndecision processes [1] (MDP) ( _S, A, p_ ) (Puterman 1994), with\nstate space _S_, action space _A_, and a joint transition and\nreward distribution _p_ ( _r, s_ _[′]_ _|s, a_ ). An agent selects actions according to its policy _π_, such that _At ∼_ _π_ ( _·|St_ ) where _π_ ( _a|s_ )\ndenotes the probability of selecting _a_ in _s_, and observes random rewards and states generated according to the MDP, resulting in trajectories _τt_ : _T_ = _{St, At, Rt_ +1 _, St_ +1 _, . . ., ST }_ .\nA central goal is to predict _returns_ of future discounted rewards (Sutton and Barto 2018)\n\n\n_Gt ≡_ _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _Rt_ +2 + _γt_ +1 _γt_ +2 _Rt_ +3 + _. . ._\n\n\n\n=\n\n\n\n_T_\n� _γt_ [(] +1 _[i][−]_ [1)] _[R][t]_ [+] _[i][,]_\n\n\n_i_ =1\n\n\n\nwhere _T_ is for instance the time the current episode terminates or _T_ = _∞_, and where _γt ∈_ [0 _,_ 1] is a (possibly constant) discount factor and _γt_ [(] _[n]_ [)] = [�] _[n]_ _k_ =0 _[−]_ [1] _[γ][t]_ [+] _[k]_ [, and] _[ γ]_ _t_ [(0)] = 1.\nThe value _vπ_ ( _s_ ) = E [ _Gt|St_ = _s, π_ ] of state _s_ is the expected return for a policy _π_ . Rather than writing the return as\na random variable _Gt_, it will be convenient to instead write it\nas an explicit function _G_ ( _τ_ ) of the random trajectory _τ_ . Note\nthat _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ).\nWe approximate the value with a function _v_ **w** ( _s_ ) _≈_ _vπ_ ( _s_ ).\nThis can for instance be a table—with a single separate entry\n_w_ [ _s_ ] for each state—a linear function of some input features,\nor a non-linear function such as a neural network with parameters **w** . The goal is to iteratively update **w** with\n\n\n**w** _t_ +1 = **w** _t_ + ∆ **w** _t_\n\n\nsuch that _v_ **w** approaches the true _vπ_ . Perhaps the simplest\nalgorithm to do so is the Monte Carlo (MC) algorithm\n\n\n∆ **w** _t ≡_ _α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._\n\n\nMonte Carlo is effective, but has high variance, which can\nlead to slow learning. TD learning (Sutton 1988; Sutton and\nBarto 2018) instead replaces the return with the current estimate of its expectation _v_ ( _St_ +1) _≈_ _G_ ( _τt_ +1: _T_ ), yielding\n\n\n∆ **w** _t ≡_ _αδt∇_ **w** _v_ **w** ( _St_ ) _,_ (1)\nwhere _δt ≡_ _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ ) _,_\n\n\nwhere _δt_ is called the temporal-difference (TD) error. We\ncan interpolate between these extremes, for instance with\n_λ_ -returns which smoothly mix values and sampled returns:\n\n\n_G_ _[λ]_ ( _τt_ : _T_ ) = _Rt_ +1+ _γt_ +1�(1 _−λ_ ) _v_ **w** ( _St_ +1)+ _λG_ _[λ]_ ( _τt_ +1: _T_ )� _._\n\n\n‘Forward view’ algorithms, like the MC algorithm, use returns\nthat depend on future trajectories and need to wait until the\n\n\n1The ideas in this paper extend naturally to POMDPs (cf. **?** ).\n\n\n\nend of an episode to construct their updates, which can take a\nlong time. Conversely, ‘backward view’ algorithms rely only\non past experiences and can update their predictions online,\nduring an episode. Such algorithms build an _eligibility trace_\n(Sutton 1988; Sutton and Barto 2018). An example is TD( _λ_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ _t,_ with _**e**_ _t_ = _γtλ_ _**e**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ ) _,_\n\n\nwhere _**e**_ _t_ is an accumulating eligibility trace. This trace can\nbe viewed as a function _**e**_ _t ≡_ _**e**_ ( _τ_ 0: _t_ ) of the trajectory of past\ntransitions. The TD update in (1) is known as TD(0), because\nit corresponds to using _λ_ = 0. TD( _λ_ = 1) corresponds to an\nonline implementation of the MC algorithm. Other variants\nexist, using other kinds of traces, and equivalences have been\nshown between these algorithms and their forward views that\nuse _λ_ -returns: these backward-view algorithms converge to\nthe same solution as the corresponding forward view, and can\nin some cases yield equivalent weight updates (Sutton 1988;\nvan Seijen and Sutton 2014; van Hasselt and Sutton 2015).\n\n\n**Expected Traces**\n\n\nThe main idea of this paper is to use the concept of an _ex-_\n_pected eligibility trace_, defined as\n\n\n_**z**_ ( _s_ ) _≡_ E [ _**e**_ _t | St_ = _s_ ] _,_\n\n\nwhere the expectation is over the agent’s policy and the MDP\ndynamics. We introduce a concrete family of algorithms,\nwhich we call ET( _λ_ ) and ET( _λ_, _η_ ), that learn expected traces\nand use them in value updates. We analyse these algorithms\ntheoretically, describe specific instances, and discuss computational and algorithmic properties.\n\n\n**ET(** _λ_ **)**\n\n\nWe propose to learn approximations _**zθ**_ ( _s_ ) _≈_ _**z**_ ( _s_ ), with parameters _**θ**_ _∈_ R _[d]_ (e.g., the weights of a neural network). One\nway to learn _**zθ**_ is by updating it toward the instantaneous\ntrace _**e**_ _t_, by minimizing an empirical loss _L_ ( _**e**_ _t,_ _**zθ**_ ( _St_ )). For\ninstance, _L_ could be a component-wise squared loss, optimized with stochastic gradient descent:\n\n\n_**θ**_ _t_ +1 = _**θ**_ _t_ + ∆ _**θ**_ _t,_ where (2)\n\n\n1\n\n∆ _**θ**_ _t_ = _−β_ _[∂]_\n\n_∂_ _**θ**_ 2 [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))] _[⊤]_ [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))]\n\n= _β_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] ( _**e**_ _t −_ _**zθ**_ ( _St_ )) _,_ (3)\n\n_∂_ _**θ**_\n\n\nwhere _[∂z]_ _**[θ]**_ _∂_ [(] _**θ**_ _[S][t]_ [)] is a _|_ _**θ**_ _| × |_ _**e**_ _|_ Jacobian [2] and _β_ is a step size.\n\nThe idea is then to use _**zθ**_ ( _s_ ) _≈_ E [ _**e**_ _t | St_ = _s_ ] in place\nof _**e**_ _t_ in the value update, which becomes\n\n\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (4)\n\n\nWe call this ET( _λ_ ). Below, we prove that this update can\nbe unbiased and can have lower variance than TD( _λ_ ). Algorithm 1 shows pseudo-code for a concrete instance of ET( _λ_ ).\n\n\n2The Jacobian-vector product can efficiently be computed (e.g.,\nvia auto-differentiation) with computational requirements that are\ncomparable to the computation of the loss.\n\n\n\n9998\n\n\n\n\n**Algorithm 1** ET( _λ_ )\n\n\n1: initialise **w**, _**θ**_\n2: **for** _M_ episodes **do**\n3: initialise _**e**_ = **0**\n\n4: observe initial state _S_\n5: **repeat** for each step in episode _m_\n6: generate _R_ and _S_ _[′]_\n\n7: _δ ←_ _R_ + _γv_ **w** ( _S_ _[′]_ ) _−_ _v_ **w** ( _S_ )\n8: _**e**_ _←_ _γλ_ _**e**_ + _∇_ **w** _v_ **w** ( _S_ )\n\n9: _**θ**_ _←_ _**θ**_ + _β_ _[∂]_ _**[z]**_ _∂_ _**[θ]**_ _**θ**_ [(] _[S]_ [)] ( _**e**_ _−_ _**zθ**_ ( _S_ ))\n\n10: **w** _←_ **w** + _αδ_ _**zθ**_ ( _S_ )\n11: **until** _S_ is terminal\n\n12: **end for**\n\n13: **Return w**\n\n\n**Interpretation and ET(** _λ, η_ **)**\n\nWe can interpret TD(0) as taking the MC update and replacing the return from the subsequent state, which is a function\nof the future trajectory, with a state-based estimate of its expectation: _v_ **w** ( _St_ +1) _≈_ E [ _G_ ( _τt_ +1: _T_ ) _|St_ +1 ]. This becomes\nmost clear when juxtaposing the updates:\n\n\n_α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _,_ (MC)\n_α_ ( _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._ (TD)\n\n\nTD( _λ_ ) also uses a function of a trajectory: the trace _**e**_ _t_ . We\npropose replacing this as well with a function of state: the\nexpected trace _**zθ**_ ( _St_ ) _≈_ E [ _**e**_ ( _τ_ 0: _t_ ) _|St_ ]. Again juxtaposing:\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ ( _τ_ 0: _t_ ) _,_ (TD( _λ_ ))\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (ET( _λ_ ))\n\n\nWe can interpolate smoothly between MC and TD(0) via\n_λ_ . This is often useful to trade off variance of the return with\npotential bias of the value estimate. For instance, we might\nnot have access to the true state _s_, and might instead have to\nrely on features **x** ( _s_ ). Then we cannot always represent or\nlearn the true values _v_ ( _s_ )—for instance different states may\nbe aliased (Whitehead and Ballard 1991).\nSimilarly, when moving from TD( _λ_ ) to ET( _λ_ ) we replaced\na trajectory-based trace with a state-based estimate. This\nmight induce bias and, again, we can smoothly interpolate by\nusing a recursively defined mixture trace _**y**_ _t_, as defined as [3]\n\n\n_**y**_ _t_ = (1 _−_ _η_ ) _**zθ**_ ( _St_ ) + _η_ � _γtλ_ _**y**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ )� _._ (5)\n\n\nThis recursive usage of the estimates _**zθ**_ ( _s_ ) at previous states\nis analogous to bootstrapping on future state values when\nusing a _λ_ -return, with the important difference that the arrow\nof time is opposite. This means we do not first have to convert\nthis into a backward view: the quantity can already be computed from past experience directly. We call the algorithm\nthat uses this mixture trace ET( _λ_, _η_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**y**_ ( _St_ ) _._ (ET( _λ_, _η_ ))\n\n\n3While _**y**_ _t_ depends on both _η_ and _λ_ we leave this dependence\nimplicit, as is conventional for traces.\n\n\n\nNote that if _η_ = 1 then _**y**_ _t_ = _**e**_ _t_ equals the instantaneous\ntrace: ET( _λ_, 1) is equivalent to TD( _λ_ ). If _η_ = 0 then _**y**_ _t_ = _**z**_ _t_\nequals the expected trace; the algorithm introduced earlier\nas ET( _λ_ ) is equivalent to ET( _λ_, 0). By setting _η ∈_ (0 _,_ 1), we\ncan smoothly interpolate between these extremes.\n\n\n**Theoretical Analysis**\n\nWe now analyse the new ET algorithms theoretically. First\nwe show that if we use _**z**_ ( _s_ ) directly and _s_ is Markov then the\nupdate has the same expectation as TD( _λ_ ) (though possibly\nwith lower variance), and therefore also inherits the same\nfixed point and convergence properties.\n\n\n**Lemma 1.** _If s is Markov, then_\n\n\nE [ _δt_ _**e**_ _t | St_ = _s_ ] = E [ _δt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] _._\n\n\n_Proof._ In Appendix .\n\n\n**Proposition 1.** _Let_ _**e**_ _t be any trace vector, updated in any_\n_way. Let_ _**z**_ ( _s_ ) = E [ _**e**_ _t | St_ = _s_ ] _. Consider the ET(λ) algo-_\n_rithm_ ∆ **w** _t_ = _αtδt_ _**z**_ ( _St_ ) _. For all Markov states s the expec-_\n_tation of this update is equal to the expected update under_\n_instantaneous trace_ _**e**_ _t, and its variance is lower or equal:_\n\n\nE [ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] = E [ _αtδt_ _**e**_ _t|St_ = _s_ ] _and_\nV[ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] _≤_ V[ _αtδt_ _**e**_ _t|St_ = _s_ ] _,_\n\n\n_where the second inequality holds component-wise for the_\n_update vector, and is strict when_ V[ _**e**_ _t|St_ ] _>_ 0 _._\n\n\n_Proof._ We have\n\n\nE [ _αtδt_ _**e**_ _t | St_ = _s_ ]\n= E [ _αtδt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] (Lemma 1)\n= E [ _αtδt | St_ = _s_ ] _**z**_ ( _s_ )\n= E [ _αtδt_ _**z**_ ( _St_ ) _| St_ = _s_ ] _._ (6)\n\n\nDenote the _i_ -th component of _**z**_ ( _St_ ) by _zt,i_ and the _i_ -th\ncomponent of _**e**_ _t_ by _et,i_ . Then, we also have\n\n\nE � ( _αtδtzt,i_ ) [2] _|St_ = _s_ � = E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � _zt,i_ [2]\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E [ _et,i|St_ = _s_ ] [2]\n\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ ��E � _e_ [2] _t,i_ _[|][S][t]_ [=] _[ s]_ � _−_ V[ _et,i|St_ = _s_ ]�\n\n_≤_ E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E � _e_ [2] _t,i_ _[|][ S][t]_ [=] _[ s]_ �\n\n= E � ( _αtδtet,i_ ) [2] _| St_ = _s_ � _,_\n\n\nwhere the last step used the fact that _s_ is Markov, and the inequality is strict when V[ _et,i|St_ ] _>_ 0. Since the expectations\nare equal, as shown in (6), the conclusion follows.\n\n\n**Interpretation** Proposition 1 is a strong result: it holds for\nany trace update, including accumulating traces (Sutton 1984,\n1988), replacing traces (Singh and Sutton 1996), dutch traces\n(van Seijen and Sutton 2014; van Hasselt, Mahmood, and\nSutton 2014; van Hasselt and Sutton 2015), and future traces\nthat may be discovered. It implies convergence of ET( _λ_ )\nunder the same conditions as TD( _λ_ ) (Dayan 1992; Peng 1993;\n\n\n\n9999\n\n\n\n\nTsitsiklis 1994) with lower variance when V[ _**e**_ _t|St_ ] _>_ 0,\nwhich is the common case.\nNext, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a\nlearned approximation _**z**_ _t_ ( _s_ ) _≈_ _**z**_ ( _s_ ) that relies solely on\nobserved experience.\n\n**Proposition 2.** _Let_ _**e**_ _t an instantaneous trace vector. Then_\n1 _nt_ ( _s_ )\n_let_ _**z**_ _t_ ( _s_ ) _be the empirical mean_ _**z**_ _t_ ( _s_ ) = _nt_ ( _s_ ) � _i_ _**e**_ _t_ _[s]_ _i_ _[,]_\n_where t_ _[s]_ _i_ _[denotes past times when we have been in state]_\n_s, that is St_ _[s]_ _i_ [=] _[ s][, and][ n][t]_ [(] _[s]_ [)] _[ is the number of visits to][ s]_\n_in the first t steps. Consider the expected trace algorithm_\n**w** _t_ +1 = **w** _t_ + _αtδt_ _**z**_ _t_ ( _St_ ) _. If St is Markov, the expectation of_\n_this update is equal to the expected update with instantaneous_\n_traces_ _**e**_ _t, while attaining a potentially lower variance:_\n\n\nE [ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] = E [ _αtδt_ _**e**_ _t | St_ ] _and_\nV[ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] _≤_ V[ _αtδt_ _**e**_ _t | St_ ] _,_\n\n\n_where the second inequality holds component-wise. The in-_\n_equality is strict when_ V[ _**e**_ _t | St_ ] _>_ 0 _._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** Proposition 2 mirrors Proposition 1 but, importantly, covers the case where we estimate the expected\ntraces from data, rather than relying on exact estimates. This\nmeans the benefits extend to this pure learning setting. Again,\nthe result holds for any trace update. The inequality is typically strict when the path leading to state _St_ = _s_ is stochastic\n(due to environment or policy).\nNext we consider what happens if we do not have Markov\nstates and instead have to rely on, possibly non-Markovian,\nfeatures **x** ( _s_ ). We then have to pick a function class and for\nthe purpose of this analysis we consider linear expected traces\n_**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) and values _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), as convergence for non-linear values can not always be assured even\nfor standard TD( _λ_ ) (Tsitsiklis and Van Roy 1997), without\nadditional assumptions (e.g., Ollivier 2018; Brandfonbrener\nand Bruna 2020).\n\n**Proposition 3.** _When using approximations z_ **Θ** ( _s_ ) = **Θx** ( _s_ )\n_and v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ) _then, if_ (1 _−_ _η_ ) **Θ** + _η_ I _is non-singular,_\n_ET(λ, η) has the same fixed point as TD(λη)._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** This result implies that linear ET( _λ_, _η_ ) converges under similar conditions as linear TD( _λ_ _[′]_ ) for _λ_ _[′]_ = _λ·η_ .\nIn particular, when **Θ** is non-singular, using the approximation _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) in ET( _λ_, 0) = ET( _λ_ ) implies convergence to the fixed point of TD(0).\nThough ET( _λ_, _η_ ) and TD( _λη_ ) have the same fixed point,\nthe algorithms are not equivalent. In general, their updates\nare not the same. Linear approximations are more general\nthan tabular functions (which are linear functions of a indicator vector for the current state), and we have already seen\nin Figure 1 that ET( _λ_ ) behaves quite differently from both\nTD(0) and TD( _λ_ ), and we have seen its variance can be lower\nin Propositions 1 and 2. Interestingly, **Θ** resembles a preconditioner that speeds up the linear semi-gradient TD update,\n\n\n10000\n\n\n\nepisode 5\n1st reward\n\n\n\nepisode 12\n2nd reward\n\n\n\nepisode 100\n20 rewards\n\n\n\nepisode 1K\n~200 rewards\n\n\n\nepisode 10K\n~2K rewards\n\n\n\nFigure 2: In the same setting as Figure 1, we show later value\nestimates after more rewards have been observed. TD(0)\nlearns slowly but steadily, TD( _λ_ ) learns faster but with higher\nvariance, and ET( _λ_ ) learns both fast and stable.\n\n\nsimilar to how second-order optimisation algorithms (Amari\n1998; Martens 2016) precondition the gradient updates.\n\n\n**Empirical Analysis**\n\nFrom the insights above, we expect that ET( _λ_ ) yields lower\nprediction errors because it has lower variance and aggregates information across episodes better. In this section we\nempirically investigate expected traces in several experiments.\nWhenever we refer to ET( _λ_ ), this is equivalent to ET( _λ_, 0).\n\n\n\n**An Open World**\n\nFirst consider the grid world depicted in Figure 1. The agent\nrandomly moves right or down (excluding moves that would\nhit a wall), starting from the top-left corner. Any action in the\nbottom-right corner terminates the episode with +1 reward\nwith probability 0 _._ 2, and 0 otherwise. All other rewards are 0.\nFigure 1 shows value estimates after the first positive reward, which occurred in the fifth episode. TD(0) updated a\nsingle state, TD( _λ_ ) updated earlier states in that episode, and\nET( _λ_ ) additionally updated states from previous episodes.\nFigure 2 additionally shows value estimates after the\nsecond reward (which occurred in episode 12), and after\nroughly 20, 200, and 2000 rewards (or 100, 1000, and 10 _,_ 000\nepisodes, respectively). ET( _λ_ ) converged faster than TD(0),\nwhich propagated information slowly, and faster than TD( _λ_ ),\nwhich exhibited higher variance. All step sizes decayed as\n_α_ = _β_ = ~~�~~ 1 _/k_, where _k_ is the current episode number.\n\n\n**A Multi-Chain**\n\nWe now consider the multi-chain shown in Figure 3. We\nfirst compare TD( _λ_ ) and ET( _λ_ ) with tabular values on various variants of the multi-chain, corresponding to _m ∈_\n_{_ 1 _,_ 2 _,_ 4 _,_ 8 _, ...,_ 128 _}_ parallel chains of length _n_ = 4. The leftmost plot in Figure 4 shows the average root mean squared\nerror (RMSE) of the value predictions after 1024 episodes.\nWe ran 10 seeds for each combination of step size 1 _/t_ _[d]_ with\n_d ∈{_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 1 _}_ and _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ .\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-4-0.png)\n\neither is +1 with probability 0 _._ 9 or _−_ 1 with probability 0 _._ 1.\n\n\nThe left plot in Figure 4 shows value errors for different\n_m_, minimized over _d_ and _λ_ . The prediction error of TD( _λ_ )\n(blue) grew quickly with the number of parallel chains. ET( _λ_ )\n(orange) scaled better, because it updates values in multiple\nchains (from past episodes) upon receiving a surprising reward (e.g., _−_ 1) on termination. The other three plots in Figure\n4 show value error as a function of _λ_ for a subset of problems\ncorresponding to _m ∈{_ 8 _,_ 32 _,_ 128 _}_ . The dependence on _λ_\ndiffers across algorithms and problem instances, but ET( _λ_ )\nconsistently achieved lower error than TD( _λ_ ), especially with\nhigh _λ_ . Further analysis, including on step-size sensitivity, is\nincluded in the appendix.\nNext, we encode each state with a feature vector **x** ( _s_ )\ncontaining a binary indicator vector of the branch, a binary\nindicator of the progress along the chain, a bias that always\nequals one, and two binary features indicating when we are in\nthe start (white) or bottleneck (orange) state. We extend the\nlengths of the chains to _n_ = 16. Both TD( _λ_ ) and ET( _λ_ ) use\na linear value function _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), and ET( _λ_ ) uses a\nlinear expected trace _z_ **Θ** ( _s_ ) = **Θx** ( _s_ ). All updates use the\nsame constant step size _α_ . The left plot in Figure 5 shows the\naverage root mean squared value error after 1024 episodes\n(averaged over 10 seeds). For each point the best constant\nstep size _α ∈{_ 0 _._ 01 _,_ 0 _._ 03 _,_ 0 _._ 1 _}_ (shared across all updates)\nand _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ is selected. ET( _λ_ ) (orange)\nattained lower errors across all values of _m_ (left plot), and\nfor all _λ_ (center two plots, for two specific _m_ ). The right plot\nshows results for smooth interpolations via _η_, for _λ_ = 0 _._ 9\nand _m_ = 16. The full expected trace ( _η_ = 0) performed well\nhere, we expect in other settings the additional flexibility of\n_η_ could be beneficial.\n\n\n**Expected Traces in Deep Reinforcement Learning**\n\n(Deep) neural networks are a common choice of function\nclass in reinforcement learning (e.g., Werbos 1990; Tesauro\n1992, 1994; Bertsekas and Tsitsiklis 1996; Prokhorov and\nWunsch 1997; Riedmiller 2005; van Hasselt 2012; Mnih\net al. 2015; van Hasselt, Guez, and Silver 2016; Wang et al.\n2016; Silver et al. 2016; Duan et al. 2016; Hessel et al. 2018).\nEligibility traces are not very commonly combined with deep\nnetworks (but see Tesauro 1992; Elfwing, Uchibe, and Doya\n2018), perhaps in part because of the popularity of experience\n\n\n10001\n\n\n\nreplay (Lin 1992; Mnih et al. 2015; Horgan et al. 2018).\nPerhaps the simplest way to extend expected traces to deep\nneural networks is to first separate the value function into\na representation **x** ( _s_ ) and a value _v_ ( **w** _,_ _**ξ**_ )( _s_ ) = **w** _[⊤]_ **x** _**ξ**_ ( _s_ ),\nwhere **x** _**ξ**_ is some (non-linear) function of the observations\n_s_ . [4] We can then apply the same expected trace algorithm as\nused in the previous sections by learning a separate linear\nfunction _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) using the representation which is\nlearned by backpropagating the value updates:\n\n\n**w** _t_ +1 = **w** _t_ + _αδ_ _**z**_ **Θ** ( _St_ ) _,_\n\n_**ξ**_ _t_ +1 = _**ξ**_ _t_ + _αδ_ _**e**_ _**[ξ]**_ _t_ _[,]_\n\nwhere _**e**_ _**[ξ]**_ _t_ [=] _[ γ][t][λ]_ _**[e][ξ]**_ _t−_ 1 [+] _[ ∇]_ _**[ξ]**_ _[v]_ [(] **[w]** _[,]_ _**[ξ]**_ [)][(] _[S][t]_ [)] _[,]_\n\n_**e**_ **[w]** _t_ [=] _[ γ][t][λ]_ _**[e]**_ **[w]** _t−_ 1 [+] _[ ∇]_ **[w]** _[v]_ ( **w** _,_ _**ξ**_ ) [(] _[S][t]_ [)] _[,]_\n\n\nand then updating **Θ** to minimise component-wise squared\ndifferences between _**e**_ **[w]** _t_ [and] _**[ z]**_ **[Θ]** _t_ [(] _[S][t]_ [)][, as in (2) and (3).]\nInteresting challenges appear outside the fully linear case.\nFirst, the representation will itself be updated and will have\nits own trace _**e**_ _**[ξ]**_ _t_ [. Second, in the control case we optimise]\nbehaviour: the policy will change. Both these properties of\nthe non-linear control setting imply that the expected traces\nmust track a non-stationary target. We found that being able to\ntrack this rather quickly improved performance: the expected\ntrace parameters **Θ** in the following experiment were updated\nwith a relatively high step size of _β_ = 0 _._ 1.\nWe tested this idea on two canonical Atari games: Pong and\nMs. Pac-Man. The results in Figure 6 show that the expected\ntraces helped speed up learning compared to the baseline\nwhich uses accumulating traces, for various step sizes. Unlike\nmost prior work on this domain, which often relies on replay\n(Mnih et al. 2015; Schaul et al. 2016; Horgan et al. 2018)\nor parallel streams of experience (Mnih et al. 2016), these\nalgorithms updated the values online from a single stream\nof experience. Further details on the experimental setup are\ngiven in the appendix.\nThese experiments demonstrate that the idea of expected\ntraces extends to non-linear function approximation, such as\ndeep neural networks. We consider this to be a rich area of\nfurther investigations. The results presented here are similar\nto earlier results (e.g., Mnih et al. 2015) and are not meant to\ncompete with state-of-the-art performance results, which often depend on replay and much larger amounts of experience\n(e.g., Horgan et al. 2018).\n\n\n**Discussion and Extensions**\n\nWe now discuss various interesting interpretations and relations, and discuss promising extensions.\n\n\n**Predecessor Features**\n\nFor linear value functions the expected trace _z_ ( _s_ ) can be\nexpressed non-recursively as follows:\n\n\n\n�\n\n\n\n_**z**_ ( _s_ ) = E\n\n\n\n_∞_\n� _λ_ [(] _t_ _[n]_ [)] _γt_ [(] _[n]_ [)] **x** _t−n | St_ = _s_\n� _n_ =0\n\n\n\n_,_ (7)\n\n\n\n4Here _s_ denotes observations to the agent, not a full environment\nstate— _s_ is not assumed to be Markovian.\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-1.png)\n\nFigure 4: Prediction errors in the multi-chain. ET( _λ_ ) (orange) consistently outperformed TD( _λ_ ) (blue). Shaded areas depict\nstandard errors across 10 seeds.\n\n\nFigure 5: Comparing value error with linear function approximation a) as function of the number of branches (left), b) as\nfunction of _λ_ (center two plots) and c) as function of _η_ (right). The left three plots show comparisons of TD( _λ_ ) (blue) and ET( _λ_ )\n(orange), showing ET( _λ_ ) attained lower prediction errors. The right plot interpolates between these algorithms via ET( _λ_, _η_ ),\nfrom ET( _λ_ ) = ET( _λ_, 0) to ET( _λ_, 1) = TD( _λ_ ), with _λ_ = 0 _._ 9 (corresponding to a vertical slice indicated in the second plot).\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-3.png)\n\nwhere _γk_ [(] _[n]_ [)] _≡_ [�] _[k]_ _j_ = _k−n_ _[γ][j]_ [. This is interestingly similar to the]\ndefinition of _successor features_ (Barreto et al. 2017):\n\n\n\n�\n\n\n\n_ψ_ ( _s_ ) = E\n\n\n\n_∞_\n� _γt_ [(] +1 _[n][−]_ [1)] **x** _t_ + _n | St_ = _s_\n� _n_ =1\n\n\n\n_._ (8)\n\n\n\nThe summation in (8) is over future features, while in (7)\nwe have a sum over features already observed by the agent.\nWe can thus think of linear expected traces as _predecessor_\n_features_ . A similar connection was made in the tabular setting by Pitis (2018), relating source traces, which aim to\nestimate the source matrix ( _I −_ _γP_ ) _[−]_ [1], to successor representations (Dayan 1993). In a sense, the above generalises\nthis insight. In addition to being interesting in its own right,\nthis connection allows for an intriguing interpretation of _**z**_ ( _s_ )\nas a multidimensional value function. Like with successor\nfeatures, the features **x** _t_ play the role of rewards, discounted\nwith _γ · λ_ rather than _γ_, and with time flowing backwards.\nAlthough the predecessor interpretation only holds in the\nlinear case, it is also of interest as a means to obtain a practical\nimplementation of expected traces with non-linear function\napproximation, for instance applied only to the linear ‘head’\nof a deep neural network. We used this ‘predecessor feature\ntrick’ in our Atari experiments described earlier.\n\n\n**Relation to Model-Based Reinforcement Learning**\n\n\nModel-based reinforcement learning provides an alternative\napproach to efficient credit assignment. The general idea is\nto construct a model that estimates state-transition dynamics,\nand to update the value function based upon hypothetical\n\n\n10002\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-4.png)\n\ntransitions drawn from the model (Sutton 1990), for example\nby prioritised sweeping (Moore and Atkeson 1993; van Seijen\nand Sutton 2013). In practice, model-based approaches have\nproven challenging in environments (such as Atari games)\nwith rich perceptual observations, compared to model-free\napproaches that more directly update the agent’s policy and\npredictions (van Hasselt, Hessel, and Aslanides 2019).\nIn some sense, expected traces also construct a model of\nthe environment—but one that differs in several key regards\nfrom standard state-to-state models used in model-based reinforcement learning. First, expected traces estimate _past_\nquantities rather than _future_ quantities. Backward planning\n(e.g., Chelu, Precup, and van Hasselt 2020) is possible with\nexplicit transition models, but is less common in practice.\nSecond, expected traces estimate the accumulation of _gradi-_\n_ents_ over a multi-step trajectory, rather than trying to learn\nthe full transition dynamics, thereby focusing only on those\naspects that matter for the eventual weight update. Third, expected traces allow credit assignment across these potential\npast trajectories with a single update, without the iterative\ncomputation that is typically required when using a dynamics\nmodel. These differences may be important to side-step some\nof the challenges faced in model-based learning.\n\n\n**Batch Learning and Replay**\n\n\nWe have mainly considered the online learning setting in this\npaper. It is often convenient to learn from batches of data, or\nreplay transitions repeatedly, to enhance data efficiency. A\nnatural extension is replay the experiences sequentially (e.g.\nKapturowski et al. 2018), but perhaps alternatives exist. We\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-1.png)\n\nFigure 6: Performance of Q( _λ_ ) ( _η_ = 1, blue) and QET( _λ_ ) ( _η_ = 0, orange) on Pong and Ms.Pac-Man for various learning rates.\nShaded regions show standard error across 10 random seeds. All results are for _λ_ = 0 _._ 95. Further implementation details and\nhyper-parameters are in the appendix.\n\n\n\nnow discuss one potential extension.\nWe defined a mixed trace _**y**_ _t_ that mixes the instantaneous\nand expected traces. Optionally the expected trace _**z**_ _t_ can\nbe updated towards the mixed trace _**y**_ _t_ as well, instead of\ntowards the instantaneous trace _**e**_ _t_ . Analogously to TD( _λ_ ) we\npropose to then use at least one real step of data:\n\n\n∆ _**θ**_ _t ≡_ _β_ ( _**∇**_ _t_ + _γtλt_ _**y**_ _t−_ 1 _−_ _**zθ**_ ( _St_ )) _[⊤]_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] _,_ (9)\n\n_∂_ _**θ**_\n\n\nwith _**∇**_ _t ≡∇_ **w** _v_ **w** ( _St_ ). This is akin to a forward-view _λ_ return update, with _∇_ **w** _v_ **w** ( _St_ ) in the role of (vector) reward,\nand _**zθ**_ of value, and discounted by _λtγt_, but reversed in time.\nIn other words, this can be considered a sampled Bellman\nequation (Bellman 1957) but backward in time.\nWhen we then choose _η_ = 0, then _**y**_ _t−_ 1 = _z_ _**θ**_ ( _St−_ 1), and\nthen the target in (9) only depends on a single transition.\nInterestingly, that means we can then learn expected traces\nfrom _individual_ transitions, sampled out of temporal order,\nfor instance in batch settings or when using replay.\n\n\n**Application to Other Traces**\n\n\nWe can apply the idea of expected trace to more traces than\nconsidered here. We can for instance consider the characteristic eligibility trace used in REINFORCE (Williams 1992)\nand related policy-gradient algorithms (Sutton et al. 2000).\nAnother appealing application is to the follow-on trace\nor _emphasis_, used in emphatic temporal difference learning\n(Sutton, Mahmood, and White 2016) and related algorithms\n(e.g., Imani, Graves, and White 2018). Emphatic TD was\nproposed to correct an important issue with off-policy learning, which can be unstable and lead to diverging learning\ndynamics. Emphatic TD weights updates according to 1) the\ninherent interest in having accurate predictions in that state\nand, 2) the importance of predictions in that state for updating\n\n\n10003\n\n\n\nother predictions. Emphatic TD uses scalar ‘follow-on’ traces\nto determine the ‘emphasis’ for each update. However, this\nfollow-on trace can have very high, even infinite, variance.\nInstead, we might estimate and use its expectation instead of\nthe instantaneous emphasis. A related idea was explored by\nZhang, Boehmer, and Whiteson (2019) to obtain off-policy\nactor critic algorithms.\n\n\n**Conclusion**\n\n\nWe have proposed a mechanism for efficient credit assignment, using the expectation of an eligibility trace. We have\ndemonstrated this can sometimes speed up credit assignment\ngreatly, and have analyzed concrete algorithms theoretically\nand empirically to increase understanding of the concept.\nExpected traces have several interpretations. First, we can\ninterpret the algorithm as counterfactually updating multiple possible trajectories leading up to the current state. Second, they can be understood as trading off bias and variance,\nwhich can be done smoothly via a unifying _η_ parameter, between standard eligibility traces (low bias, high variance) and\nestimated traces (possibly higher bias, but lower variance).\nFurthermore, with tabular or linear function approximation\nwe can interpret the resulting expected traces as predecessor\nstates or features—object analogous to successor states or features, but time-reversed. Finally, we can interpret the linear\nalgorithm as preconditioning the standard TD update, thereby\npotentially speeding up learning. These interpretations suggest that a variety of complementary ways to potentially\nextend these concepts and algorithms.\nWe have shown expected traces can already be used to\nenhance learning in non-linear settings (i.e., deep reinforcement learning), and in the control setting where we update\nthe policy. Further work is needed to determine the full extent\nof the possibilities of these new algorithms.\n\n\n\n\n**References**\n\n\nAmari, S. I. 1998. Natural gradient works efficiently in\nlearning. _Neural computation_ 10(2): 251–276. ISSN 08997667.\n\n\nBarreto, A.; Dabney, W.; Munos, R.; Hunt, J. J.; Schaul, T.;\nvan Hasselt, H. P.; and Silver, D. 2017. Successor features\nfor transfer in reinforcement learning. In _Advances in neural_\n_information processing systems_, 4055–4065.\n\n\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. _J. Artif. Intell. Res. (JAIR)_ 47:\n253–279.\n\n\nBellman, R. 1957. _Dynamic Programming_ . Princeton University Press.\n\n\nBertsekas, D. P.; and Tsitsiklis, J. N. 1996. _Neuro-dynamic_\n_Programming_ . Athena Scientific, Belmont, MA.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018a. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018b. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBrandfonbrener, D.; and Bruna, J. 2020. Geometric Insights\ninto the Convergence of Non-linear TD Learning. In _Interna-_\n_tional Conference on Learning Representations_ .\n\n\nChelu, V.; Precup, D.; and van Hasselt, H. P. 2020. Forethought and Hindsight in Credit Assignment. In Larochelle,\nH.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H.,\neds., _Advances in Neural Information Processing Systems_,\nvolume 33, 2270–2281.\n\n\nDayan, P. 1992. The convergence of TD( _λ_ ) for general\nlambda. _Machine Learning_ 8: 341–362.\n\n\nDayan, P. 1993. Improving generalization for temporal difference learning: The successor representation. _Neural Com-_\n_putation_ 5(4): 613–624.\n\n\nDuan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel,\nP. 2016. Benchmarking deep reinforcement learning for\ncontinuous control. In _International Conference on Machine_\n_Learning_, 1329–1338.\n\n\nElfwing, S.; Uchibe, E.; and Doya, K. 2018. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_ 107:\n3–11.\n\n\nHennigan, T.; Cai, T.; Norman, T.; and Babuschkin, I. 2020.\nHaiku: Sonnet for JAX. URL http://github.com/deepmind/\ndm-haiku.\n\n\nHessel, M.; Modayil, J.; van Hasselt, H. P.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2018. Rainbow: Combining Improvements in Deep\nReinforcement Learning. _AAAI_ .\n\n\n10004\n\n\n\nHorgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel,\nM.; van Hasselt, H. P.; and Silver, D. 2018. Distributed\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ .\n\n\nImani, E.; Graves, E.; and White, M. 2018. An Off-policy\nPolicy Gradient Theorem Using Emphatic Weightings. In\nBengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; CesaBianchi, N.; and Garnett, R., eds., _Advances in Neural Infor-_\n_mation Processing Systems 31_, 96–106. Curran Associates,\nInc. URL http://papers.nips.cc/paper/7295-an-off-policypolicy-gradient-theorem-using-emphatic-weightings.pdf.\n\n\nKaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1995.\nPlanning and Acting in Partially Observable Stochastic Domains. Unpublished report.\n\n\nKapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and\nDabney, W. 2018. Recurrent experience replay in distributed\nreinforcement learning. In _International conference on learn-_\n_ing representations_ .\n\n\nKingma, D. P.; and Adam, J. B. 2015. A method for stochastic optimization. In _International Conference on Learning_\n_Representation_ .\n\n\nLin, L. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_\n8(3): 293–321.\n\n\nMartens, J. 2016. _Second-order optimization for neural net-_\n_works_ . University of Toronto (Canada).\n\n\nMinsky, M. 1963. Steps Toward Artificial Intelligence.\nIn Feigenbaum, E.; and Feldman, J., eds., _Computers and_\n_Thought_, 406–450. McGraw-Hill, New York.\n\n\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous Methods for Deep Reinforcement Learning. In\n_International Conference on Machine Learning_ .\n\n\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg,\nS.; and Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. _Nature_ 518(7540): 529–533.\n\n\nMoore, A. W.; and Atkeson, C. G. 1993. Prioritized Sweeping: Reinforcement Learning with less Data and less Time.\n_Machine Learning_ 13: 103–130.\n\n\nOllivier, Y. 2018. Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies. _CoRR_\nabs/1805.00869.\n\n\nPeng, J. 1993. _Efficient dynamic programming-based learn-_\n_ing for control_ . Ph.D. thesis, Northeastern University.\n\n\nPeng, J.; and Williams, R. J. 1996. Incremental Multi-step\nQ-learning. _Machine Learning_ 22: 283–290.\n\n\nPitis, S. 2018. Source Traces for Temporal Difference Learning. In McIlraith, S. A.; and Weinberger, K. Q., eds., _Pro-_\n_ceedings of the Thirty-Second AAAI Conference on Artificial_\n_Intelligence_, 3952–3959. AAAI Press.\n\n\n\n\nPohlen, T.; Piot, B.; Hester, T.; Azar, M. G.; Horgan, D.;\nBudden, D.; Barth-Maron, G.; van Hasselt, H. P.; Quan, J.;\nVecerˇ ´ık, M.; Hessel, M.; Munos, R.; and Pietquin, O. 2018.\nObserve and look further: Achieving consistent performance\non Atari. _arXiv preprint arXiv:1805.11593_ .\n\n\nProkhorov, D. V.; and Wunsch, D. C. 1997. Adaptive critic\ndesigns. _IEEE Transactions on Neural Networks_ 8(5): 997–\n1007.\n\n\nPuterman, M. L. 1994. _Markov Decision Processes: Discrete_\n_Stochastic Dynamic Programming_ . John Wiley & Sons, Inc.\nNew York, NY, USA.\n\n\nRiedmiller, M. 2005. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning\nMethod. In Gama, J.; Camacho, R.; Brazdil, P.; Jorge, A.; and\nTorgo, L., eds., _Proceedings of the 16th European Conference_\n_on Machine Learning (ECML’05)_, 317–328. Springer.\n\n\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ . Puerto Rico.\n\n\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering\nthe game of Go with deep neural networks and tree search.\n_Nature_ 529(7587): 484–489.\n\n\nSingh, S. P.; and Sutton, R. S. 1996. Reinforcement Learning\nwith replacing eligibility traces. _Machine Learning_ 22: 123–\n158.\n\n\nSutton, R. S. 1984. _Temporal Credit Assignment in Reinforce-_\n_ment Learning_ . Ph.D. thesis, University of Massachusetts,\nDept. of Comp. and Inf. Sci.\n\n\nSutton, R. S. 1988. Learning to predict by the methods of\ntemporal differences. _Machine learning_ 3(1): 9–44.\n\n\nSutton, R. S. 1990. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Proceedings of the seventh international conference_\n_on machine learning_, 216–224.\n\n\nSutton, R. S.; and Barto, A. G. 2018. _Reinforcement Learning:_\n_An Introduction_ . The MIT press, Cambridge MA.\n\n\nSutton, R. S.; Mahmood, A. R.; and White, M. 2016. An\nEmphatic Approach to the Problem of Off-policy TemporalDifference Learning. _Journal of Machine Learning Research_\n17(73): 1–29.\n\n\nSutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y.\n2000. Policy gradient methods for reinforcement learning\nwith function approximation. _Advances in Neural Informa-_\n_tion Processing Systems 13 (NIPS-00)_ 12: 1057–1063.\n\n\nTesauro, G. 1992. Practical Issues in Temporal Difference\nLearning. In Lippman, D. S.; Moody, J. E.; and Touretzky,\nD. S., eds., _Advances in Neural Information Processing Sys-_\n_tems 4_, 259–266. San Mateo, CA: Morgan Kaufmann.\n\n\nTesauro, G. J. 1994. TD-Gammon, a self-teaching backgammon program, achieves master-level play. _Neural computa-_\n_tion_ 6(2): 215–219.\n\n\n10005\n\n\n\nTsitsiklis, J. N. 1994. Asynchronous stochastic approximation and Q-learning. _Machine Learning_ 16: 185–202.\n\nTsitsiklis, J. N.; and Van Roy, B. 1997. An analysis of\ntemporal-difference learning with function approximation.\n_IEEE Transactions on Automatic Control_ 42(5): 674–690.\n\nvan Hasselt, H. P. 2012. Reinforcement Learning in Continuous State and Action Spaces. In Wiering, M. A.; and\nvan Otterlo, M., eds., _Reinforcement Learning: State of the_\n_Art_, volume 12 of _Adaptation, Learning, and Optimization_,\n207–251. Springer.\n\n\nvan Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver,\nD. 2016. Learning values across many orders of magnitude. In _Advances in Neural Information Processing Systems_\n_29: Annual Conference on Neural Information Processing_\n_Systems 2016, December 5-10, 2016, Barcelona, Spain_, 4287–\n4295.\n\n\nvan Hasselt, H. P.; Guez, A.; and Silver, D. 2016. Deep reinforcement learning with double Q-Learning. In _Proceedings_\n_of the Thirtieth AAAI Conference on Artificial Intelligence_,\n2094–2100.\n\n\nvan Hasselt, H. P.; Hessel, M.; and Aslanides, J. 2019. When\nto use parametric models in reinforcement learning? In _Ad-_\n_vances in Neural Information Processing Systems_, volume 32,\n14322–14333.\n\nvan Hasselt, H. P.; Mahmood, A. R.; and Sutton, R. S. 2014.\nOff-policy TD( _λ_ ) with a true online equivalence. In _Pro-_\n_ceedings of the 30th Conference on Uncertainty in Artificial_\n_Intelligence_, 330–339.\n\nvan Hasselt, H. P.; Quan, J.; Hessel, M.; Xu, Z.; Borsa, D.;\nand Barreto, A. 2019. General non-linear Bellman equations.\n_arXiv preprint arXiv:1907.03687_ .\n\n\nvan Hasselt, H. P.; and Sutton, R. S. 2015. Learning to predict\nindependent of span. _CoRR_ abs/1508.04582.\n\nvan Seijen, H.; and Sutton, R. S. 2013. Planning by Prioritized Sweeping with Small Backups. In _International_\n_Conference on Machine Learning_, 361–369.\n\nvan Seijen, H.; and Sutton, R. S. 2014. True online TD( _λ_ ).\nIn _International Conference on Machine Learning_, 692–700.\n\n\nWang, Z.; de Freitas, N.; Schaul, T.; Hessel, M.; van Hasselt,\nH. P.; and Lanctot, M. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In _International_\n_Conference on Machine Learning_ . New York, NY, USA.\n\nWerbos, P. J. 1990. A menu of designs for reinforcement\nlearning over time. _Neural networks for control_ 67–95.\n\nWhitehead, S. D.; and Ballard, D. H. 1991. Learning to\nperceive and act by trial and error. _Machine Learning_ 7(1):\n45–83.\n\nWilliams, R. J. 1992. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. _Machine_\n_Learning_ 8: 229–256.\n\n\nZhang, S.; Boehmer, W.; and Whiteson, S. 2019. Generalized\noff-policy actor-critic. In _Advances in Neural Information_\n_Processing Systems_, 2001–2011.\n\n\n",
          "ranking": {
            "relevance_score": 0.7493067885948483,
            "citation_score": 0.6302697050551669,
            "recency_score": 0.3906854405837399,
            "final_score": 0.6896372370858013
          },
          "is_open_access": false,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Failure case analysis reveals challenges with highly domain-specific code patterns and unusual programming styles.",
        "chunk_index": 1
      },
      "summary": "Failure case analysis reveals challenges with highly domain-specific code patterns and unusual programming styles.",
      "vector_score": 0.664,
      "llm_score": 0.83,
      "combined_score": 0.83,
      "source_query": "mock_query_discussion"
    },
    {
      "chunk": {
        "chunk_id": "mock_15",
        "paper": {
          "id": "66d76444255be0ac378a0a93ee0379fc721a386f",
          "title": "On Q-learning Convergence for Non-Markov Decision Processes",
          "published": "2018-07-01",
          "authors": [
            "Sultan Javed Majeed",
            "Marcus Hutter"
          ],
          "summary": "Temporal-difference (TD) learning is an attractive, computationally efficient framework for model- free reinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent to learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has only been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state MDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the environment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning when applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We prove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in particular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a necessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.",
          "pdf_url": "https://doi.org/10.24963/ijcai.2018/353",
          "doi": "10.24963/ijcai.2018/353",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "International Joint Conference on Artificial Intelligence",
          "citation_count": 40,
          "bibtex": "@Article{Majeed2018OnQC,\n author = {Sultan Javed Majeed and Marcus Hutter},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {2546-2552},\n title = {On Q-learning Convergence for Non-Markov Decision Processes},\n year = {2018}\n}\n",
          "markdown_text": "[image]\n\n\nNavigation\n\n\n  - Home\n\n  - Conferences\n\n\nFuture Conferences\n\n  Past Conferences\n\n  \n\n  - Proceedings\n\n\n  - IJCAI 2025 Proceedings\n\n  - All Proceedings\n\n\n  - Awards\n\n  - Trustees/officers\n\n\nCurrent trustees\n\n  Trustees Elect\n\n  IJCAI Secretariat\n\n  \n  - IJCAI Sponsorship and Publicity Officers\nIJCAI Team\n\n  \n  - Local Arrangements Chairs\n\n  - Former Trustees serving on the Executive Committee\n\n  - Other Former Officers\n\n\n  - AI Journal\n\n  - About\n\n\nAbout IJCAI\n\n  Contact Information\n\n  \n# **On Q-learning Convergence for Non-Markov** **Decision Processes** **On Q-learning Convergence for Non-Markov** **Decision Processes**\n\n## **Sultan Javed Majeed, Marcus Hutter**\n\n\n[image]\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence\n[Main track. Pages 2546-2552. https://doi.org/10.24963/ijcai.2018/353](https://doi.org/10.24963/ijcai.2018/353)\n[PDF BibTeX](https://www.ijcai.org/proceedings/2018/0353.pdf)\n\n\nTemporal-difference (TD) learning is an attractive, computationally efficient framework for model- free\nreinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent\nto learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has\nonly been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state\nMDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the\nenvironment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning\nwhen applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We\nprove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in\nparticular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a\nnecessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.\nKeywords:\nMachine Learning: Online Learning\nMachine Learning: Reinforcement Learning\nPlanning and Scheduling: Markov Decisions Processes\n\n\nCopyright © 2025,\n\n\n",
          "ranking": {
            "relevance_score": 0.7639998734717994,
            "citation_score": 0.5680913780397937,
            "recency_score": 0.27592885552856466,
            "final_score": 0.6760110725910747
          },
          "is_open_access": true,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Computational efficiency remains competitive with existing tools while providing more comprehensive analysis.",
        "chunk_index": 2
      },
      "summary": "Computational efficiency remains competitive with existing tools while providing more comprehensive analysis.",
      "vector_score": 0.6480000000000001,
      "llm_score": 0.81,
      "combined_score": 0.81,
      "source_query": "mock_query_discussion"
    }
  ],
  "Conclusion": [
    {
      "chunk": {
        "chunk_id": "mock_16",
        "paper": {
          "id": "user_2404.15822v1",
          "title": "Recursive Backwards Q-Learning in Deterministic Environments",
          "published": "2024-04-24",
          "authors": [
            "Jan Diekhoff",
            "Jorn Fischer"
          ],
          "summary": "Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.",
          "pdf_url": "",
          "doi": "10.48550/arXiv.2404.15822",
          "fields_of_study": [
            "Computer Science"
          ],
          "venue": "arXiv.org",
          "citation_count": 0,
          "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n",
          "markdown_text": "## RECURSIVE BACKWARDS Q-LEARNING IN DETERMINISTIC ENVIRONMENTS\n\n\n\n**Jan Diekhoff**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: jan.diekhoff@web.de\n\n\n\n**Jörn Fischer**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: j.fischer@hs-mannheim.de\n\n\n\n**ABSTRACT**\n\n\nReinforcement learning is a popular method of finding optimal solutions to complex problems.\nAlgorithms like Q-learning excel at learning to solve stochastic problems without a model of their\nenvironment. However, they take longer to solve deterministic problems than is necessary. Q-learning\ncan be improved to better solve deterministic problems by introducing such a model-based approach.\nThis paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds\na model of the environment. After reaching a terminal state, it recursively propagates its value\nbackwards through this model. This lets each state be evaluated to its optimal value without a lengthy\nlearning process. In the example of finding the shortest path through a maze, this agent greatly\noutperforms a regular Q-learning agent.\n\n\n_**Keywords**_ Q-learning _·_ deterministic _·_ recursive _·_ reinforcement learning\n\n\n**1** **Introduction**\n\n\nMachine learning and reinforcement learning are increasingly popular and important fields in the modern age. There are\nproblems that reinforcement learning agents can learn to solve more efficiently and consistently than any human when\ngiven enough time to practice. However, modern approaches like Q-learning run into issues when facing certain types\nof problems. Their approach to solving problems in combination with not using a model of the environment causes\nthem to take longer than is necessary to learn to solve problems that are deterministic in nature. By working without\nmodel of the environment, information that is available and help the learning process is ignored.\n\n\nThis paper introduces an adapted Q-learning agent called the _recursive backwards Q-Learning (RBQL) agent_ . It solves\nthese types of problems by building a model of its environment as it explores and recursively applying the Q-value\nupdate rule to find an optimal policy much quicker than a regular Q-learning agent. This agent is shown to work with\nthe example of finding the fastest path through a maze. Its results are compared to the results of a regular Q-learning\nagent.\n\n\n**2** **Reinforcement Learning**\n\n\nReinforcement learning is one of the main fields of machine learning. It is commonly used for optimizing solutions to\nproblems. At its most fundamental level, a reinforcement learning method is an implementation of an agent for solving\na Markov decision process [1] by interacting with an environment. Markov decision processes describe problems as\na set of states _S_, a set of actions _A_ and a set of rewards _R_ . For every time step _t_, the agent chooses an action _a ∈_ _A_\nand receives a new state _s ∈_ _S_ and a reward _r ∈_ _R_ for the action [2]. Rewards may be positive or negative, depending\non the outcome of the action, to encourage or discourage taking that action in the future [3]. The process of the agent\ninteracting with the environment is called an episode which ends when a terminal state is reached which resets the\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nenvironment and agent to their original configuration for the start of a new episode [3]. For the purposes of this paper,\nonly finite Markov decision processes are considered, meaning the environment has at least one terminal state.\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-1-0.png)\n\n\n_St_ +1\n\n\nFigure 1: Basic agent-environment relationship in a Markov decision process. The agent chooses an action _At_ and the\nenvironment returns a new state _St_ +1 and a reward _Rt_ +1. The dotted line represents the transition from step _t_ to step\n_t_ + 1 [3].\n\n\nReinforcement learning agents learn an optimal strategy for a given Markov decision process by estimating the value of\neither being in a state or taking a certain action in a certain state. They do this through a value function or action-value\nfunction respectively. The aim of the agent is to maximize the reward they receive in an episode [3]. To achieve this,\nvalue estimations do not only consider the immediate action the agent takes but also consider all future states and actions\nthat may occur when taking the original action. Agents follow so-called policies according to which they choose which\nactions to take. Through gaining knowledge, they continuously adapt this policy in order to eventually reach an optimal\npolicy - a policy which chooses the optimal action at every step. To explore, agents have to balance between exploration\nand exploitation [3]. Exploration is the act of following suboptimal actions to attempt to find an even better policy. On\nthe other hand, exploitation is following the actions that will yield the currently highest estimated value. An agent that\nonly exploits acts _greedily_ . To ensure continual exploration so that all actions get updated given enough time, agents\ncan choose policies that are mostly greedy but choose to explore sometimes [2]. To this end, an approach like _ϵ_ -greedy\nmay be used. Here, _ϵ_ is the probability of choosing a random action and 1 _−_ _ϵ_ is the probability of acting greedily.\n\n\nA widely used modern approach to RL is temporal difference learning [4], more specifically Q-learning [2]. Q-learning\nworks with the Q-learning update formula to update its policies:\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + _α ·_\n\n[ _Rt_ +1 + _γ ·_ max _Q_ ( _St_ +1 _, a_ ) _−_ _Q_ ( _St, At_ )] (1)\n_a_\n\n\n_Q_ ( _St, At_ ) is the estimated value for any given state-action pair. The equation shows how it is updated after taking\naction _At_ from state _St_ . _Rt_ +1 represents the reward gained, max _Q_ ( _St_ +1 _, a_ ) is the value estimation of the best action\n_a_\n_a ∈_ _At_ +1 that can be taken from _St_ +1 according to the current policy, the state resulting from action _At_ . _α_ is a step-size\nparameter, also known as the _learning rate_ . Its value lies between 0 and 1 and it determines how importantly the agent\nvalues new information against the current estimate it already has. A value of 0 completely ignores new information\nwhile a value of 1 completely overrides the preexisting value estimate. _γ_ is the discount factor, weighing future rewards\nless than immediate ones. It also lies between 0 and 1, where 1 weighs the best future action equally to the current one\nand 0 does not consider it at all.\n\n\n**3** **Recursive Backwards Q-Learning**\n\n\n**3.1** **Idea**\n\n\nQ-learning agents are very widespread in modern reinforcement learning. Working free of a model allows them to\nbe generally applicable to many problems. However, some Markov decision processes take longer to solve than is\nnecessary because the agent ignores readily available information. This is noticeable in deterministic, episodic tasks\nwhere a positive reward is only given when reaching a terminal state. Before this state is reached for the first time, the\nagent appears to be moving entirely at random. Looking at figure 2, the issue becomes apparent. Even when following\nthe optimal path at every step, it still takes multiple episodes for the reward of the terminal state to propagate back to the\nstarting state. In fact, the optimal paths value estimation gets worse before it gets better. If every step has a reward of\n\n\n2\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_−_ 1, values along the optimal path get worse if they do not lead to a state that has already been reached by the terminal\nstate’s positive reward as it travels backwards.\n\n\nIn this paper, grid worlds [3] are used as an example Markov decision process for the agent to solve. Grid worlds are a\ntwo-dimensional grid in which every tile represents a state and the actions are limited to walking up, down, left or right.\nGrid worlds are useful in that they are very simple to understand and to display, they have a limited set of actions and\ntheir set of states can be as small or large as is desired. Additionally, showing the value or optimal policy for each state\nis as easy as writing a number or drawing an arrow on the corresponding tile. Actions that would place the agent off of\nthe grid simply return the state the agent is already in, but may still give a reward. Special tiles can also be defined, such\nas walls that act like the grid edge or pits that are terminal fail states because the agent cannot leave them once it has\nfallen in. Every grid world tile gives a reward of _−_ 1 to punish taking unnecessary actions in favor of taking the fastest\npath to the goal.\n\n\n_Q_ greedy policy\nw.r.t. _Q_\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -0.8\n\n\n-1\n\n\n-1\n\n\n-1 0.52\n\n\n-1\n\n\n-1\n\n\n-1 1.99\n\n\n-1\n\n\n-1\n\n\n-1 3.27\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 0.78\n\n\n-1\n\n\n-1\n\n\n-1 3.15\n\n\n-1\n\n\n-1\n\n\n-1 4.96\n\n\n-1\n\n\n-1\n\n\n-1 6.17\n\n\n-1\n\n\n-1\n\n\n-1 6.93\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 4.5\n\n\n-1\n\n\n-1\n\n\n-1 7.25\n\n\n-1\n\n\n-1\n\n\n-1 8.63\n\n\n-1\n\n\n-1\n\n\n-1 9.32\n\n\n-1\n\n\n-1\n\n\n-1 9.66\n\n\n-1\n\n\n-1\n\n\n-1 9.83\n\n\n-1\n\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n\nep. 0\n\n\nep. 1\n\n\nep. 2\n\n\nep. 3\n\n\nep. 4\n\n\nep. 5\n\n\nep. 6\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -1.6\n\n\n-1\n\n\n-1\n\n\n-1 -1.66\n\n\n-1\n\n\n-1\n\n\n-1 -1.1\n\n\n-1\n\n\n-1\n\n\n-1 -0.15\n\n\n-1\n\n\n\nFigure 2: Q-learning in a one-dimensional grid world. All Q-values are initialized as _−_ 1. Actions that lead to the\nterminal state reward 10. All other actions reward -1. The discount rate _γ_ is set to 0 _._ 9. The learning rate _α_ is set to 0 _._ 5.\nThe value of _ϵ_ is irrelevant as the only action the agent takes is _→_ .\n\n\nFigure 2 is a very simple grid world and it still takes six episodes to reach an optimal policy, even when taking the\noptimal action at every step. This problem will only grow worse and add noticeably more episodes of training for grid\nworlds that are not as trivial to solve, or even more complex tasks with more variables to consider. As stated, the issue\nis that the agent has no source of direction until it has randomly stumbled across the terminal state, its only source of\npositive rewards. The larger the state space, the longer it is blindly searching.\n\n\nReinforcement learning agents that work with a model of their environment are known as _model-based_ reinforcement\nlearning agents. They can either work with a preexisting model or, more commonly, build their own. The way they\nconstruct their models is important as having perfect knowledge of an environment is neither feasible nor sensible. In\nthe case of a grid world it is no problem, but imagining a more complex scenario like a self-driving car makes this fact\napparent. When trying to drive from one city to another, knowing every centimeter of the road with every possible place\nother cars might be on the route is resource intensive and unnecessary. Instead, an agent should attempt to simplify its\nmodel as much as possible. Instead of every bit of road, long stretches going straight can be clumped together. Similar\nsituations like a car in front slowing down can be treated the same wherever they occur.\n\n\nThe purpose of this paper is to introduce and evaluate a new type of model-based agent called the RBQL agent. The\nRBQL agent solves deterministic, episodic tasks that positively reward only the terminal state more efficiently than a\nregular Q-learning agent. It functions by building a model of its environment through exploration. When it reaches a\nterminal state, it recursively travels backwards through all previously explored states, applying a modified Q-learning\nupdate rule, the RBQL update rule. By setting the learning rate _α_ to 1, equation (1) can be simplified as such:\n\n\n3\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + 1 _·_ [ _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )]\n= _Q_ ( _St, At_ ) + _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )\n= _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n\n(2)\n\n\n\nAs can be seen in formula (2), the Q-value now exclusively depends on the reward and the discounted reward of the\nbest neighbor. Because the algorithm applies this formula starting with what is guaranteed to be the highest value of the\nenvironment and working its way away from it, the best possible neighbor for any given state is always the previously\nevaluated state.\n\n\nEvaluating all states at the end of the episode is reminiscent of dynamic programming [5] or Monte Carlo methods [3]\nand is a point of critique for those approaches. However, as will be shown in chapter 4, this evaluation method is so\neffective in RBQL that evaluating all known states in one go is still cost effective. RBQL also differs in comparison\nto dynamic programming and Monte Carlo in a few major ways. In contrast with dynamic programming, it does not\nstart out with a perfect model but has to build its own. It also propagates its reward throughout all states much more\nquickly and it uses an action-value function, not a state-value function. In contrast with Monte Carlo, it does not use\nexploring starts to guarantee exploration. It also does not only update the values that were seen in an episode. Instead,\nto facilitate exploration, it always prioritizes visiting unexplored actions, only following the greedy path when there\nare none. Because this mode of exploration still results in unexplored actions, the _ϵ_ -greedy approach is adapted for\nRBQL. Instead of exploring steps, the agent has exploration episodes. _ϵ_ serves the same purpose as before, marking\nthe probability of taking an exploration episode while 1 _−_ _ϵ_ is the probability of taking an exploitation episode. In an\nexploration episode, the agent randomly chooses an unexplored action anywhere in its model, navigates the model to\nput itself in a position to take that action and then continues to explore until it finds a known path again or the episode\nends.\n\n\nIn this paper, finding an optimal path through a randomly generated grid world maze is used as an example task for\nRBQL to solve. It is also used to compare the performance of RBQL to Q-learning.\n\n\n**3.2** **Implementation**\n\n\nTo implement RBQL [1], the Godot game engine v. 3.5 [2] was used. Godot is a free, open source engine used mainly for\nvideo game development. Its main language is GDScript, an internal language that is very similar in syntax to Python,\nthough it also supports C, C++, C# and VisualScript. Because Python is very popular for machine learning development,\nthe implementation is written in GDScript so that it is easily readable for interested parties. Godot uses a hierarchical\nstructure of objects called _nodes_ . In the implementation, there are two main nodes: the agent and the environment.\n\n\n**3.2.1** **Environment**\n\n\nThe environment is of the type `TileMap` [3] – a class designed for creating maps in grid-based environments like grid\nworlds. Before starting the first episode, the environment generates a maze given a width _w_ and a height _h_ using a\nrecursive backtracking algorithm [6]. The starting point for the agent is always (0 _,_ 0) and the goal it attempts to reach –\nthe only terminal state – is ( _w −_ 1 _, h −_ 1). To ensure that the agent has the ability to improve even after finding the goal\nin the first episode, a maze with multiple paths is needed. Because a maze generated with recursive backtracking only\nhas one path to the terminal state, a number of alternate paths are generated by taking _w · h/_ 4 random positions and a\ndirection for each position. If the position has a wall in that direction, it is removed. If not, nothing happens.\n\n\nThe environment has a function `step(state,action)` that serves as the only way for the agent to interact with it.\nThe possible moves are `UP`, `DOWN`, `LEFT` and `RIGHT` . The state is described as a coordinate of the current position. In\nGodot, the class `Vector2(x,y)` [4] is used for this purpose. `step()` checks if taking the given action from the given\nstate results in hitting a wall or not. If not, the agent moves to a new position. There are three different rewards: _−_ 1 for\nany normal tile, _−_ 5 for hitting a wall and 10 for reaching the terminal state. _−_ 1 is awarded at every step to discourage\nagents from taking unnecessary steps. Walls give _−_ 5 to quickly teach the agent to ignore them. After taking an action,\n\n\n1 The source code can be downloaded at `[https://github.com/JanDiekhoff/BackwardsLearner](https://github.com/JanDiekhoff/BackwardsLearner)`\n2 Godot v. 3.5 can be downloaded at `[https://godotengine.org/download/archive/3.5-stable/](https://godotengine.org/download/archive/3.5-stable/)`\n3 `[https://docs.godotengine.org/en/3.5/classes/class_tilemap.html](https://docs.godotengine.org/en/3.5/classes/class_tilemap.html)`\n4 `[https://docs.godotengine.org/en/3.5/classes/class_vector2.html](https://docs.godotengine.org/en/3.5/classes/class_vector2.html)`\n\n\n4\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nthe new state and reward are returned to the agent, as well as a notification if the episode has ended or not and if the\nagent has hit a wall or not.\n\n\nThe `TileMap` has a tile for each combination of having or not having a wall in each of the four directions, totaling 2 [4] or\n16 total possible tiles. Another option would be to just have a floor tile and a wall tile. However, that would make a\nmaze with an equivalent wall layout much larger, leading to a larger state set and longer solving times. To determine if a\nwall is in a certain direction, the id of each tile from 0 to 15 acts as a four-bit flag. Each direction is assigned one of the\nbits ( `UP` = 0, `RIGHT` = 1, `DOWN` = 2 and `LEFT` = 3). If the flag is set, there is a wall in the corresponding direction. The\nid for an L-shaped tile for example would be 2 [2] + 2 [3] = 12 as `DOWN` and `LEFT` have walls. The process for determining\nif the agent can move in a given direction _d_ from a position _p_ is ( _¬idp_ ) & (2 _[d]_ ), where _idp_ is the id of the tile at _p_ .\n\n\n**3.2.2** **RBQL Agent**\n\n\nThe RBQL agent is represented by a `Sprite` [5] object – a 2D image – so it can be observed while solving a maze. During\nits runtime, the agent keeps track of a few key things:\n\n\n    - A model of the environment ( `explored_map` )\n\n\n    - A list of rewards for each state-action pair ( `rewards` )\n\n\n    - The last reward received ( `reward` )\n\n\n    - A list of steps taken per episode ( `steps_taken` )\n\n\n    - The Q-table ( `qtable` )\n\n\n    - The current state ( `current_state` )\n\n\n    - The previous state ( `old_state` )\n\n\n    - The last taken action ( `action` )\n\n\nThe model of the environment starts out as an empty dictionary. Every time a new state is discovered, an entry for that state is made and initialized as an empty array. When an action is taken from this state, the resulting new state is entered into the previous state’s array at the index of the taken action’s designated number\n( `explored_map[old_state][action] = current_state` ). When hitting a wall, the “new” state is the same as the\nstate from which the action was taken. Similarly, when an action is taken, the resulting reward is saved in the rewards\nlist ( `rewards[old_state][action] = reward` ). Because the agent uses state-action values, not state values, the\ntiles are treated like nodes in a directed graph. Going from tile A to tile B might result in a different reward than when\ngoing from B to A, so when the agent learns the reward of going from A to B, it does not also learn the reward of going\nfrom B to A.\n\n\nBeing a Q-learner makes it simpler to generalize the agent for other tasks, but it causes a lot of exploratory steps and\nexploratory episodes to only explore one position at a time. If an exploration episode chooses an unexplored state-action\npair that results in hitting a wall, the exploration episode immediately ends with little information gained. To alleviate\nthis problem, the agent takes exploratory “look-ahead” steps. After entering a tile, it takes a step in every direction but\nonly saves the result if it hits a wall. This guarantees that exploratory episodes always take new paths and not just hit a\nwall and continue on the best known path.\n\n\nThe agent also keeps track of a list of the actions it has taken – except for when hitting a wall – for the case that it\nreaches a dead end, or rather a state with no unexplored neighbors. In this case, the agent would normally follow the\noptimal path until it finds a new unexplored path or reaches the terminal state. However, if the path the agent is on has\nnot been explored before it has not yet been evaluated and there is no optimal path to follow. In this case, the agent\nbacktracks by taking the opposite action of the most recent in the list, then removes it from the list, until an unexplored\ntile or an evaluated path to follow is found.\n\n\nFinally, when the terminal state is reached, the Q-table is updated with the rewards saved in `rewards` according to the\nRBQL update rule.\n```\n             qtable[state][action] =\n\n```\n\n`rewards[state][action] + discount_rate` _·_\n\n```\n             qtable[explored_map[state][action]].max()\n\n```\n\nTo do this, a copy of `explored_map` is inverted to be able to traverse it in reverse. This is then done with a breadth-first\nsearch algorithm, starting at the terminal state, and the Q-value is calculated for each state. Breadth-first search is chosen\n\n\n5 `[https://docs.godotengine.org/en/3.5/classes/class_sprite.html](https://docs.godotengine.org/en/3.5/classes/class_sprite.html)`\n\n\n5\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nover a depth-first search algorithm so that each state must only be visited once as the value is directly proportional to\nthe distance from the terminal state. With breadth-first search, each state gets the highest possible value on its first visit\nbecause it is visited from its highest possible valued neighbor.\n\n\nWhen all known states have been evaluated, a new episode begins. After the first episode, episodes are chosen to be\neither exploratory or exploitative, similar to how an _ϵ_ -greedy policy may choose exploratory actions. In an exploitative\nepisode, the agent simply follows the best path it knows, choosing at random if two states are equally good, but still\nalways exploring unknown states directly adjacent to the path above all else. In an exploratory episode, a random state\nwith an unexplored neighbor is chosen. The agent navigates to this state with the help of the A* search algorithm [7]\nand follow the unexplored path from there until it finds a known state again. This exploratory excursion may only find\none new state or it may find a vastly superior path to what was known before. _ϵ_ is decreased after every episode as\nfollows:\n_ϵ_ = `min_epsilon + (max_epsilon - min_epsilon)`\n\n\n_· e_ [(] _[−]_ `[decay_rate]` _[ ·]_ `[ current_episode]` [)]\n\n\nwhere `min_epsilon`, `max_epsilon` and `decay_rate` can be any value within a range of [0 _,_ 1] and `current_episode`\nis the number of the current episode starting with 0. Once every state is explored, the agent is guaranteed to have found\nthe optimal path, or paths, through the maze. In its entirety, the algorithm can be expressed like this:\n\n\n**Algorithm 1** Backwards Q-Learning Algorithm\n\nSet exploration_episode to false\n**while** true **do**\n\n**if** exploration_episode **then**\n\nFind unexplored path\nTravel to unexplored path\n**end if**\n**while** episode is not over **do**\n\n**if** current position has an unexplored neighbor **then**\n\nVisit unexplored neighbor\nUpdate model\nSave reward\n\n**if** no wall hit **then**\n\nSave action in action queue\n**end if**\n**else if** there is an optimal path to follow **then**\n\nVisit best neighbor\n**end if**\n**while** current pos. has no unexplored neighbor **do**\n\nBacktrack\n\n**end while**\n\n**end while**\nCreate state queue with breadth-first search\n**for** state in queue **do**\n\nApply RBQL formula\n**end for**\nSet exploration_episode to random() _<_ = _ϵ_\nApply decay to _ϵ_\n**end while**\n\n\n**3.2.3** **Q-learning agent**\n\n\nA standard Q-learning agent has been implemented in Godot as well to compare the performance of the RBQL agent to.\nThis agent is comparatively simple:\n\n\n**4** **Tests and Results**\n\n\nTo compare the performance of the two agents, three sets of tests have been done for different maze sizes: 5 _×_ 5, 10 _×_ 10\nand 15 _×_ 15. All variables have been set to common values. The decay rate is set somewhat high to account for the\nrelatively low episode amount:\n\n\n6\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**Algorithm 2** Q-Learning Algorithm\n\n\n**while** true **do**\n\n**if** random() _<_ = _ϵ_ **then**\n\nChoose random action\n\n**else**\n\nChoose greedy action\n**end if**\n\nTake action\n\nReceive new state and reward\nUpdate Q-table for old state and action\n**if** terminal state reached **then**\n\nStart new episode\n**end if**\nApply decay to _ϵ_\n**end while**\n\n\n    - _γ_ = 0 _._ 9\n\n\n    - _α_ = 0 _._ 1 (RBQL has _α_ = 1 as explained in equation (2))\n\n\n    - `min_epsilon` = 0 _._ 01\n\n\n    - `max_epsilon` = 1\n\n\n    - `decay_rate` = _−_ 0 _._ 01\n\n\nFor every maze size, each agent is given the same set of 50 randomly generated mazes. Each agent is given 25 episodes\nper maze to train. These values are chosen to offer a reasonably large sample size without requiring an enormous\namount of time to compute. Agents are compared by the number of steps taken per episode, with less steps taken being\na more desirable outcome. The step counter is increased every time `step()` is called, including the look-ahead steps of\nthe RBQL. For a sense of perspective, the best possible solution to any square maze of size _s_ [2] is 2 _s −_ 2. Assuming a\nmaze with no walls, the shortest distance between two points _A_ and _B_ can be expressed as their Manhattan distance\n_|AX −_ _BX_ _|_ + _|AY −_ _BY |_ [8]. In the corners of a square, it holds that _AX_ = _AY_ and _BX_ = _BY_, so the distance can\nbe simplified as 2 _· |A −_ _B|_ . Setting _A_ = 0 and _B_ = _s −_ 1, this further simplifies to 2 _s −_ 2. This means that while\nthe amount of states (and thereby state-action pairs) increases quadratically, the best possible solution only increases\nlinearly. This in turn means that the amount of states that are not on the optimal path that the agent has to evaluate will\noften increase drastically with the size of the maze.\n\n\nLooking at the results, a few things can be observed. First of all, the average number of steps the RBQL agent takes\nis consistently lower than the Q-learning agent in all three maze sizes. It also has much less variation in step counts,\nwhich can be seen when looking at the areas of lighter hue. The light red areas are much more sporadic and spike\nfurther away from the average. The green areas stick much closer together. If the highest two step counts per episode\nwere not removed, RBQL would also have a few small spikes. These spikes would represent exploratory episodes\nwhere a new path is explored, resulting in a higher step count. In cases where the line is flat for a long period of time, it\ncan be assumed that the optimal solution is found. This can be seen in all three figures, where both the average and\nthe min/max range become a straight line close to the minimum. Important to note is that every maze has a different\noptimal solution, hence why the average sits above the blue line which denotes the lowest possible step count in any\nmaze of this size. It can also be observed that none of the lines ever go below this boundary, as is to be expected.\n\n\nSecond, even when removing the highest two step counts per episode, many of the Q-learning agent’s step counts are so\nlarge that scaling the graphs to fit them makes the RBQL agent’s data and the lower boundary difficult to see in the\ngraphs for the larger mazes. The highest step count values that have not been cut are 858 steps in figure 3, 7,585 in\nfigure 4 and 21,147 in figure 5, while the highest in total are 3,716 steps in figure 3, 20,553 in figure 4 and 26,315 in\nfigure 5.\n\n\nThird, it is interesting to see how the differences in average step counts evolve with the grid size. Table 1 shows this\ndifference in the first and last episode. The difference between the average step counts in the last episode especially is\nstriking, as it is close to doubling from each size to the next. Further, looking at the improvement of each agent as seen\nin table 2, one can see that the factor by which RBQL improves massively increases the bigger the maze becomes while\nthe Q-learner only slightly improves its performance in comparison. Additionally, most of the improvement of RBQL is\ndone in the first two episodes, while the Q-learner has a more gradual learning curve.\n\n\n7\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**1** _**,**_ **000**\n\n\n**900**\n\n\n**800**\n\n\n**700**\n\n\n**600**\n\n\n**500**\n\n\n**400**\n\n\n**300**\n\n\n**200**\n\n\n**100**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 3: Number of steps taken to find the goal in a randomly generated grid world maze of size 5 _×_ 5. The blue line is\nthe minimum step threshold for any maze of this size. The light red area shows the range of Q-learning agent’s highest\nand lowest step count, excluding the highest and lowest two. The red line shows the average performance. Similarly, the\nlight green area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest\ntwo, and the green line shows the average performance.\n\n\n8\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-7-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**8** _**,**_ **000**\n\n\n**6** _**,**_ **000**\n\n\n**5** _**,**_ **000**\n\n\n**4** _**,**_ **000**\n\n\n**3** _**,**_ **000**\n\n\n**2** _**,**_ **000**\n\n\n**1** _**,**_ **000**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 4: Number of steps taken to find the goal in a randomly generated grid world maze of size 10 _×_ 10. The light\nred area shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two.\nThe red shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n9\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-8-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 5: Number of steps taken to find the goal in a randomly generated grid world maze of size 15 _×_ 15. The light red\narea shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two. The\nred line shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n10\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-9-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nTable 1: Difference in average step counts of the Q-learner and RBQL. The difference expresses how many times more\nsteps the Q-learner took compared to RBQL.\n\n\nGrid size Q-learner steps RBQL steps Difference\n**Episode 0**\n5 _×_ 5 278.06 191.84 1.45\n\n10 _×_ 10 3,308.46 843.52 3.92\n15 _×_ 15 7,180.98 1,965 3.65\n**Episode 24**\n5 _×_ 5 49.14 9.62 5.11\n\n10 _×_ 10 281.44 23.68 11.89\n\n15 _×_ 15 778.68 35.96 21.65\n\n\nLastly, the RBQL agent seems to find an optimal policy at around episode 4 for the 5 _×_ 5, episode 6 for the 10 _×_ 10 and\nepisode 10 for the 15 _×_ 15 grid. As the previous figures show, the Q-learning agent does not come close to similarly\nlow step counts and therefore does not reach an optimal policy at all with the same amount of training.\n\n\nTable 2: Difference in average step counts of the Q-learner and RBQL. Improvement shows the factor by which the\namount of steps is reduced from episode 0 to 24.\n\n\nGrid size Steps in episode 0 Steps in episode 24 Improvement\n**Q-learning agent**\n5 _×_ 5 278.06 49.14 5.66\n\n10 _×_ 10 3,308.46 281.44 11.76\n15 _×_ 15 7,180.98 778.68 9.22\n**RBQL agent**\n5 _×_ 5 191.84 9.62 19.94\n\n10 _×_ 10 843.52 23.68 35.62\n\n15 _×_ 15 1,965 35.96 90.76\n\n\nTo further show RBQL’s efficiency, it has also been tested under the same parameters in a grid of size 50 _×_ 50. The\nresults can be seen in figure 6. This test is done to demonstrate that even such a large maze can be explored by RBQL.\nAs with the previous examples, by far the largest policy improvement still happens in the first episode. With mazes of\nsuch a large size, a lot more spikes in step counts are seen in later episodes because there are more states to explore. The\ndifference in average step counts goes from 20,811.08 in episode 0 to 344.9 in episode 24, an improvement by a factor\nof 60.34. This is worse than the improvement in the 15 _×_ 15 mazes, but still almost double that of the 10 _×_ 10 mazes.\n\n\n**5** **Discussion**\n\n\nThis chapter explores the practicality of using this algorithm to solve other Markov decision processes. It discusses\nwhich parts of the implementation are and are not specific to the problem of fastest path through a maze, which\nimprovements can be made to make it more applicable for other problems and showcases further points for research in\nthis field. The constraints given in this paper are that the agent will attempt to solve deterministic, episodic tasks with a\nsingle terminal state as its only source of positive rewards. This chapter also discusses which of these constraints can be\ndismissed.\n\n\nThere are a few parts of the implementation as presented in chapter 3.2 that are only applicable to this specific problem.\nThis is not necessarily a bad thing, as the purpose of the RBQL agent is to utilize knowledge of its environment. As a\nresult of this, the only parts that cannot be directly adapted for other problems are the way the agent builds its model.\nIn the grid world maze, it can assume that every state has the same actions it can take and has a neighboring state in\neach direction (though it may sometimes be itself). Further, every action always has an opposite action, going up can\nalways be undone by going down for example. These assumptions allow it to easily build a model of the grid world\nand influence its policy in how it further explores it. They allow the agent to take steps in each direction to check for\nwalls and they allow the agent to backtrack when it is stuck in a dead end. These assumptions cannot be guaranteed\nfor other Markov decision processes or even for grid worlds with more complex behavior like a wind tunnel that if\nwalked through also pushes the agent one tile in the direction the wind is traveling. The way in which the agent builds a\n\n\n11\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**3**\n\n\n**2** _**.**_ **8**\n\n\n**2** _**.**_ **6**\n\n\n**2** _**.**_ **4**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 6: Number of steps taken to find the goal in a randomly generated grid world maze of size 50 _×_ 50. The light\ngreen area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest two,\nand the green line shows the average performance.\n\n\n12\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-11-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nmodel has to either be designed for each environment individually or it has to be abstracted so that it is more broadly\napplicable. Finding such an approach to model building is one area of improvement for RBQL. Importantly though,\nnone of these assumptions are required for the agent to function. Backtracking, opposite steps and the same actions for\nevery state simply make the implementation easier and more efficient. As long as no path of a directed graph would\ncause the agent to be stuck with no way to reach a terminal state, it can be explored and evaluated.\n\n\nAnother improvement to the way the implementation builds its model is to simplify it as far as possible. As the amount\nof states directly influences how long a problem takes to solve, RBQL will become more efficient the more it can\nremove unnecessary states. Currently, every position has its own state. If the agent could detect “hallways” – tiles with\nparallel walls – they could be removed without problem in favor of directly connecting the two tiles at either side of the\nhallway – only the negative rewards for the length of the hallway would have to be implemented into the model. Further,\nif there is a non-forking path that leads into a dead end, the entire path could be treated as a wall and ignored entirely.\nThis would leave only the starting state, terminal state, turns and forking paths to evaluate. Both of these additions leave\nthe key part of the algorithm, traversing the model backwards and applying the RBQL update formula, untouched.\n\n\nRBQL can be easily adapted to include multiple terminal states with the same or different rewards and this is already\nsupported by the implementation. There are two possible ways to do this. First is to create an imaginary state that\nall terminal states lead into from which the backtracking always starts. Second is to remember all terminal states and\nbacktrack from each of them. The first option is much more efficient as each state still only gets evaluated once while\nthe second version avoids having to tamper with the model.\n\n\nFinally, RBQL could be adapted to work in non-deterministic environments. To reiterate, deterministic means that a\nstate-action pair always yields the same state-reward pair. If the agent could, while building its model, also estimate the\ntransition probabilities of a state-action pair to a new state, RBQL could still be used to evaluate the states. The RBQL\nupdate rule can be generalized to\n\n\n\n_Q_ ( _St, At_ ) _←_ �\n\n_s∈St_ +1\n\n\n\n( _Rs_ + _γ_ max _Q_ ( _s, a_ )) _· p_ (3)\n_a_\n� �\n\n\n\nwhere _St_ +1 is the set of possible states when taking _At_ from _St_, _p_ is the probability of reaching _s_ when taking _At_ from\n_St_ and _Rs_ is the reward of reaching _s_ . In a deterministic environment, _St_ +1 only consists of one state with _p_ = 1,\nnegating these additions. Whether RBQL would be as effective in non-deterministic environments as in deterministic\nenvironments is something to be explored in further studies.\n\n\nThe only constraint on the algorithm that cannot easily be circumvented is its episodic nature. Because the agent relies\non a terminal state from which to propagate the rewards backwards from, a continuous task implementation seems\nimpossible to implement.\n\n\n**6** **Conclusion**\n\n\nThis paper has introduced recursive backwards Q-learning, a model-based reinforcement learning algorithm that\nevaluates all known state-action pairs of the model at the end of each episode with the Q-learning update rule. It has\nalso shown how recursive backwards Q-learning relates to, adapts and improves on them. This paper has presented\nan implementation of recursive backwards Q-learning in the Godot game engine to test its performance. Through\nmultiple tests, it has been shown to be superior in finding the shortest path through a randomly generated grid world\nmaze. It has been argued that this algorithm could be adapted to solve other deterministic, episodic tasks more quickly\nthan Q-learning. Further, it has given avenues for further research in adapting recursive backwards Q-learning for\nnon-deterministic problems.\n\n\n**References**\n\n\n[1] Richard Bellman, “A markovian decision process,” _Journal of Mathematics and Mechanics_, vol. 6, no. 5, pp. 679–\n684, 1957. [Online]. Available: `[http://www.jstor.org/stable/24900506](http://www.jstor.org/stable/24900506)` .\n\n[2] Christopher John Cornish Hellaby Watkins, “Learning from delayed rewards,” 1989.\n\n[3] Richard S Sutton and Andrew G Barto, _Reinforcement learning: An introduction_ . MIT press, 2018.\n\n[4] Richard S. Sutton, “Learning to predict by the methods of temporal differences,” _Machine Learning_, vol. 3, no. 1,\npp. 9–44, 1988. DOI: `[10.1007/bf00115009](https://doi.org/10.1007/bf00115009)` .\n\n[5] Richard Bellman, “Dynamic programming,” _Princeton, USA: Princeton University Press_, vol. 1, no. 2, p. 3, 1957.\n\n[6] Peter Gabrovšek, “Analysis of maze generating algorithms,” _IPSI Transactions on Internet Research_, vol. 15,\nno. 1, pp. 23–30, 2019.\n\n\n13\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n[7] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael, “A formal basis for the heuristic determination of minimum\ncost paths,” _IEEE Transactions on Systems Science and Cybernetics_, vol. 4, no. 2, pp. 100–107, 1968. DOI:\n`[10.1109/TSSC.1968.300136](https://doi.org/10.1109/TSSC.1968.300136)` .\n\n[8] Eugene F Krause, “Taxicab geometry,” _The Mathematics Teacher_, vol. 66, no. 8, pp. 695–706, 1973.\n\n\n14\n\n\n",
          "ranking": null,
          "is_open_access": false,
          "user_provided": true,
          "pdf_path": "output/literature/user_2404.15822v1/user_2404.15822v1.pdf"
        },
        "chunk_text": "This work demonstrates the viability of applying large-scale language models to automated code quality analysis.",
        "chunk_index": 0
      },
      "summary": "This work demonstrates the viability of applying large-scale language models to automated code quality analysis.",
      "vector_score": 0.7040000000000001,
      "llm_score": 0.88,
      "combined_score": 0.88,
      "source_query": "mock_query_conclusion"
    },
    {
      "chunk": {
        "chunk_id": "mock_17",
        "paper": {
          "id": "acda55ebdf39c6634e89a9730ff7d963471f2b0a",
          "title": "Expected Eligibility Traces",
          "published": "2020-07-03",
          "authors": [
            "H. V. Hasselt",
            "Sephora Madjiheurem",
            "Matteo Hessel",
            "David Silver",
            "André Barreto",
            "Diana Borsa"
          ],
          "summary": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state.\nIn this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that some- times substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
          "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17200/17007",
          "doi": "10.1609/aaai.v35i11.17200",
          "fields_of_study": [
            "Computer Science",
            "Mathematics"
          ],
          "venue": "AAAI Conference on Artificial Intelligence",
          "citation_count": 41,
          "bibtex": "@Article{Hasselt2020ExpectedET,\n author = {H. V. Hasselt and Sephora Madjiheurem and Matteo Hessel and David Silver and André Barreto and Diana Borsa},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Expected Eligibility Traces},\n volume = {abs/2007.01839},\n year = {2020}\n}\n",
          "markdown_text": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)\n\n# **Expected Eligibility Traces**\n\n\n**Hado van Hasselt** [1] **, Sephora Madjiheurem** [2] **, Matteo Hessel** [1]\n\n**David Silver** [1] **, Andr´e Barreto** [1] **, Diana Borsa** [1]\n\n1 DeepMind\n2 University College London, UK\n\n\n\n**Abstract**\n\n\nThe question of how to determine which states and actions\nare responsible for a certain outcome is known as the _credit_\n_assignment problem_ and remains a central research question\nin reinforcement learning and artificial intelligence. _Eligibil-_\n_ity traces_ enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not\nto counterfactual sequences that could also have led to the\ncurrent state. In this work, we introduce _expected eligibility_\n_traces_ . Expected traces allow, with a single update, to update\nstates and actions that could have preceded the current state,\neven if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous)\ntraces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide\na way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which\nensures that the resulting algorithm is a strict generalisation\nof TD( _λ_ ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.\n\n\n**Motivation and Summary**\n\n\nAppropriate credit assignment has long been a major research\ntopic in artificial intelligence (Minsky 1963). To make effective decisions and understand the world, we need to accurately associate events, like rewards or penalties, to relevant\nearlier decisions or situations. This is important both for learning accurate predictions, and for making good decisions.\n_Temporal credit assignment_ can be achieved with repeated\ntemporal-difference (TD) updates (Sutton 1988). One-step\nTD updates propagate information slowly: when a surprising value is observed, the state immediately preceding it is\nupdated, but no earlier states or decisions are updated. _Multi-_\n_step_ updates (Sutton 1988; Sutton and Barto 2018) propagate\ninformation faster over longer temporal spans, speeding up\ncredit assignment and learning. Multi-step updates can be\nimplemented online using _eligibility traces_ (Sutton 1988),\nwithout incurring significant additional computational expense, even if the time spans are long; these algorithms have\ncomputation that is independent of the temporal span of the\npredictions (van Hasselt and Sutton 2015).\n\n\nCopyright c _⃝_ 2021, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\nMDP True value TD(0) TD(λ) ET(λ)\n\n\nFigure 1: A comparison of TD(0), TD( _λ_ ), and the new\nexpected-trace algorithm ET( _λ_ ) (with _λ_ = 0 _._ 9). The MDP\nis illustrated on the left. Each episode, the agent moves randomly down and right from the top left to the bottom right,\nwhere any action terminates the episode. Reward on termination are +1 with probability 0.2, and zero otherwise—all\nother rewards are zero. We plot the value estimates after the\nfirst positive reward, which occurred in episode 5. We see\na) TD(0) only updated the last state, b) TD( _λ_ ) updated the\ntrajectory in this episode, and c) ET( _λ_ ) additionally updated\ntrajectories from earlier (unrewarding) episodes.\n\n\nTraces provide temporal credit assignment, but do not assign credit _counterfactually_ to states or actions that _could_\nhave led to the current state, but did not do so this time.\nCredit will eventually trickle backwards over the course of\nmultiple visits, but this can take many iterations. As an example, suppose we collect a key to open a door, which leads\nto an unexpected reward. Using standard one-step TD learning, we would update the state in which the door opened.\nUsing eligibility traces, we would also update the preceding\ntrajectory, including the acquisition of the key. But we would\nnot update other sequences that _could_ have led to the reward,\nsuch as collecting a spare key or finding a different entrance.\nThe problem of credit assignment to counterfactual states\nmay be addressed by learning a model, and using the model\nto propagate credit (cf. Sutton 1990; Moore and Atkeson\n1993; Chelu, Precup, and van Hasselt 2020); however, it\nhas often proven challenging to construct and use models\neffectively in complex environments (cf. van Hasselt, Hessel,\nand Aslanides 2019).\nWe introduce a new approach to counterfactual credit assignment, based on the concept of _expected eligibility traces_ .\nWe present a family of algorithms, which we call ET( _λ_ ), that\nuse expected traces to update their predictions. We analyse\nthe nature of these expected traces, and illustrate their benefits empirically in several settings—see Figure 1 for a first\n\n\n\n9997\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-1.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-3.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-4.png)\n\n\nillustration. We introduce a bootstrapping mechanism that\nprovides a spectrum of algorithms between standard eligibility traces and expected eligibility traces, and also discuss\nways to apply these ideas with deep neural networks. Finally,\nwe discuss possible extensions and connections to related\nideas such as successor features.\n\n\n**Background**\nSequential decision problems can be modelled as Markov\ndecision processes [1] (MDP) ( _S, A, p_ ) (Puterman 1994), with\nstate space _S_, action space _A_, and a joint transition and\nreward distribution _p_ ( _r, s_ _[′]_ _|s, a_ ). An agent selects actions according to its policy _π_, such that _At ∼_ _π_ ( _·|St_ ) where _π_ ( _a|s_ )\ndenotes the probability of selecting _a_ in _s_, and observes random rewards and states generated according to the MDP, resulting in trajectories _τt_ : _T_ = _{St, At, Rt_ +1 _, St_ +1 _, . . ., ST }_ .\nA central goal is to predict _returns_ of future discounted rewards (Sutton and Barto 2018)\n\n\n_Gt ≡_ _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _Rt_ +2 + _γt_ +1 _γt_ +2 _Rt_ +3 + _. . ._\n\n\n\n=\n\n\n\n_T_\n� _γt_ [(] +1 _[i][−]_ [1)] _[R][t]_ [+] _[i][,]_\n\n\n_i_ =1\n\n\n\nwhere _T_ is for instance the time the current episode terminates or _T_ = _∞_, and where _γt ∈_ [0 _,_ 1] is a (possibly constant) discount factor and _γt_ [(] _[n]_ [)] = [�] _[n]_ _k_ =0 _[−]_ [1] _[γ][t]_ [+] _[k]_ [, and] _[ γ]_ _t_ [(0)] = 1.\nThe value _vπ_ ( _s_ ) = E [ _Gt|St_ = _s, π_ ] of state _s_ is the expected return for a policy _π_ . Rather than writing the return as\na random variable _Gt_, it will be convenient to instead write it\nas an explicit function _G_ ( _τ_ ) of the random trajectory _τ_ . Note\nthat _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ).\nWe approximate the value with a function _v_ **w** ( _s_ ) _≈_ _vπ_ ( _s_ ).\nThis can for instance be a table—with a single separate entry\n_w_ [ _s_ ] for each state—a linear function of some input features,\nor a non-linear function such as a neural network with parameters **w** . The goal is to iteratively update **w** with\n\n\n**w** _t_ +1 = **w** _t_ + ∆ **w** _t_\n\n\nsuch that _v_ **w** approaches the true _vπ_ . Perhaps the simplest\nalgorithm to do so is the Monte Carlo (MC) algorithm\n\n\n∆ **w** _t ≡_ _α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._\n\n\nMonte Carlo is effective, but has high variance, which can\nlead to slow learning. TD learning (Sutton 1988; Sutton and\nBarto 2018) instead replaces the return with the current estimate of its expectation _v_ ( _St_ +1) _≈_ _G_ ( _τt_ +1: _T_ ), yielding\n\n\n∆ **w** _t ≡_ _αδt∇_ **w** _v_ **w** ( _St_ ) _,_ (1)\nwhere _δt ≡_ _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ ) _,_\n\n\nwhere _δt_ is called the temporal-difference (TD) error. We\ncan interpolate between these extremes, for instance with\n_λ_ -returns which smoothly mix values and sampled returns:\n\n\n_G_ _[λ]_ ( _τt_ : _T_ ) = _Rt_ +1+ _γt_ +1�(1 _−λ_ ) _v_ **w** ( _St_ +1)+ _λG_ _[λ]_ ( _τt_ +1: _T_ )� _._\n\n\n‘Forward view’ algorithms, like the MC algorithm, use returns\nthat depend on future trajectories and need to wait until the\n\n\n1The ideas in this paper extend naturally to POMDPs (cf. **?** ).\n\n\n\nend of an episode to construct their updates, which can take a\nlong time. Conversely, ‘backward view’ algorithms rely only\non past experiences and can update their predictions online,\nduring an episode. Such algorithms build an _eligibility trace_\n(Sutton 1988; Sutton and Barto 2018). An example is TD( _λ_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ _t,_ with _**e**_ _t_ = _γtλ_ _**e**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ ) _,_\n\n\nwhere _**e**_ _t_ is an accumulating eligibility trace. This trace can\nbe viewed as a function _**e**_ _t ≡_ _**e**_ ( _τ_ 0: _t_ ) of the trajectory of past\ntransitions. The TD update in (1) is known as TD(0), because\nit corresponds to using _λ_ = 0. TD( _λ_ = 1) corresponds to an\nonline implementation of the MC algorithm. Other variants\nexist, using other kinds of traces, and equivalences have been\nshown between these algorithms and their forward views that\nuse _λ_ -returns: these backward-view algorithms converge to\nthe same solution as the corresponding forward view, and can\nin some cases yield equivalent weight updates (Sutton 1988;\nvan Seijen and Sutton 2014; van Hasselt and Sutton 2015).\n\n\n**Expected Traces**\n\n\nThe main idea of this paper is to use the concept of an _ex-_\n_pected eligibility trace_, defined as\n\n\n_**z**_ ( _s_ ) _≡_ E [ _**e**_ _t | St_ = _s_ ] _,_\n\n\nwhere the expectation is over the agent’s policy and the MDP\ndynamics. We introduce a concrete family of algorithms,\nwhich we call ET( _λ_ ) and ET( _λ_, _η_ ), that learn expected traces\nand use them in value updates. We analyse these algorithms\ntheoretically, describe specific instances, and discuss computational and algorithmic properties.\n\n\n**ET(** _λ_ **)**\n\n\nWe propose to learn approximations _**zθ**_ ( _s_ ) _≈_ _**z**_ ( _s_ ), with parameters _**θ**_ _∈_ R _[d]_ (e.g., the weights of a neural network). One\nway to learn _**zθ**_ is by updating it toward the instantaneous\ntrace _**e**_ _t_, by minimizing an empirical loss _L_ ( _**e**_ _t,_ _**zθ**_ ( _St_ )). For\ninstance, _L_ could be a component-wise squared loss, optimized with stochastic gradient descent:\n\n\n_**θ**_ _t_ +1 = _**θ**_ _t_ + ∆ _**θ**_ _t,_ where (2)\n\n\n1\n\n∆ _**θ**_ _t_ = _−β_ _[∂]_\n\n_∂_ _**θ**_ 2 [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))] _[⊤]_ [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))]\n\n= _β_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] ( _**e**_ _t −_ _**zθ**_ ( _St_ )) _,_ (3)\n\n_∂_ _**θ**_\n\n\nwhere _[∂z]_ _**[θ]**_ _∂_ [(] _**θ**_ _[S][t]_ [)] is a _|_ _**θ**_ _| × |_ _**e**_ _|_ Jacobian [2] and _β_ is a step size.\n\nThe idea is then to use _**zθ**_ ( _s_ ) _≈_ E [ _**e**_ _t | St_ = _s_ ] in place\nof _**e**_ _t_ in the value update, which becomes\n\n\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (4)\n\n\nWe call this ET( _λ_ ). Below, we prove that this update can\nbe unbiased and can have lower variance than TD( _λ_ ). Algorithm 1 shows pseudo-code for a concrete instance of ET( _λ_ ).\n\n\n2The Jacobian-vector product can efficiently be computed (e.g.,\nvia auto-differentiation) with computational requirements that are\ncomparable to the computation of the loss.\n\n\n\n9998\n\n\n\n\n**Algorithm 1** ET( _λ_ )\n\n\n1: initialise **w**, _**θ**_\n2: **for** _M_ episodes **do**\n3: initialise _**e**_ = **0**\n\n4: observe initial state _S_\n5: **repeat** for each step in episode _m_\n6: generate _R_ and _S_ _[′]_\n\n7: _δ ←_ _R_ + _γv_ **w** ( _S_ _[′]_ ) _−_ _v_ **w** ( _S_ )\n8: _**e**_ _←_ _γλ_ _**e**_ + _∇_ **w** _v_ **w** ( _S_ )\n\n9: _**θ**_ _←_ _**θ**_ + _β_ _[∂]_ _**[z]**_ _∂_ _**[θ]**_ _**θ**_ [(] _[S]_ [)] ( _**e**_ _−_ _**zθ**_ ( _S_ ))\n\n10: **w** _←_ **w** + _αδ_ _**zθ**_ ( _S_ )\n11: **until** _S_ is terminal\n\n12: **end for**\n\n13: **Return w**\n\n\n**Interpretation and ET(** _λ, η_ **)**\n\nWe can interpret TD(0) as taking the MC update and replacing the return from the subsequent state, which is a function\nof the future trajectory, with a state-based estimate of its expectation: _v_ **w** ( _St_ +1) _≈_ E [ _G_ ( _τt_ +1: _T_ ) _|St_ +1 ]. This becomes\nmost clear when juxtaposing the updates:\n\n\n_α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _,_ (MC)\n_α_ ( _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._ (TD)\n\n\nTD( _λ_ ) also uses a function of a trajectory: the trace _**e**_ _t_ . We\npropose replacing this as well with a function of state: the\nexpected trace _**zθ**_ ( _St_ ) _≈_ E [ _**e**_ ( _τ_ 0: _t_ ) _|St_ ]. Again juxtaposing:\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ ( _τ_ 0: _t_ ) _,_ (TD( _λ_ ))\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (ET( _λ_ ))\n\n\nWe can interpolate smoothly between MC and TD(0) via\n_λ_ . This is often useful to trade off variance of the return with\npotential bias of the value estimate. For instance, we might\nnot have access to the true state _s_, and might instead have to\nrely on features **x** ( _s_ ). Then we cannot always represent or\nlearn the true values _v_ ( _s_ )—for instance different states may\nbe aliased (Whitehead and Ballard 1991).\nSimilarly, when moving from TD( _λ_ ) to ET( _λ_ ) we replaced\na trajectory-based trace with a state-based estimate. This\nmight induce bias and, again, we can smoothly interpolate by\nusing a recursively defined mixture trace _**y**_ _t_, as defined as [3]\n\n\n_**y**_ _t_ = (1 _−_ _η_ ) _**zθ**_ ( _St_ ) + _η_ � _γtλ_ _**y**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ )� _._ (5)\n\n\nThis recursive usage of the estimates _**zθ**_ ( _s_ ) at previous states\nis analogous to bootstrapping on future state values when\nusing a _λ_ -return, with the important difference that the arrow\nof time is opposite. This means we do not first have to convert\nthis into a backward view: the quantity can already be computed from past experience directly. We call the algorithm\nthat uses this mixture trace ET( _λ_, _η_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**y**_ ( _St_ ) _._ (ET( _λ_, _η_ ))\n\n\n3While _**y**_ _t_ depends on both _η_ and _λ_ we leave this dependence\nimplicit, as is conventional for traces.\n\n\n\nNote that if _η_ = 1 then _**y**_ _t_ = _**e**_ _t_ equals the instantaneous\ntrace: ET( _λ_, 1) is equivalent to TD( _λ_ ). If _η_ = 0 then _**y**_ _t_ = _**z**_ _t_\nequals the expected trace; the algorithm introduced earlier\nas ET( _λ_ ) is equivalent to ET( _λ_, 0). By setting _η ∈_ (0 _,_ 1), we\ncan smoothly interpolate between these extremes.\n\n\n**Theoretical Analysis**\n\nWe now analyse the new ET algorithms theoretically. First\nwe show that if we use _**z**_ ( _s_ ) directly and _s_ is Markov then the\nupdate has the same expectation as TD( _λ_ ) (though possibly\nwith lower variance), and therefore also inherits the same\nfixed point and convergence properties.\n\n\n**Lemma 1.** _If s is Markov, then_\n\n\nE [ _δt_ _**e**_ _t | St_ = _s_ ] = E [ _δt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] _._\n\n\n_Proof._ In Appendix .\n\n\n**Proposition 1.** _Let_ _**e**_ _t be any trace vector, updated in any_\n_way. Let_ _**z**_ ( _s_ ) = E [ _**e**_ _t | St_ = _s_ ] _. Consider the ET(λ) algo-_\n_rithm_ ∆ **w** _t_ = _αtδt_ _**z**_ ( _St_ ) _. For all Markov states s the expec-_\n_tation of this update is equal to the expected update under_\n_instantaneous trace_ _**e**_ _t, and its variance is lower or equal:_\n\n\nE [ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] = E [ _αtδt_ _**e**_ _t|St_ = _s_ ] _and_\nV[ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] _≤_ V[ _αtδt_ _**e**_ _t|St_ = _s_ ] _,_\n\n\n_where the second inequality holds component-wise for the_\n_update vector, and is strict when_ V[ _**e**_ _t|St_ ] _>_ 0 _._\n\n\n_Proof._ We have\n\n\nE [ _αtδt_ _**e**_ _t | St_ = _s_ ]\n= E [ _αtδt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] (Lemma 1)\n= E [ _αtδt | St_ = _s_ ] _**z**_ ( _s_ )\n= E [ _αtδt_ _**z**_ ( _St_ ) _| St_ = _s_ ] _._ (6)\n\n\nDenote the _i_ -th component of _**z**_ ( _St_ ) by _zt,i_ and the _i_ -th\ncomponent of _**e**_ _t_ by _et,i_ . Then, we also have\n\n\nE � ( _αtδtzt,i_ ) [2] _|St_ = _s_ � = E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � _zt,i_ [2]\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E [ _et,i|St_ = _s_ ] [2]\n\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ ��E � _e_ [2] _t,i_ _[|][S][t]_ [=] _[ s]_ � _−_ V[ _et,i|St_ = _s_ ]�\n\n_≤_ E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E � _e_ [2] _t,i_ _[|][ S][t]_ [=] _[ s]_ �\n\n= E � ( _αtδtet,i_ ) [2] _| St_ = _s_ � _,_\n\n\nwhere the last step used the fact that _s_ is Markov, and the inequality is strict when V[ _et,i|St_ ] _>_ 0. Since the expectations\nare equal, as shown in (6), the conclusion follows.\n\n\n**Interpretation** Proposition 1 is a strong result: it holds for\nany trace update, including accumulating traces (Sutton 1984,\n1988), replacing traces (Singh and Sutton 1996), dutch traces\n(van Seijen and Sutton 2014; van Hasselt, Mahmood, and\nSutton 2014; van Hasselt and Sutton 2015), and future traces\nthat may be discovered. It implies convergence of ET( _λ_ )\nunder the same conditions as TD( _λ_ ) (Dayan 1992; Peng 1993;\n\n\n\n9999\n\n\n\n\nTsitsiklis 1994) with lower variance when V[ _**e**_ _t|St_ ] _>_ 0,\nwhich is the common case.\nNext, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a\nlearned approximation _**z**_ _t_ ( _s_ ) _≈_ _**z**_ ( _s_ ) that relies solely on\nobserved experience.\n\n**Proposition 2.** _Let_ _**e**_ _t an instantaneous trace vector. Then_\n1 _nt_ ( _s_ )\n_let_ _**z**_ _t_ ( _s_ ) _be the empirical mean_ _**z**_ _t_ ( _s_ ) = _nt_ ( _s_ ) � _i_ _**e**_ _t_ _[s]_ _i_ _[,]_\n_where t_ _[s]_ _i_ _[denotes past times when we have been in state]_\n_s, that is St_ _[s]_ _i_ [=] _[ s][, and][ n][t]_ [(] _[s]_ [)] _[ is the number of visits to][ s]_\n_in the first t steps. Consider the expected trace algorithm_\n**w** _t_ +1 = **w** _t_ + _αtδt_ _**z**_ _t_ ( _St_ ) _. If St is Markov, the expectation of_\n_this update is equal to the expected update with instantaneous_\n_traces_ _**e**_ _t, while attaining a potentially lower variance:_\n\n\nE [ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] = E [ _αtδt_ _**e**_ _t | St_ ] _and_\nV[ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] _≤_ V[ _αtδt_ _**e**_ _t | St_ ] _,_\n\n\n_where the second inequality holds component-wise. The in-_\n_equality is strict when_ V[ _**e**_ _t | St_ ] _>_ 0 _._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** Proposition 2 mirrors Proposition 1 but, importantly, covers the case where we estimate the expected\ntraces from data, rather than relying on exact estimates. This\nmeans the benefits extend to this pure learning setting. Again,\nthe result holds for any trace update. The inequality is typically strict when the path leading to state _St_ = _s_ is stochastic\n(due to environment or policy).\nNext we consider what happens if we do not have Markov\nstates and instead have to rely on, possibly non-Markovian,\nfeatures **x** ( _s_ ). We then have to pick a function class and for\nthe purpose of this analysis we consider linear expected traces\n_**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) and values _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), as convergence for non-linear values can not always be assured even\nfor standard TD( _λ_ ) (Tsitsiklis and Van Roy 1997), without\nadditional assumptions (e.g., Ollivier 2018; Brandfonbrener\nand Bruna 2020).\n\n**Proposition 3.** _When using approximations z_ **Θ** ( _s_ ) = **Θx** ( _s_ )\n_and v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ) _then, if_ (1 _−_ _η_ ) **Θ** + _η_ I _is non-singular,_\n_ET(λ, η) has the same fixed point as TD(λη)._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** This result implies that linear ET( _λ_, _η_ ) converges under similar conditions as linear TD( _λ_ _[′]_ ) for _λ_ _[′]_ = _λ·η_ .\nIn particular, when **Θ** is non-singular, using the approximation _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) in ET( _λ_, 0) = ET( _λ_ ) implies convergence to the fixed point of TD(0).\nThough ET( _λ_, _η_ ) and TD( _λη_ ) have the same fixed point,\nthe algorithms are not equivalent. In general, their updates\nare not the same. Linear approximations are more general\nthan tabular functions (which are linear functions of a indicator vector for the current state), and we have already seen\nin Figure 1 that ET( _λ_ ) behaves quite differently from both\nTD(0) and TD( _λ_ ), and we have seen its variance can be lower\nin Propositions 1 and 2. Interestingly, **Θ** resembles a preconditioner that speeds up the linear semi-gradient TD update,\n\n\n10000\n\n\n\nepisode 5\n1st reward\n\n\n\nepisode 12\n2nd reward\n\n\n\nepisode 100\n20 rewards\n\n\n\nepisode 1K\n~200 rewards\n\n\n\nepisode 10K\n~2K rewards\n\n\n\nFigure 2: In the same setting as Figure 1, we show later value\nestimates after more rewards have been observed. TD(0)\nlearns slowly but steadily, TD( _λ_ ) learns faster but with higher\nvariance, and ET( _λ_ ) learns both fast and stable.\n\n\nsimilar to how second-order optimisation algorithms (Amari\n1998; Martens 2016) precondition the gradient updates.\n\n\n**Empirical Analysis**\n\nFrom the insights above, we expect that ET( _λ_ ) yields lower\nprediction errors because it has lower variance and aggregates information across episodes better. In this section we\nempirically investigate expected traces in several experiments.\nWhenever we refer to ET( _λ_ ), this is equivalent to ET( _λ_, 0).\n\n\n\n**An Open World**\n\nFirst consider the grid world depicted in Figure 1. The agent\nrandomly moves right or down (excluding moves that would\nhit a wall), starting from the top-left corner. Any action in the\nbottom-right corner terminates the episode with +1 reward\nwith probability 0 _._ 2, and 0 otherwise. All other rewards are 0.\nFigure 1 shows value estimates after the first positive reward, which occurred in the fifth episode. TD(0) updated a\nsingle state, TD( _λ_ ) updated earlier states in that episode, and\nET( _λ_ ) additionally updated states from previous episodes.\nFigure 2 additionally shows value estimates after the\nsecond reward (which occurred in episode 12), and after\nroughly 20, 200, and 2000 rewards (or 100, 1000, and 10 _,_ 000\nepisodes, respectively). ET( _λ_ ) converged faster than TD(0),\nwhich propagated information slowly, and faster than TD( _λ_ ),\nwhich exhibited higher variance. All step sizes decayed as\n_α_ = _β_ = ~~�~~ 1 _/k_, where _k_ is the current episode number.\n\n\n**A Multi-Chain**\n\nWe now consider the multi-chain shown in Figure 3. We\nfirst compare TD( _λ_ ) and ET( _λ_ ) with tabular values on various variants of the multi-chain, corresponding to _m ∈_\n_{_ 1 _,_ 2 _,_ 4 _,_ 8 _, ...,_ 128 _}_ parallel chains of length _n_ = 4. The leftmost plot in Figure 4 shows the average root mean squared\nerror (RMSE) of the value predictions after 1024 episodes.\nWe ran 10 seeds for each combination of step size 1 _/t_ _[d]_ with\n_d ∈{_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 1 _}_ and _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ .\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-4-0.png)\n\neither is +1 with probability 0 _._ 9 or _−_ 1 with probability 0 _._ 1.\n\n\nThe left plot in Figure 4 shows value errors for different\n_m_, minimized over _d_ and _λ_ . The prediction error of TD( _λ_ )\n(blue) grew quickly with the number of parallel chains. ET( _λ_ )\n(orange) scaled better, because it updates values in multiple\nchains (from past episodes) upon receiving a surprising reward (e.g., _−_ 1) on termination. The other three plots in Figure\n4 show value error as a function of _λ_ for a subset of problems\ncorresponding to _m ∈{_ 8 _,_ 32 _,_ 128 _}_ . The dependence on _λ_\ndiffers across algorithms and problem instances, but ET( _λ_ )\nconsistently achieved lower error than TD( _λ_ ), especially with\nhigh _λ_ . Further analysis, including on step-size sensitivity, is\nincluded in the appendix.\nNext, we encode each state with a feature vector **x** ( _s_ )\ncontaining a binary indicator vector of the branch, a binary\nindicator of the progress along the chain, a bias that always\nequals one, and two binary features indicating when we are in\nthe start (white) or bottleneck (orange) state. We extend the\nlengths of the chains to _n_ = 16. Both TD( _λ_ ) and ET( _λ_ ) use\na linear value function _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), and ET( _λ_ ) uses a\nlinear expected trace _z_ **Θ** ( _s_ ) = **Θx** ( _s_ ). All updates use the\nsame constant step size _α_ . The left plot in Figure 5 shows the\naverage root mean squared value error after 1024 episodes\n(averaged over 10 seeds). For each point the best constant\nstep size _α ∈{_ 0 _._ 01 _,_ 0 _._ 03 _,_ 0 _._ 1 _}_ (shared across all updates)\nand _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ is selected. ET( _λ_ ) (orange)\nattained lower errors across all values of _m_ (left plot), and\nfor all _λ_ (center two plots, for two specific _m_ ). The right plot\nshows results for smooth interpolations via _η_, for _λ_ = 0 _._ 9\nand _m_ = 16. The full expected trace ( _η_ = 0) performed well\nhere, we expect in other settings the additional flexibility of\n_η_ could be beneficial.\n\n\n**Expected Traces in Deep Reinforcement Learning**\n\n(Deep) neural networks are a common choice of function\nclass in reinforcement learning (e.g., Werbos 1990; Tesauro\n1992, 1994; Bertsekas and Tsitsiklis 1996; Prokhorov and\nWunsch 1997; Riedmiller 2005; van Hasselt 2012; Mnih\net al. 2015; van Hasselt, Guez, and Silver 2016; Wang et al.\n2016; Silver et al. 2016; Duan et al. 2016; Hessel et al. 2018).\nEligibility traces are not very commonly combined with deep\nnetworks (but see Tesauro 1992; Elfwing, Uchibe, and Doya\n2018), perhaps in part because of the popularity of experience\n\n\n10001\n\n\n\nreplay (Lin 1992; Mnih et al. 2015; Horgan et al. 2018).\nPerhaps the simplest way to extend expected traces to deep\nneural networks is to first separate the value function into\na representation **x** ( _s_ ) and a value _v_ ( **w** _,_ _**ξ**_ )( _s_ ) = **w** _[⊤]_ **x** _**ξ**_ ( _s_ ),\nwhere **x** _**ξ**_ is some (non-linear) function of the observations\n_s_ . [4] We can then apply the same expected trace algorithm as\nused in the previous sections by learning a separate linear\nfunction _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) using the representation which is\nlearned by backpropagating the value updates:\n\n\n**w** _t_ +1 = **w** _t_ + _αδ_ _**z**_ **Θ** ( _St_ ) _,_\n\n_**ξ**_ _t_ +1 = _**ξ**_ _t_ + _αδ_ _**e**_ _**[ξ]**_ _t_ _[,]_\n\nwhere _**e**_ _**[ξ]**_ _t_ [=] _[ γ][t][λ]_ _**[e][ξ]**_ _t−_ 1 [+] _[ ∇]_ _**[ξ]**_ _[v]_ [(] **[w]** _[,]_ _**[ξ]**_ [)][(] _[S][t]_ [)] _[,]_\n\n_**e**_ **[w]** _t_ [=] _[ γ][t][λ]_ _**[e]**_ **[w]** _t−_ 1 [+] _[ ∇]_ **[w]** _[v]_ ( **w** _,_ _**ξ**_ ) [(] _[S][t]_ [)] _[,]_\n\n\nand then updating **Θ** to minimise component-wise squared\ndifferences between _**e**_ **[w]** _t_ [and] _**[ z]**_ **[Θ]** _t_ [(] _[S][t]_ [)][, as in (2) and (3).]\nInteresting challenges appear outside the fully linear case.\nFirst, the representation will itself be updated and will have\nits own trace _**e**_ _**[ξ]**_ _t_ [. Second, in the control case we optimise]\nbehaviour: the policy will change. Both these properties of\nthe non-linear control setting imply that the expected traces\nmust track a non-stationary target. We found that being able to\ntrack this rather quickly improved performance: the expected\ntrace parameters **Θ** in the following experiment were updated\nwith a relatively high step size of _β_ = 0 _._ 1.\nWe tested this idea on two canonical Atari games: Pong and\nMs. Pac-Man. The results in Figure 6 show that the expected\ntraces helped speed up learning compared to the baseline\nwhich uses accumulating traces, for various step sizes. Unlike\nmost prior work on this domain, which often relies on replay\n(Mnih et al. 2015; Schaul et al. 2016; Horgan et al. 2018)\nor parallel streams of experience (Mnih et al. 2016), these\nalgorithms updated the values online from a single stream\nof experience. Further details on the experimental setup are\ngiven in the appendix.\nThese experiments demonstrate that the idea of expected\ntraces extends to non-linear function approximation, such as\ndeep neural networks. We consider this to be a rich area of\nfurther investigations. The results presented here are similar\nto earlier results (e.g., Mnih et al. 2015) and are not meant to\ncompete with state-of-the-art performance results, which often depend on replay and much larger amounts of experience\n(e.g., Horgan et al. 2018).\n\n\n**Discussion and Extensions**\n\nWe now discuss various interesting interpretations and relations, and discuss promising extensions.\n\n\n**Predecessor Features**\n\nFor linear value functions the expected trace _z_ ( _s_ ) can be\nexpressed non-recursively as follows:\n\n\n\n�\n\n\n\n_**z**_ ( _s_ ) = E\n\n\n\n_∞_\n� _λ_ [(] _t_ _[n]_ [)] _γt_ [(] _[n]_ [)] **x** _t−n | St_ = _s_\n� _n_ =0\n\n\n\n_,_ (7)\n\n\n\n4Here _s_ denotes observations to the agent, not a full environment\nstate— _s_ is not assumed to be Markovian.\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-1.png)\n\nFigure 4: Prediction errors in the multi-chain. ET( _λ_ ) (orange) consistently outperformed TD( _λ_ ) (blue). Shaded areas depict\nstandard errors across 10 seeds.\n\n\nFigure 5: Comparing value error with linear function approximation a) as function of the number of branches (left), b) as\nfunction of _λ_ (center two plots) and c) as function of _η_ (right). The left three plots show comparisons of TD( _λ_ ) (blue) and ET( _λ_ )\n(orange), showing ET( _λ_ ) attained lower prediction errors. The right plot interpolates between these algorithms via ET( _λ_, _η_ ),\nfrom ET( _λ_ ) = ET( _λ_, 0) to ET( _λ_, 1) = TD( _λ_ ), with _λ_ = 0 _._ 9 (corresponding to a vertical slice indicated in the second plot).\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-3.png)\n\nwhere _γk_ [(] _[n]_ [)] _≡_ [�] _[k]_ _j_ = _k−n_ _[γ][j]_ [. This is interestingly similar to the]\ndefinition of _successor features_ (Barreto et al. 2017):\n\n\n\n�\n\n\n\n_ψ_ ( _s_ ) = E\n\n\n\n_∞_\n� _γt_ [(] +1 _[n][−]_ [1)] **x** _t_ + _n | St_ = _s_\n� _n_ =1\n\n\n\n_._ (8)\n\n\n\nThe summation in (8) is over future features, while in (7)\nwe have a sum over features already observed by the agent.\nWe can thus think of linear expected traces as _predecessor_\n_features_ . A similar connection was made in the tabular setting by Pitis (2018), relating source traces, which aim to\nestimate the source matrix ( _I −_ _γP_ ) _[−]_ [1], to successor representations (Dayan 1993). In a sense, the above generalises\nthis insight. In addition to being interesting in its own right,\nthis connection allows for an intriguing interpretation of _**z**_ ( _s_ )\nas a multidimensional value function. Like with successor\nfeatures, the features **x** _t_ play the role of rewards, discounted\nwith _γ · λ_ rather than _γ_, and with time flowing backwards.\nAlthough the predecessor interpretation only holds in the\nlinear case, it is also of interest as a means to obtain a practical\nimplementation of expected traces with non-linear function\napproximation, for instance applied only to the linear ‘head’\nof a deep neural network. We used this ‘predecessor feature\ntrick’ in our Atari experiments described earlier.\n\n\n**Relation to Model-Based Reinforcement Learning**\n\n\nModel-based reinforcement learning provides an alternative\napproach to efficient credit assignment. The general idea is\nto construct a model that estimates state-transition dynamics,\nand to update the value function based upon hypothetical\n\n\n10002\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-4.png)\n\ntransitions drawn from the model (Sutton 1990), for example\nby prioritised sweeping (Moore and Atkeson 1993; van Seijen\nand Sutton 2013). In practice, model-based approaches have\nproven challenging in environments (such as Atari games)\nwith rich perceptual observations, compared to model-free\napproaches that more directly update the agent’s policy and\npredictions (van Hasselt, Hessel, and Aslanides 2019).\nIn some sense, expected traces also construct a model of\nthe environment—but one that differs in several key regards\nfrom standard state-to-state models used in model-based reinforcement learning. First, expected traces estimate _past_\nquantities rather than _future_ quantities. Backward planning\n(e.g., Chelu, Precup, and van Hasselt 2020) is possible with\nexplicit transition models, but is less common in practice.\nSecond, expected traces estimate the accumulation of _gradi-_\n_ents_ over a multi-step trajectory, rather than trying to learn\nthe full transition dynamics, thereby focusing only on those\naspects that matter for the eventual weight update. Third, expected traces allow credit assignment across these potential\npast trajectories with a single update, without the iterative\ncomputation that is typically required when using a dynamics\nmodel. These differences may be important to side-step some\nof the challenges faced in model-based learning.\n\n\n**Batch Learning and Replay**\n\n\nWe have mainly considered the online learning setting in this\npaper. It is often convenient to learn from batches of data, or\nreplay transitions repeatedly, to enhance data efficiency. A\nnatural extension is replay the experiences sequentially (e.g.\nKapturowski et al. 2018), but perhaps alternatives exist. We\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-1.png)\n\nFigure 6: Performance of Q( _λ_ ) ( _η_ = 1, blue) and QET( _λ_ ) ( _η_ = 0, orange) on Pong and Ms.Pac-Man for various learning rates.\nShaded regions show standard error across 10 random seeds. All results are for _λ_ = 0 _._ 95. Further implementation details and\nhyper-parameters are in the appendix.\n\n\n\nnow discuss one potential extension.\nWe defined a mixed trace _**y**_ _t_ that mixes the instantaneous\nand expected traces. Optionally the expected trace _**z**_ _t_ can\nbe updated towards the mixed trace _**y**_ _t_ as well, instead of\ntowards the instantaneous trace _**e**_ _t_ . Analogously to TD( _λ_ ) we\npropose to then use at least one real step of data:\n\n\n∆ _**θ**_ _t ≡_ _β_ ( _**∇**_ _t_ + _γtλt_ _**y**_ _t−_ 1 _−_ _**zθ**_ ( _St_ )) _[⊤]_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] _,_ (9)\n\n_∂_ _**θ**_\n\n\nwith _**∇**_ _t ≡∇_ **w** _v_ **w** ( _St_ ). This is akin to a forward-view _λ_ return update, with _∇_ **w** _v_ **w** ( _St_ ) in the role of (vector) reward,\nand _**zθ**_ of value, and discounted by _λtγt_, but reversed in time.\nIn other words, this can be considered a sampled Bellman\nequation (Bellman 1957) but backward in time.\nWhen we then choose _η_ = 0, then _**y**_ _t−_ 1 = _z_ _**θ**_ ( _St−_ 1), and\nthen the target in (9) only depends on a single transition.\nInterestingly, that means we can then learn expected traces\nfrom _individual_ transitions, sampled out of temporal order,\nfor instance in batch settings or when using replay.\n\n\n**Application to Other Traces**\n\n\nWe can apply the idea of expected trace to more traces than\nconsidered here. We can for instance consider the characteristic eligibility trace used in REINFORCE (Williams 1992)\nand related policy-gradient algorithms (Sutton et al. 2000).\nAnother appealing application is to the follow-on trace\nor _emphasis_, used in emphatic temporal difference learning\n(Sutton, Mahmood, and White 2016) and related algorithms\n(e.g., Imani, Graves, and White 2018). Emphatic TD was\nproposed to correct an important issue with off-policy learning, which can be unstable and lead to diverging learning\ndynamics. Emphatic TD weights updates according to 1) the\ninherent interest in having accurate predictions in that state\nand, 2) the importance of predictions in that state for updating\n\n\n10003\n\n\n\nother predictions. Emphatic TD uses scalar ‘follow-on’ traces\nto determine the ‘emphasis’ for each update. However, this\nfollow-on trace can have very high, even infinite, variance.\nInstead, we might estimate and use its expectation instead of\nthe instantaneous emphasis. A related idea was explored by\nZhang, Boehmer, and Whiteson (2019) to obtain off-policy\nactor critic algorithms.\n\n\n**Conclusion**\n\n\nWe have proposed a mechanism for efficient credit assignment, using the expectation of an eligibility trace. We have\ndemonstrated this can sometimes speed up credit assignment\ngreatly, and have analyzed concrete algorithms theoretically\nand empirically to increase understanding of the concept.\nExpected traces have several interpretations. First, we can\ninterpret the algorithm as counterfactually updating multiple possible trajectories leading up to the current state. Second, they can be understood as trading off bias and variance,\nwhich can be done smoothly via a unifying _η_ parameter, between standard eligibility traces (low bias, high variance) and\nestimated traces (possibly higher bias, but lower variance).\nFurthermore, with tabular or linear function approximation\nwe can interpret the resulting expected traces as predecessor\nstates or features—object analogous to successor states or features, but time-reversed. Finally, we can interpret the linear\nalgorithm as preconditioning the standard TD update, thereby\npotentially speeding up learning. These interpretations suggest that a variety of complementary ways to potentially\nextend these concepts and algorithms.\nWe have shown expected traces can already be used to\nenhance learning in non-linear settings (i.e., deep reinforcement learning), and in the control setting where we update\nthe policy. Further work is needed to determine the full extent\nof the possibilities of these new algorithms.\n\n\n\n\n**References**\n\n\nAmari, S. I. 1998. Natural gradient works efficiently in\nlearning. _Neural computation_ 10(2): 251–276. ISSN 08997667.\n\n\nBarreto, A.; Dabney, W.; Munos, R.; Hunt, J. J.; Schaul, T.;\nvan Hasselt, H. P.; and Silver, D. 2017. Successor features\nfor transfer in reinforcement learning. In _Advances in neural_\n_information processing systems_, 4055–4065.\n\n\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. _J. Artif. Intell. Res. (JAIR)_ 47:\n253–279.\n\n\nBellman, R. 1957. _Dynamic Programming_ . Princeton University Press.\n\n\nBertsekas, D. P.; and Tsitsiklis, J. N. 1996. _Neuro-dynamic_\n_Programming_ . Athena Scientific, Belmont, MA.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018a. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018b. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBrandfonbrener, D.; and Bruna, J. 2020. Geometric Insights\ninto the Convergence of Non-linear TD Learning. In _Interna-_\n_tional Conference on Learning Representations_ .\n\n\nChelu, V.; Precup, D.; and van Hasselt, H. P. 2020. Forethought and Hindsight in Credit Assignment. In Larochelle,\nH.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H.,\neds., _Advances in Neural Information Processing Systems_,\nvolume 33, 2270–2281.\n\n\nDayan, P. 1992. The convergence of TD( _λ_ ) for general\nlambda. _Machine Learning_ 8: 341–362.\n\n\nDayan, P. 1993. Improving generalization for temporal difference learning: The successor representation. _Neural Com-_\n_putation_ 5(4): 613–624.\n\n\nDuan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel,\nP. 2016. Benchmarking deep reinforcement learning for\ncontinuous control. In _International Conference on Machine_\n_Learning_, 1329–1338.\n\n\nElfwing, S.; Uchibe, E.; and Doya, K. 2018. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_ 107:\n3–11.\n\n\nHennigan, T.; Cai, T.; Norman, T.; and Babuschkin, I. 2020.\nHaiku: Sonnet for JAX. URL http://github.com/deepmind/\ndm-haiku.\n\n\nHessel, M.; Modayil, J.; van Hasselt, H. P.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2018. Rainbow: Combining Improvements in Deep\nReinforcement Learning. _AAAI_ .\n\n\n10004\n\n\n\nHorgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel,\nM.; van Hasselt, H. P.; and Silver, D. 2018. Distributed\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ .\n\n\nImani, E.; Graves, E.; and White, M. 2018. An Off-policy\nPolicy Gradient Theorem Using Emphatic Weightings. In\nBengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; CesaBianchi, N.; and Garnett, R., eds., _Advances in Neural Infor-_\n_mation Processing Systems 31_, 96–106. Curran Associates,\nInc. URL http://papers.nips.cc/paper/7295-an-off-policypolicy-gradient-theorem-using-emphatic-weightings.pdf.\n\n\nKaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1995.\nPlanning and Acting in Partially Observable Stochastic Domains. Unpublished report.\n\n\nKapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and\nDabney, W. 2018. Recurrent experience replay in distributed\nreinforcement learning. In _International conference on learn-_\n_ing representations_ .\n\n\nKingma, D. P.; and Adam, J. B. 2015. A method for stochastic optimization. In _International Conference on Learning_\n_Representation_ .\n\n\nLin, L. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_\n8(3): 293–321.\n\n\nMartens, J. 2016. _Second-order optimization for neural net-_\n_works_ . University of Toronto (Canada).\n\n\nMinsky, M. 1963. Steps Toward Artificial Intelligence.\nIn Feigenbaum, E.; and Feldman, J., eds., _Computers and_\n_Thought_, 406–450. McGraw-Hill, New York.\n\n\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous Methods for Deep Reinforcement Learning. In\n_International Conference on Machine Learning_ .\n\n\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg,\nS.; and Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. _Nature_ 518(7540): 529–533.\n\n\nMoore, A. W.; and Atkeson, C. G. 1993. Prioritized Sweeping: Reinforcement Learning with less Data and less Time.\n_Machine Learning_ 13: 103–130.\n\n\nOllivier, Y. 2018. Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies. _CoRR_\nabs/1805.00869.\n\n\nPeng, J. 1993. _Efficient dynamic programming-based learn-_\n_ing for control_ . Ph.D. thesis, Northeastern University.\n\n\nPeng, J.; and Williams, R. J. 1996. Incremental Multi-step\nQ-learning. _Machine Learning_ 22: 283–290.\n\n\nPitis, S. 2018. Source Traces for Temporal Difference Learning. In McIlraith, S. A.; and Weinberger, K. Q., eds., _Pro-_\n_ceedings of the Thirty-Second AAAI Conference on Artificial_\n_Intelligence_, 3952–3959. AAAI Press.\n\n\n\n\nPohlen, T.; Piot, B.; Hester, T.; Azar, M. G.; Horgan, D.;\nBudden, D.; Barth-Maron, G.; van Hasselt, H. P.; Quan, J.;\nVecerˇ ´ık, M.; Hessel, M.; Munos, R.; and Pietquin, O. 2018.\nObserve and look further: Achieving consistent performance\non Atari. _arXiv preprint arXiv:1805.11593_ .\n\n\nProkhorov, D. V.; and Wunsch, D. C. 1997. Adaptive critic\ndesigns. _IEEE Transactions on Neural Networks_ 8(5): 997–\n1007.\n\n\nPuterman, M. L. 1994. _Markov Decision Processes: Discrete_\n_Stochastic Dynamic Programming_ . John Wiley & Sons, Inc.\nNew York, NY, USA.\n\n\nRiedmiller, M. 2005. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning\nMethod. In Gama, J.; Camacho, R.; Brazdil, P.; Jorge, A.; and\nTorgo, L., eds., _Proceedings of the 16th European Conference_\n_on Machine Learning (ECML’05)_, 317–328. Springer.\n\n\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ . Puerto Rico.\n\n\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering\nthe game of Go with deep neural networks and tree search.\n_Nature_ 529(7587): 484–489.\n\n\nSingh, S. P.; and Sutton, R. S. 1996. Reinforcement Learning\nwith replacing eligibility traces. _Machine Learning_ 22: 123–\n158.\n\n\nSutton, R. S. 1984. _Temporal Credit Assignment in Reinforce-_\n_ment Learning_ . Ph.D. thesis, University of Massachusetts,\nDept. of Comp. and Inf. Sci.\n\n\nSutton, R. S. 1988. Learning to predict by the methods of\ntemporal differences. _Machine learning_ 3(1): 9–44.\n\n\nSutton, R. S. 1990. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Proceedings of the seventh international conference_\n_on machine learning_, 216–224.\n\n\nSutton, R. S.; and Barto, A. G. 2018. _Reinforcement Learning:_\n_An Introduction_ . The MIT press, Cambridge MA.\n\n\nSutton, R. S.; Mahmood, A. R.; and White, M. 2016. An\nEmphatic Approach to the Problem of Off-policy TemporalDifference Learning. _Journal of Machine Learning Research_\n17(73): 1–29.\n\n\nSutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y.\n2000. Policy gradient methods for reinforcement learning\nwith function approximation. _Advances in Neural Informa-_\n_tion Processing Systems 13 (NIPS-00)_ 12: 1057–1063.\n\n\nTesauro, G. 1992. Practical Issues in Temporal Difference\nLearning. In Lippman, D. S.; Moody, J. E.; and Touretzky,\nD. S., eds., _Advances in Neural Information Processing Sys-_\n_tems 4_, 259–266. San Mateo, CA: Morgan Kaufmann.\n\n\nTesauro, G. J. 1994. TD-Gammon, a self-teaching backgammon program, achieves master-level play. _Neural computa-_\n_tion_ 6(2): 215–219.\n\n\n10005\n\n\n\nTsitsiklis, J. N. 1994. Asynchronous stochastic approximation and Q-learning. _Machine Learning_ 16: 185–202.\n\nTsitsiklis, J. N.; and Van Roy, B. 1997. An analysis of\ntemporal-difference learning with function approximation.\n_IEEE Transactions on Automatic Control_ 42(5): 674–690.\n\nvan Hasselt, H. P. 2012. Reinforcement Learning in Continuous State and Action Spaces. In Wiering, M. A.; and\nvan Otterlo, M., eds., _Reinforcement Learning: State of the_\n_Art_, volume 12 of _Adaptation, Learning, and Optimization_,\n207–251. Springer.\n\n\nvan Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver,\nD. 2016. Learning values across many orders of magnitude. In _Advances in Neural Information Processing Systems_\n_29: Annual Conference on Neural Information Processing_\n_Systems 2016, December 5-10, 2016, Barcelona, Spain_, 4287–\n4295.\n\n\nvan Hasselt, H. P.; Guez, A.; and Silver, D. 2016. Deep reinforcement learning with double Q-Learning. In _Proceedings_\n_of the Thirtieth AAAI Conference on Artificial Intelligence_,\n2094–2100.\n\n\nvan Hasselt, H. P.; Hessel, M.; and Aslanides, J. 2019. When\nto use parametric models in reinforcement learning? In _Ad-_\n_vances in Neural Information Processing Systems_, volume 32,\n14322–14333.\n\nvan Hasselt, H. P.; Mahmood, A. R.; and Sutton, R. S. 2014.\nOff-policy TD( _λ_ ) with a true online equivalence. In _Pro-_\n_ceedings of the 30th Conference on Uncertainty in Artificial_\n_Intelligence_, 330–339.\n\nvan Hasselt, H. P.; Quan, J.; Hessel, M.; Xu, Z.; Borsa, D.;\nand Barreto, A. 2019. General non-linear Bellman equations.\n_arXiv preprint arXiv:1907.03687_ .\n\n\nvan Hasselt, H. P.; and Sutton, R. S. 2015. Learning to predict\nindependent of span. _CoRR_ abs/1508.04582.\n\nvan Seijen, H.; and Sutton, R. S. 2013. Planning by Prioritized Sweeping with Small Backups. In _International_\n_Conference on Machine Learning_, 361–369.\n\nvan Seijen, H.; and Sutton, R. S. 2014. True online TD( _λ_ ).\nIn _International Conference on Machine Learning_, 692–700.\n\n\nWang, Z.; de Freitas, N.; Schaul, T.; Hessel, M.; van Hasselt,\nH. P.; and Lanctot, M. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In _International_\n_Conference on Machine Learning_ . New York, NY, USA.\n\nWerbos, P. J. 1990. A menu of designs for reinforcement\nlearning over time. _Neural networks for control_ 67–95.\n\nWhitehead, S. D.; and Ballard, D. H. 1991. Learning to\nperceive and act by trial and error. _Machine Learning_ 7(1):\n45–83.\n\nWilliams, R. J. 1992. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. _Machine_\n_Learning_ 8: 229–256.\n\n\nZhang, S.; Boehmer, W.; and Whiteson, S. 2019. Generalized\noff-policy actor-critic. In _Advances in Neural Information_\n_Processing Systems_, 2001–2011.\n\n\n",
          "ranking": {
            "relevance_score": 0.7493067885948483,
            "citation_score": 0.6302697050551669,
            "recency_score": 0.3906854405837399,
            "final_score": 0.6896372370858013
          },
          "is_open_access": false,
          "user_provided": false,
          "pdf_path": null
        },
        "chunk_text": "Future work should explore integration with development workflows and real-time analysis capabilities.",
        "chunk_index": 1
      },
      "summary": "Future work should explore integration with development workflows and real-time analysis capabilities.",
      "vector_score": 0.672,
      "llm_score": 0.84,
      "combined_score": 0.84,
      "source_query": "mock_query_conclusion"
    }
  ]
}