[
  {
    "id": "http://arxiv.org/abs/2407.08803v2",
    "title": "PID Accelerated Temporal Difference Algorithms",
    "published": "2024-07-11T18:23:46Z",
    "updated": "2024-09-03T16:59:07Z",
    "authors": [
      "Mark Bedaywi",
      "Amin Rakhsha",
      "Amir-massoud Farahmand"
    ],
    "summary": "Long-horizon tasks, which have a large discount factor, pose a challenge for\nmost conventional reinforcement learning (RL) algorithms. Algorithms such as\nValue Iteration and Temporal Difference (TD) learning have a slow convergence\nrate and become inefficient in these tasks. When the transition distributions\nare given, PID VI was recently introduced to accelerate the convergence of\nValue Iteration using ideas from control theory. Inspired by this, we introduce\nPID TD Learning and PID Q-Learning algorithms for the RL setting, in which only\nsamples from the environment are available. We give a theoretical analysis of\nthe convergence of PID TD Learning and its acceleration compared to the\nconventional TD Learning. We also introduce a method for adapting PID gains in\nthe presence of noise and empirically verify its effectiveness.",
    "pdf_url": "http://arxiv.org/pdf/2407.08803v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Bedaywi2024PIDAT,\n author = {Mark Bedaywi and Amin Rakhsha and Amir-massoud Farahmand},\n booktitle = {RLJ},\n journal = {RLJ},\n pages = {2071-2095},\n title = {PID Accelerated Temporal Difference Algorithms},\n volume = {5},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2306.00867v1",
    "title": "IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive\n  Control",
    "published": "2023-06-01T16:24:40Z",
    "updated": "2023-06-01T16:24:40Z",
    "authors": [
      "Rohan Chitnis",
      "Yingchen Xu",
      "Bobak Hashemi",
      "Lucas Lehnert",
      "Urun Dogan",
      "Zheqing Zhu",
      "Olivier Delalleau"
    ],
    "summary": "Model-based reinforcement learning (RL) has shown great promise due to its\nsample efficiency, but still struggles with long-horizon sparse-reward tasks,\nespecially in offline settings where the agent learns from a fixed dataset. We\nhypothesize that model-based RL agents struggle in these environments due to a\nlack of long-term planning capabilities, and that planning in a temporally\nabstract model of the environment can alleviate this issue. In this paper, we\nmake two key contributions: 1) we introduce an offline model-based RL\nalgorithm, IQL-TD-MPC, that extends the state-of-the-art Temporal Difference\nLearning for Model Predictive Control (TD-MPC) with Implicit Q-Learning (IQL);\n2) we propose to use IQL-TD-MPC as a Manager in a hierarchical setting with any\noff-the-shelf offline RL algorithm as a Worker. More specifically, we pre-train\na temporally abstract IQL-TD-MPC Manager to predict \"intent embeddings\", which\nroughly correspond to subgoals, via planning. We empirically show that\naugmenting state representations with intent embeddings generated by an\nIQL-TD-MPC manager significantly improves off-the-shelf offline RL agents'\nperformance on some of the most challenging D4RL benchmark tasks. For instance,\nthe offline RL algorithms AWAC, TD3-BC, DT, and CQL all get zero or near-zero\nnormalized evaluation scores on the medium and large antmaze tasks, while our\nmodification gives an average score over 40.",
    "pdf_url": "http://arxiv.org/pdf/2306.00867v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Short version published at ICRA 2024\n  (https://tinyurl.com/icra24-iqltdmpc)",
    "citation_count": 9,
    "bibtex": "@Article{Chitnis2023IQLTDMPCIQ,\n author = {Rohan Chitnis and Yingchen Xu and B. Hashemi and Lucas Lehnert and Ürün Dogan and Zheqing Zhu and Olivier Delalleau},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2024 IEEE International Conference on Robotics and Automation (ICRA)},\n pages = {9154-9160},\n title = {IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1904.10945v3",
    "title": "Target-Based Temporal Difference Learning",
    "published": "2019-04-24T17:41:58Z",
    "updated": "2019-09-20T20:23:44Z",
    "authors": [
      "Donghwan Lee",
      "Niao He"
    ],
    "summary": "The use of target networks has been a popular and key component of recent\ndeep Q-learning algorithms for reinforcement learning, yet little is known from\nthe theory side. In this work, we introduce a new family of target-based\ntemporal difference (TD) learning algorithms and provide theoretical analysis\non their convergences. In contrast to the standard TD-learning, target-based TD\nalgorithms maintain two separate learning parameters-the target variable and\nonline variable. Particularly, we introduce three members in the family, called\nthe averaging TD, double TD, and periodic TD, where the target variable is\nupdated through an averaging, symmetric, or periodic fashion, mirroring those\ntechniques used in deep Q-learning practice.\n  We establish asymptotic convergence analyses for both averaging TD and double\nTD and a finite sample analysis for periodic TD. In addition, we also provide\nsome simulation results showing potentially superior convergence of these\ntarget-based TD algorithms compared to the standard TD-learning. While this\nwork focuses on linear function approximation and policy evaluation setting, we\nconsider this as a meaningful step towards the theoretical understanding of\ndeep Q-learning variants with target networks.",
    "pdf_url": "http://arxiv.org/pdf/1904.10945v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.SY",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 33,
    "bibtex": "@Article{Lee2019TargetBasedTD,\n author = {Donghwan Lee and Niao He},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Target-Based Temporal Difference Learning},\n volume = {abs/1904.10945},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1905.07727v1",
    "title": "Reinforcement Learning for Learning of Dynamical Systems in Uncertain\n  Environment: a Tutorial",
    "published": "2019-05-19T11:29:01Z",
    "updated": "2019-05-19T11:29:01Z",
    "authors": [
      "Mehran Attar",
      "Mohammadreza Dabirian"
    ],
    "summary": "In this paper, a review of model-free reinforcement learning for learning of\ndynamical systems in uncertain environments has discussed. For this purpose,\nthe Markov Decision Process (MDP) will be reviewed. Furthermore, some learning\nalgorithms such as Temporal Difference (TD) learning, Q-Learning, and\nApproximate Q-learning as model-free algorithms which constitute the main part\nof this article have been investigated, and benefits and drawbacks of each\nalgorithm will be discussed. The discussed concepts in each section are\nexplaining with details and examples.",
    "pdf_url": "http://arxiv.org/pdf/1905.07727v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Attar2019ReinforcementLF,\n author = {Mehran Attar and Mohammadreza Dabirian},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement Learning for Learning of Dynamical Systems in Uncertain Environment: a Tutorial},\n volume = {abs/1905.07727},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1711.01569v1",
    "title": "Double Q($σ$) and Q($σ, λ$): Unifying Reinforcement\n  Learning Control Algorithms",
    "published": "2017-11-05T12:05:31Z",
    "updated": "2017-11-05T12:05:31Z",
    "authors": [
      "Markus Dumke"
    ],
    "summary": "Temporal-difference (TD) learning is an important field in reinforcement\nlearning. Sarsa and Q-Learning are among the most used TD algorithms. The\nQ($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper\nextends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma,\n\\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the\nextension of Q($\\sigma$) to double learning. Experiments suggest that the new\nQ($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods\nSarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$).",
    "pdf_url": "http://arxiv.org/pdf/1711.01569v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Dumke2017DoubleQA,\n author = {M. Dumke},\n journal = {arXiv: Artificial Intelligence},\n title = {Double Q($\\sigma$) and Q($\\sigma, \\lambda$): Unifying Reinforcement Learning Control Algorithms},\n year = {2017}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2404.00686v2",
    "title": "Utilizing Maximum Mean Discrepancy Barycenter for Propagating the\n  Uncertainty of Value Functions in Reinforcement Learning",
    "published": "2024-03-31T13:41:56Z",
    "updated": "2024-04-03T14:32:17Z",
    "authors": [
      "Srinjoy Roy",
      "Swagatam Das"
    ],
    "summary": "Accounting for the uncertainty of value functions boosts exploration in\nReinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy\nQ-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty\npropagation during Temporal Difference (TD) updates. MMD-QL uses the MMD\nbarycenter for this purpose, as MMD provides a tighter estimate of closeness\nbetween probability measures than the Wasserstein distance. Firstly, we\nestablish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under\nthe average loss metric. Concerning the accumulated rewards, experiments on\ntabular environments show that MMD-QL outperforms WQL and other algorithms.\nSecondly, we incorporate deep networks into MMD-QL to create MMD Q-Network\n(MMD-QN). Making reasonable assumptions, we analyze the convergence rates of\nMMD-QN using function approximation. Empirical results on challenging Atari\ngames demonstrate that MMD-QN performs well compared to benchmark deep RL\nalgorithms, highlighting its effectiveness in handling large state-action\nspaces.",
    "pdf_url": "http://arxiv.org/pdf/2404.00686v2",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "We found some flaws in our analysis and we are in the process of\n  rectifying those",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Roy2024UtilizingMM,\n author = {Srinjoy Roy and Swagatam Das},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning},\n volume = {abs/2404.00686},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2506.00458v1",
    "title": "Reinforcement Learning for Hanabi",
    "published": "2025-05-31T08:24:16Z",
    "updated": "2025-05-31T08:24:16Z",
    "authors": [
      "Nina Cohen",
      "Kordel K. France"
    ],
    "summary": "Hanabi has become a popular game for research when it comes to reinforcement\nlearning (RL) as it is one of the few cooperative card games where you have\nincomplete knowledge of the entire environment, thus presenting a challenge for\na RL agent. We explored different tabular and deep reinforcement learning\nalgorithms to see which had the best performance both against an agent of the\nsame type and also against other types of agents. We establish that certain\nagents played their highest scoring games against specific agents while others\nexhibited higher scores on average by adapting to the opposing agent's\nbehavior. We attempted to quantify the conditions under which each algorithm\nprovides the best advantage and identified the most interesting interactions\nbetween agents of different types. In the end, we found that temporal\ndifference (TD) algorithms had better overall performance and balancing of play\ntypes compared to tabular agents. Specifically, tabular Expected SARSA and deep\nQ-Learning agents showed the best performance.",
    "pdf_url": "http://arxiv.org/pdf/2506.00458v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Cohen2025ReinforcementLF,\n author = {Nina Cohen and Kordel K. France},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement Learning for Hanabi},\n volume = {abs/2506.00458},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1806.02450v2",
    "title": "A Finite Time Analysis of Temporal Difference Learning With Linear\n  Function Approximation",
    "published": "2018-06-06T22:57:08Z",
    "updated": "2018-11-06T07:34:09Z",
    "authors": [
      "Jalaj Bhandari",
      "Daniel Russo",
      "Raghav Singal"
    ],
    "summary": "Temporal difference learning (TD) is a simple iterative algorithm used to\nestimate the value function corresponding to a given policy in a Markov\ndecision process. Although TD is one of the most widely used algorithms in\nreinforcement learning, its theoretical analysis has proved challenging and few\nguarantees on its statistical efficiency are available. In this work, we\nprovide a simple and explicit finite time analysis of temporal difference\nlearning with linear function approximation. Except for a few key insights, our\nanalysis mirrors standard techniques for analyzing stochastic gradient descent\nalgorithms, and therefore inherits the simplicity and elegance of that\nliterature. Final sections of the paper show how all of our main results extend\nto the study of TD learning with eligibility traces, known as TD($\\lambda$),\nand to Q-learning applied in high-dimensional optimal stopping problems.",
    "pdf_url": "http://arxiv.org/pdf/1806.02450v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 363,
    "bibtex": "@Article{Bhandari2018AFT,\n author = {Jalaj Bhandari and Daniel Russo and Raghav Singal},\n booktitle = {Annual Conference Computational Learning Theory},\n journal = {Oper. Res.},\n pages = {950-973},\n title = {A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation},\n volume = {69},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2006.00997v1",
    "title": "Temporal-Differential Learning in Continuous Environments",
    "published": "2020-06-01T15:01:03Z",
    "updated": "2020-06-01T15:01:03Z",
    "authors": [
      "Tao Bian",
      "Zhong-Ping Jiang"
    ],
    "summary": "In this paper, a new reinforcement learning (RL) method known as the method\nof temporal differential is introduced. Compared to the traditional\ntemporal-difference learning method, it plays a crucial role in developing\nnovel RL techniques for continuous environments. In particular, the\ncontinuous-time least squares policy evaluation (CT-LSPE) and the\ncontinuous-time temporal-differential (CT-TD) learning methods are developed.\nBoth theoretical and empirical evidences are provided to demonstrate the\neffectiveness of the proposed temporal-differential learning methodology.",
    "pdf_url": "http://arxiv.org/pdf/2006.00997v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Bian2020TemporalDifferentialLI,\n author = {T. Bian and Zhong-Ping Jiang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Temporal-Differential Learning in Continuous Environments},\n volume = {abs/2006.00997},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2007.00611v4",
    "title": "Gradient Temporal-Difference Learning with Regularized Corrections",
    "published": "2020-07-01T16:56:56Z",
    "updated": "2020-09-17T21:17:00Z",
    "authors": [
      "Sina Ghiassian",
      "Andrew Patterson",
      "Shivam Garg",
      "Dhawal Gupta",
      "Adam White",
      "Martha White"
    ],
    "summary": "It is still common to use Q-learning and temporal difference (TD)\nlearning-even though they have divergence issues and sound Gradient TD\nalternatives exist-because divergence seems rare and they typically perform\nwell. However, recent work with large neural network learning systems reveals\nthat instability is more common than previously thought. Practitioners face a\ndifficult dilemma: choose an easy to use and performant TD method, or a more\ncomplex algorithm that is more sound but harder to tune and all but unexplored\nwith non-linear function approximation or control. In this paper, we introduce\na new method called TD with Regularized Corrections (TDRC), that attempts to\nbalance ease of use, soundness, and performance. It behaves as well as TD, when\nTD performs well, but is sound in cases where TD diverges. We empirically\ninvestigate TDRC across a range of problems, for both prediction and control,\nand for both linear and non-linear function approximation, and show,\npotentially for the first time, that gradient TD methods could be a better\nalternative to TD and Q-learning.",
    "pdf_url": "http://arxiv.org/pdf/2007.00611v4",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Appeared in Proceedings of the 37th International Conference on\n  Machine Learning (ICML2020)",
    "journal_ref": null,
    "citation_count": 43,
    "bibtex": "@Article{Ghiassian2020GradientTL,\n author = {Sina Ghiassian and Andrew Patterson and Shivam Garg and Dhawal Gupta and Adam White and Martha White},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Gradient Temporal-Difference Learning with Regularized Corrections},\n volume = {abs/2007.00611},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2508.05960v1",
    "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement\n  Learning",
    "published": "2025-08-08T02:48:26Z",
    "updated": "2025-08-08T02:48:26Z",
    "authors": [
      "Haohui Chen",
      "Zhiyong Chen"
    ],
    "summary": "Offline reinforcement learning (RL) seeks to learn optimal policies from\nstatic datasets without further environment interaction. A key challenge is the\ndistribution shift between the learned and behavior policies, leading to\nout-of-distribution (OOD) actions and overestimation. To prevent gross\noverestimation, the value function must remain conservative; however, excessive\nconservatism may hinder performance improvement. To address this, we propose\nthe mildly conservative regularized evaluation (MCRE) framework, which balances\nconservatism and performance by combining temporal difference (TD) error with a\nbehavior cloning term in the Bellman backup. Building on this, we develop the\nmildly conservative regularized Q-learning (MCRQ) algorithm, which integrates\nMCRE into an off-policy actor-critic framework. Experiments show that MCRQ\noutperforms strong baselines and state-of-the-art offline RL algorithms on\nbenchmark datasets.",
    "pdf_url": "http://arxiv.org/pdf/2508.05960v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Chen2025MildlyCR,\n author = {Haohui Chen and Zhiyong Chen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning},\n volume = {abs/2508.05960},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2301.00944v3",
    "title": "Temporal Difference Learning with Compressed Updates: Error-Feedback\n  meets Reinforcement Learning",
    "published": "2023-01-03T04:09:38Z",
    "updated": "2024-06-04T15:40:42Z",
    "authors": [
      "Aritra Mitra",
      "George J. Pappas",
      "Hamed Hassani"
    ],
    "summary": "In large-scale distributed machine learning, recent works have studied the\neffects of compressing gradients in stochastic optimization to alleviate the\ncommunication bottleneck. These works have collectively revealed that\nstochastic gradient descent (SGD) is robust to structured perturbations such as\nquantization, sparsification, and delays. Perhaps surprisingly, despite the\nsurge of interest in multi-agent reinforcement learning, almost nothing is\nknown about the analogous question: Are common reinforcement learning (RL)\nalgorithms also robust to similar perturbations? We investigate this question\nby studying a variant of the classical temporal difference (TD) learning\nalgorithm with a perturbed update direction, where a general compression\noperator is used to model the perturbation. Our work makes three important\ntechnical contributions. First, we prove that compressed TD algorithms, coupled\nwith an error-feedback mechanism used widely in optimization, exhibit the same\nnon-asymptotic theoretical guarantees as their SGD counterparts. Second, we\nshow that our analysis framework extends seamlessly to nonlinear stochastic\napproximation schemes that subsume Q-learning. Third, we prove that for\nmulti-agent TD learning, one can achieve linear convergence speedups with\nrespect to the number of agents while communicating just $\\tilde{O}(1)$ bits\nper iteration. Notably, these are the first finite-time results in RL that\naccount for general compression operators and error-feedback in tandem with\nlinear function approximation and Markovian sampling. Our proofs hinge on the\nconstruction of novel Lyapunov functions that capture the dynamics of a memory\nvariable introduced by error-feedback.",
    "pdf_url": "http://arxiv.org/pdf/2301.00944v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to Transactions on Machine Learning Research",
    "journal_ref": null,
    "citation_count": 13,
    "bibtex": "@Article{Mitra2023TemporalDL,\n author = {A. Mitra and George Pappas and Hamed Hassani},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning},\n volume = {2024},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1905.10027v2",
    "title": "Neural Temporal-Difference and Q-Learning Provably Converge to Global\n  Optima",
    "published": "2019-05-24T04:36:42Z",
    "updated": "2020-04-15T07:34:21Z",
    "authors": [
      "Qi Cai",
      "Zhuoran Yang",
      "Jason D. Lee",
      "Zhaoran Wang"
    ],
    "summary": "Temporal-difference learning (TD), coupled with neural networks, is among the\nmost fundamental building blocks of deep reinforcement learning. However, due\nto the nonlinearity in value function approximation, such a coupling leads to\nnonconvexity and even divergence in optimization. As a result, the global\nconvergence of neural TD remains unclear. In this paper, we prove for the first\ntime that neural TD converges at a sublinear rate to the global optimum of the\nmean-squared projected Bellman error for policy evaluation. In particular, we\nshow how such global convergence is enabled by the overparametrization of\nneural networks, which also plays a vital role in the empirical success of\nneural TD. Beyond policy evaluation, we establish the global convergence of\nneural (soft) Q-learning, which is further connected to that of policy gradient\nalgorithms.",
    "pdf_url": "http://arxiv.org/pdf/1905.10027v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 34,
    "bibtex": "@Article{Cai2019NeuralTD,\n author = {Qi Cai and Zhuoran Yang and Jason D. Lee and Zhaoran Wang},\n booktitle = {Mathematics of Operations Research},\n journal = {Math. Oper. Res.},\n pages = {619-651},\n title = {Neural Temporal Difference and Q Learning Provably Converge to Global Optima},\n volume = {49},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1803.05402v5",
    "title": "Imitation Learning with Concurrent Actions in 3D Games",
    "published": "2018-03-14T16:59:17Z",
    "updated": "2018-09-06T12:16:17Z",
    "authors": [
      "Jack Harmer",
      "Linus Gisslén",
      "Jorge del Val",
      "Henrik Holst",
      "Joakim Bergdahl",
      "Tom Olsson",
      "Kristoffer Sjöö",
      "Magnus Nordin"
    ],
    "summary": "In this work we describe a novel deep reinforcement learning architecture\nthat allows multiple actions to be selected at every time-step in an efficient\nmanner. Multi-action policies allow complex behaviours to be learnt that would\notherwise be hard to achieve when using single action selection techniques. We\nuse both imitation learning and temporal difference (TD) reinforcement learning\n(RL) to provide a 4x improvement in training time and 2.5x improvement in\nperformance over single action selection TD RL. We demonstrate the capabilities\nof this network using a complex in-house 3D game. Mimicking the behavior of the\nexpert teacher significantly improves world state exploration and allows the\nagents vision system to be trained more rapidly than TD RL alone. This initial\ntraining technique kick-starts TD learning and the agent quickly learns to\nsurpass the capabilities of the expert.",
    "pdf_url": "http://arxiv.org/pdf/1803.05402v5",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 50,
    "bibtex": "@Article{Harmer2018ImitationLW,\n author = {Jack Harmer and Linus Gisslén and Henrik Holst and Joakim Bergdahl and Tom Olsson and K. Sjöö and Magnus Nordin},\n booktitle = {IEEE Conference on Computational Intelligence and Games},\n journal = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},\n pages = {1-8},\n title = {Imitation Learning with Concurrent Actions in 3D Games},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2509.15110v2",
    "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and\n  Inference",
    "published": "2025-09-18T16:14:34Z",
    "updated": "2025-09-29T10:46:05Z",
    "authors": [
      "Dan Zhang",
      "Min Cai",
      "Jonathan Light",
      "Ziniu Hu",
      "Yisong Yue",
      "Jie Tang"
    ],
    "summary": "Reward models are central to both reinforcement learning (RL) with language\nmodels and inference-time verification. However, existing reward models often\nlack temporal consistency, leading to ineffective policy updates and unstable\nRL training. We introduce TDRM, a method for learning smoother and more\nreliable reward models by minimizing temporal differences (TD) for\ntraining-time reinforcement learning and inference-time verification.\nExperiments show that TD-trained process reward models (PRMs) improve\nperformance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)\nsettings. When combined with Reinforcement Learning with Verifiable Rewards\n(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable\nperformance with just 2.5k data to what baseline methods require 50.1k data to\nattain -- and yield higher-quality language model policies in 8 model variants\n(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,\nQwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release\nall code at https://github.com/THUDM/TDRM.",
    "pdf_url": "http://arxiv.org/pdf/2509.15110v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 figures, 7 tables",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Zhang2025TDRMSR,\n author = {Dan Zhang and Min Cai and Jonathan Light and Ziniu Hu and Yisong Yue and Jie Tang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference},\n volume = {abs/2509.15110},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/cs/9501103v1",
    "title": "Truncating Temporal Differences: On the Efficient Implementation of\n  TD(lambda) for Reinforcement Learning",
    "published": "1995-01-01T00:00:00Z",
    "updated": "1995-01-01T00:00:00Z",
    "authors": [
      "P. Cichosz"
    ],
    "summary": "Temporal difference (TD) methods constitute a class of methods for learning\npredictions in multi-step prediction problems, parameterized by a recency\nfactor lambda. Currently the most important application of these methods is to\ntemporal credit assignment in reinforcement learning. Well known reinforcement\nlearning algorithms, such as AHC or Q-learning, may be viewed as instances of\nTD learning. This paper examines the issues of the efficient and general\nimplementation of TD(lambda) for arbitrary lambda, for use with reinforcement\nlearning algorithms optimizing the discounted sum of rewards. The traditional\napproach, based on eligibility traces, is argued to suffer from both\ninefficiency and lack of generality. The TTD (Truncated Temporal Differences)\nprocedure is proposed as an alternative, that indeed only approximates\nTD(lambda), but requires very little computation per action and can be used\nwith arbitrary function representation methods. The idea from which it is\nderived is fairly simple and not new, but probably unexplored so far.\nEncouraging experimental results are presented, suggesting that using lambda\n&gt 0 with the TTD procedure allows one to obtain a significant learning\nspeedup at essentially the same cost as usual TD(0) learning.",
    "pdf_url": "http://arxiv.org/pdf/cs/9501103v1",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "See http://www.jair.org/ for any accompanying files",
    "journal_ref": "Journal of Artificial Intelligence Research, Vol 2, (1995),\n  287-318",
    "citation_count": null,
    "bibtex": null
  },
  {
    "id": "http://arxiv.org/abs/1106.0707v1",
    "title": "Efficient Reinforcement Learning Using Recursive Least-Squares Methods",
    "published": "2011-06-03T16:44:06Z",
    "updated": "2011-06-03T16:44:06Z",
    "authors": [
      "H. He",
      "D. Hu",
      "X. Xu"
    ],
    "summary": "The recursive least-squares (RLS) algorithm is one of the most well-known\nalgorithms used in adaptive filtering, system identification and adaptive\ncontrol. Its popularity is mainly due to its fast convergence speed, which is\nconsidered to be optimal in practice. In this paper, RLS methods are used to\nsolve reinforcement learning problems, where two new reinforcement learning\nalgorithms using linear value function approximators are proposed and analyzed.\nThe two algorithms are called RLS-TD(lambda) and Fast-AHC (Fast Adaptive\nHeuristic Critic), respectively. RLS-TD(lambda) can be viewed as the extension\nof RLS-TD(0) from lambda=0 to general lambda within interval [0,1], so it is a\nmulti-step temporal-difference (TD) learning algorithm using RLS methods. The\nconvergence with probability one and the limit of convergence of RLS-TD(lambda)\nare proved for ergodic Markov chains. Compared to the existing LS-TD(lambda)\nalgorithm, RLS-TD(lambda) has advantages in computation and is more suitable\nfor online learning. The effectiveness of RLS-TD(lambda) is analyzed and\nverified by learning prediction experiments of Markov chains with a wide range\nof parameter settings. The Fast-AHC algorithm is derived by applying the\nproposed RLS-TD(lambda) algorithm in the critic network of the adaptive\nheuristic critic method. Unlike conventional AHC algorithm, Fast-AHC makes use\nof RLS methods to improve the learning-prediction efficiency in the critic.\nLearning control experiments of the cart-pole balancing and the acrobot\nswing-up problems are conducted to compare the data efficiency of Fast-AHC with\nconventional AHC. From the experimental results, it is shown that the data\nefficiency of learning control can also be improved by using RLS methods in the\nlearning-prediction process of the critic. The performance of Fast-AHC is also\ncompared with that of the AHC method using LS-TD(lambda). Furthermore, it is\ndemonstrated in the experiments that different initial values of the variance\nmatrix in RLS-TD(lambda) are required to get better performance not only in\nlearning prediction but also in learning control. The experimental results are\nanalyzed based on the existing theoretical work on the transient phase of\nforgetting factor RLS methods.",
    "pdf_url": "http://arxiv.org/pdf/1106.0707v1",
    "doi": "10.1613/jair.946",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  259-292, 2002",
    "citation_count": 150,
    "bibtex": "@Article{He2011EfficientRL,\n author = {He He and D. Hu and X. Xu},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {259-292},\n title = {Efficient Reinforcement Learning Using Recursive Least-Squares Methods},\n volume = {16},\n year = {2011}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2307.08171v1",
    "title": "Credit Assignment: Challenges and Opportunities in Developing Human-like\n  AI Agents",
    "published": "2023-07-16T23:11:26Z",
    "updated": "2023-07-16T23:11:26Z",
    "authors": [
      "Thuy Ngoc Nguyen",
      "Chase McDonald",
      "Cleotilde Gonzalez"
    ],
    "summary": "Temporal credit assignment is crucial for learning and skill development in\nnatural and artificial intelligence. While computational methods like the TD\napproach in reinforcement learning have been proposed, it's unclear if they\naccurately represent how humans handle feedback delays. Cognitive models intend\nto represent the mental steps by which humans solve problems and perform a\nnumber of tasks, but limited research in cognitive science has addressed the\ncredit assignment problem in humans and cognitive models. Our research uses a\ncognitive model based on a theory of decisions from experience, Instance-Based\nLearning Theory (IBLT), to test different credit assignment mechanisms in a\ngoal-seeking navigation task with varying levels of decision complexity.\nInstance-Based Learning (IBL) models simulate the process of making sequential\nchoices with different credit assignment mechanisms, including a new IBL-TD\nmodel that combines the IBL decision mechanism with the TD approach. We found\nthat (1) An IBL model that gives equal credit assignment to all decisions is\nable to match human performance better than other models, including IBL-TD and\nQ-learning; (2) IBL-TD and Q-learning models underperform compared to humans\ninitially, but eventually, they outperform humans; (3) humans are influenced by\ndecision complexity, while models are not. Our study provides insights into the\nchallenges of capturing human behavior and the potential opportunities to use\nthese models in future AI systems to support human activities.",
    "pdf_url": "http://arxiv.org/pdf/2307.08171v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "11 figures; 3 tables",
    "journal_ref": null,
    "citation_count": 10,
    "bibtex": "@Article{Nguyen2023CreditAC,\n author = {T. Nguyen and Chase McDonald and Cleotilde González},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Credit Assignment: Challenges and Opportunities in Developing Human-like AI Agents},\n volume = {abs/2307.08171},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2205.12770v2",
    "title": "An Experimental Comparison Between Temporal Difference and Residual\n  Gradient with Neural Network Approximation",
    "published": "2022-05-25T13:37:52Z",
    "updated": "2022-11-14T08:29:53Z",
    "authors": [
      "Shuyu Yin",
      "Tao Luo",
      "Peilin Liu",
      "Zhi-Qin John Xu"
    ],
    "summary": "Gradient descent or its variants are popular in training neural networks.\nHowever, in deep Q-learning with neural network approximation, a type of\nreinforcement learning, gradient descent (also known as Residual Gradient (RG))\nis barely used to solve Bellman residual minimization problem. On the contrary,\nTemporal Difference (TD), an incomplete gradient descent method prevails. In\nthis work, we perform extensive experiments to show that TD outperforms RG,\nthat is, when the training leads to a small Bellman residual error, the\nsolution found by TD has a better policy and is more robust against the\nperturbation of neural network parameters. We further use experiments to reveal\na key difference between reinforcement learning and supervised learning, that\nis, a small Bellman residual error can correspond to a bad policy in\nreinforcement learning while the test loss function in supervised learning is a\nstandard index to indicate the performance. We also empirically examine that\nthe missing term in TD is a key reason why RG performs badly. Our work shows\nthat the performance of a deep Q-learning solution is closely related to the\ntraining dynamics and how an incomplete gradient descent method can find a good\npolicy is interesting for future study.",
    "pdf_url": "http://arxiv.org/pdf/2205.12770v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Yin2022AnEC,\n author = {Shuyu Yin and Tao Luo and Peilin Liu and Z. Xu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Experimental Comparison Between Temporal Difference and Residual Gradient with Neural Network Approximation},\n volume = {abs/2205.12770},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2302.09875v3",
    "title": "Backstepping Temporal Difference Learning",
    "published": "2023-02-20T10:06:49Z",
    "updated": "2025-04-18T04:46:08Z",
    "authors": [
      "Han-Dong Lim",
      "Donghwan Lee"
    ],
    "summary": "Off-policy learning ability is an important feature of reinforcement learning\n(RL) for practical applications. However, even one of the most elementary RL\nalgorithms, temporal-difference (TD) learning, is known to suffer form\ndivergence issue when the off-policy scheme is used together with linear\nfunction approximation. To overcome the divergent behavior, several off-policy\nTD-learning algorithms, including gradient-TD learning (GTD), and TD-learning\nwith correction (TDC), have been developed until now. In this work, we provide\na unified view of such algorithms from a purely control-theoretic perspective,\nand propose a new convergent algorithm. Our method relies on the backstepping\ntechnique, which is widely used in nonlinear control theory. Finally,\nconvergence of the proposed algorithm is experimentally verified in\nenvironments where the standard TD-learning is known to be unstable.",
    "pdf_url": "http://arxiv.org/pdf/2302.09875v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR2023",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Lim2023BacksteppingTD,\n author = {Han-Dong Lim and Dong-hwan Lee},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Backstepping Temporal Difference Learning},\n volume = {abs/2302.09875},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2411.03604v1",
    "title": "Temporal-Difference Learning Using Distributed Error Signals",
    "published": "2024-11-06T01:49:13Z",
    "updated": "2024-11-06T01:49:13Z",
    "authors": [
      "Jonas Guan",
      "Shon Eduard Verch",
      "Claas Voelcker",
      "Ethan C. Jackson",
      "Nicolas Papernot",
      "William A. Cunningham"
    ],
    "summary": "A computational problem in biological reward-based learning is how credit\nassignment is performed in the nucleus accumbens (NAc). Much research suggests\nthat NAc dopamine encodes temporal-difference (TD) errors for learning value\npredictions. However, dopamine is synchronously distributed in regionally\nhomogeneous concentrations, which does not support explicit credit assignment\n(like used by backpropagation). It is unclear whether distributed errors alone\nare sufficient for synapses to make coordinated updates to learn complex,\nnonlinear reward-based learning tasks. We design a new deep Q-learning\nalgorithm, Artificial Dopamine, to computationally demonstrate that\nsynchronously distributed, per-layer TD errors may be sufficient to learn\nsurprisingly complex RL tasks. We empirically evaluate our algorithm on\nMinAtar, the DeepMind Control Suite, and classic control tasks, and show it\noften achieves comparable performance to deep RL algorithms that use\nbackpropagation.",
    "pdf_url": "http://arxiv.org/pdf/2411.03604v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, to be published at NeurIPS 2024",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Guan2024TemporalDifferenceLU,\n author = {Jonas Guan and S. Verch and C. Voelcker and Ethan C. Jackson and Nicolas Papernot and William A. Cunningham},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Temporal-Difference Learning Using Distributed Error Signals},\n volume = {abs/2411.03604},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2108.05338v2",
    "title": "Truncated Emphatic Temporal Difference Methods for Prediction and\n  Control",
    "published": "2021-08-11T17:26:38Z",
    "updated": "2022-05-10T22:06:07Z",
    "authors": [
      "Shangtong Zhang",
      "Shimon Whiteson"
    ],
    "summary": "Emphatic Temporal Difference (TD) methods are a class of off-policy\nReinforcement Learning (RL) methods involving the use of followon traces.\nDespite the theoretical success of emphatic TD methods in addressing the\nnotorious deadly triad of off-policy RL, there are still two open problems.\nFirst, followon traces typically suffer from large variance, making them hard\nto use in practice. Second, though Yu (2015) confirms the asymptotic\nconvergence of some emphatic TD methods for prediction problems, there is still\nno finite sample analysis for any emphatic TD method for prediction, much less\ncontrol. In this paper, we address those two open problems simultaneously via\nusing truncated followon traces in emphatic TD methods. Unlike the original\nfollowon traces, which depend on all previous history, truncated followon\ntraces depend on only finite history, reducing variance and enabling the finite\nsample analysis of our proposed emphatic TD methods for both prediction and\ncontrol.",
    "pdf_url": "http://arxiv.org/pdf/2108.05338v2",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Journal of Machine Learning Research 2022",
    "journal_ref": null,
    "citation_count": 14,
    "bibtex": "@Article{Zhang2021TruncatedET,\n author = {Shangtong Zhang and S. Whiteson},\n booktitle = {Journal of machine learning research},\n journal = {ArXiv},\n title = {Truncated Emphatic Temporal Difference Methods for Prediction and Control},\n volume = {abs/2108.05338},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2310.08091v2",
    "title": "Discerning Temporal Difference Learning",
    "published": "2023-10-12T07:38:10Z",
    "updated": "2024-02-10T14:27:29Z",
    "authors": [
      "Jianfei Ma"
    ],
    "summary": "Temporal difference learning (TD) is a foundational concept in reinforcement\nlearning (RL), aimed at efficiently assessing a policy's value function.\nTD($\\lambda$), a potent variant, incorporates a memory trace to distribute the\nprediction error into the historical context. However, this approach often\nneglects the significance of historical states and the relative importance of\npropagating the TD error, influenced by challenges such as visitation imbalance\nor outcome noise. To address this, we propose a novel TD algorithm named\ndiscerning TD learning (DTD), which allows flexible emphasis\nfunctions$-$predetermined or adapted during training$-$to allocate efforts\neffectively across states. We establish the convergence properties of our\nmethod within a specific class of emphasis functions and showcase its promising\npotential for adaptation to deep RL contexts. Empirical results underscore that\nemploying a judicious emphasis function not only improves value estimation but\nalso expedites learning across diverse scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2310.08091v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Ma2023DiscerningTD,\n author = {Jianfei Ma},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Discerning Temporal Difference Learning},\n volume = {abs/2310.08091},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2505.01361v2",
    "title": "Stabilizing Temporal Difference Learning via Implicit Stochastic\n  Recursion",
    "published": "2025-05-02T15:57:54Z",
    "updated": "2025-06-22T22:31:04Z",
    "authors": [
      "Hwanwoo Kim",
      "Panos Toulis",
      "Eric Laber"
    ],
    "summary": "Temporal difference (TD) learning is a foundational algorithm in\nreinforcement learning (RL). For nearly forty years, TD learning has served as\na workhorse for applied RL as well as a building block for more complex and\nspecialized algorithms. However, despite its widespread use, TD procedures are\ngenerally sensitive to step size specification. A poor choice of step size can\ndramatically increase variance and slow convergence in both on-policy and\noff-policy evaluation tasks. In practice, researchers use trial and error to\nidentify stable step sizes, but these approaches tend to be ad hoc and\ninefficient. As an alternative, we propose implicit TD algorithms that\nreformulate TD updates into fixed point equations. Such updates are more stable\nand less sensitive to step size without sacrificing computational efficiency.\nMoreover, we derive asymptotic convergence guarantees and finite-time error\nbounds for our proposed implicit TD algorithms, which include implicit TD(0),\nTD($\\lambda$), and TD with gradient correction (TDC). Our results show that\nimplicit TD algorithms are applicable to a much broader range of step sizes,\nand thus provide a robust and versatile framework for policy evaluation and\nvalue approximation in modern RL tasks. We demonstrate these benefits\nempirically through extensive numerical examples spanning both on-policy and\noff-policy tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.01361v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "A substantial amount of content has been added regarding the theory\n  and numerical experiments of the implicit version of temporal difference\n  learning with gradient correction (TDC), which is newly proposed in this\n  manuscript",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Kim2025StabilizingTD,\n author = {Hwanwoo Kim and Panos Toulis and Eric Laber},\n title = {Stabilizing Temporal Difference Learning via Implicit Stochastic Recursion},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2110.14818v1",
    "title": "Temporal-Difference Value Estimation via Uncertainty-Guided Soft Updates",
    "published": "2021-10-28T00:07:19Z",
    "updated": "2021-10-28T00:07:19Z",
    "authors": [
      "Litian Liang",
      "Yaosheng Xu",
      "Stephen McAleer",
      "Dailin Hu",
      "Alexander Ihler",
      "Pieter Abbeel",
      "Roy Fox"
    ],
    "summary": "Temporal-Difference (TD) learning methods, such as Q-Learning, have proven\neffective at learning a policy to perform control tasks. One issue with methods\nlike Q-Learning is that the value update introduces bias when predicting the TD\ntarget of a unfamiliar state. Estimation noise becomes a bias after the max\noperator in the policy improvement step, and carries over to value estimations\nof other states, causing Q-Learning to overestimate the Q value. Algorithms\nlike Soft Q-Learning (SQL) introduce the notion of a soft-greedy policy, which\nreduces the estimation bias via soft updates in early stages of training.\nHowever, the inverse temperature $\\beta$ that controls the softness of an\nupdate is usually set by a hand-designed heuristic, which can be inaccurate at\ncapturing the uncertainty in the target estimate. Under the belief that $\\beta$\nis closely related to the (state dependent) model uncertainty, Entropy\nRegularized Q-Learning (EQL) further introduces a principled scheduling of\n$\\beta$ by maintaining a collection of the model parameters that characterizes\nmodel uncertainty. In this paper, we present Unbiased Soft Q-Learning (UQL),\nwhich extends the work of EQL from two action, finite state spaces to\nmulti-action, infinite state space Markov Decision Processes. We also provide a\nprincipled numerical scheduling of $\\beta$, extended from SQL and using model\nuncertainty, during the optimization process. We show the theoretical\nguarantees and the effectiveness of this update method in experiments on\nseveral discrete control environments.",
    "pdf_url": "http://arxiv.org/pdf/2110.14818v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to Deep Reinforcement Learning Workshop @ NeurIPS 2021",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Liang2021TemporalDifferenceVE,\n author = {Litian Liang and Yaosheng Xu and S. McAleer and Dailin Hu and A. Ihler and P. Abbeel and Roy Fox},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Temporal-Difference Value Estimation via Uncertainty-Guided Soft Updates},\n volume = {abs/2110.14818},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2404.15822v1",
    "title": "Recursive Backwards Q-Learning in Deterministic Environments",
    "published": "2024-04-24T11:54:53Z",
    "updated": "2024-04-24T11:54:53Z",
    "authors": [
      "Jan Diekhoff",
      "Jörn Fischer"
    ],
    "summary": "Reinforcement learning is a popular method of finding optimal solutions to\ncomplex problems. Algorithms like Q-learning excel at learning to solve\nstochastic problems without a model of their environment. However, they take\nlonger to solve deterministic problems than is necessary. Q-learning can be\nimproved to better solve deterministic problems by introducing such a\nmodel-based approach. This paper introduces the recursive backwards Q-learning\n(RBQL) agent, which explores and builds a model of the environment. After\nreaching a terminal state, it recursively propagates its value backwards\nthrough this model. This lets each state be evaluated to its optimal value\nwithout a lengthy learning process. In the example of finding the shortest path\nthrough a maze, this agent greatly outperforms a regular Q-learning agent.",
    "pdf_url": "http://arxiv.org/pdf/2404.15822v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2204.02662v2",
    "title": "Accelerating Backward Aggregation in GCN Training with Execution Path\n  Preparing on GPUs",
    "published": "2022-04-06T08:22:03Z",
    "updated": "2022-10-10T01:18:30Z",
    "authors": [
      "Shaoxian Xu",
      "Zhiyuan Shao",
      "Ci Yang",
      "Xiaofei Liao",
      "Hai Jin"
    ],
    "summary": "The emerging Graph Convolutional Network (GCN) has now been widely used in\nmany domains, and it is challenging to improve the efficiencies of applications\nby accelerating the GCN trainings. For the sparsity nature and exploding scales\nof input real-world graphs, state-of-the-art GCN training systems (e.g.,\nGNNAdvisor) employ graph processing techniques to accelerate the message\nexchanging (i.e. aggregations) among the graph vertices. Nevertheless, these\nsystems treat both the aggregation stages of forward and backward propagation\nphases as all-active graph processing procedures that indiscriminately conduct\ncomputation on all vertices of an input graph.\n  In this paper, we first point out that in a GCN training problem with a given\ntraining set, the aggregation stages of its backward propagation phase (called\nas backward aggregations in this paper) can be converted to partially-active\ngraph processing procedures, which conduct computation on only partial vertices\nof the input graph. By leveraging such a finding, we propose an execution path\npreparing method that collects and coalesces the data used during backward\npropagations of GCN training conducted on GPUs. The experimental results show\nthat compared with GNNAdvisor, our approach improves the performance of the\nbackward aggregation of GCN trainings on typical real-world graphs by\n1.48x~5.65x. Moreover, the execution path preparing can be conducted either\nbefore the training (during preprocessing) or on-the-fly with the training.\nWhen used during preprocessing, our approach improves the overall GCN training\nby 1.05x~1.37x. And when used on-the-fly, our approach improves the overall GCN\ntraining by 1.03x~1.35x.",
    "pdf_url": "http://arxiv.org/pdf/2204.02662v2",
    "doi": "10.1109/TPDS.2022.3205642",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "in IEEE Transactions on Parallel & Distributed Systems, vol. 33,\n  no. 12, pp. 4891-4902, 2022",
    "citation_count": 1,
    "bibtex": "@Article{Xu2022AcceleratingBA,\n author = {Shaoxian Xu and Zhiyuan Shao and Ci-Syuan Yang and Xiaofei Liao and Hai Jin},\n booktitle = {IEEE Transactions on Parallel and Distributed Systems},\n journal = {IEEE Transactions on Parallel and Distributed Systems},\n pages = {4891-4902},\n title = {Accelerating Backward Aggregation in GCN Training With Execution Path Preparing on GPUs},\n volume = {33},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2502.01693v3",
    "title": "Predicting Steady-State Behavior in Complex Networks with Graph Neural\n  Networks",
    "published": "2025-02-02T17:29:10Z",
    "updated": "2025-09-08T03:35:37Z",
    "authors": [
      "Priodyuti Pradhan",
      "Amit Reza"
    ],
    "summary": "In complex systems, information propagation can be defined as diffused or\ndelocalized, weakly localized, and strongly localized. This study investigates\nthe application of graph neural network models to learn the behavior of a\nlinear dynamical system on networks. A graph convolution and attention-based\nneural network framework has been developed to identify the steady-state\nbehavior of the linear dynamical system. We reveal that our trained model\ndistinguishes the different states with high accuracy. Furthermore, we have\nevaluated model performance with real-world data. In addition, to understand\nthe explainability of our model, we provide an analytical derivation for the\nforward and backward propagation of our framework.",
    "pdf_url": "http://arxiv.org/pdf/2502.01693v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "nlin.AO"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 15 figures (including Appendix)",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Pradhan2025PredictingSB,\n author = {Priodyuti Pradhan and Amit Reza},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Predicting Steady-State Behavior in Complex Networks with Graph Neural Networks},\n volume = {abs/2502.01693},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2509.22613v1",
    "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective",
    "published": "2025-09-26T17:39:48Z",
    "updated": "2025-09-26T17:39:48Z",
    "authors": [
      "Siwei Wang",
      "Yifei Shen",
      "Haoran Sun",
      "Shi Feng",
      "Shang-Hua Teng",
      "Li Dong",
      "Yaru Hao",
      "Wei Chen"
    ],
    "summary": "Recent reinforcement learning (RL) methods have substantially enhanced the\nplanning capabilities of Large Language Models (LLMs), yet the theoretical\nbasis for their effectiveness remains elusive. In this work, we investigate\nRL's benefits and limitations through a tractable graph-based abstraction,\nfocusing on policy gradient (PG) and Q-learning methods. Our theoretical\nanalyses reveal that supervised fine-tuning (SFT) may introduce\nco-occurrence-based spurious solutions, whereas RL achieves correct planning\nprimarily through exploration, underscoring exploration's role in enabling\nbetter generalization. However, we also show that PG suffers from diversity\ncollapse, where output diversity decreases during training and persists even\nafter perfect accuracy is attained. By contrast, Q-learning provides two key\nadvantages: off-policy learning and diversity preservation at convergence. We\nfurther demonstrate that careful reward design is necessary to prevent reward\nhacking in Q-learning. Finally, applying our framework to the real-world\nplanning benchmark Blocksworld, we confirm that these behaviors manifest in\npractice.",
    "pdf_url": "http://arxiv.org/pdf/2509.22613v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Wang2025BenefitsAP,\n author = {Siwei Wang and Yifei Shen and Haoran Sun and Shi Feng and Shang-Hua Teng and Li Dong and Y. Hao and Wei Chen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective},\n volume = {abs/2509.22613},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1905.06684v6",
    "title": "Formal derivation of Mesh Neural Networks with their Forward-Only\n  gradient Propagation",
    "published": "2019-05-16T12:22:26Z",
    "updated": "2021-09-30T10:23:43Z",
    "authors": [
      "Federico A. Galatolo",
      "Mario G. C. A. Cimino",
      "Gigliola Vaglini"
    ],
    "summary": "This paper proposes the Mesh Neural Network (MNN), a novel architecture which\nallows neurons to be connected in any topology, to efficiently route\ninformation. In MNNs, information is propagated between neurons throughout a\nstate transition function. State and error gradients are then directly computed\nfrom state updates without backward computation. The MNN architecture and the\nerror propagation schema is formalized and derived in tensor algebra. The\nproposed computational model can fully supply a gradient descent process, and\nis potentially suitable for very large scale sparse NNs, due to its\nexpressivity and training efficiency, with respect to NNs based on\nback-propagation and computational graphs.",
    "pdf_url": "http://arxiv.org/pdf/1905.06684v6",
    "doi": "10.1007/s11063-021-10490-1",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Galatolo, F. A., Cimino, M. G., & Vaglini, G. (2021). Formal\n  Derivation of Mesh Neural Networks with Their Forward-Only Gradient\n  Propagation. Neural Processing Letters, 1-16",
    "citation_count": 3,
    "bibtex": "@Article{Galatolo2019FormalDO,\n author = {F. Galatolo and M. Cimino and G. Vaglini},\n booktitle = {Neural Processing Letters},\n journal = {Neural Processing Letters},\n pages = {1963 - 1978},\n title = {Formal Derivation of Mesh Neural Networks with Their Forward-Only Gradient Propagation},\n volume = {53},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2208.02434v1",
    "title": "Backward Imitation and Forward Reinforcement Learning via Bi-directional\n  Model Rollouts",
    "published": "2022-08-04T04:04:05Z",
    "updated": "2022-08-04T04:04:05Z",
    "authors": [
      "Yuxin Pan",
      "Fangzhen Lin"
    ],
    "summary": "Traditional model-based reinforcement learning (RL) methods generate forward\nrollout traces using the learnt dynamics model to reduce interactions with the\nreal environment. The recent model-based RL method considers the way to learn a\nbackward model that specifies the conditional probability of the previous state\ngiven the previous action and the current state to additionally generate\nbackward rollout trajectories. However, in this type of model-based method, the\nsamples derived from backward rollouts and those from forward rollouts are\nsimply aggregated together to optimize the policy via the model-free RL\nalgorithm, which may decrease both the sample efficiency and the convergence\nrate. This is because such an approach ignores the fact that backward rollout\ntraces are often generated starting from some high-value states and are\ncertainly more instructive for the agent to improve the behavior. In this\npaper, we propose the backward imitation and forward reinforcement learning\n(BIFRL) framework where the agent treats backward rollout traces as expert\ndemonstrations for the imitation of excellent behaviors, and then collects\nforward rollout transitions for policy reinforcement. Consequently, BIFRL\nempowers the agent to both reach to and explore from high-value states in a\nmore efficient manner, and further reduces the real interactions, making it\npotentially more suitable for real-robot learning. Moreover, a\nvalue-regularized generative adversarial network is introduced to augment the\nvaluable states which are infrequently received by the agent. Theoretically, we\nprovide the condition where BIFRL is superior to the baseline methods.\nExperimentally, we demonstrate that BIFRL acquires the better sample efficiency\nand produces the competitive asymptotic performance on various MuJoCo\nlocomotion tasks compared against state-of-the-art model-based methods.",
    "pdf_url": "http://arxiv.org/pdf/2208.02434v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IROS2022",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Pan2022BackwardIA,\n author = {Yuxin Pan and Fangzhen Lin},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {9040-9047},\n title = {Backward Imitation and Forward Reinforcement Learning via Bi-directional Model Rollouts},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2501.12113v2",
    "title": "Dual NUP Representations and Min-Maximization in Factor Graphs",
    "published": "2025-01-21T13:17:16Z",
    "updated": "2025-04-23T09:42:32Z",
    "authors": [
      "Yun-Peng Li",
      "Hans-Andrea Loeliger"
    ],
    "summary": "Normals with unknown parameters (NUP) can be used to convert nontrivial\nmodel-based estimation problems into iterations of linear least-squares or\nGaussian estimation problems. In this paper, we extend this approach by\naugmenting factor graphs with convex-dual variables and pertinent NUP\nrepresentations. In particular, in a state space setting, we propose a new\niterative forward-backward algorithm that is dual to a recently proposed\nbackward-forward algorithm.",
    "pdf_url": "http://arxiv.org/pdf/2501.12113v2",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "stat.ML",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Li2025DualNR,\n author = {Yun-Peng Li and H.-A. Loeliger},\n booktitle = {International Symposium on Information Theory},\n journal = {2025 IEEE International Symposium on Information Theory (ISIT)},\n pages = {1-6},\n title = {Dual Nup Representations and Min-Maximization in Factor Graphs},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2408.14785v1",
    "title": "Unsupervised-to-Online Reinforcement Learning",
    "published": "2024-08-27T05:23:45Z",
    "updated": "2024-08-27T05:23:45Z",
    "authors": [
      "Junsu Kim",
      "Seohong Park",
      "Sergey Levine"
    ],
    "summary": "Offline-to-online reinforcement learning (RL), a framework that trains a\npolicy with offline RL and then further fine-tunes it with online RL, has been\nconsidered a promising recipe for data-driven decision-making. While sensible,\nthis framework has drawbacks: it requires domain-specific offline RL\npre-training for each task, and is often brittle in practice. In this work, we\npropose unsupervised-to-online RL (U2O RL), which replaces domain-specific\nsupervised offline RL with unsupervised offline RL, as a better alternative to\noffline-to-online RL. U2O RL not only enables reusing a single pre-trained\nmodel for multiple downstream tasks, but also learns better representations,\nwhich often result in even better performance and stability than supervised\noffline-to-online RL. To instantiate U2O RL in practice, we propose a general\nrecipe for U2O RL to bridge task-agnostic unsupervised offline skill-based\npolicy pre-training and supervised online fine-tuning. Throughout our\nexperiments in nine state-based and pixel-based environments, we empirically\ndemonstrate that U2O RL achieves strong performance that matches or even\noutperforms previous offline-to-online RL approaches, while being able to reuse\na single pre-trained model for a number of different downstream tasks.",
    "pdf_url": "http://arxiv.org/pdf/2408.14785v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Kim2024UnsupervisedtoOnlineRL,\n author = {Junsu Kim and Seohong Park and Sergey Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Unsupervised-to-Online Reinforcement Learning},\n volume = {abs/2408.14785},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1712.03563v1",
    "title": "DGCNN: Disordered Graph Convolutional Neural Network Based on the\n  Gaussian Mixture Model",
    "published": "2017-12-10T17:38:25Z",
    "updated": "2017-12-10T17:38:25Z",
    "authors": [
      "Bo Wu",
      "Yang Liu",
      "Bo Lang",
      "Lei Huang"
    ],
    "summary": "Convolutional neural networks (CNNs) can be applied to graph similarity\nmatching, in which case they are called graph CNNs. Graph CNNs are attracting\nincreasing attention due to their effectiveness and efficiency. However, the\nexisting convolution approaches focus only on regular data forms and require\nthe transfer of the graph or key node neighborhoods of the graph into the same\nfixed form. During this transfer process, structural information of the graph\ncan be lost, and some redundant information can be incorporated. To overcome\nthis problem, we propose the disordered graph convolutional neural network\n(DGCNN) based on the mixed Gaussian model, which extends the CNN by adding a\npreprocessing layer called the disordered graph convolutional layer (DGCL). The\nDGCL uses a mixed Gaussian function to realize the mapping between the\nconvolution kernel and the nodes in the neighborhood of the graph. The output\nof the DGCL is the input of the CNN. We further implement a\nbackward-propagation optimization process of the convolutional layer by which\nwe incorporate the feature-learning model of the irregular node neighborhood\nstructure into the network. Thereafter, the optimization of the convolution\nkernel becomes part of the neural network learning process. The DGCNN can\naccept arbitrary scaled and disordered neighborhood graph structures as the\nreceptive fields of CNNs, which reduces information loss during graph\ntransformation. Finally, we perform experiments on multiple standard graph\ndatasets. The results show that the proposed method outperforms the\nstate-of-the-art methods in graph classification and retrieval.",
    "pdf_url": "http://arxiv.org/pdf/1712.03563v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages,8 figures",
    "journal_ref": null,
    "citation_count": 72,
    "bibtex": "@Article{Wu2017DGCNNDG,\n author = {Bo Wu and Yang Liu and B. Lang and Lei Huang},\n booktitle = {Neurocomputing},\n journal = {ArXiv},\n title = {DGCNN: Disordered Graph Convolutional Neural Network Based on the Gaussian Mixture Model},\n volume = {abs/1712.03563},\n year = {2017}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2311.11423v1",
    "title": "Offline Reinforcement Learning for Wireless Network Optimization with\n  Mixture Datasets",
    "published": "2023-11-19T21:02:17Z",
    "updated": "2023-11-19T21:02:17Z",
    "authors": [
      "Kun Yang",
      "Cong Shen",
      "Jing Yang",
      "Shu-ping Yeh",
      "Jerry Sydir"
    ],
    "summary": "The recent development of reinforcement learning (RL) has boosted the\nadoption of online RL for wireless radio resource management (RRM). However,\nonline RL algorithms require direct interactions with the environment, which\nmay be undesirable given the potential performance loss due to the unavoidable\nexploration in RL. In this work, we first investigate the use of \\emph{offline}\nRL algorithms in solving the RRM problem. We evaluate several state-of-the-art\noffline RL algorithms, including behavior constrained Q-learning (BCQ),\nconservative Q-learning (CQL), and implicit Q-learning (IQL), for a specific\nRRM problem that aims at maximizing a linear combination {of sum and}\n5-percentile rates via user scheduling. We observe that the performance of\noffline RL for the RRM problem depends critically on the behavior policy used\nfor data collection, and further propose a novel offline RL solution that\nleverages heterogeneous datasets collected by different behavior policies. We\nshow that with a proper mixture of the datasets, offline RL can produce a\nnear-optimal RL policy even when all involved behavior policies are highly\nsuboptimal.",
    "pdf_url": "http://arxiv.org/pdf/2311.11423v1",
    "doi": null,
    "categories": [
      "cs.IT",
      "cs.LG",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "This paper is the camera ready version for Asilomar 2023",
    "journal_ref": null,
    "citation_count": 14,
    "bibtex": "@Article{Yang2023OfflineRL,\n author = {Kun Yang and Cong Shen and Jing Yang and Shu-ping Yeh and J. Sydir},\n booktitle = {Asilomar Conference on Signals, Systems and Computers},\n journal = {2023 57th Asilomar Conference on Signals, Systems, and Computers},\n pages = {629-633},\n title = {Offline Reinforcement Learning for Wireless Network Optimization with Mixture Datasets},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2306.15503v2",
    "title": "Prioritized Trajectory Replay: A Replay Memory for Data-driven\n  Reinforcement Learning",
    "published": "2023-06-27T14:29:44Z",
    "updated": "2025-03-21T02:41:44Z",
    "authors": [
      "Jinyi Liu",
      "Yi Ma",
      "Jianye Hao",
      "Yujing Hu",
      "Yan Zheng",
      "Tangjie Lv",
      "Changjie Fan"
    ],
    "summary": "In recent years, data-driven reinforcement learning (RL), also known as\noffline RL, have gained significant attention. However, the role of data\nsampling techniques in offline RL has been overlooked despite its potential to\nenhance online RL performance. Recent research suggests applying sampling\ntechniques directly to state-transitions does not consistently improve\nperformance in offline RL. Therefore, in this study, we propose a memory\ntechnique, (Prioritized) Trajectory Replay (TR/PTR), which extends the sampling\nperspective to trajectories for more comprehensive information extraction from\nlimited data. TR enhances learning efficiency by backward sampling of\ntrajectories that optimizes the use of subsequent state information. Building\non TR, we build the weighted critic target to avoid sampling unseen actions in\noffline training, and Prioritized Trajectory Replay (PTR) that enables more\nefficient trajectory sampling, prioritized by various trajectory priority\nmetrics. We demonstrate the benefits of integrating TR and PTR with existing\noffline RL algorithms on D4RL. In summary, our research emphasizes the\nsignificance of trajectory-based data sampling techniques in enhancing the\nefficiency and performance of offline RL algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2306.15503v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAMAS 2024, see\n  https://dl.acm.org/doi/10.5555/3635637.3662980",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Liu2023PrioritizedTR,\n author = {Jinyi Liu and Y. Ma and Jianye Hao and Yujing Hu and Yan Zheng and Tangjie Lv and Changjie Fan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning},\n volume = {abs/2306.15503},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2102.07210v2",
    "title": "Reversible Action Design for Combinatorial Optimization with\n  Reinforcement Learning",
    "published": "2021-02-14T18:05:42Z",
    "updated": "2022-09-02T17:34:47Z",
    "authors": [
      "Fan Yao",
      "Renqin Cai",
      "Hongning Wang"
    ],
    "summary": "Combinatorial optimization problem (COP) over graphs is a fundamental\nchallenge in optimization. Reinforcement learning (RL) has recently emerged as\na new framework to tackle these problems and has demonstrated promising\nresults. However, most RL solutions employ a greedy manner to construct the\nsolution incrementally, thus inevitably pose unnecessary dependency on action\nsequences and need a lot of problem-specific designs. We propose a general RL\nframework that not only exhibits state-of-the-art empirical performance but\nalso generalizes to a variety class of COPs. Specifically, we define state as a\nsolution to a problem instance and action as a perturbation to this solution.\nWe utilize graph neural networks (GNN) to extract latent representations for\ngiven problem instances for state-action encoding, and then apply deep\nQ-learning to obtain a policy that gradually refines the solution by flipping\nor swapping vertex labels. Experiments are conducted on Maximum $k$-Cut and\nTraveling Salesman Problem and performance improvement is achieved against a\nset of learning-based and heuristic baselines.",
    "pdf_url": "http://arxiv.org/pdf/2102.07210v2",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Yao2021ReversibleAD,\n author = {Fan Yao and Renqin Cai and Hongning Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reversible Action Design for Combinatorial Optimization with Reinforcement Learning},\n volume = {abs/2102.07210},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2210.11579v1",
    "title": "Model-based Lifelong Reinforcement Learning with Bayesian Exploration",
    "published": "2022-10-20T20:40:47Z",
    "updated": "2022-10-20T20:40:47Z",
    "authors": [
      "Haotian Fu",
      "Shangqun Yu",
      "Michael Littman",
      "George Konidaris"
    ],
    "summary": "We propose a model-based lifelong reinforcement-learning approach that\nestimates a hierarchical Bayesian posterior distilling the common structure\nshared across different tasks. The learned posterior combined with a\nsample-based Bayesian exploration procedure increases the sample efficiency of\nlearning across a family of related tasks. We first derive an analysis of the\nrelationship between the sample complexity and the initialization quality of\nthe posterior in the finite MDP setting. We next scale the approach to\ncontinuous-state domains by introducing a Variational Bayesian Lifelong\nReinforcement Learning algorithm that can be combined with recent model-based\ndeep RL methods, and that exhibits backward transfer. Experimental results on\nseveral challenging domains show that our algorithms achieve both better\nforward and backward transfer performance than state-of-the-art lifelong RL\nmethods.",
    "pdf_url": "http://arxiv.org/pdf/2210.11579v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2022",
    "journal_ref": null,
    "citation_count": 14,
    "bibtex": "@Article{Fu2022ModelbasedLR,\n author = {Haotian Fu and Shangqun Yu and Michael S. Littman and G. Konidaris},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Model-based Lifelong Reinforcement Learning with Bayesian Exploration},\n volume = {abs/2210.11579},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2203.15845v3",
    "title": "Topological Experience Replay",
    "published": "2022-03-29T18:28:20Z",
    "updated": "2023-06-26T21:12:17Z",
    "authors": [
      "Zhang-Wei Hong",
      "Tao Chen",
      "Yen-Chen Lin",
      "Joni Pajarinen",
      "Pulkit Agrawal"
    ],
    "summary": "State-of-the-art deep Q-learning methods update Q-values using state\ntransition tuples sampled from the experience replay buffer. This strategy\noften uniformly and randomly samples or prioritizes data sampling based on\nmeasures such as the temporal difference (TD) error. Such sampling strategies\ncan be inefficient at learning Q-function because a state's Q-value depends on\nthe Q-value of successor states. If the data sampling strategy ignores the\nprecision of the Q-value estimate of the next state, it can lead to useless and\noften incorrect updates to the Q-values. To mitigate this issue, we organize\nthe agent's experience into a graph that explicitly tracks the dependency\nbetween Q-values of states. Each edge in the graph represents a transition\nbetween two states by executing a single action. We perform value backups via a\nbreadth-first search starting from that expands vertices in the graph starting\nfrom the set of terminal states and successively moving backward. We\nempirically show that our method is substantially more data-efficient than\nseveral baselines on a diverse range of goal-reaching tasks. Notably, the\nproposed method also outperforms baselines that consume more batches of\ntraining experience and operates from high-dimensional observational data such\nas images.",
    "pdf_url": "http://arxiv.org/pdf/2203.15845v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Published at ICLR 2022",
    "citation_count": 19,
    "bibtex": "@Article{Hong2022TopologicalER,\n author = {Zhang-Wei Hong and Tao Chen and Yen-Chen Lin and J. Pajarinen and Pulkit Agrawal},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Topological Experience Replay},\n volume = {abs/2203.15845},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2412.05265v4",
    "title": "Reinforcement Learning: An Overview",
    "published": "2024-12-06T18:53:49Z",
    "updated": "2025-09-12T22:08:16Z",
    "authors": [
      "Kevin Murphy"
    ],
    "summary": "This manuscript gives a big-picture, up-to-date overview of the field of\n(deep) reinforcement learning and sequential decision making, covering\nvalue-based methods, policy-based methods, model-based methods, multi-agent RL,\nLLMs and RL, and various other topics (e.g., offline RL, hierarchical RL,\nintrinsic reward).",
    "pdf_url": "http://arxiv.org/pdf/2412.05265v4",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Murphy2024ReinforcementLA,\n author = {Kevin Murphy},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement Learning: An Overview},\n volume = {abs/2412.05265},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2407.08964v1",
    "title": "Communication-Aware Reinforcement Learning for Cooperative Adaptive\n  Cruise Control",
    "published": "2024-07-12T03:28:24Z",
    "updated": "2024-07-12T03:28:24Z",
    "authors": [
      "Sicong Jiang",
      "Seongjin Choi",
      "Lijun Sun"
    ],
    "summary": "Cooperative Adaptive Cruise Control (CACC) plays a pivotal role in enhancing\ntraffic efficiency and safety in Connected and Autonomous Vehicles (CAVs).\nReinforcement Learning (RL) has proven effective in optimizing complex\ndecision-making processes in CACC, leading to improved system performance and\nadaptability. Among RL approaches, Multi-Agent Reinforcement Learning (MARL)\nhas shown remarkable potential by enabling coordinated actions among multiple\nCAVs through Centralized Training with Decentralized Execution (CTDE). However,\nMARL often faces scalability issues, particularly when CACC vehicles suddenly\njoin or leave the platoon, resulting in performance degradation. To address\nthese challenges, we propose Communication-Aware Reinforcement Learning\n(CA-RL). CA-RL includes a communication-aware module that extracts and\ncompresses vehicle communication information through forward and backward\ninformation transmission modules. This enables efficient cyclic information\npropagation within the CACC traffic flow, ensuring policy consistency and\nmitigating the scalability problems of MARL in CACC. Experimental results\ndemonstrate that CA-RL significantly outperforms baseline methods in various\ntraffic scenarios, achieving superior scalability, robustness, and overall\nsystem performance while maintaining reliable performance despite changes in\nthe number of participating vehicles.",
    "pdf_url": "http://arxiv.org/pdf/2407.08964v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Jiang2024CommunicationAwareRL,\n author = {Sicong Jiang and Seongjin Choi and Lijun Sun},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Communication-Aware Reinforcement Learning for Cooperative Adaptive Cruise Control},\n volume = {abs/2407.08964},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2005.04444v1",
    "title": "Reinforcement Learning for Thermostatically Controlled Loads Control\n  using Modelica and Python",
    "published": "2020-05-09T13:35:49Z",
    "updated": "2020-05-09T13:35:49Z",
    "authors": [
      "Oleh Lukianykhin",
      "Tetiana Bogodorova"
    ],
    "summary": "The aim of the project is to investigate and assess opportunities for\napplying reinforcement learning (RL) for power system control. As a proof of\nconcept (PoC), voltage control of thermostatically controlled loads (TCLs) for\npower consumption regulation was developed using Modelica-based pipeline. The\nQ-learning RL algorithm has been validated for deterministic and stochastic\ninitialization of TCLs. The latter modelling is closer to real grid behaviour,\nwhich challenges the control development, considering the stochastic nature of\nload switching. In addition, the paper shows the influence of Q-learning\nparameters, including discretization of state-action space, on the controller\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2005.04444v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted for the Asian Modelica Conference 2020",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Lukianykhin2020ReinforcementLF,\n author = {Oleh Lukianykhin and T. Bogodorova},\n booktitle = {Proceedings of Asian Modelica Conference 2020, Tokyo, Japan, October 08-09, 2020},\n journal = {ArXiv},\n title = {Reinforcement Learning for Thermostatically Controlled Loads Control using Modelica and Python},\n volume = {abs/2005.04444},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2402.00348v1",
    "title": "ODICE: Revealing the Mystery of Distribution Correction Estimation via\n  Orthogonal-gradient Update",
    "published": "2024-02-01T05:30:51Z",
    "updated": "2024-02-01T05:30:51Z",
    "authors": [
      "Liyuan Mao",
      "Haoran Xu",
      "Weinan Zhang",
      "Xianyuan Zhan"
    ],
    "summary": "In this study, we investigate the DIstribution Correction Estimation (DICE)\nmethods, an important line of work in offline reinforcement learning (RL) and\nimitation learning (IL). DICE-based methods impose state-action-level behavior\nconstraint, which is an ideal choice for offline learning. However, they\ntypically perform much worse than current state-of-the-art (SOTA) methods that\nsolely use action-level behavior constraint. After revisiting DICE-based\nmethods, we find there exist two gradient terms when learning the value\nfunction using true-gradient update: forward gradient (taken on the current\nstate) and backward gradient (taken on the next state). Using forward gradient\nbears a large similarity to many offline RL methods, and thus can be regarded\nas applying action-level constraint. However, directly adding the backward\ngradient may degenerate or cancel out its effect if these two gradients have\nconflicting directions. To resolve this issue, we propose a simple yet\neffective modification that projects the backward gradient onto the normal\nplane of the forward gradient, resulting in an orthogonal-gradient update, a\nnew learning rule for DICE-based methods. We conduct thorough theoretical\nanalyses and find that the projected backward gradient brings state-level\nbehavior regularization, which reveals the mystery of DICE-based methods: the\nvalue learning objective does try to impose state-action-level constraint, but\nneeds to be used in a corrected way. Through toy examples and extensive\nexperiments on complex offline RL and IL tasks, we demonstrate that DICE-based\nmethods using orthogonal-gradient updates (O-DICE) achieve SOTA performance and\ngreat robustness.",
    "pdf_url": "http://arxiv.org/pdf/2402.00348v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Spotlight @ ICLR 2024, first two authors contribute equally",
    "journal_ref": null,
    "citation_count": 17,
    "bibtex": "@Article{Mao2024ODICERT,\n author = {Liyuan Mao and Haoran Xu and Weinan Zhang and Xianyuan Zhan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update},\n volume = {abs/2402.00348},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2411.14726v1",
    "title": "Enhancing Molecular Design through Graph-based Topological Reinforcement\n  Learning",
    "published": "2024-11-22T04:45:55Z",
    "updated": "2024-11-22T04:45:55Z",
    "authors": [
      "Xiangyu Zhang"
    ],
    "summary": "The generation of drug-like molecules is crucial for drug design. Existing\nreinforcement learning (RL) methods often overlook structural information.\nHowever, feature engineering-based methods usually merely focus on binding\naffinity prediction without substantial molecular modification. To address\nthis, we present Graph-based Topological Reinforcement Learning (GraphTRL),\nwhich integrates both chemical and structural data for improved molecular\ngeneration. GraphTRL leverages multiscale weighted colored graphs (MWCG) and\npersistent homology, combined with molecular fingerprints, as the state space\nfor RL. Evaluations show that GraphTRL outperforms existing methods in binding\naffinity prediction, offering a promising approach to accelerate drug\ndiscovery.",
    "pdf_url": "http://arxiv.org/pdf/2411.14726v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Zhang2024EnhancingMD,\n author = {Xiangyu Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Enhancing Molecular Design through Graph-based Topological Reinforcement Learning},\n volume = {abs/2411.14726},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2103.06257v2",
    "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
    "published": "2021-03-10T18:45:48Z",
    "updated": "2022-05-05T17:01:45Z",
    "authors": [
      "Benjamin Eysenbach",
      "Sergey Levine"
    ],
    "summary": "Many potential applications of reinforcement learning (RL) require guarantees\nthat the agent will perform well in the face of disturbances to the dynamics or\nreward function. In this paper, we prove theoretically that maximum entropy\n(MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be\nused to learn policies that are robust to some disturbances in the dynamics and\nthe reward function. While this capability of MaxEnt RL has been observed\nempirically in prior work, to the best of our knowledge our work provides the\nfirst rigorous proof and theoretical characterization of the MaxEnt RL robust\nset. While a number of prior robust RL algorithms have been designed to handle\nsimilar disturbances to the reward function or dynamics, these methods\ntypically require additional moving parts and hyperparameters on top of a base\nRL algorithm. In contrast, our results suggest that MaxEnt RL by itself is\nrobust to certain disturbances, without requiring any additional modifications.\nWhile this does not imply that MaxEnt RL is the best available robust RL\nmethod, MaxEnt RL is a simple robust RL method with appealing formal\nguarantees.",
    "pdf_url": "http://arxiv.org/pdf/2103.06257v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at ICLR 2022. Blog post and videos:\n  https://bair.berkeley.edu/blog/2021/03/10/maxent-robust-rl/. arXiv admin\n  note: text overlap with arXiv:1910.01913",
    "journal_ref": null,
    "citation_count": 210,
    "bibtex": "@Article{Eysenbach2021MaximumER,\n author = {Benjamin Eysenbach and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Maximum Entropy RL (Provably) Solves Some Robust RL Problems},\n volume = {abs/2103.06257},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2105.08764v2",
    "title": "OpenGraphGym-MG: Using Reinforcement Learning to Solve Large Graph\n  Optimization Problems on MultiGPU Systems",
    "published": "2021-05-18T18:24:42Z",
    "updated": "2021-06-24T02:01:48Z",
    "authors": [
      "Weijian Zheng",
      "Dali Wang",
      "Fengguang Song"
    ],
    "summary": "Large scale graph optimization problems arise in many fields. This paper\npresents an extensible, high performance framework (named OpenGraphGym-MG) that\nuses deep reinforcement learning and graph embedding to solve large graph\noptimization problems with multiple GPUs. The paper uses a common RL algorithm\n(deep Q-learning) and a representative graph embedding (structure2vec) to\ndemonstrate the extensibility of the framework and, most importantly, to\nillustrate the novel optimization techniques, such as spatial parallelism,\ngraph-level and node-level batched processing, distributed sparse graph\nstorage, efficient parallel RL training and inference algorithms, repeated\ngradient descent iterations, and adaptive multiple-node selections. This study\nperforms a comprehensive performance analysis on parallel efficiency and memory\ncost that proves the parallel RL training and inference algorithms are\nefficient and highly scalable on a number of GPUs. This study also conducts a\nrange of large graph experiments, with both generated graphs (over 30 million\nedges) and real-world graphs, using a single compute node (with six GPUs) of\nthe Summit supercomputer. Good scalability in both RL training and inference is\nachieved: as the number of GPUs increases from one to six, the time of a single\nstep of RL training and a single step of RL inference on large graphs with more\nthan 30 million edges, is reduced from 316.4s to 54.5s, and 23.8s to 3.4s,\nrespectively. The research results on a single node lay out a solid foundation\nfor the future work to address graph optimization problems with a large number\nof GPUs across multiple nodes in the Summit.",
    "pdf_url": "http://arxiv.org/pdf/2105.08764v2",
    "doi": null,
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Zheng2021OpenGraphGymMGUR,\n author = {Weijian Zheng and Dali Wang and Fengguang Song},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {OpenGraphGym-MG: Using Reinforcement Learning to Solve Large Graph Optimization Problems on MultiGPU Systems},\n volume = {abs/2105.08764},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2507.11367v1",
    "title": "Local Pairwise Distance Matching for Backpropagation-Free Reinforcement\n  Learning",
    "published": "2025-07-15T14:39:41Z",
    "updated": "2025-07-15T14:39:41Z",
    "authors": [
      "Daniel Tanneberg"
    ],
    "summary": "Training neural networks with reinforcement learning (RL) typically relies on\nbackpropagation (BP), necessitating storage of activations from the forward\npass for subsequent backward updates. Furthermore, backpropagating error\nsignals through multiple layers often leads to vanishing or exploding\ngradients, which can degrade learning performance and stability. We propose a\nnovel approach that trains each layer of the neural network using local signals\nduring the forward pass in RL settings. Our approach introduces local,\nlayer-wise losses leveraging the principle of matching pairwise distances from\nmulti-dimensional scaling, enhanced with optional reward-driven guidance. This\nmethod allows each hidden layer to be trained using local signals computed\nduring forward propagation, thus eliminating the need for backward passes and\nstoring intermediate activations. Our experiments, conducted with policy\ngradient methods across common RL benchmarks, demonstrate that this\nbackpropagation-free method achieves competitive performance compared to their\nclassical BP-based counterpart. Additionally, the proposed method enhances\nstability and consistency within and across runs, and improves performance\nespecially in challenging environments.",
    "pdf_url": "http://arxiv.org/pdf/2507.11367v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted at the European Conference on Artificial Intelligence (ECAI\n  2025)",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Tanneberg2025LocalPD,\n author = {Daniel Tanneberg},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning},\n volume = {abs/2507.11367},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2211.16759v1",
    "title": "General policy mapping: online continual reinforcement learning inspired\n  on the insect brain",
    "published": "2022-11-30T05:54:19Z",
    "updated": "2022-11-30T05:54:19Z",
    "authors": [
      "Angel Yanguas-Gil",
      "Sandeep Madireddy"
    ],
    "summary": "We have developed a model for online continual or lifelong reinforcement\nlearning (RL) inspired on the insect brain. Our model leverages the offline\ntraining of a feature extraction and a common general policy layer to enable\nthe convergence of RL algorithms in online settings. Sharing a common policy\nlayer across tasks leads to positive backward transfer, where the agent\ncontinuously improved in older tasks sharing the same underlying general\npolicy. Biologically inspired restrictions to the agent's network are key for\nthe convergence of RL algorithms. This provides a pathway towards efficient\nonline RL in resource-constrained scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2211.16759v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Yanguas-Gil2022GeneralPM,\n author = {A. Yanguas-Gil and Sandeep Madireddy},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {General policy mapping: online continual reinforcement learning inspired on the insect brain},\n volume = {abs/2211.16759},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2502.07978v1",
    "title": "A Survey of In-Context Reinforcement Learning",
    "published": "2025-02-11T21:52:19Z",
    "updated": "2025-02-11T21:52:19Z",
    "authors": [
      "Amir Moeini",
      "Jiuqi Wang",
      "Jacob Beck",
      "Ethan Blaser",
      "Shimon Whiteson",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "summary": "Reinforcement learning (RL) agents typically optimize their policies by\nperforming expensive backward passes to update their network parameters.\nHowever, some agents can solve new tasks without updating any parameters by\nsimply conditioning on additional context such as their action-observation\nhistories. This paper surveys work on such behavior, known as in-context\nreinforcement learning.",
    "pdf_url": "http://arxiv.org/pdf/2502.07978v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 16,
    "bibtex": "@Article{Moeini2025ASO,\n author = {Amir Moeini and Jiuqi Wang and Jacob Beck and Ethan Blaser and S. Whiteson and Rohan Chandra and Shangtong Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey of In-Context Reinforcement Learning},\n volume = {abs/2502.07978},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2207.07570v1",
    "title": "The Nature of Temporal Difference Errors in Multi-step Distributional\n  Reinforcement Learning",
    "published": "2022-07-15T16:19:23Z",
    "updated": "2022-07-15T16:19:23Z",
    "authors": [
      "Yunhao Tang",
      "Mark Rowland",
      "Rémi Munos",
      "Bernardo Ávila Pires",
      "Will Dabney",
      "Marc G. Bellemare"
    ],
    "summary": "We study the multi-step off-policy learning approach to distributional RL.\nDespite the apparent similarity between value-based RL and distributional RL,\nour study reveals intriguing and fundamental differences between the two cases\nin the multi-step setting. We identify a novel notion of path-dependent\ndistributional TD error, which is indispensable for principled multi-step\ndistributional RL. The distinction from the value-based case bears important\nimplications on concepts such as backward-view algorithms. Our work provides\nthe first theoretical guarantees on multi-step off-policy distributional RL\nalgorithms, including results that apply to the small number of existing\napproaches to multi-step distributional RL. In addition, we derive a novel\nalgorithm, Quantile Regression-Retrace, which leads to a deep RL agent\nQR-DQN-Retrace that shows empirical improvements over QR-DQN on the Atari-57\nbenchmark. Collectively, we shed light on how unique challenges in multi-step\ndistributional RL can be addressed both in theory and practice.",
    "pdf_url": "http://arxiv.org/pdf/2207.07570v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 12,
    "bibtex": "@Article{Tang2022TheNO,\n author = {Yunhao Tang and Mark Rowland and R. Munos and B. '. Pires and Will Dabney and Marc G. Bellemare},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {The Nature of Temporal Difference Errors in Multi-step Distributional Reinforcement Learning},\n volume = {abs/2207.07570},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2509.12833v1",
    "title": "Safe Reinforcement Learning using Action Projection: Safeguard the\n  Policy or the Environment?",
    "published": "2025-09-16T08:56:38Z",
    "updated": "2025-09-16T08:56:38Z",
    "authors": [
      "Hannah Markgraf",
      "Shamburaj Sawant",
      "Hanna Krasowski",
      "Lukas Schäfer",
      "Sebastien Gros",
      "Matthias Althoff"
    ],
    "summary": "Projection-based safety filters, which modify unsafe actions by mapping them\nto the closest safe alternative, are widely used to enforce safety constraints\nin reinforcement learning (RL). Two integration strategies are commonly\nconsidered: Safe environment RL (SE-RL), where the safeguard is treated as part\nof the environment, and safe policy RL (SP-RL), where it is embedded within the\npolicy through differentiable optimization layers. Despite their practical\nrelevance in safety-critical settings, a formal understanding of their\ndifferences is lacking. In this work, we present a theoretical comparison of\nSE-RL and SP-RL. We identify a key distinction in how each approach is affected\nby action aliasing, a phenomenon in which multiple unsafe actions are projected\nto the same safe action, causing information loss in the policy gradients. In\nSE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it\nmanifests directly as rank-deficient Jacobians during backpropagation through\nthe safeguard. Our contributions are threefold: (i) a unified formalization of\nSE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical\nanalysis of their respective policy gradient estimates, highlighting the role\nof action aliasing, and (iii) a comparative study of mitigation strategies,\nincluding a novel penalty-based improvement for SP-RL that aligns with\nestablished SE-RL practices. Empirical results support our theoretical\npredictions, showing that action aliasing is more detrimental for SP-RL than\nfor SE-RL. However, with appropriate improvement strategies, SP-RL can match or\noutperform improved SE-RL across a range of environments. These findings\nprovide actionable insights for choosing and refining projection-based safe RL\nmethods based on task characteristics.",
    "pdf_url": "http://arxiv.org/pdf/2509.12833v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Markgraf2025SafeRL,\n author = {H. Markgraf and Shamburaj Sawant and Hanna Krasowski and Lukas Schäfer and Sebastien Gros and Matthias Althoff},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?},\n volume = {abs/2509.12833},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2506.17919v1",
    "title": "Permutation Equivariant Model-based Offline Reinforcement Learning for\n  Auto-bidding",
    "published": "2025-06-22T06:58:36Z",
    "updated": "2025-06-22T06:58:36Z",
    "authors": [
      "Zhiyu Mou",
      "Miao Xu",
      "Wei Chen",
      "Rongquan Bai",
      "Chuan Yu",
      "Jian Xu"
    ],
    "summary": "Reinforcement learning (RL) for auto-bidding has shifted from using\nsimplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL\non fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are\nlimited by the dataset's state space coverage, offering modest gains. While\nSRLB expands state coverage, its simulator-reality gap risks misleading\npolicies. This paper introduces Model-based RL Bidding (MRLB), which learns an\nenvironment model from real data to bridge this gap. MRLB trains policies using\nboth real and model-generated data, expanding state coverage beyond ORLB. To\nensure model reliability, we propose: 1) A permutation equivariant model\narchitecture for better generalization, and 2) A robust offline Q-learning\nmethod that pessimistically penalizes model errors. These form the Permutation\nEquivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments\nshow that PE-MORL outperforms state-of-the-art auto-bidding methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.17919v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Mou2025PermutationEM,\n author = {Zhiyu Mou and Miao Xu and Wei Chen and Rongquan Bai and Chuan Yu and Jian Xu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding},\n volume = {abs/2506.17919},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2402.07928v1",
    "title": "Abstracted Trajectory Visualization for Explainability in Reinforcement\n  Learning",
    "published": "2024-02-05T21:17:44Z",
    "updated": "2024-02-05T21:17:44Z",
    "authors": [
      "Yoshiki Takagi",
      "Roderick Tabalba",
      "Nurit Kirshenbaum",
      "Jason Leigh"
    ],
    "summary": "Explainable AI (XAI) has demonstrated the potential to help reinforcement\nlearning (RL) practitioners to understand how RL models work. However, XAI for\nusers who do not have RL expertise (non-RL experts), has not been studied\nsufficiently. This results in a difficulty for the non-RL experts to\nparticipate in the fundamental discussion of how RL models should be designed\nfor an incoming society where humans and AI coexist. Solving such a problem\nwould enable RL experts to communicate with the non-RL experts in producing\nmachine learning solutions that better fit our society. We argue that\nabstracted trajectories, that depicts transitions between the major states of\nthe RL model, will be useful for non-RL experts to build a mental model of the\nagents. Our early results suggest that by leveraging a visualization of the\nabstracted trajectories, users without RL expertise are able to infer the\nbehavior patterns of RL.",
    "pdf_url": "http://arxiv.org/pdf/2402.07928v1",
    "doi": null,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "14pages, 11figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Takagi2024AbstractedTV,\n author = {Yoshiki Takagi and Roderick S. Tabalba and Nurit Kirshenbaum and Jason Leigh},\n booktitle = {Conference on Algebraic Informatics},\n journal = {2024 IEEE Conference on Artificial Intelligence (CAI)},\n pages = {75-82},\n title = {Abstracted Trajectory Visualization for Explainability in Reinforcement Learning},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1901.00188v2",
    "title": "Complementary reinforcement learning towards explainable agents",
    "published": "2019-01-01T18:14:35Z",
    "updated": "2019-01-24T02:29:24Z",
    "authors": [
      "Jung Hoon Lee"
    ],
    "summary": "Reinforcement learning (RL) algorithms allow agents to learn skills and\nstrategies to perform complex tasks without detailed instructions or expensive\nlabelled training examples. That is, RL agents can learn, as we learn. Given\nthe importance of learning in our intelligence, RL has been thought to be one\nof key components to general artificial intelligence, and recent breakthroughs\nin deep reinforcement learning suggest that neural networks (NN) are natural\nplatforms for RL agents. However, despite the efficiency and versatility of\nNN-based RL agents, their decision-making remains incomprehensible, reducing\ntheir utilities. To deploy RL into a wider range of applications, it is\nimperative to develop explainable NN-based RL agents. Here, we propose a method\nto derive a secondary comprehensible agent from a NN-based RL agent, whose\ndecision-makings are based on simple rules. Our empirical evaluation of this\nsecondary agent's performance supports the possibility of building a\ncomprehensible and transparent agent using a NN-based RL agent.",
    "pdf_url": "http://arxiv.org/pdf/1901.00188v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 figures",
    "journal_ref": null,
    "citation_count": 12,
    "bibtex": "@Article{Lee2019ComplementaryRL,\n author = {J. H. Lee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Complementary reinforcement learning towards explainable agents},\n volume = {abs/1901.00188},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2003.09540v1",
    "title": "Distributed Reinforcement Learning for Cooperative Multi-Robot Object\n  Manipulation",
    "published": "2020-03-21T00:43:54Z",
    "updated": "2020-03-21T00:43:54Z",
    "authors": [
      "Guohui Ding",
      "Joewie J. Koh",
      "Kelly Merckaert",
      "Bram Vanderborght",
      "Marco M. Nicotra",
      "Christoffer Heckman",
      "Alessandro Roncone",
      "Lijun Chen"
    ],
    "summary": "We consider solving a cooperative multi-robot object manipulation task using\nreinforcement learning (RL). We propose two distributed multi-agent RL\napproaches: distributed approximate RL (DA-RL), where each agent applies\nQ-learning with individual reward functions; and game-theoretic RL (GT-RL),\nwhere the agents update their Q-values based on the Nash equilibrium of a\nbimatrix Q-value game. We validate the proposed approaches in the setting of\ncooperative object manipulation with two simulated robot arms. Although we\nfocus on a small system of two agents in this paper, both DA-RL and GT-RL apply\nto general multi-agent systems, and are expected to scale well to large\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2003.09540v1",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.GT",
      "cs.LG",
      "cs.MA",
      "I.2.9; I.2.6; I.2.11"
    ],
    "primary_category": "cs.RO",
    "comment": "3 pages, 3 figures",
    "journal_ref": "Proceedings of the 19th International Conference on Autonomous\n  Agents and Multiagent Systems (AAMAS), 2020, pp. 1831-1833",
    "citation_count": 24,
    "bibtex": "@Article{Ding2020DistributedRL,\n author = {Guohui Ding and Joewie J. Koh and Kelly Merckaert and B. Vanderborght and M. Nicotra and C. Heckman and Alessandro Roncone and Lijun Chen},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {1831-1833},\n title = {Distributed Reinforcement Learning for Cooperative Multi-Robot Object Manipulation},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1707.04192v3",
    "title": "One-shot learning and behavioral eligibility traces in sequential\n  decision making",
    "published": "2017-07-13T16:04:34Z",
    "updated": "2019-11-12T10:00:22Z",
    "authors": [
      "Marco Lehmann",
      "He Xu",
      "Vasiliki Liakoni",
      "Michael Herzog",
      "Wulfram Gerstner",
      "Kerstin Preuschoff"
    ],
    "summary": "In many daily tasks we make multiple decisions before reaching a goal. In\norder to learn such sequences of decisions, a mechanism to link earlier actions\nto later reward is necessary. Reinforcement learning theory suggests two\nclasses of algorithms solving this credit assignment problem: In classic\ntemporal-difference learning, earlier actions receive reward information only\nafter multiple repetitions of the task, whereas models with eligibility traces\nreinforce entire sequences of actions from a single experience (one-shot). Here\nwe asked whether humans use eligibility traces. We developed a novel paradigm\nto directly observe which actions and states along a multi-step sequence are\nreinforced after a single reward. By focusing our analysis on those states for\nwhich RL with and without eligibility trace make qualitatively distinct\npredictions, we find direct behavioral (choice probability) and physiological\n(pupil dilation) signatures of reinforcement learning with eligibility trace\nacross multiple sensory modalities.",
    "pdf_url": "http://arxiv.org/pdf/1707.04192v3",
    "doi": "10.7554/eLife.47463",
    "categories": [
      "q-bio.NC"
    ],
    "primary_category": "q-bio.NC",
    "comment": null,
    "journal_ref": "eLife 2019; 8:e47463",
    "citation_count": 15,
    "bibtex": "@Article{Lehmann2017OneshotLA,\n author = {Marco P Lehmann and He A. Xu and Vasiliki Liakoni and M. Herzog and W. Gerstner and K. Preuschoff},\n booktitle = {eLife},\n journal = {eLife},\n title = {One-shot learning and behavioral eligibility traces in sequential decision making},\n volume = {8},\n year = {2017}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2112.12281v1",
    "title": "Improving the Efficiency of Off-Policy Reinforcement Learning by\n  Accounting for Past Decisions",
    "published": "2021-12-23T00:07:28Z",
    "updated": "2021-12-23T00:07:28Z",
    "authors": [
      "Brett Daley",
      "Christopher Amato"
    ],
    "summary": "Off-policy learning from multistep returns is crucial for sample-efficient\nreinforcement learning, particularly in the experience replay setting now\ncommonly used with deep neural networks. Classically, off-policy estimation\nbias is corrected in a per-decision manner: past temporal-difference errors are\nre-weighted by the instantaneous Importance Sampling (IS) ratio (via\neligibility traces) after each action. Many important off-policy algorithms\nsuch as Tree Backup and Retrace rely on this mechanism along with differing\nprotocols for truncating (\"cutting\") the ratios (\"traces\") to counteract the\nexcessive variance of the IS estimator. Unfortunately, cutting traces on a\nper-decision basis is not necessarily efficient; once a trace has been cut\naccording to local information, the effect cannot be reversed later,\npotentially resulting in the premature truncation of estimated returns and\nslower learning. In the interest of motivating efficient off-policy algorithms,\nwe propose a multistep operator that permits arbitrary past-dependent traces.\nWe prove that our operator is convergent for policy evaluation, and for optimal\ncontrol when targeting greedy-in-the-limit policies. Our theorems establish the\nfirst convergence guarantees for many existing algorithms including Truncated\nIS, Non-Markov Retrace, and history-dependent TD($\\lambda$). Our theoretical\nresults also provide guidance for the development of new algorithms that\njointly consider multiple past decisions for better credit assignment and\nfaster learning.",
    "pdf_url": "http://arxiv.org/pdf/2112.12281v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 0 figures",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Daley2021ImprovingTE,\n author = {Brett Daley and Chris Amato},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving the Efficiency of Off-Policy Reinforcement Learning by Accounting for Past Decisions},\n volume = {abs/2112.12281},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2103.06224v1",
    "title": "An Information-Theoretic Perspective on Credit Assignment in\n  Reinforcement Learning",
    "published": "2021-03-10T17:50:15Z",
    "updated": "2021-03-10T17:50:15Z",
    "authors": [
      "Dilip Arumugam",
      "Peter Henderson",
      "Pierre-Luc Bacon"
    ],
    "summary": "How do we formalize the challenge of credit assignment in reinforcement\nlearning? Common intuition would draw attention to reward sparsity as a key\ncontributor to difficult credit assignment and traditional heuristics would\nlook to temporal recency for the solution, calling upon the classic eligibility\ntrace. We posit that it is not the sparsity of the reward itself that causes\ndifficulty in credit assignment, but rather the \\emph{information sparsity}. We\npropose to use information theory to define this notion, which we then use to\ncharacterize when credit assignment is an obstacle to efficient learning. With\nthis perspective, we outline several information-theoretic mechanisms for\nmeasuring credit under a fixed behavior policy, highlighting the potential of\ninformation theory as a key tool towards provably-efficient credit assignment.",
    "pdf_url": "http://arxiv.org/pdf/2103.06224v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "Workshop on Biological and Artificial Reinforcement Learning (NeurIPS\n  2020)",
    "journal_ref": null,
    "citation_count": 18,
    "bibtex": "@Article{Arumugam2021AnIP,\n author = {Dilip Arumugam and Peter Henderson and Pierre-Luc Bacon},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning},\n volume = {abs/2103.06224},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2510.17916v1",
    "title": "Self-Evidencing Through Hierarchical Gradient Decomposition: A\n  Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing\n  Variational Free Energy",
    "published": "2025-10-20T00:19:32Z",
    "updated": "2025-10-20T00:19:32Z",
    "authors": [
      "Michael James McCulloch"
    ],
    "summary": "The Free Energy Principle (FEP) states that self-organizing systems must\nminimize variational free energy to persist, but the path from principle to\nimplementable algorithm has remained unclear. We present a constructive proof\nthat the FEP can be realized through exact local credit assignment. The system\ndecomposes gradient computation hierarchically: spatial credit via feedback\nalignment, temporal credit via eligibility traces, and structural credit via a\nTrophic Field Map (TFM) that estimates expected gradient magnitude for each\nconnection block. We prove these mechanisms are exact at their respective\nlevels and validate the central claim empirically: the TFM achieves 0.9693\nPearson correlation with oracle gradients. This exactness produces emergent\ncapabilities including 98.6% retention after task interference, autonomous\nrecovery from 75% structural damage, self-organized criticality (spectral\nradius p ~= 1.0$), and sample-efficient reinforcement learning on continuous\ncontrol tasks without replay buffers. The architecture unifies Prigogine's\ndissipative structures, Friston's free energy minimization, and Hopfield's\nattractor dynamics, demonstrating that exact hierarchical inference over\nnetwork topology can be implemented with local, biologically plausible rules.",
    "pdf_url": "http://arxiv.org/pdf/2510.17916v1",
    "doi": null,
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.NE",
    "comment": "30 pages, 13 Figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{McCulloch2025SelfEvidencingTH,\n author = {Michael James McCulloch},\n title = {Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2305.08124v1",
    "title": "Theta sequences as eligibility traces: a biological solution to credit\n  assignment",
    "published": "2023-05-14T11:04:36Z",
    "updated": "2023-05-14T11:04:36Z",
    "authors": [
      "Tom M George"
    ],
    "summary": "Credit assignment problems, for example policy evaluation in RL, often\nrequire bootstrapping prediction errors through preceding states \\textit{or}\nmaintaining temporally extended memory traces; solutions which are unfavourable\nor implausible for biological networks of neurons. We propose theta sequences\n-- chains of neural activity during theta oscillations in the hippocampus,\nthought to represent rapid playthroughs of awake behaviour -- as a solution. By\nanalysing and simulating a model for theta sequences we show they compress\nbehaviour such that existing but short $\\mathsf{O}(10)$ ms neuronal memory\ntraces are effectively extended allowing for bootstrap-free credit assignment\nwithout long memory traces, equivalent to the use of eligibility traces in\nTD($\\lambda$).",
    "pdf_url": "http://arxiv.org/pdf/2305.08124v1",
    "doi": null,
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{George2023ThetaSA,\n author = {Tom M George},\n booktitle = {Tiny Papers @ ICLR},\n journal = {ArXiv},\n title = {Theta sequences as eligibility traces: a biological solution to credit assignment},\n volume = {abs/2305.08124},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2507.09127v1",
    "title": "A Study of Value-Aware Eigenoptions",
    "published": "2025-07-12T03:29:59Z",
    "updated": "2025-07-12T03:29:59Z",
    "authors": [
      "Harshil Kotamreddy",
      "Marlos C. Machado"
    ],
    "summary": "Options, which impose an inductive bias toward temporal and hierarchical\nstructure, offer a powerful framework for reinforcement learning (RL). While\neffective in sequential decision-making, they are often handcrafted rather than\nlearned. Among approaches for discovering options, eigenoptions have shown\nstrong performance in exploration, but their role in credit assignment remains\nunderexplored. In this paper, we investigate whether eigenoptions can\naccelerate credit assignment in model-free RL, evaluating them in tabular and\npixel-based gridworlds. We find that pre-specified eigenoptions aid not only\nexploration but also credit assignment, whereas online discovery can bias the\nagent's experience too strongly and hinder learning. In the context of deep RL,\nwe also propose a method for learning option-values under non-linear function\napproximation, highlighting the impact of termination conditions on\nperformance. Our findings reveal both the promise and complexity of using\neigenoptions, and options more broadly, to simultaneously support credit\nassignment and exploration in reinforcement learning.",
    "pdf_url": "http://arxiv.org/pdf/2507.09127v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at the RLC Workshop on Inductive Biases in Reinforcement\n  Learning 2025",
    "journal_ref": null,
    "citation_count": null,
    "bibtex": null
  },
  {
    "id": "http://arxiv.org/abs/1805.04514v2",
    "title": "Metatrace Actor-Critic: Online Step-size Tuning by Meta-gradient Descent\n  for Reinforcement Learning Control",
    "published": "2018-05-10T20:00:50Z",
    "updated": "2019-05-24T17:40:08Z",
    "authors": [
      "Kenny Young",
      "Baoxiang Wang",
      "Matthew E. Taylor"
    ],
    "summary": "Reinforcement learning (RL) has had many successes in both \"deep\" and\n\"shallow\" settings. In both cases, significant hyperparameter tuning is often\nrequired to achieve good performance. Furthermore, when nonlinear function\napproximation is used, non-stationarity in the state representation can lead to\nlearning instability. A variety of techniques exist to combat this --- most\nnotably large experience replay buffers or the use of multiple parallel actors.\nThese techniques come at the cost of moving away from the online RL problem as\nit is traditionally formulated (i.e., a single agent learning online without\nmaintaining a large database of training examples). Meta-learning can\npotentially help with both these issues by tuning hyperparameters online and\nallowing the algorithm to more robustly adjust to non-stationarity in a\nproblem. This paper applies meta-gradient descent to derive a set of step-size\ntuning algorithms specifically for online RL control with eligibility traces.\nOur novel technique, Metatrace, makes use of an eligibility trace analogous to\nmethods like $TD(\\lambda)$. We explore tuning both a single scalar step-size\nand a separate step-size for each learned parameter. We evaluate Metatrace\nfirst for control with linear function approximation in the classic mountain\ncar problem and then in a noisy, non-stationary version. Finally, we apply\nMetatrace for control with nonlinear function approximation in 5 games in the\nArcade Learning Environment where we explore how it impacts learning speed and\nrobustness to initial step-size choice. Results show that the meta-step-size\nparameter of Metatrace is easy to set, Metatrace can speed learning, and\nMetatrace can allow an RL algorithm to deal with non-stationarity in the\nlearning task.",
    "pdf_url": "http://arxiv.org/pdf/1805.04514v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 15,
    "bibtex": "@Article{Young2018MetatraceOS,\n author = {K. Young and Baoxiang Wang and Matthew E. Taylor},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Metatrace: Online Step-size Tuning by Meta-gradient Descent for Reinforcement Learning Control},\n volume = {abs/1805.04514},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2105.12991v2",
    "title": "Optimistic Reinforcement Learning by Forward Kullback-Leibler Divergence\n  Optimization",
    "published": "2021-05-27T08:24:51Z",
    "updated": "2022-04-22T03:17:04Z",
    "authors": [
      "Taisuke Kobayashi"
    ],
    "summary": "This paper addresses a new interpretation of the traditional optimization\nmethod in reinforcement learning (RL) as optimization problems using reverse\nKullback-Leibler (KL) divergence, and derives a new optimization method using\nforward KL divergence, instead of reverse KL divergence in the optimization\nproblems. Although RL originally aims to maximize return indirectly through\noptimization of policy, the recent work by Levine has proposed a different\nderivation process with explicit consideration of optimality as stochastic\nvariable. This paper follows this concept and formulates the traditional\nlearning laws for both value function and policy as the optimization problems\nwith reverse KL divergence including optimality. Focusing on the asymmetry of\nKL divergence, the new optimization problems with forward KL divergence are\nderived. Remarkably, such new optimization problems can be regarded as\noptimistic RL. That optimism is intuitively specified by a hyperparameter\nconverted from an uncertainty parameter. In addition, it can be enhanced when\nit is integrated with prioritized experience replay and eligibility traces,\nboth of which accelerate learning. The effects of this expected optimism was\ninvestigated through learning tendencies on numerical simulations using\nPybullet. As a result, moderate optimism accelerated learning and yielded\nhigher rewards. In a realistic robotic simulation, the proposed method with the\nmoderate optimism outperformed one of the state-of-the-art RL method.",
    "pdf_url": "http://arxiv.org/pdf/2105.12991v2",
    "doi": "10.1016/j.neunet.2022.04.021",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 6 figures",
    "journal_ref": "Neural Networks, 2022",
    "citation_count": 17,
    "bibtex": "@Article{Kobayashi2021OptimisticRL,\n author = {Taisuke Kobayashi},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          169-180\n        },\n title = {Optimistic Reinforcement Learning by Forward Kullback-Leibler Divergence Optimization},\n volume = {152},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2506.02522v1",
    "title": "Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for\n  Large-Scale Decision Making",
    "published": "2025-06-03T06:52:37Z",
    "updated": "2025-06-03T06:52:37Z",
    "authors": [
      "Xu Wan",
      "Wenyue Xu",
      "Chao Yang",
      "Mingyang Sun"
    ],
    "summary": "Recent advancements in Large Language Models (LLMs) and Reinforcement\nLearning (RL) have shown significant promise in decision-making tasks.\nNevertheless, for large-scale industrial decision problems, both approaches\nface distinct challenges: LLMs lack real-time long-sequence decision-making\ncapabilities, while RL struggles with sample efficiency in vast action spaces.\nTo bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic\nframework between LLMs and RL agents for large-scale decision-making scenarios.\nACE introduces a dual-role trajectory refinement mechanism where LLMs act as\nboth Policy Actor and Value Critic during RL's training: the Actor refines\nsuboptimal actions via multi-step reasoning and environment validation, while\nthe Critic performs temporal credit assignment through trajectory-level reward\nshaping. Concurrently, RL agent enhances LLMs' task-specific decision-making\nwith high-quality fine-tuning datasets generated via prioritized experience\nreplay. Through extensive experiments across multiple power grid operation\nchallenges with action spaces exceeding 60K discrete actions, ACE demonstrates\nsuperior performance over existing RL methods and LLM-based methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.02522v1",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Wan2025ThinkTA,\n author = {Xu Wan and Wenyue Xu and Chao Yang and Mingyang Sun},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making},\n volume = {abs/2506.02522},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/0904.0546v1",
    "title": "Eligibility Propagation to Speed up Time Hopping for Reinforcement\n  Learning",
    "published": "2009-04-03T10:42:28Z",
    "updated": "2009-04-03T10:42:28Z",
    "authors": [
      "Petar Kormushev",
      "Kohei Nomoto",
      "Fangyan Dong",
      "Kaoru Hirota"
    ],
    "summary": "A mechanism called Eligibility Propagation is proposed to speed up the Time\nHopping technique used for faster Reinforcement Learning in simulations.\nEligibility Propagation provides for Time Hopping similar abilities to what\neligibility traces provide for conventional Reinforcement Learning. It\npropagates values from one state to all of its temporal predecessors using a\nstate transitions graph. Experiments on a simulated biped crawling robot\nconfirm that Eligibility Propagation accelerates the learning process more than\n3 times.",
    "pdf_url": "http://arxiv.org/pdf/0904.0546v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Kormushev2009EligibilityPT,\n author = {Petar Kormushev and K. Nomoto and F. Dong and K. Hirota},\n booktitle = {Journal of Advanced Computational Intelligence and Intelligent Informatics},\n journal = {J. Adv. Comput. Intell. Intell. Informatics},\n pages = {600-607},\n title = {Eligibility Propagation to Speed up Time Hopping for Reinforcement Learning},\n volume = {13},\n year = {2009}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2510.00194v1",
    "title": "GRPO-$λ$: Credit Assignment improves LLM Reasoning",
    "published": "2025-09-30T19:11:10Z",
    "updated": "2025-09-30T19:11:10Z",
    "authors": [
      "Prasanna Parthasarathi",
      "Mathieu Reymond",
      "Boxing Chen",
      "Yufei Cui",
      "Sarath Chandar"
    ],
    "summary": "Large language models (LLMs) are increasingly deployed for tasks requiring\ncomplex reasoning, prompting significant interest in improving their reasoning\nabilities through post-training. Especially RL based methods using verifiable\nreward, like the state-of-the-art GRPO, have shown to tremendously improve\nreasoning behaviors when applied as post-training methods. However, the lack of\nan explicit reward or critic model limits GRPO's ability to assign fine-grained\ncredit across token sequences. In this work, we present GRPO-$\\lambda$, a novel\nextension to GRPO that enhances credit assignment in RL finetuning of LLMs for\ncomplex reasoning tasks. We approximate learning from $\\lambda$-return with a\nreformulation of eligibility traces using token-level log-probabilities applied\nafter each sequence generation, and a novel critic-free approximation of the\ntemporal-difference error. We introduce a few variations for the weighting of\nthe $\\lambda$-return, and their applications to the eligibility-trace, where\nall the variations provide significant gains over GRPO. We compare\nGRPO-$\\lambda$ against GRPO by training models from 1.5B to 7B parameters on\n$4$ different math reasoning datasets. The training plots demonstrate 30-40%\nimproved performance during RL training on both LLaMA-3.1 and Qwen-2.5\narchitectures. Finally, we show that with GRPO-$\\lambda$, the resulting average\nperformance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves\nover GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.",
    "pdf_url": "http://arxiv.org/pdf/2510.00194v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Parthasarathi2025GRPOlambdaCA,\n author = {Prasanna Parthasarathi and Mathieu Reymond and Boxing Chen and Yufei Cui and Sarath Chandar},\n title = {GRPO-$\\lambda$: Credit Assignment improves LLM Reasoning},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2206.00303v3",
    "title": "Predecessor Features",
    "published": "2022-06-01T08:05:59Z",
    "updated": "2022-07-25T07:51:14Z",
    "authors": [
      "Duncan Bailey",
      "Marcelo G. Mattar"
    ],
    "summary": "Any reinforcement learning system must be able to identify which past events\ncontributed to observed outcomes, a problem known as credit assignment. A\ncommon solution to this problem is to use an eligibility trace to assign credit\nto recency-weighted set of experienced events. However, in many realistic\ntasks, the set of recently experienced events are only one of the many possible\naction events that could have preceded the current outcome. This suggests that\nreinforcement learning can be made more efficient by allowing credit assignment\nto any viable preceding state, rather than only those most recently\nexperienced. Accordingly, we examine ``Predecessor Features'', the fully\nbootstrapped version of van Hasselt's ``Expected Trace'', an algorithm that\nachieves this richer form of credit assignment. By maintaining a representation\nthat approximates the expected sum of past occupancies, this algorithm allows\ntemporal difference (TD) errors to be propagated accurately to a larger number\nof predecessor states than conventional methods, greatly improving learning\nspeed. The algorithm can also be naturally extended from tabular state\nrepresentation to feature representations allowing for increased performance on\na wide range of environments. We demonstrate several use cases for Predecessor\nFeatures and compare its performance with other approaches.",
    "pdf_url": "http://arxiv.org/pdf/2206.00303v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to RLDM 2022",
    "journal_ref": null,
    "citation_count": 7,
    "bibtex": "@Article{Bailey2022PredecessorF,\n author = {D. Bailey},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Predecessor Features},\n volume = {abs/2206.00303},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2007.01839v2",
    "title": "Expected Eligibility Traces",
    "published": "2020-07-03T17:46:16Z",
    "updated": "2021-02-08T13:02:30Z",
    "authors": [
      "Hado van Hasselt",
      "Sephora Madjiheurem",
      "Matteo Hessel",
      "David Silver",
      "André Barreto",
      "Diana Borsa"
    ],
    "summary": "The question of how to determine which states and actions are responsible for\na certain outcome is known as the credit assignment problem and remains a\ncentral research question in reinforcement learning and artificial\nintelligence. Eligibility traces enable efficient credit assignment to the\nrecent sequence of states and actions experienced by the agent, but not to\ncounterfactual sequences that could also have led to the current state. In this\nwork, we introduce expected eligibility traces. Expected traces allow, with a\nsingle update, to update states and actions that could have preceded the\ncurrent state, even if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous) traces in\ntemporal-difference learning, and show that sometimes substantial improvements\ncan be attained. We provide a way to smoothly interpolate between instantaneous\nand expected traces by a mechanism similar to bootstrapping, which ensures that\nthe resulting algorithm is a strict generalisation of TD($\\lambda$). Finally,\nwe discuss possible extensions and connections to related ideas, such as\nsuccessor features.",
    "pdf_url": "http://arxiv.org/pdf/2007.01839v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI, distinguished paper award",
    "journal_ref": null,
    "citation_count": 39,
    "bibtex": "@Article{Hasselt2020ExpectedET,\n author = {H. V. Hasselt and Sephora Madjiheurem and Matteo Hessel and David Silver and André Barreto and Diana Borsa},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Expected Eligibility Traces},\n volume = {abs/2007.01839},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2507.09087v2",
    "title": "Deep Reinforcement Learning with Gradient Eligibility Traces",
    "published": "2025-07-12T00:12:05Z",
    "updated": "2025-09-18T18:17:44Z",
    "authors": [
      "Esraa Elelimy",
      "Brett Daley",
      "Andrew Patterson",
      "Marlos C. Machado",
      "Adam White",
      "Martha White"
    ],
    "summary": "Achieving fast and stable off-policy learning in deep reinforcement learning\n(RL) is challenging. Most existing methods rely on semi-gradient\ntemporal-difference (TD) methods for their simplicity and efficiency, but are\nconsequently susceptible to divergence. While more principled approaches like\nGradient TD (GTD) methods have strong convergence guarantees, they have rarely\nbeen used in deep RL. Recent work introduced the generalized Projected Bellman\nError ($\\overline{\\text{PBE}}$), enabling GTD methods to work efficiently with\nnonlinear function approximation. However, this work is limited to one-step\nmethods, which are slow at credit assignment and require a large number of\nsamples. In this paper, we extend the generalized $\\overline{\\text{PBE}}$\nobjective to support multistep credit assignment based on the $\\lambda$-return\nand derive three gradient-based methods that optimize this new objective. We\nprovide both a forward-view formulation compatible with experience replay and a\nbackward-view formulation compatible with streaming algorithms. Finally, we\nevaluate the proposed algorithms and show that they outperform both PPO and\nStreamQ in MuJoCo and MinAtar environments, respectively. Code available at\nhttps://github.com/esraaelelimy/gtd\\_algos",
    "pdf_url": "http://arxiv.org/pdf/2507.09087v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Reinforcement Learning Journal, 2025",
    "citation_count": 1,
    "bibtex": "@Article{Elelimy2025DeepRL,\n author = {Esraa Elelimy and Brett Daley and Andrew Patterson and Marlos C. Machado and Adam White and Martha White},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Reinforcement Learning with Gradient Eligibility Traces},\n volume = {abs/2507.09087},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2501.03142v1",
    "title": "Co-Activation Graph Analysis of Safety-Verified and Explainable Deep\n  Reinforcement Learning Policies",
    "published": "2025-01-06T17:07:44Z",
    "updated": "2025-01-06T17:07:44Z",
    "authors": [
      "Dennis Gross",
      "Helge Spieker"
    ],
    "summary": "Deep reinforcement learning (RL) policies can demonstrate unsafe behaviors\nand are challenging to interpret. To address these challenges, we combine RL\npolicy model checking--a technique for determining whether RL policies exhibit\nunsafe behaviors--with co-activation graph analysis--a method that maps neural\nnetwork inner workings by analyzing neuron activation patterns--to gain insight\ninto the safe RL policy's sequential decision-making. This combination lets us\ninterpret the RL policy's inner workings for safe decision-making. We\ndemonstrate its applicability in various experiments.",
    "pdf_url": "http://arxiv.org/pdf/2501.03142v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Gross2025CoActivationGA,\n author = {Dennis Gross and Helge Spieker},\n booktitle = {International Conference on Agents and Artificial Intelligence},\n pages = {611-621},\n title = {Co-Activation Graph Analysis of Safety-Verified and Explainable Deep Reinforcement Learning Policies},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2501.19133v1",
    "title": "Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning",
    "published": "2025-01-31T13:38:57Z",
    "updated": "2025-01-31T13:38:57Z",
    "authors": [
      "Burcu Küçükoğlu",
      "Sander Dalm",
      "Marcel van Gerven"
    ],
    "summary": "The effectiveness of credit assignment in reinforcement learning (RL) when\ndealing with high-dimensional data is influenced by the success of\nrepresentation learning via deep neural networks, and has implications for the\nsample efficiency of deep RL algorithms. Input decorrelation has been\npreviously introduced as a method to speed up optimization in neural networks,\nand has proven impactful in both efficient deep learning and as a method for\neffective representation learning for deep RL algorithms. We propose a novel\napproach to online decorrelation in deep RL based on the decorrelated\nbackpropagation algorithm that seamlessly integrates the decorrelation process\ninto the RL training pipeline. Decorrelation matrices are added to each layer,\nwhich are updated using a separate decorrelation learning rule that minimizes\nthe total decorrelation loss across all layers, in parallel to minimizing the\nusual RL loss. We used our approach in combination with the soft actor-critic\n(SAC) method, which we refer to as decorrelated soft actor-critic (DSAC).\nExperiments on the Atari 100k benchmark with DSAC shows, compared to the\nregular SAC baseline, faster training in five out of the seven games tested and\nimproved reward performance in two games with around 50% reduction in\nwall-clock time, while maintaining performance levels on the other games. These\nresults demonstrate the positive impact of network-wide decorrelation in deep\nRL for speeding up its sample efficiency through more effective credit\nassignment.",
    "pdf_url": "http://arxiv.org/pdf/2501.19133v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Küçükoğlu2025DecorrelatedSA,\n author = {Burcu Küçükoğlu and Sander Dalm and M. Gerven},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning},\n volume = {abs/2501.19133},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2509.13053v2",
    "title": "Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning\n  in Spiking Neural Networks",
    "published": "2025-09-16T13:11:52Z",
    "updated": "2025-10-17T12:31:33Z",
    "authors": [
      "Lorenzo Pes",
      "Bojian Yin",
      "Sander Stuijk",
      "Federico Corradi"
    ],
    "summary": "Spiking Neural Networks (SNNs) provide an efficient framework for processing\ndynamic spatio-temporal signals and for investigating the learning principles\nunderlying biological neural systems. A key challenge in training SNNs is to\nsolve both spatial and temporal credit assignment. The dominant approach for\ntraining SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.\nHowever, BPTT is in stark contrast with the spatial and temporal locality\nobserved in biological neural systems and leads to high computational and\nmemory demands, limiting efficient training strategies and on-device learning.\nAlthough existing local learning rules achieve local temporal credit assignment\nby leveraging eligibility traces, they fail to address the spatial credit\nassignment without resorting to auxiliary layer-wise matrices, which increase\nmemory overhead and hinder scalability, especially on embedded devices. In this\nwork, we propose Traces Propagation (TP), a forward-only, memory-efficient,\nscalable, and fully local learning rule that combines eligibility traces with a\nlayer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP\noutperforms other fully local learning rules on NMNIST and SHD datasets. On\nmore complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases\ncompetitive performance and scales effectively to deeper SNN architectures such\nas VGG-9, while providing favorable memory scaling compared to prior fully\nlocal scalable rules, for datasets with a significant number of classes.\nFinally, we show that TP is well suited for practical fine-tuning tasks, such\nas keyword spotting on the Google Speech Commands dataset, thus paving the way\nfor efficient learning at the edge.",
    "pdf_url": "http://arxiv.org/pdf/2509.13053v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Pes2025TracesPM,\n author = {Lorenzo Pes and Bojian Yin and S. Stuijk and Federico Corradi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks},\n volume = {abs/2509.13053},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1810.06784v4",
    "title": "ProMP: Proximal Meta-Policy Search",
    "published": "2018-10-16T01:43:51Z",
    "updated": "2022-02-11T12:46:43Z",
    "authors": [
      "Jonas Rothfuss",
      "Dennis Lee",
      "Ignasi Clavera",
      "Tamim Asfour",
      "Pieter Abbeel"
    ],
    "summary": "Credit assignment in Meta-reinforcement learning (Meta-RL) is still poorly\nunderstood. Existing methods either neglect credit assignment to pre-adaptation\nbehavior or implement it naively. This leads to poor sample-efficiency during\nmeta-training as well as ineffective task identification strategies. This paper\nprovides a theoretical analysis of credit assignment in gradient-based Meta-RL.\nBuilding on the gained insights we develop a novel meta-learning algorithm that\novercomes both the issue of poor credit assignment and previous difficulties in\nestimating meta-policy gradients. By controlling the statistical distance of\nboth pre-adaptation and adapted policies during meta-policy search, the\nproposed algorithm endows efficient and stable meta-learning. Our approach\nleads to superior pre-adaptation policy behavior and consistently outperforms\nprevious Meta-RL algorithms in sample-efficiency, wall-clock time, and\nasymptotic performance.",
    "pdf_url": "http://arxiv.org/pdf/1810.06784v4",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "The first three authors contributed equally. Published at ICLR 2019",
    "journal_ref": null,
    "citation_count": 212,
    "bibtex": "@Article{Rothfuss2018ProMPPM,\n author = {Jonas Rothfuss and Dennis Lee and I. Clavera and T. Asfour and P. Abbeel},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {ProMP: Proximal Meta-Policy Search},\n volume = {abs/1810.06784},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1910.02532v1",
    "title": "Probabilistic Successor Representations with Kalman Temporal Differences",
    "published": "2019-10-06T21:32:46Z",
    "updated": "2019-10-06T21:32:46Z",
    "authors": [
      "Jesse P. Geerts",
      "Kimberly L. Stachenfeld",
      "Neil Burgess"
    ],
    "summary": "The effectiveness of Reinforcement Learning (RL) depends on an animal's\nability to assign credit for rewards to the appropriate preceding stimuli. One\naspect of understanding the neural underpinnings of this process involves\nunderstanding what sorts of stimulus representations support generalisation.\nThe Successor Representation (SR), which enforces generalisation over states\nthat predict similar outcomes, has become an increasingly popular model in this\nspace of inquiries. Another dimension of credit assignment involves\nunderstanding how animals handle uncertainty about learned associations, using\nprobabilistic methods such as Kalman Temporal Differences (KTD). Combining\nthese approaches, we propose using KTD to estimate a distribution over the SR.\nKTD-SR captures uncertainty about the estimated SR as well as covariances\nbetween different long-term predictions. We show that because of this, KTD-SR\nexhibits partial transition revaluation as humans do in this experiment without\nadditional replay, unlike the standard TD-SR algorithm. We conclude by\ndiscussing future applications of the KTD-SR as a model of the interaction\nbetween predictive and probabilistic animal reasoning.",
    "pdf_url": "http://arxiv.org/pdf/1910.02532v1",
    "doi": "10.32470/CCN.2019.1323-0",
    "categories": [
      "q-bio.NC",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "Conference on Cognitive Computational Neuroscience",
    "journal_ref": null,
    "citation_count": 13,
    "bibtex": "@Article{Geerts2019ProbabilisticSR,\n author = {J. Geerts and Kimberly L. Stachenfeld and N. Burgess},\n booktitle = {2019 Conference on Cognitive Computational Neuroscience},\n journal = {ArXiv},\n title = {Probabilistic Successor Representations with Kalman Temporal Differences},\n volume = {abs/1910.02532},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1702.08887v3",
    "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement\n  Learning",
    "published": "2017-02-28T17:56:41Z",
    "updated": "2018-05-21T08:24:02Z",
    "authors": [
      "Jakob Foerster",
      "Nantas Nardelli",
      "Gregory Farquhar",
      "Triantafyllos Afouras",
      "Philip H. S. Torr",
      "Pushmeet Kohli",
      "Shimon Whiteson"
    ],
    "summary": "Many real-world problems, such as network packet routing and urban traffic\ncontrol, are naturally modeled as multi-agent reinforcement learning (RL)\nproblems. However, existing multi-agent RL methods typically scale poorly in\nthe problem size. Therefore, a key challenge is to translate the success of\ndeep learning on single-agent RL to the multi-agent setting. A major stumbling\nblock is that independent Q-learning, the most popular multi-agent RL method,\nintroduces nonstationarity that makes it incompatible with the experience\nreplay memory on which deep Q-learning relies. This paper proposes two methods\nthat address this problem: 1) using a multi-agent variant of importance\nsampling to naturally decay obsolete data and 2) conditioning each agent's\nvalue function on a fingerprint that disambiguates the age of the data sampled\nfrom the replay memory. Results on a challenging decentralised variant of\nStarCraft unit micromanagement confirm that these methods enable the successful\ncombination of experience replay with multi-agent RL.",
    "pdf_url": "http://arxiv.org/pdf/1702.08887v3",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Camera-ready version, International Conference of Machine Learning\n  2017; updated to fix print-breaking image",
    "journal_ref": null,
    "citation_count": 620,
    "bibtex": "@Article{Foerster2017StabilisingER,\n author = {Jakob N. Foerster and Nantas Nardelli and Gregory Farquhar and Triantafyllos Afouras and Philip H. S. Torr and Pushmeet Kohli and S. Whiteson},\n booktitle = {International Conference on Machine Learning},\n pages = {1146-1155},\n title = {Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning},\n year = {2017}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2501.15971v1",
    "title": "REINFORCE-ING Chemical Language Models in Drug Design",
    "published": "2025-01-27T11:30:45Z",
    "updated": "2025-01-27T11:30:45Z",
    "authors": [
      "Morgan Thomas",
      "Albert Bou",
      "Gianni De Fabritiis"
    ],
    "summary": "Chemical language models, combined with reinforcement learning, have shown\nsignificant promise to efficiently traverse large chemical spaces in drug\ndesign. However, the performance of various RL algorithms and their best\npractices for practical drug design are still unclear. Here, starting from the\nprinciples of the REINFORCE algorithm, we investigate the effect of different\ncomponents from RL theory including experience replay, hill-climbing, baselines\nto reduce variance, and alternative reward shaping. Additionally we demonstrate\nhow RL hyperparameters can be fine-tuned for effectiveness, efficiency, or\nchemical regularization as demonstrated using the MolOpt benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2501.15971v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Thomas2025REINFORCEINGCL,\n author = {Morgan Thomas and Albert Bou and G. D. Fabritiis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {REINFORCE-ING Chemical Language Models in Drug Design},\n volume = {abs/2501.15971},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1904.11439v6",
    "title": "META-Learning State-based Eligibility Traces for More Sample-Efficient\n  Policy Evaluation",
    "published": "2019-04-25T16:32:21Z",
    "updated": "2020-05-16T18:15:11Z",
    "authors": [
      "Mingde Zhao",
      "Sitao Luan",
      "Ian Porada",
      "Xiao-Wen Chang",
      "Doina Precup"
    ],
    "summary": "Temporal-Difference (TD) learning is a standard and very successful\nreinforcement learning approach, at the core of both algorithms that learn the\nvalue of a given policy, as well as algorithms which learn how to improve\npolicies. TD-learning with eligibility traces provides a way to boost sample\nefficiency by temporal credit assignment, i.e. deciding which portion of a\nreward should be assigned to predecessor states that occurred at different\nprevious times, controlled by a parameter $\\lambda$. However, tuning this\nparameter can be time-consuming, and not tuning it can lead to inefficient\nlearning. For better sample efficiency of TD-learning, we propose a\nmeta-learning method for adjusting the eligibility trace parameter, in a\nstate-dependent manner. The adaptation is achieved with the help of auxiliary\nlearners that learn distributional information about the update targets online,\nincurring roughly the same computational complexity per step as the usual value\nlearner. Our approach can be used both in on-policy and off-policy learning. We\nprove that, under some assumptions, the proposed method improves the overall\nquality of the update targets, by minimizing the overall target error. This\nmethod can be viewed as a plugin to assist prediction with function\napproximation by meta-learning feature (observation)-based $\\lambda$ online, or\neven in the control case to assist policy improvement. Our empirical evaluation\ndemonstrates significant performance improvements, as well as improved\nrobustness of the proposed algorithm to learning rate variation.",
    "pdf_url": "http://arxiv.org/pdf/1904.11439v6",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAMAS 2020",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Zhao2019FasterAM,\n author = {Mingde Zhao and Ian Porada and Sitao Luan and X. Chang and Doina Precup},\n title = {Faster and More Accurate Trace-based Policy Evaluation via Overall Target Error Meta-Optimization},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2006.08906v1",
    "title": "META-Learning Eligibility Traces for More Sample Efficient Temporal\n  Difference Learning",
    "published": "2020-06-16T03:41:07Z",
    "updated": "2020-06-16T03:41:07Z",
    "authors": [
      "Mingde Zhao"
    ],
    "summary": "Temporal-Difference (TD) learning is a standard and very successful\nreinforcement learning approach, at the core of both algorithms that learn the\nvalue of a given policy, as well as algorithms which learn how to improve\npolicies. TD-learning with eligibility traces provides a way to do temporal\ncredit assignment, i.e. decide which portion of a reward should be assigned to\npredecessor states that occurred at different previous times, controlled by a\nparameter $\\lambda$. However, tuning this parameter can be time-consuming, and\nnot tuning it can lead to inefficient learning. To improve the sample\nefficiency of TD-learning, we propose a meta-learning method for adjusting the\neligibility trace parameter, in a state-dependent manner. The adaptation is\nachieved with the help of auxiliary learners that learn distributional\ninformation about the update targets online, incurring roughly the same\ncomputational complexity per step as the usual value learner. Our approach can\nbe used both in on-policy and off-policy learning. We prove that, under some\nassumptions, the proposed method improves the overall quality of the update\ntargets, by minimizing the overall target error. This method can be viewed as a\nplugin which can also be used to assist prediction with function approximation\nby meta-learning feature (observation)-based $\\lambda$ online, or even in the\ncontrol case to assist policy improvement. Our empirical evaluation\ndemonstrates significant performance improvements, as well as improved\nrobustness of the proposed algorithm to learning rate variation.",
    "pdf_url": "http://arxiv.org/pdf/2006.08906v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "A thesis submitted to McGill University in partial fulfillment of the\n  requirements of the degree of Master of Computer Science",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Zhao2020METALearningET,\n author = {Mingde Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {META-Learning Eligibility Traces for More Sample Efficient Temporal Difference Learning},\n volume = {abs/2006.08906},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2303.04150v4",
    "title": "Evolutionary Reinforcement Learning: A Survey",
    "published": "2023-03-07T01:38:42Z",
    "updated": "2023-08-30T01:47:53Z",
    "authors": [
      "Hui Bai",
      "Ran Cheng",
      "Yaochu Jin"
    ],
    "summary": "Reinforcement learning (RL) is a machine learning approach that trains agents\nto maximize cumulative rewards through interactions with environments. The\nintegration of RL with deep learning has recently resulted in impressive\nachievements in a wide range of challenging tasks, including board games,\narcade games, and robot control. Despite these successes, there remain several\ncrucial challenges, including brittle convergence properties caused by\nsensitive hyperparameters, difficulties in temporal credit assignment with long\ntime horizons and sparse rewards, a lack of diverse exploration, especially in\ncontinuous search space scenarios, difficulties in credit assignment in\nmulti-agent reinforcement learning, and conflicting objectives for rewards.\nEvolutionary computation (EC), which maintains a population of learning agents,\nhas demonstrated promising performance in addressing these limitations. This\narticle presents a comprehensive survey of state-of-the-art methods for\nintegrating EC into RL, referred to as evolutionary reinforcement learning\n(EvoRL). We categorize EvoRL methods according to key research fields in RL,\nincluding hyperparameter optimization, policy search, exploration, reward\nshaping, meta-RL, and multi-objective RL. We then discuss future research\ndirections in terms of efficient methods, benchmarks, and scalable platforms.\nThis survey serves as a resource for researchers and practitioners interested\nin the field of EvoRL, highlighting the important challenges and opportunities\nfor future research. With the help of this survey, researchers and\npractitioners can develop more efficient methods and tailored benchmarks for\nEvoRL, further advancing this promising cross-disciplinary research field.",
    "pdf_url": "http://arxiv.org/pdf/2303.04150v4",
    "doi": "10.34133/icomputing.0025",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": null,
    "journal_ref": "INTELLIGENT COMPUTING, 21 Apr 2023",
    "citation_count": 67,
    "bibtex": "@Article{Bai2023EvolutionaryRL,\n author = {Hui Bai and Ran Cheng and Yaochu Jin},\n booktitle = {Intelligent Computing},\n journal = {ArXiv},\n title = {Evolutionary Reinforcement Learning: A Survey},\n volume = {abs/2303.04150},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2103.13861v1",
    "title": "Hierarchical Program-Triggered Reinforcement Learning Agents For\n  Automated Driving",
    "published": "2021-03-25T14:19:54Z",
    "updated": "2021-03-25T14:19:54Z",
    "authors": [
      "Briti Gangopadhyay",
      "Harshit Soora",
      "Pallab Dasgupta"
    ],
    "summary": "Recent advances in Reinforcement Learning (RL) combined with Deep Learning\n(DL) have demonstrated impressive performance in complex tasks, including\nautonomous driving. The use of RL agents in autonomous driving leads to a\nsmooth human-like driving experience, but the limited interpretability of Deep\nReinforcement Learning (DRL) creates a verification and certification\nbottleneck. Instead of relying on RL agents to learn complex tasks, we propose\nHPRL - Hierarchical Program-triggered Reinforcement Learning, which uses a\nhierarchy consisting of a structured program along with multiple RL agents,\neach trained to perform a relatively simple task. The focus of verification\nshifts to the master program under simple guarantees from the RL agents,\nleading to a significantly more interpretable and verifiable implementation as\ncompared to a complex RL agent. The evaluation of the framework is demonstrated\non different driving tasks, and NHTSA precrash scenarios using CARLA, an\nopen-source dynamic urban simulation environment.",
    "pdf_url": "http://arxiv.org/pdf/2103.13861v1",
    "doi": "10.1109/TITS.2021.3096998",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "The paper is under consideration in Transactions on Intelligent\n  Transportation Systems",
    "journal_ref": null,
    "citation_count": 37,
    "bibtex": "@Article{Gangopadhyay2021HierarchicalPR,\n author = {Briti Gangopadhyay and Harshit Soora and P. Dasgupta},\n booktitle = {IEEE transactions on intelligent transportation systems (Print)},\n journal = {IEEE Transactions on Intelligent Transportation Systems},\n pages = {10902-10911},\n title = {Hierarchical Program-Triggered Reinforcement Learning Agents for Automated Driving},\n volume = {23},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2311.07558v2",
    "title": "Data-Efficient Task Generalization via Probabilistic Model-based Meta\n  Reinforcement Learning",
    "published": "2023-11-13T18:51:57Z",
    "updated": "2024-02-06T20:41:01Z",
    "authors": [
      "Arjun Bhardwaj",
      "Jonas Rothfuss",
      "Bhavya Sukhija",
      "Yarden As",
      "Marco Hutter",
      "Stelian Coros",
      "Andreas Krause"
    ],
    "summary": "We introduce PACOH-RL, a novel model-based Meta-Reinforcement Learning\n(Meta-RL) algorithm designed to efficiently adapt control policies to changing\ndynamics. PACOH-RL meta-learns priors for the dynamics model, allowing swift\nadaptation to new dynamics with minimal interaction data. Existing Meta-RL\nmethods require abundant meta-learning data, limiting their applicability in\nsettings such as robotics, where data is costly to obtain. To address this,\nPACOH-RL incorporates regularization and epistemic uncertainty quantification\nin both the meta-learning and task adaptation stages. When facing new dynamics,\nwe use these uncertainty estimates to effectively guide exploration and data\ncollection. Overall, this enables positive transfer, even when access to data\nfrom prior tasks or dynamic settings is severely limited. Our experiment\nresults demonstrate that PACOH-RL outperforms model-based RL and model-based\nMeta-RL baselines in adapting to new dynamic conditions. Finally, on a real\nrobotic car, we showcase the potential for efficient RL policy adaptation in\ndiverse, data-scarce conditions.",
    "pdf_url": "http://arxiv.org/pdf/2311.07558v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Article{Bhardwaj2023DataEfficientTG,\n author = {Arjun Bhardwaj and Jonas Rothfuss and Bhavya Sukhija and Yarden As and Marco Hutter and Stelian Coros and Andreas Krause},\n booktitle = {IEEE Robotics and Automation Letters},\n journal = {IEEE Robotics and Automation Letters},\n pages = {3918-3925},\n title = {Data-Efficient Task Generalization via Probabilistic Model-Based Meta Reinforcement Learning},\n volume = {9},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2301.11321v2",
    "title": "Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement\n  Learning",
    "published": "2023-01-26T18:57:41Z",
    "updated": "2023-05-31T05:13:15Z",
    "authors": [
      "Brett Daley",
      "Martha White",
      "Christopher Amato",
      "Marlos C. Machado"
    ],
    "summary": "Off-policy learning from multistep returns is crucial for sample-efficient\nreinforcement learning, but counteracting off-policy bias without exacerbating\nvariance is challenging. Classically, off-policy bias is corrected in a\nper-decision manner: past temporal-difference errors are re-weighted by the\ninstantaneous Importance Sampling (IS) ratio after each action via eligibility\ntraces. Many off-policy algorithms rely on this mechanism, along with differing\nprotocols for cutting the IS ratios to combat the variance of the IS estimator.\nUnfortunately, once a trace has been fully cut, the effect cannot be reversed.\nThis has led to the development of credit-assignment strategies that account\nfor multiple past experiences at a time. These trajectory-aware methods have\nnot been extensively analyzed, and their theoretical justification remains\nuncertain. In this paper, we propose a multistep operator that can express both\nper-decision and trajectory-aware methods. We prove convergence conditions for\nour operator in the tabular setting, establishing the first guarantees for\nseveral existing methods as well as many new ones. Finally, we introduce\nRecency-Bounded Importance Sampling (RBIS), which leverages trajectory\nawareness to perform robustly across $\\lambda$-values in an off-policy control\ntask.",
    "pdf_url": "http://arxiv.org/pdf/2301.11321v2",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2023. 8 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2112.12281",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Daley2023TrajectoryAwareET,\n author = {Brett Daley and Martha White and Chris Amato and Marlos C. Machado},\n booktitle = {International Conference on Machine Learning},\n pages = {6818-6835},\n title = {Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2312.01072v2",
    "title": "A Survey of Temporal Credit Assignment in Deep Reinforcement Learning",
    "published": "2023-12-02T08:49:51Z",
    "updated": "2024-07-04T09:32:18Z",
    "authors": [
      "Eduardo Pignatelli",
      "Johan Ferret",
      "Matthieu Geist",
      "Thomas Mesnard",
      "Hado van Hasselt",
      "Olivier Pietquin",
      "Laura Toni"
    ],
    "summary": "The Credit Assignment Problem (CAP) refers to the longstanding challenge of\nReinforcement Learning (RL) agents to associate actions with their long-term\nconsequences. Solving the CAP is a crucial step towards the successful\ndeployment of RL in the real world since most decision problems provide\nfeedback that is noisy, delayed, and with little or no information about the\ncauses. These conditions make it hard to distinguish serendipitous outcomes\nfrom those caused by informed decision-making. However, the mathematical nature\nof credit and the CAP remains poorly understood and defined. In this survey, we\nreview the state of the art of Temporal Credit Assignment (CA) in deep RL. We\npropose a unifying formalism for credit that enables equitable comparisons of\nstate-of-the-art algorithms and improves our understanding of the trade-offs\nbetween the various methods. We cast the CAP as the problem of learning the\ninfluence of an action over an outcome from a finite amount of experience. We\ndiscuss the challenges posed by delayed effects, transpositions, and a lack of\naction influence, and analyse how existing methods aim to address them.\nFinally, we survey the protocols to evaluate a credit assignment method and\nsuggest ways to diagnose the sources of struggle for different methods.\nOverall, this survey provides an overview of the field for new-entry\npractitioners and researchers, it offers a coherent perspective for scholars\nlooking to expedite the starting stages of a new study on the CAP, and it\nsuggests potential directions for future research.",
    "pdf_url": "http://arxiv.org/pdf/2312.01072v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "56 pages, 2 figures, 4 tables",
    "journal_ref": null,
    "citation_count": 29,
    "bibtex": "@Article{Pignatelli2023ASO,\n author = {Eduardo Pignatelli and Johan Ferret and Matthieu Geist and Thomas Mesnard and Hado van Hasselt and Laura Toni},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey of Temporal Credit Assignment in Deep Reinforcement Learning},\n volume = {abs/2312.01072},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2509.16513v1",
    "title": "Trace Replay Simulation of MIT SuperCloud for Studying Optimal\n  Sustainability Policies",
    "published": "2025-09-20T03:20:37Z",
    "updated": "2025-09-20T03:20:37Z",
    "authors": [
      "Wesley Brewer",
      "Matthias Maiterth",
      "Damien Fay"
    ],
    "summary": "The rapid growth of AI supercomputing is creating unprecedented power\ndemands, with next-generation GPU datacenters requiring hundreds of megawatts\nand producing fast, large swings in consumption. To address the resulting\nchallenges for utilities and system operators, we extend ExaDigiT, an\nopen-source digital twin framework for modeling power, cooling, and scheduling\nof supercomputers. Originally developed for replaying traces from\nleadership-class HPC systems, ExaDigiT now incorporates heterogeneity,\nmulti-tenancy, and cloud-scale workloads. In this work, we focus on trace\nreplay and rescheduling of jobs on the MIT SuperCloud TX-GAIA system to enable\nreinforcement learning (RL)-based experimentation with sustainability policies.\nThe RAPS module provides a simulation environment with detailed power and\nperformance statistics, supporting the study of scheduling strategies,\nincentive structures, and hardware/software prototyping. Preliminary RL\nexperiments using Proximal Policy Optimization demonstrate the feasibility of\nlearning energy-aware scheduling decisions, highlighting ExaDigiT's potential\nas a platform for exploring optimal policies to improve throughput, efficiency,\nand sustainability.",
    "pdf_url": "http://arxiv.org/pdf/2509.16513v1",
    "doi": null,
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC",
    "comment": "2 pages, 2 figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Brewer2025TraceRS,\n author = {Wesley Brewer and Matthias Maiterth and Damien Fay},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Trace Replay Simulation of MIT SuperCloud for Studying Optimal Sustainability Policies},\n volume = {abs/2509.16513},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2212.07632v2",
    "title": "Reinforcement Learning in Credit Scoring and Underwriting",
    "published": "2022-12-15T06:36:14Z",
    "updated": "2024-06-27T02:32:42Z",
    "authors": [
      "Seksan Kiatsupaibul",
      "Pakawan Chansiripas",
      "Pojtanut Manopanjasiri",
      "Kantapong Visantavarakul",
      "Zheng Wen"
    ],
    "summary": "This paper proposes a novel reinforcement learning (RL) framework for credit\nunderwriting that tackles ungeneralizable contextual challenges. We adapt RL\nprinciples for credit scoring, incorporating action space renewal and\nmulti-choice actions. Our work demonstrates that the traditional underwriting\napproach aligns with the RL greedy strategy. We introduce two new RL-based\ncredit underwriting algorithms to enable more informed decision-making.\nSimulations show these new approaches outperform the traditional method in\nscenarios where the data aligns with the model. However, complex situations\nhighlight model limitations, emphasizing the importance of powerful machine\nlearning models for optimal performance. Future research directions include\nexploring more sophisticated models alongside efficient exploration mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2212.07632v2",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Kiatsupaibul2022ReinforcementLI,\n author = {S. Kiatsupaibul and Pakawan Chansiripas and Pojtanut Manopanjasiri and Kantapong Visantavarakul and Zheng Wen},\n title = {Reinforcement Learning in Credit Scoring and Underwriting},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2304.00803v1",
    "title": "A Tutorial Introduction to Reinforcement Learning",
    "published": "2023-04-03T08:50:58Z",
    "updated": "2023-04-03T08:50:58Z",
    "authors": [
      "Mathukumalli Vidyasagar"
    ],
    "summary": "In this paper, we present a brief survey of Reinforcement Learning (RL), with\nparticular emphasis on Stochastic Approximation (SA) as a unifying theme. The\nscope of the paper includes Markov Reward Processes, Markov Decision Processes,\nStochastic Approximation algorithms, and widely used algorithms such as\nTemporal Difference Learning and $Q$-learning.",
    "pdf_url": "http://arxiv.org/pdf/2304.00803v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 9,
    "bibtex": "@Article{Vidyasagar2023ATI,\n author = {M. Vidyasagar},\n booktitle = {SICE Journal of Control Measurement and System Integration},\n journal = {SICE Journal of Control, Measurement, and System Integration},\n pages = {172 - 191},\n title = {A tutorial introduction to reinforcement learning},\n volume = {16},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2506.09487v1",
    "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for\n  Long-Term Audio Generation",
    "published": "2025-06-11T07:57:05Z",
    "updated": "2025-06-11T07:57:05Z",
    "authors": [
      "Taesoo Park",
      "Mungwi Jeong",
      "Mingyu Park",
      "Narae Kim",
      "Junyoung Kim",
      "Mujung Kim",
      "Jisang Yoo",
      "Hoyun Lee",
      "Sanghoon Kim",
      "Soonchul Kwon"
    ],
    "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.",
    "pdf_url": "http://arxiv.org/pdf/2506.09487v1",
    "doi": null,
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "eess.AS",
      "I.2.6; H.5.5; I.5.1"
    ],
    "primary_category": "cs.SD",
    "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Park2025BemaGANv2AT,\n author = {Taesoo Park and Mungwi Jeong and Mingyu Park and Narae Kim and Junyoung Kim and Mujung Kim and Jisang Yoo and Hoyun Lee and Sanghoon Kim and Soonchul Kwon},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation},\n volume = {abs/2506.09487},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2403.05192v4",
    "title": "An End-to-End Pipeline Perspective on Video Streaming in Best-Effort\n  Networks: A Survey and Tutorial",
    "published": "2024-03-08T10:14:32Z",
    "updated": "2025-07-15T18:32:35Z",
    "authors": [
      "Leonardo Peroni",
      "Sergey Gorinsky"
    ],
    "summary": "Remaining a dominant force in Internet traffic, video streaming captivates\nend users, service providers, and researchers. This paper takes a pragmatic\napproach to reviewing recent advances in the field by focusing on the prevalent\nstreaming paradigm that involves delivering long-form two-dimensional videos\nover the best-effort Internet with client-side adaptive bitrate (ABR)\nalgorithms and assistance from content delivery networks (CDNs). To enhance\naccessibility, we supplement the survey with tutorial material. Unlike existing\nsurveys that offer fragmented views, our work provides a holistic perspective\non the entire end-to-end streaming pipeline, from video capture by a\ncamera-equipped device to playback by the end user. Our novel perspective\ncovers the ingestion, processing, and distribution stages of the pipeline and\naddresses key challenges such as video compression, upload, transcoding, ABR\nalgorithms, CDN support, and quality of experience. We review over 200 papers\nand classify streaming designs by their problem-solving methodology, whether\nbased on intuition (simple heuristics), theory (formal optimization), or\nmachine learning (generalizable data patterns). The survey further refines\nthese methodology-based categories and characterizes each design by additional\ntraits such as compatible codecs and use of super resolution. We connect the\nreviewed research to real-world applications by discussing the practices of\ncommercial streaming platforms. Finally, the survey highlights prominent\ncurrent trends and outlines future directions in video streaming.",
    "pdf_url": "http://arxiv.org/pdf/2403.05192v4",
    "doi": "10.1145/3742472",
    "categories": [
      "cs.NI",
      "cs.MM"
    ],
    "primary_category": "cs.NI",
    "comment": null,
    "journal_ref": "ACM Comput. Surv. 57, 12, Article 322 (December 2025), 47 pages",
    "citation_count": 6,
    "bibtex": "@Article{Peroni2024AnEP,\n author = {Leonardo Peroni and Sergey Gorinsky},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys},\n title = {An End-to-End Pipeline Perspective on Video Streaming in Best-Effort Networks: A Survey and Tutorial},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2410.08048v1",
    "title": "VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based\n  Verifiers",
    "published": "2024-10-10T15:43:55Z",
    "updated": "2024-10-10T15:43:55Z",
    "authors": [
      "Jianing Qi",
      "Hao Tang",
      "Zhigang Zhu"
    ],
    "summary": "Recent advancements in test time compute, particularly through the use of\nverifier models, have significantly enhanced the reasoning capabilities of\nLarge Language Models (LLMs). This generator-verifier approach closely\nresembles the actor-critic framework in reinforcement learning (RL). However,\ncurrent verifier models in LLMs often rely on supervised fine-tuning without\ntemporal difference learning such as Q-learning. This paper introduces\nVerifierQ, a novel approach that integrates Offline Q-learning into LLM\nverifier models. We address three key challenges in applying Q-learning to\nLLMs: (1) handling utterance-level Markov Decision Processes (MDPs), (2)\nmanaging large action spaces, and (3) mitigating overestimation bias. VerifierQ\nintroduces a modified Bellman update for bounded Q-values, incorporates\nImplicit Q-learning (IQL) for efficient action space management, and integrates\na novel Conservative Q-learning (CQL) formulation for balanced Q-value\nestimation. Our method enables parallel Q-value computation and improving\ntraining efficiency. While recent work has explored RL techniques like MCTS for\ngenerators, VerifierQ is among the first to investigate the verifier (critic)\naspect in LLMs through Q-learning. This integration of RL principles into\nverifier models complements existing advancements in generator techniques,\npotentially enabling more robust and adaptive reasoning in LLMs. Experimental\nresults on mathematical reasoning tasks demonstrate VerifierQ's superior\nperformance compared to traditional supervised fine-tuning approaches, with\nimprovements in efficiency, accuracy and robustness. By enhancing the synergy\nbetween generation and evaluation capabilities, VerifierQ contributes to the\nongoing evolution of AI systems in addressing complex cognitive tasks across\nvarious domains.",
    "pdf_url": "http://arxiv.org/pdf/2410.08048v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Qi2024VerifierQEL,\n author = {Jianing Qi and Hao Tang and Zhigang Zhu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {VerifierQ: Enhancing LLM Test Time Compute with Q-Learning-based Verifiers},\n volume = {abs/2410.08048},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2401.16249v2",
    "title": "Molecular dynamics simulations of heat transport using machine-learned\n  potentials: A mini review and tutorial on GPUMD with neuroevolution\n  potentials",
    "published": "2024-01-29T15:52:11Z",
    "updated": "2024-04-24T18:24:50Z",
    "authors": [
      "Haikuan Dong",
      "Yongbo Shi",
      "Penghua Ying",
      "Ke Xu",
      "Ting Liang",
      "Yanzhou Wang",
      "Zezhu Zeng",
      "Xin Wu",
      "Wenjiang Zhou",
      "Shiyun Xiong",
      "Shunda Chen",
      "Zheyong Fan"
    ],
    "summary": "Molecular dynamics (MD) simulations play an important role in understanding\nand engineering heat transport properties of complex materials. An essential\nrequirement for reliably predicting heat transport properties is the use of\naccurate and efficient interatomic potentials. Recently, machine-learned\npotentials (MLPs) have shown great promise in providing the required accuracy\nfor a broad range of materials. In this mini review and tutorial, we delve into\nthe fundamentals of heat transport, explore pertinent MD simulation methods,\nand survey the applications of MLPs in MD simulations of heat transport.\nFurthermore, we provide a step-by-step tutorial on developing MLPs for highly\nefficient and predictive heat transport simulations, utilizing the\nneuroevolution potentials (NEPs) as implemented in the GPUMD package. Our aim\nwith this mini review and tutorial is to empower researchers with valuable\ninsights into cutting-edge methodologies that can significantly enhance the\naccuracy and efficiency of MD simulations for heat transport studies.",
    "pdf_url": "http://arxiv.org/pdf/2401.16249v2",
    "doi": "10.1063/5.0200833",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.stat-mech",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "25 pages, 9 figures. This paper is part of the special topic, Machine\n  Learning for Thermal Transport",
    "journal_ref": "J. Appl. Phys. 135, 161101 (2024)",
    "citation_count": 46,
    "bibtex": "@Article{None,\n booktitle = {Journal of Applied Physics},\n journal = {Journal of Applied Physics},\n title = {Molecular dynamics simulations of heat transport using machine-learned potentials: A mini-review and tutorial on GPUMD with neuroevolution potentials},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2102.03861v1",
    "title": "Dynamic Movement Primitives in Robotics: A Tutorial Survey",
    "published": "2021-02-07T17:43:51Z",
    "updated": "2021-02-07T17:43:51Z",
    "authors": [
      "Matteo Saveriano",
      "Fares J. Abu-Dakka",
      "Aljaz Kramberger",
      "Luka Peternel"
    ],
    "summary": "Biological systems, including human beings, have the innate ability to\nperform complex tasks in versatile and agile manner. Researchers in\nsensorimotor control have tried to understand and formally define this innate\nproperty. The idea, supported by several experimental findings, that biological\nsystems are able to combine and adapt basic units of motion into complex tasks\nfinally lead to the formulation of the motor primitives theory. In this\nrespect, Dynamic Movement Primitives (DMPs) represent an elegant mathematical\nformulation of the motor primitives as stable dynamical systems, and are well\nsuited to generate motor commands for artificial systems like robots. In the\nlast decades, DMPs have inspired researchers in different robotic fields\nincluding imitation and reinforcement learning, optimal control,physical\ninteraction, and human-robot co-working, resulting a considerable amount of\npublished papers. The goal of this tutorial survey is two-fold. On one side, we\npresent the existing DMPs formulations in rigorous mathematical terms,and\ndiscuss advantages and limitations of each approach as well as practical\nimplementation details. In the tutorial vein, we also search for existing\nimplementations of presented approaches and release several others. On the\nother side, we provide a systematic and comprehensive review of existing\nliterature and categorize state of the art work on DMP. The paper concludes\nwith a discussion on the limitations of DMPs and an outline of possible\nresearch directions.",
    "pdf_url": "http://arxiv.org/pdf/2102.03861v1",
    "doi": "10.1177/02783649231201196",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "comment": "43 pages, 21 figures, 5 tables",
    "journal_ref": null,
    "citation_count": 207,
    "bibtex": "@Article{Saveriano2021DynamicMP,\n author = {Matteo Saveriano and Fares J. Abu-Dakka and Aljaz Kramberger and L. Peternel},\n booktitle = {Int. J. Robotics Res.},\n journal = {The International Journal of Robotics Research},\n pages = {1133 - 1184},\n title = {Dynamic movement primitives in robotics: A tutorial survey},\n volume = {42},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2507.00275v1",
    "title": "Double Q-learning for Value-based Deep Reinforcement Learning, Revisited",
    "published": "2025-06-30T21:32:46Z",
    "updated": "2025-06-30T21:32:46Z",
    "authors": [
      "Prabhat Nagarajan",
      "Martha White",
      "Marlos C. Machado"
    ],
    "summary": "Overestimation is pervasive in reinforcement learning (RL), including in\nQ-learning, which forms the algorithmic basis for many value-based deep RL\nalgorithms. Double Q-learning is an algorithm introduced to address\nQ-learning's overestimation by training two Q-functions and using both to\nde-correlate action-selection and action-evaluation in bootstrap targets.\nShortly after Q-learning was adapted to deep RL in the form of deep Q-networks\n(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.\nHowever, Double DQN only loosely adapts Double Q-learning, forgoing the\ntraining of two different Q-functions that bootstrap off one another. In this\npaper, we study algorithms that adapt this core idea of Double Q-learning for\nvalue-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our\naim is to understand whether DDQL exhibits less overestimation than Double DQN\nand whether performant instantiations of DDQL exist. We answer both questions\naffirmatively, demonstrating that DDQL reduces overestimation and outperforms\nDouble DQN in aggregate across 57 Atari 2600 games, without requiring\nadditional hyperparameters. We also study several aspects of DDQL, including\nits network architecture, replay ratio, and minibatch sampling strategy.",
    "pdf_url": "http://arxiv.org/pdf/2507.00275v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "44 pages",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Nagarajan2025DoubleQF,\n author = {P. Nagarajan and Martha White and Marlos C. Machado},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Double Q-learning for Value-based Deep Reinforcement Learning, Revisited},\n volume = {abs/2507.00275},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2011.00901v1",
    "title": "Sampling Algorithms, from Survey Sampling to Monte Carlo Methods:\n  Tutorial and Literature Review",
    "published": "2020-11-02T11:27:23Z",
    "updated": "2020-11-02T11:27:23Z",
    "authors": [
      "Benyamin Ghojogh",
      "Hadi Nekoei",
      "Aydin Ghojogh",
      "Fakhri Karray",
      "Mark Crowley"
    ],
    "summary": "This paper is a tutorial and literature review on sampling algorithms. We\nhave two main types of sampling in statistics. The first type is survey\nsampling which draws samples from a set or population. The second type is\nsampling from probability distribution where we have a probability density or\nmass function. In this paper, we cover both types of sampling. First, we review\nsome required background on mean squared error, variance, bias, maximum\nlikelihood estimation, Bernoulli, Binomial, and Hypergeometric distributions,\nthe Horvitz-Thompson estimator, and the Markov property. Then, we explain the\ntheory of simple random sampling, bootstrapping, stratified sampling, and\ncluster sampling. We also briefly introduce multistage sampling, network\nsampling, and snowball sampling. Afterwards, we switch to sampling from\ndistribution. We explain sampling from cumulative distribution function, Monte\nCarlo approximation, simple Monte Carlo methods, and Markov Chain Monte Carlo\n(MCMC) methods. For simple Monte Carlo methods, whose iterations are\nindependent, we cover importance sampling and rejection sampling. For MCMC\nmethods, we cover Metropolis algorithm, Metropolis-Hastings algorithm, Gibbs\nsampling, and slice sampling. Then, we explain the random walk behaviour of\nMonte Carlo methods and more efficient Monte Carlo methods, including\nHamiltonian (or hybrid) Monte Carlo, Adler's overrelaxation, and ordered\noverrelaxation. Finally, we summarize the characteristics, pros, and cons of\nsampling methods compared to each other. This paper can be useful for different\nfields of statistics, machine learning, reinforcement learning, and\ncomputational physics.",
    "pdf_url": "http://arxiv.org/pdf/2011.00901v1",
    "doi": null,
    "categories": [
      "stat.ME",
      "physics.comp-ph",
      "physics.data-an",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "comment": "The first three authors contributed equally to this work",
    "journal_ref": null,
    "citation_count": 15,
    "bibtex": "@Article{Ghojogh2020SamplingAF,\n author = {Benyamin Ghojogh and Hadi Nekoei and Aydin Ghojogh and F. Karray and Mark Crowley},\n journal = {arXiv: Methodology},\n title = {Sampling Algorithms, from Survey Sampling to Monte Carlo Methods: Tutorial and Literature Review.},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2106.11086v1",
    "title": "Analytically Tractable Bayesian Deep Q-Learning",
    "published": "2021-06-21T13:11:52Z",
    "updated": "2021-06-21T13:11:52Z",
    "authors": [
      "Luong Ha",
      "Nguyen",
      "James-A. Goulet"
    ],
    "summary": "Reinforcement learning (RL) has gained increasing interest since the\ndemonstration it was able to reach human performance on video game benchmarks\nusing deep Q-learning (DQN). The current consensus for training neural networks\non such complex environments is to rely on gradient-based optimization.\nAlthough alternative Bayesian deep learning methods exist, most of them still\nrely on gradient-based optimization, and they typically do not scale on\nbenchmarks such as the Atari game environment. Moreover none of these\napproaches allow performing the analytical inference for the weights and biases\ndefining the neural network. In this paper, we present how we can adapt the\ntemporal difference Q-learning framework to make it compatible with the\ntractable approximate Gaussian inference (TAGI), which allows learning the\nparameters of a neural network using a closed-form analytical method.\nThroughout the experiments with on- and off-policy reinforcement learning\napproaches, we demonstrate that TAGI can reach a performance comparable to\nbackpropagation-trained networks while using fewer hyperparameters, and without\nrelying on gradient-based optimization.",
    "pdf_url": "http://arxiv.org/pdf/2106.11086v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 4 figures",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Nguyen2021AnalyticallyTB,\n author = {L. Nguyen and J. Goulet},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Analytically Tractable Bayesian Deep Q-Learning},\n volume = {abs/2106.11086},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2210.13623v3",
    "title": "Reinforcement Learning and Bandits for Speech and Language Processing:\n  Tutorial, Review and Outlook",
    "published": "2022-10-24T21:49:12Z",
    "updated": "2023-10-19T13:15:48Z",
    "authors": [
      "Baihan Lin"
    ],
    "summary": "In recent years, reinforcement learning and bandits have transformed a wide\nrange of real-world applications including healthcare, finance, recommendation\nsystems, robotics, and last but not least, the speech and natural language\nprocessing. While most speech and language applications of reinforcement\nlearning algorithms are centered around improving the training of deep neural\nnetworks with its flexible optimization properties, there are still many\ngrounds to explore to utilize the benefits of reinforcement learning, such as\nits reward-driven adaptability, state representations, temporal structures and\ngeneralizability. In this survey, we present an overview of recent advancements\nof reinforcement learning and bandits, and discuss how they can be effectively\nemployed to solve speech and natural language processing problems with models\nthat are adaptive, interactive and scalable.",
    "pdf_url": "http://arxiv.org/pdf/2210.13623v3",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in Expert Systems with Applications. Accompanying\n  INTERSPEECH 2022 Tutorial on the same topic. Including latest advancements in\n  large language models (LLMs)",
    "journal_ref": null,
    "citation_count": 28,
    "bibtex": "@Article{Lin2022ReinforcementLA,\n author = {Baihan Lin},\n booktitle = {Expert systems with applications},\n journal = {Expert Syst. Appl.},\n pages = {122254},\n title = {Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook},\n volume = {238},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2510.19731v1",
    "title": "Bridging Earth and Space: A Survey on HAPS for Non-Terrestrial Networks",
    "published": "2025-10-22T16:22:31Z",
    "updated": "2025-10-22T16:22:31Z",
    "authors": [
      "G. Svistunov",
      "A. Akhtarshenas",
      "D. López-Pérez",
      "M. Giordani",
      "G. Geraci",
      "H. Yanikomeroglu"
    ],
    "summary": "HAPS are emerging as key enablers in the evolution of 6G wireless networks,\nbridging terrestrial and non-terrestrial infrastructures. Operating in the\nstratosphere, HAPS can provide wide-area coverage, low-latency,\nenergy-efficient broadband communications with flexible deployment options for\ndiverse applications. This survey delivers a comprehensive overview of HAPS use\ncases, technologies, and integration strategies within the 6G ecosystem. The\nroles of HAPS in extending connectivity to underserved regions, supporting\ndynamic backhauling, enabling massive IoT, and delivering reliable low-latency\ncommunications for autonomous and immersive services are discussed. The paper\nreviews state-of-the-art architectures for terrestrial and non-terrestrial\nnetwork integration, highlights recent field trials. Furthermore, key enabling\ntechnologies such as channel modeling, AI-driven resource allocation,\ninterference control, mobility management, and energy-efficient communications\nare examined. The paper also outlines open research challenges. By addressing\nexisting gaps in the literature, this survey positions HAPS as a foundational\ncomponent of globally integrated, resilient, and sustainable 6G networks.",
    "pdf_url": "http://arxiv.org/pdf/2510.19731v1",
    "doi": null,
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "30 pages. This work has been submitted to IEEE Communications Surveys\n  & Tutorials (under review)",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Svistunov2025BridgingEA,\n author = {G. Svistunov and A. Akhtarshenas and D. L'opez-P'erez and M. Giordani and G. Geraci and H. Y. U. P. D. Valencia and U. Padova and U. P. Fabra and Carleton University},\n title = {Bridging Earth and Space: A Survey on HAPS for Non-Terrestrial Networks},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2008.06799v1",
    "title": "Chrome Dino Run using Reinforcement Learning",
    "published": "2020-08-15T22:18:20Z",
    "updated": "2020-08-15T22:18:20Z",
    "authors": [
      "Divyanshu Marwah",
      "Sneha Srivastava",
      "Anusha Gupta",
      "Shruti Verma"
    ],
    "summary": "Reinforcement Learning is one of the most advanced set of algorithms known to\nmankind which can compete in games and perform at par or even better than\nhumans. In this paper we study most popular model free reinforcement learning\nalgorithms along with convolutional neural network to train the agent for\nplaying the game of Chrome Dino Run. We have used two of the popular temporal\ndifference approaches namely Deep Q-Learning, and Expected SARSA and also\nimplemented Double DQN model to train the agent and finally compare the scores\nwith respect to the episodes and convergence of algorithms with respect to\ntimesteps.",
    "pdf_url": "http://arxiv.org/pdf/2008.06799v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Marwah2020ChromeDR,\n author = {Divyanshu Marwah and S. Srivastava and Anusha Gupta and S. Verma},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Chrome Dino Run using Reinforcement Learning},\n volume = {abs/2008.06799},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1905.03949v2",
    "title": "Sensing, Computing, and Communication for Energy Harvesting IoTs: A\n  Survey",
    "published": "2019-05-10T05:42:57Z",
    "updated": "2019-12-24T00:47:58Z",
    "authors": [
      "Dong Ma",
      "Guohao Lan",
      "Mahbub Hassan",
      "Wen Hu",
      "Sajal K. Das"
    ],
    "summary": "With the growing number of deployments of Internet of Things (IoT)\ninfrastructure for a wide variety of applications, the battery maintenance has\nbecome a major limitation for the sustainability of such infrastructure. To\novercome this problem, energy harvesting offers a viable alternative to\nautonomously power IoT devices, resulting in a number of battery-less energy\nharvesting IoTs (or EH-IoTs) appearing in the market in recent years. Standards\nactivities are also underway, which involve wireless protocol design suitable\nfor EH-IoTs as well as testing procedures for various energy harvesting\nmethods. Despite the early commercial and standards activities, IoT sensing,\ncomputing and communications under unpredictable power supply still face\nsignificant research challenges. This paper systematically surveys recent\nadvances in EH-IoTs from several perspectives. First, it reviews the recent\ncommercial developments for EH-IoT in terms of both products and services,\nfollowed by initial standards activities in this space. Then it surveys methods\nthat enable the use of energy harvesting hardware as a proxy for conventional\nsensors to detect contexts in energy efficient manner. Next it reviews the\nadvancements in efficient checkpointing and timekeeping for intermittently\npowered IoT devices. We also survey recent research in novel wireless\ncommunication techniques for EH-IoTs, such as the applications of reinforcement\nlearning to optimize power allocations on-the-fly under unpredictable energy\nproductions, and packet-less IoT communications and backscatter communication\ntechniques for energy impoverished environments. The paper is concluded with a\ndiscussion of future research directions.",
    "pdf_url": "http://arxiv.org/pdf/1905.03949v2",
    "doi": "10.1109/COMST.2019.2962526",
    "categories": [
      "eess.SP",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "Accpeted for publication in IEEE Communications Surveys & Tutorials\n  (2019)",
    "journal_ref": null,
    "citation_count": 107,
    "bibtex": "@Article{Ma2019SensingCA,\n author = {Dong Ma and Guohao Lan and M. Hassan and Wen Hu and Sajal K. Das},\n booktitle = {IEEE Communications Surveys and Tutorials},\n journal = {IEEE Communications Surveys & Tutorials},\n pages = {1222-1250},\n title = {Sensing, Computing, and Communications for Energy Harvesting IoTs: A Survey},\n volume = {22},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2201.08610v2",
    "title": "Deep Q-learning: a robust control approach",
    "published": "2022-01-21T09:47:34Z",
    "updated": "2022-11-07T13:02:42Z",
    "authors": [
      "Balazs Varga",
      "Balazs Kulcsar",
      "Morteza Haghir Chehreghani"
    ],
    "summary": "In this paper, we place deep Q-learning into a control-oriented perspective\nand study its learning dynamics with well-established techniques from robust\ncontrol. We formulate an uncertain linear time-invariant model by means of the\nneural tangent kernel to describe learning. We show the instability of learning\nand analyze the agent's behavior in frequency-domain. Then, we ensure\nconvergence via robust controllers acting as dynamical rewards in the loss\nfunction. We synthesize three controllers: state-feedback gain scheduling H2,\ndynamic Hinf, and constant gain Hinf controllers. Setting up the learning agent\nwith a control-oriented tuning methodology is more transparent and has\nwell-established literature compared to the heuristics in reinforcement\nlearning. In addition, our approach does not use a target network and\nrandomized replay memory. The role of the target network is overtaken by the\ncontrol input, which also exploits the temporal dependency of samples (opposed\nto a randomized memory buffer). Numerical simulations in different OpenAI Gym\nenvironments suggest that the Hinf controlled learning performs slightly better\nthan Double deep Q-learning.",
    "pdf_url": "http://arxiv.org/pdf/2201.08610v2",
    "doi": "10.1002/rnc.6457",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Int J Robust Nonlinear Control. 2022; 1- 19",
    "citation_count": 13,
    "bibtex": "@Article{Varga2022DeepQA,\n author = {B. Varga and B. Kulcs'ar and M. Chehreghani},\n booktitle = {International Journal of Robust and Nonlinear Control},\n journal = {International Journal of Robust and Nonlinear Control},\n pages = {526 - 544},\n title = {Deep Q‐learning: A robust control approach},\n volume = {33},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2009.08136v1",
    "title": "Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and\n  Survey",
    "published": "2020-09-17T08:12:25Z",
    "updated": "2020-09-17T08:12:25Z",
    "authors": [
      "Benyamin Ghojogh",
      "Ali Ghodsi",
      "Fakhri Karray",
      "Mark Crowley"
    ],
    "summary": "Multidimensional Scaling (MDS) is one of the first fundamental manifold\nlearning methods. It can be categorized into several methods, i.e., classical\nMDS, kernel classical MDS, metric MDS, and non-metric MDS. Sammon mapping and\nIsomap can be considered as special cases of metric MDS and kernel classical\nMDS, respectively. In this tutorial and survey paper, we review the theory of\nMDS, Sammon mapping, and Isomap in detail. We explain all the mentioned\ncategories of MDS. Then, Sammon mapping, Isomap, and kernel Isomap are\nexplained. Out-of-sample embedding for MDS and Isomap using eigenfunctions and\nkernel mapping are introduced. Then, Nystrom approximation and its use in\nlandmark MDS and landmark Isomap are introduced for big data embedding. We also\nprovide some simulations for illustrating the embedding by these methods.",
    "pdf_url": "http://arxiv.org/pdf/2009.08136v1",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "To appear as a part of an upcoming academic book on dimensionality\n  reduction and manifold learning",
    "journal_ref": null,
    "citation_count": 32,
    "bibtex": "@Article{Ghojogh2020MultidimensionalSS,\n author = {Benyamin Ghojogh and A. Ghodsi and F. Karray and Mark Crowley},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Multidimensional Scaling, Sammon Mapping, and Isomap: Tutorial and Survey},\n volume = {abs/2009.08136},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2506.17702v1",
    "title": "Lower Bounds for Conjunctive Query Evaluation",
    "published": "2025-06-21T12:29:29Z",
    "updated": "2025-06-21T12:29:29Z",
    "authors": [
      "Stefan Mengel"
    ],
    "summary": "In this tutorial, we will survey known results on the complexity of\nconjunctive query evaluation in different settings, ranging from Boolean\nqueries over counting to more complex models like enumeration and direct\naccess. A particular focus will be on showing how different relatively recent\nhypotheses from complexity theory connect to query answering and allow showing\nthat known algorithms in several cases can likely not be improved.",
    "pdf_url": "http://arxiv.org/pdf/2506.17702v1",
    "doi": null,
    "categories": [
      "cs.DB",
      "cs.CC"
    ],
    "primary_category": "cs.DB",
    "comment": "paper for the tutorial at PODS 2025",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Mengel2025LowerBF,\n author = {Stefan Mengel},\n booktitle = {PODS Companion},\n journal = {Companion of the 44th Symposium on Principles of Database Systems},\n title = {Lower Bounds for Conjunctive Query Evaluation},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2106.08443v1",
    "title": "Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions,\n  Nyström Method, and Use of Kernels in Machine Learning: Tutorial and Survey",
    "published": "2021-06-15T21:29:12Z",
    "updated": "2021-06-15T21:29:12Z",
    "authors": [
      "Benyamin Ghojogh",
      "Ali Ghodsi",
      "Fakhri Karray",
      "Mark Crowley"
    ],
    "summary": "This is a tutorial and survey paper on kernels, kernel methods, and related\nfields. We start with reviewing the history of kernels in functional analysis\nand machine learning. Then, Mercer kernel, Hilbert and Banach spaces,\nReproducing Kernel Hilbert Space (RKHS), Mercer's theorem and its proof,\nfrequently used kernels, kernel construction from distance metric, important\nclasses of kernels (including bounded, integrally positive definite, universal,\nstationary, and characteristic kernels), kernel centering and normalization,\nand eigenfunctions are explained in detail. Then, we introduce types of use of\nkernels in machine learning including kernel methods (such as kernel support\nvector machines), kernel learning by semi-definite programming, Hilbert-Schmidt\nindependence criterion, maximum mean discrepancy, kernel mean embedding, and\nkernel dimensionality reduction. We also cover rank and factorization of kernel\nmatrix as well as the approximation of eigenfunctions and kernels using the\nNystr{\\\"o}m method. This paper can be useful for various fields of science\nincluding machine learning, dimensionality reduction, functional analysis in\nmathematics, and mathematical physics in quantum mechanics.",
    "pdf_url": "http://arxiv.org/pdf/2106.08443v1",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.FA"
    ],
    "primary_category": "stat.ML",
    "comment": "To appear as a part of an upcoming textbook on dimensionality\n  reduction and manifold learning",
    "journal_ref": null,
    "citation_count": 46,
    "bibtex": "@Article{Ghojogh2021ReproducingKH,\n author = {Benyamin Ghojogh and A. Ghodsi and F. Karray and Mark Crowley},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nyström Method, and Use of Kernels in Machine Learning: Tutorial and Survey},\n volume = {abs/2106.08443},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2103.11874v2",
    "title": "Applications of Game Theory in Vehicular Networks: A Survey",
    "published": "2021-03-22T14:09:33Z",
    "updated": "2022-01-05T13:22:20Z",
    "authors": [
      "Zemin Sun",
      "Yanheng Liu",
      "Jian Wang",
      "Guofa Li",
      "Carie Anil",
      "Keqiang Li",
      "Xinyu Guo",
      "Geng Sun",
      "Daxin Tian",
      "Dongpu Cao"
    ],
    "summary": "In the Internet of Things (IoT) era, vehicles and other intelligent\ncomponents in an intelligent transportation system (ITS) are connected, forming\nVehicular Networks (VNs) that provide efficient and secure traffic and\nubiquitous access to various applications. However, as the number of nodes in\nITS increases, it is challenging to satisfy a varied and large number of\nservice requests with different Quality of Service and security requirements in\nhighly dynamic VNs. Intelligent nodes in VNs can compete or cooperate for\nlimited network resources to achieve either an individual or a group's\nobjectives. Game Theory (GT), a theoretical framework designed for strategic\ninteractions among rational decision-makers sharing scarce resources, can be\nused to model and analyze individual or group behaviors of communicating\nentities in VNs. This paper primarily surveys the recent developments of GT in\nsolving various challenges of VNs. This survey starts with an introduction to\nthe background of VNs. A review of GT models studied in the VNs is then\nintroduced, including its basic concepts, classifications, and applicable\nvehicular issues. After discussing the requirements of VNs and the motivation\nof using GT, a comprehensive literature review on GT applications in dealing\nwith the challenges of current VNs is provided. Furthermore, recent\ncontributions of GT to VNs integrating with diverse emerging 5G technologies\nare surveyed. Finally, the lessons learned are given, and several key research\nchallenges and possible solutions for applying GT in VNs are outlined.",
    "pdf_url": "http://arxiv.org/pdf/2103.11874v2",
    "doi": "10.1109/COMST.2021.3108466",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT",
    "comment": "It has been published on \"IEEE communications surveys and tutorials\"\n  (https://ieeexplore.ieee.org/document/9524815)",
    "journal_ref": "IEEE Communications Surveys & Tutorials, vol. 23, no. 4, pp.\n  2660-2710, Fourthquarter 2021",
    "citation_count": 44,
    "bibtex": "@Article{Sun2021ApplicationsOG,\n author = {Zemin Sun and Yanheng Liu and Jian Wang and Guofa Li and C. Anil and Keqiang Li and Xinyu Guo and Geng Sun and Daxin Tian and Dongpu Cao},\n booktitle = {IEEE Communications Surveys and Tutorials},\n journal = {IEEE Communications Surveys & Tutorials},\n pages = {2660-2710},\n title = {Applications of Game Theory in Vehicular Networks: A Survey},\n volume = {23},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2304.13807v2",
    "title": "A Survey on Solving and Discovering Differential Equations Using Deep\n  Neural Networks",
    "published": "2023-04-26T20:14:25Z",
    "updated": "2023-06-19T13:56:29Z",
    "authors": [
      "Hyeonjung",
      "Jung",
      "Jayant Gupta",
      "Bharat Jayaprakash",
      "Matthew Eagon",
      "Harish Panneer Selvam",
      "Carl Molnar",
      "William Northrop",
      "Shashi Shekhar"
    ],
    "summary": "Ordinary and partial differential equations (DE) are used extensively in\nscientific and mathematical domains to model physical systems. Current\nliterature has focused primarily on deep neural network (DNN) based methods for\nsolving a specific DE or a family of DEs. Research communities with a history\nof using DE models may view DNN-based differential equation solvers (DNN-DEs)\nas a faster and transferable alternative to current numerical methods. However,\nthere is a lack of systematic surveys detailing the use of DNN-DE methods\nacross physical application domains and a generalized taxonomy to guide future\nresearch. This paper surveys and classifies previous works and provides an\neducational tutorial for senior practitioners, professionals, and graduate\nstudents in engineering and computer science. First, we propose a taxonomy to\nnavigate domains of DE systems studied under the umbrella of DNN-DE. Second, we\nexamine the theory and performance of the Physics Informed Neural Network\n(PINN) to demonstrate how the influential DNN-DE architecture mathematically\nsolves a system of equations. Third, to reinforce the key ideas of solving and\ndiscovery of DEs using DNN, we provide a tutorial using DeepXDE, a Python\npackage for developing PINNs, to develop DNN-DEs for solving and discovering a\nclassic DE, the linear transport equation.",
    "pdf_url": "http://arxiv.org/pdf/2304.13807v2",
    "doi": null,
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE",
    "comment": "Under review for ACM Computing Surveys journal. 29 pages",
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Article{Jung2023ASO,\n author = {Hyeonjung Jung and Jayant Gupta and B. Jayaprakash and Matthew J. Eagon and Harish Selvam and Carl Molnar and W. Northrop and Shashi Shekhar},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey on Solving and Discovering Differential Equations Using Deep Neural Networks},\n volume = {abs/2304.13807},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2006.16471v4",
    "title": "Object Detection Under Rainy Conditions for Autonomous Vehicles: A\n  Review of State-of-the-Art and Emerging Techniques",
    "published": "2020-06-30T02:05:10Z",
    "updated": "2021-02-12T02:16:15Z",
    "authors": [
      "Mazin Hnewa",
      "Hayder Radha"
    ],
    "summary": "Advanced automotive active-safety systems, in general, and autonomous\nvehicles, in particular, rely heavily on visual data to classify and localize\nobjects such as pedestrians, traffic signs and lights, and other nearby cars,\nto assist the corresponding vehicles maneuver safely in their environments.\nHowever, the performance of object detection methods could degrade rather\nsignificantly under challenging weather scenarios including rainy conditions.\nDespite major advancements in the development of deraining approaches, the\nimpact of rain on object detection has largely been understudied, especially in\nthe context of autonomous driving. The main objective of this paper is to\npresent a tutorial on state-of-the-art and emerging techniques that represent\nleading candidates for mitigating the influence of rainy conditions on an\nautonomous vehicle's ability to detect objects. Our goal includes surveying and\nanalyzing the performance of object detection methods trained and tested using\nvisual data captured under clear and rainy conditions. Moreover, we survey and\nevaluate the efficacy and limitations of leading deraining approaches,\ndeep-learning based domain adaptation, and image translation frameworks that\nare being considered for addressing the problem of object detection under rainy\nconditions. Experimental results of a variety of the surveyed techniques are\npresented as part of this tutorial.",
    "pdf_url": "http://arxiv.org/pdf/2006.16471v4",
    "doi": "10.1109/MSP.2020.2984801",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "comment": null,
    "journal_ref": "IEEE Signal Processing Magazine, vol. 38, no. 1, pp. 53-67, Jan.\n  2021",
    "citation_count": 157,
    "bibtex": "@Article{Hnewa2020ObjectDU,\n author = {Mazin Hnewa and H. Radha},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {53-67},\n title = {Object Detection Under Rainy Conditions for Autonomous Vehicles: A Review of State-of-the-Art and Emerging Techniques},\n volume = {38},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1908.09381v5",
    "title": "Tutorial and Survey on Probabilistic Graphical Model and Variational\n  Inference in Deep Reinforcement Learning",
    "published": "2019-08-25T19:36:36Z",
    "updated": "2019-12-08T11:44:35Z",
    "authors": [
      "Xudong Sun",
      "Bernd Bischl"
    ],
    "summary": "Aiming at a comprehensive and concise tutorial survey, recap of variational\ninference and reinforcement learning with Probabilistic Graphical Models are\ngiven with detailed derivations. Reviews and comparisons on recent advances in\ndeep reinforcement learning are made from various aspects. We offer detailed\nderivations to a taxonomy of Probabilistic Graphical Model and Variational\nInference methods in deep reinforcement learning, which serves as a\ncomplementary material on top of the original contributions.",
    "pdf_url": "http://arxiv.org/pdf/1908.09381v5",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "2019 IEEE Symposium on Computational Intelligence, Symposium on\n  Adaptive Dynamic Programming and Reinforcement Learning",
    "journal_ref": null,
    "citation_count": 9,
    "bibtex": "@Article{Sun2019TutorialAS,\n author = {Xudong Sun and B. Bischl},\n booktitle = {IEEE Symposium Series on Computational Intelligence},\n journal = {2019 IEEE Symposium Series on Computational Intelligence (SSCI)},\n pages = {110-119},\n title = {Tutorial and Survey on Probabilistic Graphical Model and Variational Inference in Deep Reinforcement Learning},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1803.03939v1",
    "title": "50 Years of Permutation, Spatial and Index Modulation: From Classic RF\n  to Visible Light Communications and Data Storage",
    "published": "2018-03-11T10:52:09Z",
    "updated": "2018-03-11T10:52:09Z",
    "authors": [
      "Naoki Ishikawa",
      "Shinya Sugiura",
      "Lajos Hanzo"
    ],
    "summary": "In this treatise, we provide an interdisciplinary survey on spatial\nmodulation (SM), where multiple-input multiple-output microwave and visible\nlight, as well as single and multicarrier communications are considered.\nSpecifically, we first review the permutation modulation (PM) concept, which\nwas originally proposed by Slepian in 1965. The PM concept has been applied to\na wide range of applications, including wired and wireless communications and\ndata storage. By introducing a three-dimensional signal representation, which\nconsists of spatial, temporal and frequency axes, the hybrid PM concept is\nshown to be equivalent to the recently proposed SM family. In contrast to other\nsurvey papers, this treatise aims for celebrating the hitherto overlooked\nstudies, including papers and patents that date back to the 1960s, before the\ninvention of SM. We also provide simulation results that demonstrate the pros\nand cons of PM-aided low-complexity schemes over conventional multiplexing\nschemes.",
    "pdf_url": "http://arxiv.org/pdf/1803.03939v1",
    "doi": null,
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP",
    "comment": "34 pages, 28 figures, 10 tables, accepted for publication in IEEE\n  Communications Surveys & Tutorials",
    "journal_ref": null,
    "citation_count": 152,
    "bibtex": "@Article{Ishikawa201850YO,\n author = {Naoki Ishikawa and S. Sugiura and L. Hanzo},\n booktitle = {IEEE Communications Surveys and Tutorials},\n journal = {IEEE Communications Surveys & Tutorials},\n pages = {1905-1938},\n title = {50 Years of Permutation, Spatial and Index Modulation: From Classic RF to Visible Light Communications and Data Storage},\n volume = {20},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1707.03770v2",
    "title": "Fastest Convergence for Q-learning",
    "published": "2017-07-12T15:44:22Z",
    "updated": "2018-03-21T18:38:35Z",
    "authors": [
      "Adithya M. Devraj",
      "Sean P. Meyn"
    ],
    "summary": "The Zap Q-learning algorithm introduced in this paper is an improvement of\nWatkins' original algorithm and recent competitors in several respects. It is a\nmatrix-gain algorithm designed so that its asymptotic variance is optimal.\nMoreover, an ODE analysis suggests that the transient behavior is a close match\nto a deterministic Newton-Raphson implementation. This is made possible by a\ntwo time-scale update equation for the matrix gain sequence.\n  The analysis suggests that the approach will lead to stable and efficient\ncomputation even for non-ideal parameterized settings. Numerical experiments\nconfirm the quick convergence, even in such non-ideal cases.\n  A secondary goal of this paper is tutorial. The first half of the paper\ncontains a survey on reinforcement learning algorithms, with a focus on minimum\nvariance algorithms.",
    "pdf_url": "http://arxiv.org/pdf/1707.03770v2",
    "doi": null,
    "categories": [
      "cs.SY",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.SY",
    "comment": null,
    "journal_ref": null,
    "citation_count": 36,
    "bibtex": "@Article{Devraj2017FastestCF,\n author = {Adithya M. Devraj and Sean P. Meyn},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Fastest Convergence for Q-learning},\n volume = {abs/1707.03770},\n year = {2017}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1810.07862v1",
    "title": "Applications of Deep Reinforcement Learning in Communications and\n  Networking: A Survey",
    "published": "2018-10-18T01:47:19Z",
    "updated": "2018-10-18T01:47:19Z",
    "authors": [
      "Nguyen Cong Luong",
      "Dinh Thai Hoang",
      "Shimin Gong",
      "Dusit Niyato",
      "Ping Wang",
      "Ying-Chang Liang",
      "Dong In Kim"
    ],
    "summary": "This paper presents a comprehensive literature review on applications of deep\nreinforcement learning in communications and networking. Modern networks, e.g.,\nInternet of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks, become\nmore decentralized and autonomous. In such networks, network entities need to\nmake decisions locally to maximize the network performance under uncertainty of\nnetwork environment. Reinforcement learning has been efficiently used to enable\nthe network entities to obtain the optimal policy including, e.g., decisions or\nactions, given their states when the state and action spaces are small.\nHowever, in complex and large-scale networks, the state and action spaces are\nusually large, and the reinforcement learning may not be able to find the\noptimal policy in reasonable time. Therefore, deep reinforcement learning, a\ncombination of reinforcement learning with deep learning, has been developed to\novercome the shortcomings. In this survey, we first give a tutorial of deep\nreinforcement learning from fundamental concepts to advanced models. Then, we\nreview deep reinforcement learning approaches proposed to address emerging\nissues in communications and networking. The issues include dynamic network\naccess, data rate control, wireless caching, data offloading, network security,\nand connectivity preservation which are all important to next generation\nnetworks such as 5G and beyond. Furthermore, we present applications of deep\nreinforcement learning for traffic routing, resource sharing, and data\ncollection. Finally, we highlight important challenges, open issues, and future\nresearch directions of applying deep reinforcement learning.",
    "pdf_url": "http://arxiv.org/pdf/1810.07862v1",
    "doi": null,
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "37 pages, 13 figures, 6 tables, 174 reference papers",
    "journal_ref": null,
    "citation_count": 1524,
    "bibtex": "@Article{Luong2018ApplicationsOD,\n author = {Nguyen Cong Luong and D. Hoang and Shimin Gong and Dusist Niyato and Ping Wang and Ying-Chang Liang and Dong In Kim},\n booktitle = {IEEE Communications Surveys and Tutorials},\n journal = {IEEE Communications Surveys & Tutorials},\n pages = {3133-3174},\n title = {Applications of Deep Reinforcement Learning in Communications and Networking: A Survey},\n volume = {21},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2403.08049v1",
    "title": "TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial\n  Creation on Physical Tasks",
    "published": "2024-03-12T19:46:59Z",
    "updated": "2024-03-12T19:46:59Z",
    "authors": [
      "Yuexi Chen",
      "Vlad I. Morariu",
      "Anh Truong",
      "Zhicheng Liu"
    ],
    "summary": "Mixed-media tutorials, which integrate videos, images, text, and diagrams to\nteach procedural skills, offer more browsable alternatives than timeline-based\nvideos. However, manually creating such tutorials is tedious, and existing\nautomated solutions are often restricted to a particular domain. While AI\nmodels hold promise, it is unclear how to effectively harness their powers,\ngiven the multi-modal data involved and the vast landscape of models. We\npresent TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial\ncreation on physical tasks. First, we distill common tutorial components by\nsurveying existing work; then, we present an approach to identify, assemble,\nand evaluate AI models for component extraction; finally, we propose guidelines\nfor designing user interfaces (UI) that support tutorial creation based on\nAI-generated components. We show that TutoAI has achieved higher or similar\nquality compared to a baseline model in preliminary user studies.",
    "pdf_url": "http://arxiv.org/pdf/2403.08049v1",
    "doi": null,
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "CHI 2024, supplementary materials:\n  https://hdi.cs.umd.edu/papers/TutoAI_CHI24_Supp.pdf",
    "journal_ref": null,
    "citation_count": 10,
    "bibtex": "@Article{Chen2024TutoAIAC,\n author = {Yuexi Chen and Vlad I. Morariu and Anh Truong and Zhicheng Liu},\n booktitle = {International Conference on Human Factors in Computing Systems},\n journal = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},\n title = {TutoAI: a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1812.08856v10",
    "title": "Machine Learning for Wireless Link Quality Estimation: A Survey",
    "published": "2018-12-07T13:27:00Z",
    "updated": "2021-01-18T10:16:21Z",
    "authors": [
      "Gregor Cerar",
      "Halil Yetgin",
      "Mihael Mohorčič",
      "Carolina Fortuna"
    ],
    "summary": "Since the emergence of wireless communication networks, a plethora of\nresearch papers focus their attention on the quality aspects of wireless links.\n  The analysis of the rich body of existing literature on link quality\nestimation using models developed from data traces indicates that the\ntechniques used for modeling link quality estimation are becoming increasingly\nsophisticated. A number of recent estimators leverage machine learning (ML)\ntechniques that require a sophisticated design and development process, each of\nwhich has a great potential to significantly affect the overall model\nperformance.\n  In this paper, we provide a comprehensive survey on link quality estimators\ndeveloped from empirical data and then focus on the subset that use ML\nalgorithms. We analyze ML-based link quality estimation (LQE) models from two\nperspectives using performance data. Firstly, we focus on how they address\nquality requirements that are important from the perspective of the\napplications they serve. Secondly, we analyze how they approach the standard\ndesign steps commonly used in the ML community. Having analyzed the scientific\nbody of the survey, we review existing open source datasets suitable for LQE\nresearch. Finally, we round up our survey with the lessons learned and design\nguidelines for ML-based LQE development and dataset collection.",
    "pdf_url": "http://arxiv.org/pdf/1812.08856v10",
    "doi": "10.1109/COMST.2021.3053615",
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted in IEEE Communications Surveys and Tutorials (COMST)",
    "journal_ref": "IEEE Communications Surveys and Tutorials, 2021",
    "citation_count": 2,
    "bibtex": "@Inproceedings{Cerar2018AnalysisOM,\n author = {Gregor Cerar and Mihael Mohorvcivc and Timotej Gale and C. Fortuna},\n title = {Analysis of Machine Learning for Link Quality Estimation},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2111.13485v2",
    "title": "Learning Long-Term Reward Redistribution via Randomized Return\n  Decomposition",
    "published": "2021-11-26T13:23:36Z",
    "updated": "2022-03-17T07:01:28Z",
    "authors": [
      "Zhizhou Ren",
      "Ruihan Guo",
      "Yuan Zhou",
      "Jian Peng"
    ],
    "summary": "Many practical applications of reinforcement learning require agents to learn\nfrom sparse and delayed rewards. It challenges the ability of agents to\nattribute their actions to future outcomes. In this paper, we consider the\nproblem formulation of episodic reinforcement learning with trajectory\nfeedback. It refers to an extreme delay of reward signals, in which the agent\ncan only obtain one reward signal at the end of each trajectory. A popular\nparadigm for this problem setting is learning with a designed auxiliary dense\nreward function, namely proxy reward, instead of sparse environmental signals.\nBased on this framework, this paper proposes a novel reward redistribution\nalgorithm, randomized return decomposition (RRD), to learn a proxy reward\nfunction for episodic reinforcement learning. We establish a surrogate problem\nby Monte-Carlo sampling that scales up least-squares-based reward\nredistribution to long-horizon problems. We analyze our surrogate loss function\nby connection with existing methods in the literature, which illustrates the\nalgorithmic properties of our approach. In experiments, we extensively evaluate\nour proposed method on a variety of benchmark tasks with episodic rewards and\ndemonstrate substantial improvement over baseline algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2111.13485v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Tenth International Conference on Learning Representations (ICLR 2022\n  Spotlight)",
    "journal_ref": null,
    "citation_count": 41,
    "bibtex": "@Article{Ren2021LearningLR,\n author = {Zhizhou Ren and Ruihan Guo and Yuanshuo Zhou and Jian Peng},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Learning Long-Term Reward Redistribution via Randomized Return Decomposition},\n volume = {abs/2111.13485},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2305.17250v1",
    "title": "Self-Supervised Reinforcement Learning that Transfers using Random\n  Features",
    "published": "2023-05-26T20:37:06Z",
    "updated": "2023-05-26T20:37:06Z",
    "authors": [
      "Boyuan Chen",
      "Chuning Zhu",
      "Pulkit Agrawal",
      "Kaiqing Zhang",
      "Abhishek Gupta"
    ],
    "summary": "Model-free reinforcement learning algorithms have exhibited great potential\nin solving single-task sequential decision-making problems with\nhigh-dimensional observations and long horizons, but are known to be hard to\ngeneralize across tasks. Model-based RL, on the other hand, learns\ntask-agnostic models of the world that naturally enables transfer across\ndifferent reward functions, but struggles to scale to complex environments due\nto the compounding error. To get the best of both worlds, we propose a\nself-supervised reinforcement learning method that enables the transfer of\nbehaviors across tasks with different rewards, while circumventing the\nchallenges of model-based RL. In particular, we show self-supervised\npre-training of model-free reinforcement learning with a number of random\nfeatures as rewards allows implicit modeling of long-horizon environment\ndynamics. Then, planning techniques like model-predictive control using these\nimplicit models enable fast adaptation to problems with new reward functions.\nOur method is self-supervised in that it can be trained on offline datasets\nwithout reward labels, but can then be quickly deployed on new tasks. We\nvalidate that our proposed method enables transfer across tasks on a variety of\nmanipulation and locomotion domains in simulation, opening the door to\ngeneralist decision-making agents.",
    "pdf_url": "http://arxiv.org/pdf/2305.17250v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Chen2023SelfSupervisedRL,\n author = {Boyuan Chen and Chuning Zhu and Pulkit Agrawal and K. Zhang and Abhishek Gupta},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Self-Supervised Reinforcement Learning that Transfers using Random Features},\n volume = {abs/2305.17250},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2211.15183v3",
    "title": "Continuous Episodic Control",
    "published": "2022-11-28T09:48:42Z",
    "updated": "2023-04-23T09:21:14Z",
    "authors": [
      "Zhao Yang",
      "Thomas M. Moerland",
      "Mike Preuss",
      "Aske Plaat"
    ],
    "summary": "Non-parametric episodic memory can be used to quickly latch onto\nhigh-rewarded experience in reinforcement learning tasks. In contrast to\nparametric deep reinforcement learning approaches in which reward signals need\nto be back-propagated slowly, these methods only need to discover the solution\nonce, and may then repeatedly solve the task. However, episodic control\nsolutions are stored in discrete tables, and this approach has so far only been\napplied to discrete action space problems. Therefore, this paper introduces\nContinuous Episodic Control (CEC), a novel non-parametric episodic memory\nalgorithm for sequential decision making in problems with a continuous action\nspace. Results on several sparse-reward continuous control environments show\nthat our proposed method learns faster than state-of-the-art model-free RL and\nmemory-augmented RL algorithms, while maintaining good long-run performance as\nwell. In short, CEC can be a fast approach for learning in continuous control\ntasks.",
    "pdf_url": "http://arxiv.org/pdf/2211.15183v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Yang2022ContinuousEC,\n author = {Zhao Yang and Thomas M. Moerland and M. Preuss and Aske Plaat},\n booktitle = {2023 IEEE Conference on Games (CoG)},\n journal = {2023 IEEE Conference on Games (CoG)},\n pages = {1-8},\n title = {Continuous Episodic Control},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2205.08926v2",
    "title": "Generating Explanations from Deep Reinforcement Learning Using Episodic\n  Memory",
    "published": "2022-05-18T13:46:38Z",
    "updated": "2022-07-24T17:29:28Z",
    "authors": [
      "Sam Blakeman",
      "Denis Mareschal"
    ],
    "summary": "Deep Reinforcement Learning (RL) involves the use of Deep Neural Networks\n(DNNs) to make sequential decisions in order to maximize reward. For many tasks\nthe resulting sequence of actions produced by a Deep RL policy can be long and\ndifficult to understand for humans. A crucial component of human explanations\nis selectivity, whereby only key decisions and causes are recounted. Imbuing\nDeep RL agents with such an ability would make their resulting policies easier\nto understand from a human perspective and generate a concise set of\ninstructions to aid the learning of future agents. To this end we use a Deep RL\nagent with an episodic memory system to identify and recount key decisions\nduring policy execution. We show that these decisions form a short, human\nreadable explanation that can also be used to speed up the learning of naive\nDeep RL agents in an algorithm-independent manner.",
    "pdf_url": "http://arxiv.org/pdf/2205.08926v2",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Blakeman2022GeneratingEF,\n author = {Sam Blakeman and D. Mareschal},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Generating Explanations from Deep Reinforcement Learning Using Episodic Memory},\n volume = {abs/2205.08926},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2502.15214v1",
    "title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning",
    "published": "2025-02-21T05:01:30Z",
    "updated": "2025-02-21T05:01:30Z",
    "authors": [
      "Sheila Schoepp",
      "Masoud Jafaripour",
      "Yingyue Cao",
      "Tianpei Yang",
      "Fatemeh Abdollahi",
      "Shadan Golestan",
      "Zahin Sufiyan",
      "Osmar R. Zaiane",
      "Matthew E. Taylor"
    ],
    "summary": "Reinforcement learning (RL) has shown impressive results in sequential\ndecision-making tasks. Meanwhile, Large Language Models (LLMs) and\nVision-Language Models (VLMs) have emerged, exhibiting impressive capabilities\nin multimodal understanding and reasoning. These advances have led to a surge\nof research integrating LLMs and VLMs into RL. In this survey, we review\nrepresentative works in which LLMs and VLMs are used to overcome key challenges\nin RL, such as lack of prior knowledge, long-horizon planning, and reward\ndesign. We present a taxonomy that categorizes these LLM/VLM-assisted RL\napproaches into three roles: agent, planner, and reward. We conclude by\nexploring open problems, including grounding, bias mitigation, improved\nrepresentations, and action advice. By consolidating existing research and\nidentifying future directions, this survey establishes a framework for\nintegrating LLMs and VLMs into RL, advancing approaches that unify natural\nlanguage and visual understanding with sequential decision-making.",
    "pdf_url": "http://arxiv.org/pdf/2502.15214v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Schoepp2024TheEL,\n author = {Sheila Schoepp and Masoud Jafaripour and Yingyue Cao and Tianpei Yang and Fatemeh Abdollahi and Shadan Golestan and Zahin Sufiyan and Osmar R. Zaiane and Matthew E. Taylor},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {10641-10649},\n title = {The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2405.20692v1",
    "title": "In-Context Decision Transformer: Reinforcement Learning via Hierarchical\n  Chain-of-Thought",
    "published": "2024-05-31T08:38:25Z",
    "updated": "2024-05-31T08:38:25Z",
    "authors": [
      "Sili Huang",
      "Jifeng Hu",
      "Hechang Chen",
      "Lichao Sun",
      "Bo Yang"
    ],
    "summary": "In-context learning is a promising approach for offline reinforcement\nlearning (RL) to handle online tasks, which can be achieved by providing task\nprompts. Recent works demonstrated that in-context RL could emerge with\nself-improvement in a trial-and-error manner when treating RL tasks as an\nacross-episodic sequential prediction problem. Despite the self-improvement not\nrequiring gradient updates, current works still suffer from high computational\ncosts when the across-episodic sequence increases with task horizons. To this\nend, we propose an In-context Decision Transformer (IDT) to achieve\nself-improvement in a high-level trial-and-error manner. Specifically, IDT is\ninspired by the efficient hierarchical structure of human decision-making and\nthus reconstructs the sequence to consist of high-level decisions instead of\nlow-level actions that interact with environments. As one high-level decision\ncan guide multi-step low-level actions, IDT naturally avoids excessively long\nsequences and solves online tasks more efficiently. Experimental results show\nthat IDT achieves state-of-the-art in long-horizon tasks over current\nin-context RL methods. In particular, the online evaluation time of our IDT is\n\\textbf{36$\\times$} times faster than baselines in the D4RL benchmark and\n\\textbf{27$\\times$} times faster in the Grid World benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2405.20692v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 17,
    "bibtex": "@Article{Huang2024InContextDT,\n author = {Sili Huang and Jifeng Hu and Hechang Chen and Lichao Sun and Bo Yang},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought},\n volume = {abs/2405.20692},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2410.11324v1",
    "title": "Diffusion-Based Offline RL for Improved Decision-Making in Augmented ARC\n  Task",
    "published": "2024-10-15T06:48:27Z",
    "updated": "2024-10-15T06:48:27Z",
    "authors": [
      "Yunho Kim",
      "Jaehyun Park",
      "Heejun Kim",
      "Sejin Kim",
      "Byung-Jun Lee",
      "Sundong Kim"
    ],
    "summary": "Effective long-term strategies enable AI systems to navigate complex\nenvironments by making sequential decisions over extended horizons. Similarly,\nreinforcement learning (RL) agents optimize decisions across sequences to\nmaximize rewards, even without immediate feedback. To verify that Latent\nDiffusion-Constrained Q-learning (LDCQ), a prominent diffusion-based offline RL\nmethod, demonstrates strong reasoning abilities in multi-step decision-making,\nwe aimed to evaluate its performance on the Abstraction and Reasoning Corpus\n(ARC). However, applying offline RL methodologies to enhance strategic\nreasoning in AI for solving tasks in ARC is challenging due to the lack of\nsufficient experience data in the ARC training set. To address this limitation,\nwe introduce an augmented offline RL dataset for ARC, called Synthesized\nOffline Learning Data for Abstraction and Reasoning (SOLAR), along with the\nSOLAR-Generator, which generates diverse trajectory data based on predefined\nrules. SOLAR enables the application of offline RL methods by offering\nsufficient experience data. We synthesized SOLAR for a simple task and used it\nto train an agent with the LDCQ method. Our experiments demonstrate the\neffectiveness of the offline RL approach on a simple ARC task, showing the\nagent's ability to make multi-step sequential decisions and correctly identify\nanswer states. These results highlight the potential of the offline RL approach\nto enhance AI's strategic reasoning capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2410.11324v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint, Under review. Comments welcome",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Kim2024DiffusionBasedOR,\n author = {Yunho Kim and Jaehyun Park and Heejun Kim and Sejin Kim and Byung-Jun Lee and Sundong Kim},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Diffusion-Based Offline RL for Improved Decision-Making in Augmented ARC Task},\n volume = {abs/2410.11324},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2403.03141v1",
    "title": "Language Guided Exploration for RL Agents in Text Environments",
    "published": "2024-03-05T17:26:41Z",
    "updated": "2024-03-05T17:26:41Z",
    "authors": [
      "Hitesh Golchha",
      "Sahil Yerawar",
      "Dhruvesh Patel",
      "Soham Dan",
      "Keerthiram Murugesan"
    ],
    "summary": "Real-world sequential decision making is characterized by sparse rewards and\nlarge decision spaces, posing significant difficulty for experiential learning\nsystems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large\nLanguage Models (LLMs), with a wealth of world knowledge, can help RL agents\nlearn quickly and adapt to distribution shifts. In this work, we introduce\nLanguage Guided Exploration (LGE) framework, which uses a pre-trained language\nmodel (called GUIDE ) to provide decision-level guidance to an RL agent (called\nEXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging\ntext environment, LGE outperforms vanilla RL agents significantly and also\noutperforms other sophisticated methods like Behaviour Cloning and Text\nDecision Transformer.",
    "pdf_url": "http://arxiv.org/pdf/2403.03141v1",
    "doi": null,
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "comment": null,
    "journal_ref": null,
    "citation_count": 7,
    "bibtex": "@Article{Golchha2024LanguageGE,\n author = {Hitesh Golchha and Sahil Yerawar and Dhruvesh Patel and Soham Dan and K. Murugesan},\n booktitle = {NAACL-HLT},\n pages = {93-102},\n title = {Language Guided Exploration for RL Agents in Text Environments},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2510.20725v1",
    "title": "No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes\n  with Gaussian Processes",
    "published": "2025-10-23T16:44:31Z",
    "updated": "2025-10-23T16:44:31Z",
    "authors": [
      "Jasmine Bayrooti",
      "Sattar Vakili",
      "Amanda Prorok",
      "Carl Henrik Ek"
    ],
    "summary": "Thompson sampling (TS) is a powerful and widely used strategy for sequential\ndecision-making, with applications ranging from Bayesian optimization to\nreinforcement learning (RL). Despite its success, the theoretical foundations\nof TS remain limited, particularly in settings with complex temporal structure\nsuch as RL. We address this gap by establishing no-regret guarantees for TS\nusing models with Gaussian marginal distributions. Specifically, we consider TS\nin episodic RL with joint Gaussian process (GP) priors over rewards and\ntransitions. We prove a regret bound of\n$\\mathcal{\\tilde{O}}(\\sqrt{KH\\Gamma(KH)})$ over $K$ episodes of horizon $H$,\nwhere $\\Gamma(\\cdot)$ captures the complexity of the GP model. Our analysis\naddresses several challenges, including the non-Gaussian nature of value\nfunctions and the recursive structure of Bellman updates, and extends classical\ntools such as the elliptical potential lemma to multi-output settings. This\nwork advances the understanding of TS in RL and highlights how structural\nassumptions and model uncertainty shape its performance in finite-horizon\nMarkov Decision Processes.",
    "pdf_url": "http://arxiv.org/pdf/2510.20725v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Appearing in NeurIPS, 2025",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Bayrooti2025NoRegretTS,\n author = {Jasmine Bayrooti and Sattar Vakili and Amanda Prorok and Carl Henrik Ek},\n title = {No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2102.02639v1",
    "title": "Improving Reinforcement Learning with Human Assistance: An Argument for\n  Human Subject Studies with HIPPO Gym",
    "published": "2021-02-02T12:56:02Z",
    "updated": "2021-02-02T12:56:02Z",
    "authors": [
      "Matthew E. Taylor",
      "Nicholas Nissen",
      "Yuan Wang",
      "Neda Navidi"
    ],
    "summary": "Reinforcement learning (RL) is a popular machine learning paradigm for game\nplaying, robotics control, and other sequential decision tasks. However, RL\nagents often have long learning times with high data requirements because they\nbegin by acting randomly. In order to better learn in complex tasks, this\narticle argues that an external teacher can often significantly help the RL\nagent learn.\n  OpenAI Gym is a common framework for RL research, including a large number of\nstandard environments and agents, making RL research significantly more\naccessible. This article introduces our new open-source RL framework, the Human\nInput Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions\nthat went into its creation. The goal of this platform is to facilitate\nhuman-RL research, again lowering the bar so that more researchers can quickly\ninvestigate different ways that human teachers could assist RL agents,\nincluding learning from demonstrations, learning from feedback, or curriculum\nlearning.",
    "pdf_url": "http://arxiv.org/pdf/2102.02639v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Article{Taylor2021ImprovingRL,\n author = {Matthew E. Taylor and Nicholas Nissen and Yuan Wang and N. Navidi},\n booktitle = {Neural computing & applications (Print)},\n journal = {Neural Computing and Applications},\n pages = {23429-23439},\n title = {Improving reinforcement learning with human assistance: an argument for human subject studies with HIPPO Gym},\n volume = {35},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1907.12894v1",
    "title": "Reward Learning for Efficient Reinforcement Learning in Extractive\n  Document Summarisation",
    "published": "2019-07-30T13:31:07Z",
    "updated": "2019-07-30T13:31:07Z",
    "authors": [
      "Yang Gao",
      "Christian M. Meyer",
      "Mohsen Mesgar",
      "Iryna Gurevych"
    ],
    "summary": "Document summarisation can be formulated as a sequential decision-making\nproblem, which can be solved by Reinforcement Learning (RL) algorithms. The\npredominant RL paradigm for summarisation learns a cross-input policy, which\nrequires considerable time, data and parameter tuning due to the huge search\nspaces and the delayed rewards. Learning input-specific RL policies is a more\nefficient alternative but so far depends on handcrafted rewards, which are\ndifficult to design and yield poor performance. We propose RELIS, a novel RL\nparadigm that learns a reward function with Learning-to-Rank (L2R) algorithms\nat training time and uses this reward function to train an input-specific RL\npolicy at test time. We prove that RELIS guarantees to generate near-optimal\nsummaries with appropriate L2R and RL algorithms. Empirically, we evaluate our\napproach on extractive multi-document summarisation. We show that RELIS reduces\nthe training time by two orders of magnitude compared to the state-of-the-art\nmodels while performing on par with them.",
    "pdf_url": "http://arxiv.org/pdf/1907.12894v1",
    "doi": null,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to IJCAI 2019",
    "journal_ref": null,
    "citation_count": 24,
    "bibtex": "@Article{Gao2019RewardLF,\n author = {Yang Gao and Yang Gao and Christian M. Meyer and Mohsen Mesgar and Iryna Gurevych},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation},\n volume = {abs/1907.12894},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1604.06508v1",
    "title": "HIRL: Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks\n  with Delayed Rewards",
    "published": "2016-04-21T22:14:11Z",
    "updated": "2016-04-21T22:14:11Z",
    "authors": [
      "Sanjay Krishnan",
      "Animesh Garg",
      "Richard Liaw",
      "Lauren Miller",
      "Florian T. Pokorny",
      "Ken Goldberg"
    ],
    "summary": "Reinforcement Learning (RL) struggles in problems with delayed rewards, and\none approach is to segment the task into sub-tasks with incremental rewards. We\npropose a framework called Hierarchical Inverse Reinforcement Learning (HIRL),\nwhich is a model for learning sub-task structure from demonstrations. HIRL\ndecomposes the task into sub-tasks based on transitions that are consistent\nacross demonstrations. These transitions are defined as changes in local\nlinearity w.r.t to a kernel function. Then, HIRL uses the inferred structure to\nlearn reward functions local to the sub-tasks but also handle any global\ndependencies such as sequentiality.\n  We have evaluated HIRL on several standard RL benchmarks: Parallel Parking\nwith noisy dynamics, Two-Link Pendulum, 2D Noisy Motion Planning, and a Pinball\nenvironment. In the parallel parking task, we find that rewards constructed\nwith HIRL converge to a policy with an 80% success rate in 32% fewer time-steps\nthan those constructed with Maximum Entropy Inverse RL (MaxEnt IRL), and with\npartial state observation, the policies learned with IRL fail to achieve this\naccuracy while HIRL still converges. We further find that that the rewards\nlearned with HIRL are robust to environment noise where they can tolerate 1\nstdev. of random perturbation in the poses in the environment obstacles while\nmaintaining roughly the same convergence rate. We find that HIRL rewards can\nconverge up-to 6x faster than rewards constructed with IRL.",
    "pdf_url": "http://arxiv.org/pdf/1604.06508v1",
    "doi": null,
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "comment": "12 pages",
    "journal_ref": null,
    "citation_count": 40,
    "bibtex": "@Article{Krishnan2016HIRLHI,\n author = {S. Krishnan and Animesh Garg and Richard Liaw and Lauren Miller and Florian T. Pokorny and Ken Goldberg},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {HIRL: Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks with Delayed Rewards},\n volume = {abs/1604.06508},\n year = {2016}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2201.05000v1",
    "title": "Automated Reinforcement Learning: An Overview",
    "published": "2022-01-13T14:28:06Z",
    "updated": "2022-01-13T14:28:06Z",
    "authors": [
      "Reza Refaei Afshar",
      "Yingqian Zhang",
      "Joaquin Vanschoren",
      "Uzay Kaymak"
    ],
    "summary": "Reinforcement Learning and recently Deep Reinforcement Learning are popular\nmethods for solving sequential decision making problems modeled as Markov\nDecision Processes. RL modeling of a problem and selecting algorithms and\nhyper-parameters require careful considerations as different configurations may\nentail completely different performances. These considerations are mainly the\ntask of RL experts; however, RL is progressively becoming popular in other\nfields where the researchers and system designers are not RL experts. Besides,\nmany modeling decisions, such as defining state and action space, size of\nbatches and frequency of batch updating, and number of timesteps are typically\nmade manually. For these reasons, automating different components of RL\nframework is of great importance and it has attracted much attention in recent\nyears. Automated RL provides a framework in which different components of RL\nincluding MDP modeling, algorithm selection and hyper-parameter optimization\nare modeled and defined automatically. In this article, we explore the\nliterature and present recent work that can be used in automated RL. Moreover,\nwe discuss the challenges, open questions and research directions in AutoRL.",
    "pdf_url": "http://arxiv.org/pdf/2201.05000v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 16,
    "bibtex": "@Article{Afshar2022AutomatedRL,\n author = {Reza Refaei Afshar and Yingqian Zhang and J. Vanschoren and U. Kaymak},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Automated Reinforcement Learning: An Overview},\n volume = {abs/2201.05000},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2502.07949v2",
    "title": "Advancing Autonomous VLM Agents via Variational Subgoal-Conditioned\n  Reinforcement Learning",
    "published": "2025-02-11T20:57:46Z",
    "updated": "2025-05-20T19:54:36Z",
    "authors": [
      "Qingyuan Wu",
      "Jianheng Liu",
      "Jianye Hao",
      "Jun Wang",
      "Kun Shao"
    ],
    "summary": "State-of-the-art (SOTA) reinforcement learning (RL) methods have enabled\nvision-language model (VLM) agents to learn from interaction with online\nenvironments without human supervision. However, these methods often struggle\nwith learning inefficiencies when applied to complex, real-world\ndecision-making tasks with sparse rewards and long-horizon dependencies. We\npropose a novel framework, Variational Subgoal-Conditioned Reinforcement\nLearning (VSC-RL), advancing the VLM agents in resolving challenging\ndecision-making tasks. Fundamentally distinct from existing methods, VSC-RL\nreformulates the decision-making problem as a variational subgoal-conditioned\nRL problem with the newly derived optimization objective, Subgoal Evidence\nLower BOund (SGC-ELBO), which comprises two key components: (a) maximizing the\nsubgoal-conditioned return, and (b) minimizing the divergence from a reference\ngoal-conditioned policy. We theoretically and empirically demonstrate that the\nVSC-RL can efficiently improve the learning efficiency without compromising\nperformance guarantees. Across a diverse set of challenging benchmarks,\nincluding mobile device and web control tasks, VSC-RL consistently outperforms\nexisting SOTA methods, achieving superior learning efficiency and performance.",
    "pdf_url": "http://arxiv.org/pdf/2502.07949v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Inproceedings{Wu2025AdvancingAV,\n author = {Qingyuan Wu and Jianheng Liu and Jianye Hao and Jun Wang and Kun Shao},\n title = {Advancing Autonomous VLM Agents via Variational Subgoal-Conditioned Reinforcement Learning},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2106.02757v2",
    "title": "Heuristic-Guided Reinforcement Learning",
    "published": "2021-06-05T00:04:09Z",
    "updated": "2021-11-22T17:42:43Z",
    "authors": [
      "Ching-An Cheng",
      "Andrey Kolobov",
      "Adith Swaminathan"
    ],
    "summary": "We provide a framework for accelerating reinforcement learning (RL)\nalgorithms by heuristics constructed from domain knowledge or offline data.\nTabula rasa RL algorithms require environment interactions or computation that\nscales with the horizon of the sequential decision-making task. Using our\nframework, we show how heuristic-guided RL induces a much shorter-horizon\nsubproblem that provably solves the original task. Our framework can be viewed\nas a horizon-based regularization for controlling bias and variance in RL under\na finite interaction budget. On the theoretical side, we characterize\nproperties of a good heuristic and its impact on RL acceleration. In\nparticular, we introduce the novel concept of an improvable heuristic, a\nheuristic that allows an RL agent to extrapolate beyond its prior knowledge. On\nthe empirical side, we instantiate our framework to accelerate several\nstate-of-the-art algorithms in simulated robotic control tasks and procedurally\ngenerated games. Our framework complements the rich literature on warm-starting\nRL with expert demonstrations or exploratory datasets, and introduces a\nprincipled method for injecting prior knowledge into RL.",
    "pdf_url": "http://arxiv.org/pdf/2106.02757v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 65,
    "bibtex": "@Article{Cheng2021HeuristicGuidedRL,\n author = {Ching-An Cheng and A. Kolobov and Adith Swaminathan},\n booktitle = {Neural Information Processing Systems},\n pages = {13550-13563},\n title = {Heuristic-Guided Reinforcement Learning},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2411.08637v1",
    "title": "Robot See, Robot Do: Imitation Reward for Noisy Financial Environments",
    "published": "2024-11-13T14:24:47Z",
    "updated": "2024-11-13T14:24:47Z",
    "authors": [
      "Sven Goluža",
      "Tomislav Kovačević",
      "Stjepan Begušić",
      "Zvonko Kostanjčar"
    ],
    "summary": "The sequential nature of decision-making in financial asset trading aligns\nnaturally with the reinforcement learning (RL) framework, making RL a common\napproach in this domain. However, the low signal-to-noise ratio in financial\nmarkets results in noisy estimates of environment components, including the\nreward function, which hinders effective policy learning by RL agents. Given\nthe critical importance of reward function design in RL problems, this paper\nintroduces a novel and more robust reward function by leveraging imitation\nlearning, where a trend labeling algorithm acts as an expert. We integrate\nimitation (expert's) feedback with reinforcement (agent's) feedback in a\nmodel-free RL algorithm, effectively embedding the imitation learning problem\nwithin the RL paradigm to handle the stochasticity of reward signals. Empirical\nresults demonstrate that this novel approach improves financial performance\nmetrics compared to traditional benchmarks and RL agents trained solely using\nreinforcement feedback.",
    "pdf_url": "http://arxiv.org/pdf/2411.08637v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.RO",
      "q-fin.TR"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Goluža2024RobotSR,\n author = {Sven Goluža and Tomislav Kovacevic and Stjepan Begušić and Z. Kostanjčar},\n booktitle = {BigData Congress [Services Society]},\n journal = {2024 IEEE International Conference on Big Data (BigData)},\n pages = {4884-4891},\n title = {Robot See, Robot Do: Imitation Reward for Noisy Financial Environments},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2003.04203v1",
    "title": "Human AI interaction loop training: New approach for interactive\n  reinforcement learning",
    "published": "2020-03-09T15:27:48Z",
    "updated": "2020-03-09T15:27:48Z",
    "authors": [
      "Neda Navidi"
    ],
    "summary": "Reinforcement Learning (RL) in various decision-making tasks of machine\nlearning provides effective results with an agent learning from a stand-alone\nreward function. However, it presents unique challenges with large amounts of\nenvironment states and action spaces, as well as in the determination of\nrewards. This complexity, coming from high dimensionality and continuousness of\nthe environments considered herein, calls for a large number of learning trials\nto learn about the environment through Reinforcement Learning. Imitation\nLearning (IL) offers a promising solution for those challenges using a teacher.\nIn IL, the learning process can take advantage of human-sourced assistance\nand/or control over the agent and environment. A human teacher and an agent\nlearner are considered in this study. The teacher takes part in the agent\ntraining towards dealing with the environment, tackling a specific objective,\nand achieving a predefined goal. Within that paradigm, however, existing IL\napproaches have the drawback of expecting extensive demonstration information\nin long-horizon problems. This paper proposes a novel approach combining IL\nwith different types of RL methods, namely state action reward state action\n(SARSA) and asynchronous advantage actor-critic (A3C) agents, to overcome the\nproblems of both stand-alone systems. It is addressed how to effectively\nleverage the teacher feedback, be it direct binary or indirect detailed for the\nagent learner to learn sequential decision-making policies. The results of this\nstudy on various OpenAI Gym environments show that this algorithmic method can\nbe incorporated with different combinations, significantly decreases both human\nendeavor and tedious exploration process.",
    "pdf_url": "http://arxiv.org/pdf/2003.04203v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Article{Navidi2020HumanAI,\n author = {N. Navidi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Human AI interaction loop training: New approach for interactive reinforcement learning},\n volume = {abs/2003.04203},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1902.02893v1",
    "title": "Rethinking the Discount Factor in Reinforcement Learning: A Decision\n  Theoretic Approach",
    "published": "2019-02-08T00:30:53Z",
    "updated": "2019-02-08T00:30:53Z",
    "authors": [
      "Silviu Pitis"
    ],
    "summary": "Reinforcement learning (RL) agents have traditionally been tasked with\nmaximizing the value function of a Markov decision process (MDP), either in\ncontinuous settings, with fixed discount factor $\\gamma < 1$, or in episodic\nsettings, with $\\gamma = 1$. While this has proven effective for specific tasks\nwith well-defined objectives (e.g., games), it has never been established that\nfixed discounting is suitable for general purpose use (e.g., as a model of\nhuman preferences). This paper characterizes rationality in sequential decision\nmaking using a set of seven axioms and arrives at a form of discounting that\ngeneralizes traditional fixed discounting. In particular, our framework admits\na state-action dependent \"discount\" factor that is not constrained to be less\nthan 1, so long as there is eventual long run discounting. Although this\nbroadens the range of possible preference structures in continuous settings, we\nshow that there exists a unique \"optimizing MDP\" with fixed $\\gamma < 1$ whose\noptimal value function matches the true utility of the optimal policy, and we\nquantify the difference between value and utility for suboptimal policies. Our\nwork can be seen as providing a normative justification for (a slight\ngeneralization of) Martha White's RL task formalism (2017) and other recent\ndepartures from the traditional RL, and is relevant to task specification in\nRL, inverse RL and preference-based RL.",
    "pdf_url": "http://arxiv.org/pdf/1902.02893v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages + 1 page supplement. In proceedings of AAAI 2019. Slides,\n  poster and bibtex available at\n  https://silviupitis.com/#rethinking-the-discount-factor-in-reinforcement-learning-a-decision-theoretic-approach",
    "journal_ref": null,
    "citation_count": 53,
    "bibtex": "@Article{Pitis2019RethinkingTD,\n author = {Silviu Pitis},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Rethinking the Discount Factor in Reinforcement Learning: A Decision Theoretic Approach},\n volume = {abs/1902.02893},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2205.10032v1",
    "title": "Survey on Fair Reinforcement Learning: Theory and Practice",
    "published": "2022-05-20T09:07:28Z",
    "updated": "2022-05-20T09:07:28Z",
    "authors": [
      "Pratik Gajane",
      "Akrati Saxena",
      "Maryam Tavakol",
      "George Fletcher",
      "Mykola Pechenizkiy"
    ],
    "summary": "Fairness-aware learning aims at satisfying various fairness constraints in\naddition to the usual performance criteria via data-driven machine learning\ntechniques. Most of the research in fairness-aware learning employs the setting\nof fair-supervised learning. However, many dynamic real-world applications can\nbe better modeled using sequential decision-making problems and fair\nreinforcement learning provides a more suitable alternative for addressing\nthese problems. In this article, we provide an extensive overview of fairness\napproaches that have been implemented via a reinforcement learning (RL)\nframework. We discuss various practical applications in which RL methods have\nbeen applied to achieve a fair solution with high accuracy. We further include\nvarious facets of the theory of fair reinforcement learning, organizing them\ninto single-agent RL, multi-agent RL, long-term fairness via RL, and offline\nlearning. Moreover, we highlight a few major issues to explore in order to\nadvance the field of fair-RL, namely - i) correcting societal biases, ii)\nfeasibility of group fairness or individual fairness, and iii) explainability\nin RL. Our work is beneficial for both researchers and practitioners as we\ndiscuss articles providing mathematical guarantees as well as articles with\nempirical studies on real-world problems.",
    "pdf_url": "http://arxiv.org/pdf/2205.10032v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 18,
    "bibtex": "@Article{Gajane2022SurveyOF,\n author = {Pratik Gajane and A. Saxena and M. Tavakol and George Fletcher and Mykola Pechenizkiy},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Survey on Fair Reinforcement Learning: Theory and Practice},\n volume = {abs/2205.10032},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2312.10303v2",
    "title": "Online Restless Multi-Armed Bandits with Long-Term Fairness Constraints",
    "published": "2023-12-16T03:35:56Z",
    "updated": "2023-12-22T01:40:28Z",
    "authors": [
      "Shufan Wang",
      "Guojun Xiong",
      "Jian Li"
    ],
    "summary": "Restless multi-armed bandits (RMAB) have been widely used to model sequential\ndecision making problems with constraints. The decision maker (DM) aims to\nmaximize the expected total reward over an infinite horizon under an\n\"instantaneous activation constraint\" that at most B arms can be activated at\nany decision epoch, where the state of each arm evolves stochastically\naccording to a Markov decision process (MDP). However, this basic model fails\nto provide any fairness guarantee among arms. In this paper, we introduce\nRMAB-F, a new RMAB model with \"long-term fairness constraints\", where the\nobjective now is to maximize the long term reward while a minimum long-term\nactivation fraction for each arm must be satisfied. For the online RMAB-F\nsetting (i.e., the underlying MDPs associated with each arm are unknown to the\nDM), we develop a novel reinforcement learning (RL) algorithm named Fair-UCRL.\nWe prove that Fair-UCRL ensures probabilistic sublinear bounds on both the\nreward regret and the fairness violation regret. Compared with off-the-shelf RL\nmethods, our Fair-UCRL is much more computationally efficient since it contains\na novel exploitation that leverages a low-complexity index policy for making\ndecisions. Experimental results further demonstrate the effectiveness of our\nFair-UCRL.",
    "pdf_url": "http://arxiv.org/pdf/2312.10303v2",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2024",
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Wang2023OnlineRM,\n author = {Shu-Fan Wang and Guojun Xiong and Jian Li},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {15616-15624},\n title = {Online Restless Multi-Armed Bandits with Long-Term Fairness Constraints},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2303.04118v1",
    "title": "A Multiplicative Value Function for Safe and Efficient Reinforcement\n  Learning",
    "published": "2023-03-07T18:29:15Z",
    "updated": "2023-03-07T18:29:15Z",
    "authors": [
      "Nick Bührer",
      "Zhejun Zhang",
      "Alexander Liniger",
      "Fisher Yu",
      "Luc Van Gool"
    ],
    "summary": "An emerging field of sequential decision problems is safe Reinforcement\nLearning (RL), where the objective is to maximize the reward while obeying\nsafety constraints. Being able to handle constraints is essential for deploying\nRL agents in real-world environments, where constraint violations can harm the\nagent and the environment. To this end, we propose a safe model-free RL\nalgorithm with a novel multiplicative value function consisting of a safety\ncritic and a reward critic. The safety critic predicts the probability of\nconstraint violation and discounts the reward critic that only estimates\nconstraint-free returns. By splitting responsibilities, we facilitate the\nlearning task leading to increased sample efficiency. We integrate our approach\ninto two popular RL algorithms, Proximal Policy Optimization and Soft\nActor-Critic, and evaluate our method in four safety-focused environments,\nincluding classical RL benchmarks augmented with safety constraints and robot\nnavigation tasks with images and raw Lidar scans as observations. Finally, we\nmake the zero-shot sim-to-real transfer where a differential drive robot has to\nnavigate through a cluttered room. Our code can be found at\nhttps://github.com/nikeke19/Safe-Mult-RL.",
    "pdf_url": "http://arxiv.org/pdf/2303.04118v1",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Repository available at https://github.com/nikeke19/Safe-Mult-RL",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Bührer2023AMV,\n author = {Nick Bührer and Zhejun Zhang and Alexander Liniger and F. Yu and L. Gool},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {5582-5589},\n title = {A Multiplicative Value Function for Safe and Efficient Reinforcement Learning},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2502.04864v1",
    "title": "$TAR^2$: Temporal-Agent Reward Redistribution for Optimal Policy\n  Preservation in Multi-Agent Reinforcement Learning",
    "published": "2025-02-07T12:07:57Z",
    "updated": "2025-02-07T12:07:57Z",
    "authors": [
      "Aditya Kapoor",
      "Kale-ab Tessera",
      "Mayank Baranwal",
      "Harshad Khadilkar",
      "Stefano Albrecht",
      "Mingfei Sun"
    ],
    "summary": "In cooperative multi-agent reinforcement learning (MARL), learning effective\npolicies is challenging when global rewards are sparse and delayed. This\ndifficulty arises from the need to assign credit across both agents and time\nsteps, a problem that existing methods often fail to address in episodic,\nlong-horizon tasks. We propose Temporal-Agent Reward Redistribution $TAR^2$, a\nnovel approach that decomposes sparse global rewards into agent-specific,\ntime-step-specific components, thereby providing more frequent and accurate\nfeedback for policy learning. Theoretically, we show that $TAR^2$ (i) aligns\nwith potential-based reward shaping, preserving the same optimal policies as\nthe original environment, and (ii) maintains policy gradient update directions\nidentical to those under the original sparse reward, ensuring unbiased credit\nsignals. Empirical results on two challenging benchmarks, SMACLite and Google\nResearch Football, demonstrate that $TAR^2$ significantly stabilizes and\naccelerates convergence, outperforming strong baselines like AREL and STAS in\nboth learning speed and final performance. These findings establish $TAR^2$ as\na principled and practical solution for agent-temporal credit assignment in\nsparse-reward multi-agent systems.",
    "pdf_url": "http://arxiv.org/pdf/2502.04864v1",
    "doi": null,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "23 pages, 5 figures, 4 tables",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Kapoor2025TAR2TR,\n author = {Aditya Kapoor and K. Tessera and Mayank Baranwal and H. Khadilkar and Stefano V. Albrecht and Mingfei Sun},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {TAR2: Temporal-Agent Reward Redistribution for Optimal Policy Preservation in Multi-Agent Reinforcement Learning},\n volume = {abs/2502.04864},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2503.14554v1",
    "title": "Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot",
    "published": "2025-03-17T22:24:39Z",
    "updated": "2025-03-17T22:24:39Z",
    "authors": [
      "Ali Parsaee",
      "Fahim Shahriar",
      "Chuxin He",
      "Ruiqing Tan"
    ],
    "summary": "In recent times, reinforcement learning (RL) with physical robots has\nattracted the attention of a wide range of researchers. However,\nstate-of-the-art RL algorithms do not consider that physical environments do\nnot wait for the RL agent to make decisions or updates. RL agents learn by\nperiodically conducting computationally expensive gradient updates. When\ndecision-making and gradient update tasks are carried out sequentially by the\nRL agent in a physical robot, it significantly increases the agent's response\ntime. In a rapidly changing environment, this increased response time may be\ndetrimental to the performance of the learning agent. Asynchronous RL methods,\nwhich separate the computation of decision-making and gradient updates, are a\npotential solution to this problem. However, only a few comparisons between\nasynchronous and synchronous RL have been made with physical robots. For this\nreason, the exact performance benefits of using asynchronous RL methods over\nsynchronous RL methods are still unclear. In this study, we provide a\nperformance comparison between asynchronous and synchronous RL using a physical\nrobotic arm called Franka Emika Panda. Our experiments show that the agents\nlearn faster and attain significantly more returns using asynchronous RL. Our\nexperiments also demonstrate that the learning agent with a faster response\ntime performs better than the agent with a slower response time, even if the\nagent with a slower response time performs a higher number of gradient updates.",
    "pdf_url": "http://arxiv.org/pdf/2503.14554v1",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Presented at Alberta Robotics & Intelligent Systems Expo (RISE)\n  Conference",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Parsaee2025SynchronousVA,\n author = {Ali Parsaee and Fahim Shahriar and Chuxin He and Ruiqing Tan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot},\n volume = {abs/2503.14554},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2102.05815v1",
    "title": "Representation Matters: Offline Pretraining for Sequential Decision\n  Making",
    "published": "2021-02-11T02:38:12Z",
    "updated": "2021-02-11T02:38:12Z",
    "authors": [
      "Mengjiao Yang",
      "Ofir Nachum"
    ],
    "summary": "The recent success of supervised learning methods on ever larger offline\ndatasets has spurred interest in the reinforcement learning (RL) field to\ninvestigate whether the same paradigms can be translated to RL algorithms. This\nresearch area, known as offline RL, has largely focused on offline policy\noptimization, aiming to find a return-maximizing policy exclusively from\noffline data. In this paper, we consider a slightly different approach to\nincorporating offline data into sequential decision-making. We aim to answer\nthe question, what unsupervised objectives applied to offline datasets are able\nto learn state representations which elevate performance on downstream tasks,\nwhether those downstream tasks be online RL, imitation learning from expert\ndemonstrations, or even offline policy optimization based on the same offline\ndataset? Through a variety of experiments utilizing standard offline RL\ndatasets, we find that the use of pretraining with unsupervised learning\nobjectives can dramatically improve the performance of policy learning\nalgorithms that otherwise yield mediocre performance on their own. Extensive\nablations further provide insights into what components of these unsupervised\nobjectives -- e.g., reward prediction, continuous or discrete representations,\npretraining or finetuning -- are most important and in which settings.",
    "pdf_url": "http://arxiv.org/pdf/2102.05815v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 122,
    "bibtex": "@Article{Yang2021RepresentationMO,\n author = {Mengjiao Yang and Ofir Nachum},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Representation Matters: Offline Pretraining for Sequential Decision Making},\n volume = {abs/2102.05815},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2303.16563v2",
    "title": "Skill Reinforcement Learning and Planning for Open-World Long-Horizon\n  Tasks",
    "published": "2023-03-29T09:45:50Z",
    "updated": "2023-12-04T14:53:15Z",
    "authors": [
      "Haoqi Yuan",
      "Chi Zhang",
      "Hongcheng Wang",
      "Feiyang Xie",
      "Penglin Cai",
      "Hao Dong",
      "Zongqing Lu"
    ],
    "summary": "We study building multi-task agents in open-world environments. Without human\ndemonstrations, learning to accomplish long-horizon tasks in a large open-world\nenvironment with reinforcement learning (RL) is extremely inefficient. To\ntackle this challenge, we convert the multi-task learning problem into learning\nbasic skills and planning over the skills. Using the popular open-world game\nMinecraft as the testbed, we propose three types of fine-grained basic skills,\nand use RL with intrinsic rewards to acquire skills. A novel Finding-skill that\nperforms exploration to find diverse items provides better initialization for\nother skills, improving the sample efficiency for skill learning. In skill\nplanning, we leverage the prior knowledge in Large Language Models to find the\nrelationships between skills and build a skill graph. When the agent is solving\na task, our skill search algorithm walks on the skill graph and generates the\nproper skill plans for the agent. In experiments, our method accomplishes 40\ndiverse Minecraft tasks, where many tasks require sequentially executing for\nmore than 10 skills. Our method outperforms baselines by a large margin and is\nthe most sample-efficient demonstration-free RL method to solve Minecraft Tech\nTree tasks. The project's website and code can be found at\nhttps://sites.google.com/view/plan4mc.",
    "pdf_url": "http://arxiv.org/pdf/2303.16563v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, presented in Foundation Models for Decision Making Workshop\n  at NeurIPS 2023",
    "journal_ref": null,
    "citation_count": 24,
    "bibtex": "@Inproceedings{Yuan2023SkillRL,\n author = {Haoqi Yuan and Chi Zhang and Hongchen Wang and Feiyang Xie and Penglin Cai and Hao Dong and Zongqing Lu},\n title = {Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2307.12158v1",
    "title": "DIP-RL: Demonstration-Inferred Preference Learning in Minecraft",
    "published": "2023-07-22T20:05:31Z",
    "updated": "2023-07-22T20:05:31Z",
    "authors": [
      "Ellen Novoseller",
      "Vinicius G. Goecks",
      "David Watkins",
      "Josh Miller",
      "Nicholas Waytowich"
    ],
    "summary": "In machine learning for sequential decision-making, an algorithmic agent\nlearns to interact with an environment while receiving feedback in the form of\na reward signal. However, in many unstructured real-world settings, such a\nreward signal is unknown and humans cannot reliably craft a reward signal that\ncorrectly captures desired behavior. To solve tasks in such unstructured and\nopen-ended environments, we present Demonstration-Inferred Preference\nReinforcement Learning (DIP-RL), an algorithm that leverages human\ndemonstrations in three distinct ways, including training an autoencoder,\nseeding reinforcement learning (RL) training batches with demonstration data,\nand inferring preferences over behaviors to learn a reward function to guide\nRL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest\nthat the method can guide an RL agent to learn a reward function that reflects\nhuman preferences and that DIP-RL performs competitively relative to baselines.\nDIP-RL is inspired by our previous work on combining demonstrations and\npairwise preferences in Minecraft, which was awarded a research prize at the\n2022 NeurIPS MineRL BASALT competition, Learning from Human Feedback in\nMinecraft. Example trajectory rollouts of DIP-RL and baselines are located at\nhttps://sites.google.com/view/dip-rl.",
    "pdf_url": "http://arxiv.org/pdf/2307.12158v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "I.2.6; G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted at The Many Facets of Preference Learning Workshop at\n  the International Conference on Machine Learning (ICML), Honolulu, Hawaii,\n  USA, 2023",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Novoseller2023DIPRLDP,\n author = {Ellen R. Novoseller and Vinicius G. Goecks and David Watkins and J. Miller and Nicholas R. Waytowich},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {DIP-RL: Demonstration-Inferred Preference Learning in Minecraft},\n volume = {abs/2307.12158},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2312.10642v1",
    "title": "Episodic Return Decomposition by Difference of Implicitly Assigned\n  Sub-Trajectory Reward",
    "published": "2023-12-17T07:58:19Z",
    "updated": "2023-12-17T07:58:19Z",
    "authors": [
      "Haoxin Lin",
      "Hongqiu Wu",
      "Jiaji Zhang",
      "Yihao Sun",
      "Junyin Ye",
      "Yang Yu"
    ],
    "summary": "Real-world decision-making problems are usually accompanied by delayed\nrewards, which affects the sample efficiency of Reinforcement Learning,\nespecially in the extremely delayed case where the only feedback is the\nepisodic reward obtained at the end of an episode. Episodic return\ndecomposition is a promising way to deal with the episodic-reward setting.\nSeveral corresponding algorithms have shown remarkable effectiveness of the\nlearned step-wise proxy rewards from return decomposition. However, these\nexisting methods lack either attribution or representation capacity, leading to\ninefficient decomposition in the case of long-term episodes. In this paper, we\npropose a novel episodic return decomposition method called Diaster (Difference\nof implicitly assigned sub-trajectory reward). Diaster decomposes any episodic\nreward into credits of two divided sub-trajectories at any cut point, and the\nstep-wise proxy rewards come from differences in expectation. We theoretically\nand empirically verify that the decomposed proxy reward function can guide the\npolicy to be nearly optimal. Experimental results show that our method\noutperforms previous state-of-the-art methods in terms of both sample\nefficiency and performance.",
    "pdf_url": "http://arxiv.org/pdf/2312.10642v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Lin2023EpisodicRD,\n author = {Hao-Chu Lin and Hongqiu Wu and Jiaji Zhang and Yihao Sun and Junyin Ye and Yang Yu},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Episodic Return Decomposition by Difference of Implicitly Assigned Sub-Trajectory Reward},\n volume = {abs/2312.10642},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2502.10732v1",
    "title": "Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision\n  Optimization for Resource Allocation with Language Agents",
    "published": "2025-02-15T09:01:31Z",
    "updated": "2025-02-15T09:01:31Z",
    "authors": [
      "Mauricio Tec",
      "Guojun Xiong",
      "Haichuan Wang",
      "Francesca Dominici",
      "Milind Tambe"
    ],
    "summary": "Deep Reinforcement Learning (RL) is remarkably effective in addressing\nsequential resource allocation problems in domains such as healthcare, public\npolicy, and resource management. However, deep RL policies often lack\ntransparency and adaptability, challenging their deployment alongside human\ndecision-makers. In contrast, Language Agents, powered by large language models\n(LLMs), provide human-understandable reasoning but may struggle with effective\ndecision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement\nLearning (RBRL), a novel framework that jointly optimizes decision and\nexplanations. At each step, RBRL generates candidate rules with an LLM, selects\namong them using an attention-based RL policy, and determines the environment\naction with an explanation via chain-of-thought reasoning. The RL rule\nselection is optimized using the environment rewards and an explainability\nmetric judged by the LLM. Evaluations in real-world scenarios highlight RBRL's\ncompetitive performance with deep RL and efficiency gains over LLM fine-tuning.\nA survey further confirms the enhanced quality of its explanations.",
    "pdf_url": "http://arxiv.org/pdf/2502.10732v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Tec2025RuleBottleneckRL,\n author = {M. Tec and Guojun Xiong and Haichuan Wang and Francesca Dominici and Milind Tambe},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents},\n volume = {abs/2502.10732},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2111.14629v1",
    "title": "Improving Zero-shot Generalization in Offline Reinforcement Learning\n  using Generalized Similarity Functions",
    "published": "2021-11-29T15:42:54Z",
    "updated": "2021-11-29T15:42:54Z",
    "authors": [
      "Bogdan Mazoure",
      "Ilya Kostrikov",
      "Ofir Nachum",
      "Jonathan Tompson"
    ],
    "summary": "Reinforcement learning (RL) agents are widely used for solving complex\nsequential decision making tasks, but still exhibit difficulty in generalizing\nto scenarios not seen during training. While prior online approaches\ndemonstrated that using additional signals beyond the reward function can lead\nto better generalization capabilities in RL agents, i.e. using self-supervised\nlearning (SSL), they struggle in the offline RL setting, i.e. learning from a\nstatic dataset. We show that performance of online algorithms for\ngeneralization in RL can be hindered in the offline setting due to poor\nestimation of similarity between observations. We propose a new\ntheoretically-motivated framework called Generalized Similarity Functions\n(GSF), which uses contrastive learning to train an offline RL agent to\naggregate observations based on the similarity of their expected future\nbehavior, where we quantify this similarity using \\emph{generalized value\nfunctions}. We show that GSF is general enough to recover existing SSL\nobjectives while also improving zero-shot generalization performance on a\ncomplex offline RL benchmark, offline Procgen.",
    "pdf_url": "http://arxiv.org/pdf/2111.14629v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Offline RL workshop at NeurIPS 2021",
    "journal_ref": null,
    "citation_count": 24,
    "bibtex": "@Article{Mazoure2021ImprovingZG,\n author = {Bogdan Mazoure and Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Improving Zero-shot Generalization in Offline Reinforcement Learning using Generalized Similarity Functions},\n volume = {abs/2111.14629},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1802.09081v2",
    "title": "Temporal Difference Models: Model-Free Deep RL for Model-Based Control",
    "published": "2018-02-25T21:14:44Z",
    "updated": "2020-02-24T06:34:11Z",
    "authors": [
      "Vitchyr Pong",
      "Shixiang Gu",
      "Murtaza Dalal",
      "Sergey Levine"
    ],
    "summary": "Model-free reinforcement learning (RL) is a powerful, general tool for\nlearning complex behaviors. However, its sample efficiency is often\nimpractically large for solving challenging real-world problems, even with\noff-policy algorithms such as Q-learning. A limiting factor in classic\nmodel-free RL is that the learning signal consists only of scalar rewards,\nignoring much of the rich information contained in state transition tuples.\nModel-based RL uses this information, by training a predictive model, but often\ndoes not achieve the same asymptotic performance as model-free RL due to model\nbias. We introduce temporal difference models (TDMs), a family of\ngoal-conditioned value functions that can be trained with model-free learning\nand used for model-based control. TDMs combine the benefits of model-free and\nmodel-based RL: they leverage the rich information in state transitions to\nlearn very efficiently, while still attaining asymptotic performance that\nexceeds that of direct model-based RL methods. Our experimental results show\nthat, on a range of continuous control tasks, TDMs provide a substantial\nimprovement in efficiency compared to state-of-the-art model-based and\nmodel-free methods.",
    "pdf_url": "http://arxiv.org/pdf/1802.09081v2",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Appeared in ICLR 2018; typos corrected",
    "journal_ref": null,
    "citation_count": 247,
    "bibtex": "@Article{Pong2018TemporalDM,\n author = {Vitchyr H. Pong and S. Gu and Murtaza Dalal and S. Levine},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Temporal Difference Models: Model-Free Deep RL for Model-Based Control},\n volume = {abs/1802.09081},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2102.03261v1",
    "title": "Revisiting Prioritized Experience Replay: A Value Perspective",
    "published": "2021-02-05T16:09:07Z",
    "updated": "2021-02-05T16:09:07Z",
    "authors": [
      "Ang A. Li",
      "Zongqing Lu",
      "Chenglin Miao"
    ],
    "summary": "Experience replay enables off-policy reinforcement learning (RL) agents to\nutilize past experiences to maximize the cumulative reward. Prioritized\nexperience replay that weighs experiences by the magnitude of their\ntemporal-difference error ($|\\text{TD}|$) significantly improves the learning\nefficiency. But how $|\\text{TD}|$ is related to the importance of experience is\nnot well understood. We address this problem from an economic perspective, by\nlinking $|\\text{TD}|$ to value of experience, which is defined as the value\nadded to the cumulative reward by accessing the experience. We theoretically\nshow the value metrics of experience are upper-bounded by $|\\text{TD}|$ for\nQ-learning. Furthermore, we successfully extend our theoretical framework to\nmaximum-entropy RL by deriving the lower and upper bounds of these value\nmetrics for soft Q-learning, which turn out to be the product of $|\\text{TD}|$\nand \"on-policyness\" of the experiences. Our framework links two important\nquantities in RL: $|\\text{TD}|$ and value of experience. We empirically show\nthat the bounds hold in practice, and experience replay using the upper bound\nas priority improves maximum-entropy RL in Atari games.",
    "pdf_url": "http://arxiv.org/pdf/2102.03261v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "journal_ref": null,
    "citation_count": 9,
    "bibtex": "@Article{Li2021RevisitingPE,\n author = {Ang Li and Zongqing Lu and Chenglin Miao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Revisiting Prioritized Experience Replay: A Value Perspective},\n volume = {abs/2102.03261},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2307.11166v1",
    "title": "Exploring reinforcement learning techniques for discrete and continuous\n  control tasks in the MuJoCo environment",
    "published": "2023-07-20T18:01:48Z",
    "updated": "2023-07-20T18:01:48Z",
    "authors": [
      "Vaddadi Sai Rahul",
      "Debajyoti Chakraborty"
    ],
    "summary": "We leverage the fast physics simulator, MuJoCo to run tasks in a continuous\ncontrol environment and reveal details like the observation space, action\nspace, rewards, etc. for each task. We benchmark value-based methods for\ncontinuous control by comparing Q-learning and SARSA through a discretization\napproach, and using them as baselines, progressively moving into one of the\nstate-of-the-art deep policy gradient method DDPG. Over a large number of\nepisodes, Qlearning outscored SARSA, but DDPG outperformed both in a small\nnumber of episodes. Lastly, we also fine-tuned the model hyper-parameters\nexpecting to squeeze more performance but using lesser time and resources. We\nanticipated that the new design for DDPG would vastly improve performance, yet\nafter only a few episodes, we were able to achieve decent average rewards. We\nexpect to improve the performance provided adequate time and computational\nresources.",
    "pdf_url": "http://arxiv.org/pdf/2307.11166v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Released @ Dec 2021. For associated project files, see\n  https://github.com/chakrabortyde/mujoco-control-tasks",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Rahul2023ExploringRL,\n author = {Vaddadi Sai Rahul and Debajyoti Chakraborty},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Exploring reinforcement learning techniques for discrete and continuous control tasks in the MuJoCo environment},\n volume = {abs/2307.11166},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2201.06027v1",
    "title": "A Reliable Reinforcement Learning for Resource Allocation in Uplink\n  NOMA-URLLC Networks",
    "published": "2022-01-16T11:58:05Z",
    "updated": "2022-01-16T11:58:05Z",
    "authors": [
      "Waleed Ahsan",
      "Wenqiang Yi",
      "Yuanwei Liu",
      "Arumugam Nallanathan"
    ],
    "summary": "In this paper, we propose a deep state-action-reward-state-action (SARSA)\n$\\lambda$ learning approach for optimising the uplink resource allocation in\nnon-orthogonal multiple access (NOMA) aided ultra-reliable low-latency\ncommunication (URLLC). To reduce the mean decoding error probability in\ntime-varying network environments, this work designs a reliable learning\nalgorithm for providing a long-term resource allocation, where the reward\nfeedback is based on the instantaneous network performance. With the aid of the\nproposed algorithm, this paper addresses three main challenges of the reliable\nresource sharing in NOMA-URLLC networks: 1) user clustering; 2) Instantaneous\nfeedback system; and 3) Optimal resource allocation. All of these designs\ninteract with the considered communication environment. Lastly, we compare the\nperformance of the proposed algorithm with conventional Q-learning and SARSA\nQ-learning algorithms. The simulation outcomes show that: 1) Compared with the\ntraditional Q learning algorithms, the proposed solution is able to converges\nwithin \\myb{200} episodes for providing as low as $10^{-2}$ long-term mean\nerror; 2) NOMA assisted URLLC outperforms traditional OMA systems in terms of\ndecoding error probabilities; and 3) The proposed feedback system is efficient\nfor the long-term learning process.",
    "pdf_url": "http://arxiv.org/pdf/2201.06027v1",
    "doi": null,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "32 pages, 8 figures",
    "journal_ref": null,
    "citation_count": 17,
    "bibtex": "@Article{Ahsan2022ARR,\n author = {Waleed Ahsan and Wenqiang Yi and Yuanwei Liu and A. Nallanathan},\n booktitle = {IEEE Transactions on Wireless Communications},\n journal = {IEEE Transactions on Wireless Communications},\n pages = {5989-6002},\n title = {A Reliable Reinforcement Learning for Resource Allocation in Uplink NOMA-URLLC Networks},\n volume = {21},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2210.01800v1",
    "title": "Bayesian Q-learning With Imperfect Expert Demonstrations",
    "published": "2022-10-01T17:38:19Z",
    "updated": "2022-10-01T17:38:19Z",
    "authors": [
      "Fengdi Che",
      "Xiru Zhu",
      "Doina Precup",
      "David Meger",
      "Gregory Dudek"
    ],
    "summary": "Guided exploration with expert demonstrations improves data efficiency for\nreinforcement learning, but current algorithms often overuse expert\ninformation. We propose a novel algorithm to speed up Q-learning with the help\nof a limited amount of imperfect expert demonstrations. The algorithm avoids\nexcessive reliance on expert data by relaxing the optimal expert assumption and\ngradually reducing the usage of uninformative expert data. Experimentally, we\nevaluate our approach on a sparse-reward chain environment and six more\ncomplicated Atari games with delayed rewards. With the proposed methods, we can\nachieve better results than Deep Q-learning from Demonstrations (Hester et al.,\n2017) in most environments.",
    "pdf_url": "http://arxiv.org/pdf/2210.01800v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Che2022BayesianQW,\n author = {Fengdi Che and Xiru Zhu and Doina Precup and D. Meger and Gregory Dudek},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Bayesian Q-learning With Imperfect Expert Demonstrations},\n volume = {abs/2210.01800},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1210.3569v2",
    "title": "Autonomous Reinforcement of Behavioral Sequences in Neural Dynamics",
    "published": "2012-10-12T16:41:58Z",
    "updated": "2013-05-14T15:07:35Z",
    "authors": [
      "Sohrob Kazerounian",
      "Matthew Luciw",
      "Mathis Richter",
      "Yulia Sandamirskaya"
    ],
    "summary": "We introduce a dynamic neural algorithm called Dynamic Neural (DN)\nSARSA(\\lambda) for learning a behavioral sequence from delayed reward.\nDN-SARSA(\\lambda) combines Dynamic Field Theory models of behavioral sequence\nrepresentation, classical reinforcement learning, and a computational\nneuroscience model of working memory, called Item and Order working memory,\nwhich serves as an eligibility trace. DN-SARSA(\\lambda) is implemented on both\na simulated and real robot that must learn a specific rewarding sequence of\nelementary behaviors from exploration. Results show DN-SARSA(\\lambda) performs\non the level of the discrete SARSA(\\lambda), validating the feasibility of\ngeneral reinforcement learning without compromising neural dynamics.",
    "pdf_url": "http://arxiv.org/pdf/1210.3569v2",
    "doi": null,
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE",
    "comment": "Sohrob Kazerounian, Matthew Luciw are Joint first authors",
    "journal_ref": null,
    "citation_count": 13,
    "bibtex": "@Article{Kazerounian2012AutonomousRO,\n author = {Sohrob Kazerounian and M. Luciw and Yulia Sandamirskaya and Mathis Richter and J. Schmidhuber and G. Schöner},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {The 2013 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {Autonomous reinforcement of behavioral sequences in neural dynamics},\n year = {2012}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1804.08607v1",
    "title": "Benchmarking projective simulation in navigation problems",
    "published": "2018-04-23T17:58:27Z",
    "updated": "2018-04-23T17:58:27Z",
    "authors": [
      "Alexey A. Melnikov",
      "Adi Makmal",
      "Hans J. Briegel"
    ],
    "summary": "Projective simulation (PS) is a model for intelligent agents with a\ndeliberation capacity that is based on episodic memory. The model has been\nshown to provide a flexible framework for constructing reinforcement-learning\nagents, and it allows for quantum mechanical generalization, which leads to a\nspeed-up in deliberation time. PS agents have been applied successfully in the\ncontext of complex skill learning in robotics, and in the design of\nstate-of-the-art quantum experiments. In this paper, we study the performance\nof projective simulation in two benchmarking problems in navigation, namely the\ngrid world and the mountain car problem. The performance of PS is compared to\nstandard tabular reinforcement learning approaches, Q-learning and SARSA. Our\ncomparison demonstrates that the performance of PS and standard learning\napproaches are qualitatively and quantitatively similar, while it is much\neasier to choose optimal model parameters in case of projective simulation,\nwith a reduced computational effort of one to two orders of magnitude. Our\nresults show that the projective simulation model stands out for its simplicity\nin terms of the number of model parameters, which makes it simple to set up the\nlearning agent in unknown task environments.",
    "pdf_url": "http://arxiv.org/pdf/1804.08607v1",
    "doi": "10.1109/ACCESS.2018.2876494",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 10 figures",
    "journal_ref": "IEEE Access 6, 64639 (2018)",
    "citation_count": 20,
    "bibtex": "@Article{Melnikov2018BenchmarkingPS,\n author = {A. Melnikov and A. Makmal and H. Briegel},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {64639-64648},\n title = {Benchmarking Projective Simulation in Navigation Problems},\n volume = {6},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2011.03780v1",
    "title": "Deep Reinforcement Learning Based Dynamic Power and Beamforming Design\n  for Time-Varying Wireless Downlink Interference Channel",
    "published": "2020-11-07T14:23:44Z",
    "updated": "2020-11-07T14:23:44Z",
    "authors": [
      "Mengfan Liu",
      "Rui Wang"
    ],
    "summary": "With the high development of wireless communication techniques, it is widely\nused in various fields for convenient and efficient data transmission.\nDifferent from commonly used assumption of the time-invariant wireless channel,\nwe focus on the research on the time-varying wireless downlink channel to get\nclose to the practical situation. Our objective is to gain the maximum value of\nsum rate in the time-varying channel under the some constraints about cut-off\nsignal-to-interference and noise ratio (SINR), transmitted power and\nbeamforming. In order to adapt the rapid changing channel, we abandon the\nfrequently used algorithm convex optimization and deep reinforcement learning\nalgorithms are used in this paper. From the view of the ordinary measures such\nas power control, interference incoordination and beamforming, continuous\nchanges of measures should be put into consideration while sparse reward\nproblem due to the abortion of episodes as an important bottleneck should not\nbe ignored. Therefore, with the analysis of relevant algorithms, we proposed\ntwo algorithms, Deep Deterministic Policy Gradient algorithm (DDPG) and\nhierarchical DDPG, in our work. As for these two algorithms, in order to solve\nthe discrete output, DDPG is established by combining the Actor-Critic\nalgorithm with Deep Q-learning (DQN), so that it can output the continuous\nactions without sacrificing the existed advantages brought by DQN and also can\nimprove the performance. Also, to address the challenge of sparse reward, we\ntake advantage of meta policy from the idea of hierarchical theory to divide\none agent in DDPG into one meta-controller and one controller as hierarchical\nDDPG. Our simulation results demonstrate that the proposed DDPG and\nhierarchical DDPG performs well from the views of coverage, convergence and sum\nrate performance.",
    "pdf_url": "http://arxiv.org/pdf/2011.03780v1",
    "doi": null,
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Liu2020DeepRL,\n author = {Mengfan Liu and Rui Wang},\n booktitle = {IEEE Wireless Communications and Networking Conference},\n journal = {2022 IEEE Wireless Communications and Networking Conference (WCNC)},\n pages = {471-476},\n title = {Deep Reinforcement Learning Based Dynamic Power and Beamforming Design for Time-Varying Wireless Downlink Interference Channel},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2411.14783v4",
    "title": "Segmenting Action-Value Functions Over Time-Scales in SARSA via\n  TD($Δ$)",
    "published": "2024-11-22T07:52:28Z",
    "updated": "2025-09-04T07:01:56Z",
    "authors": [
      "Mahammad Humayoo"
    ],
    "summary": "In numerous episodic reinforcement learning (RL) environments, SARSA-based\nmethodologies are employed to enhance policies aimed at maximizing returns over\nlong horizons. Traditional SARSA algorithms face challenges in achieving an\noptimal balance between bias and variation, primarily due to their dependence\non a single, constant discount factor ($\\eta$). This investigation enhances the\ntemporal difference decomposition method, TD($\\Delta$), by applying it to the\nSARSA algorithm, now designated as SARSA($\\Delta$). SARSA is a widely used\non-policy RL method that enhances action-value functions via temporal\ndifference updates. By splitting the action-value function down into components\nthat are linked to specific discount factors, SARSA($\\Delta$) makes learning\neasier across a range of time scales. This analysis makes learning more\neffective and ensures consistency, particularly in situations where\nlong-horizon improvement is needed. The results of this research show that the\nsuggested strategy works to lower bias in SARSA's updates and speed up\nconvergence in both deterministic and stochastic settings, even in dense reward\nAtari environments. Experimental results from a variety of benchmark settings\nshow that the proposed SARSA($\\Delta$) outperforms existing TD learning\ntechniques in both tabular and deep RL environments.",
    "pdf_url": "http://arxiv.org/pdf/2411.14783v4",
    "doi": null,
    "categories": [
      "cs.LG",
      "F.2.2, I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages. arXiv admin note: text overlap with arXiv:2411.14019",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Humayoo2024SegmentingAF,\n author = {Mahammad Humayoo},\n title = {Segmenting Action-Value Functions Over Time-Scales in SARSA via TD($\\Delta$)},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1909.05912v2",
    "title": "Joint Inference of Reward Machines and Policies for Reinforcement\n  Learning",
    "published": "2019-09-12T19:09:13Z",
    "updated": "2022-02-08T20:02:19Z",
    "authors": [
      "Zhe Xu",
      "Ivan Gavran",
      "Yousef Ahmad",
      "Rupak Majumdar",
      "Daniel Neider",
      "Ufuk Topcu",
      "Bo Wu"
    ],
    "summary": "Incorporating high-level knowledge is an effective way to expedite\nreinforcement learning (RL), especially for complex tasks with sparse rewards.\nWe investigate an RL problem where the high-level knowledge is in the form of\nreward machines, i.e., a type of Mealy machine that encodes the reward\nfunctions. We focus on a setting in which this knowledge is a priori not\navailable to the learning agent. We develop an iterative algorithm that\nperforms joint inference of reward machines and policies for RL (more\nspecifically, q-learning). In each iteration, the algorithm maintains a\nhypothesis reward machine and a sample of RL episodes. It derives q-functions\nfrom the current hypothesis reward machine, and performs RL to update the\nq-functions. While performing RL, the algorithm updates the sample by adding RL\nepisodes along which the obtained rewards are inconsistent with the rewards\nbased on the current hypothesis reward machine. In the next iteration, the\nalgorithm infers a new hypothesis reward machine from the updated sample. Based\non an equivalence relationship we defined between states of reward machines, we\ntransfer the q-functions between the hypothesis reward machines in consecutive\niterations. We prove that the proposed algorithm converges almost surely to an\noptimal policy in the limit if a minimal reward machine can be inferred and the\nmaximal length of each RL episode is sufficiently long. The experiments show\nthat learning high-level knowledge in the form of reward machines can lead to\nfast convergence to optimal policies in RL, while standard RL methods such as\nq-learning and hierarchical RL methods fail to converge to optimal policies\nafter a substantial number of training steps in many tasks.",
    "pdf_url": "http://arxiv.org/pdf/1909.05912v2",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Fixed incorrect references in proof of Lemma 4",
    "journal_ref": null,
    "citation_count": 102,
    "bibtex": "@Article{Xu2019JointIO,\n author = {Zhe Xu and I. Gavran and Yousef Ahmad and R. Majumdar and D. Neider and U. Topcu and Bo Wu},\n booktitle = {International Conference on Automated Planning and Scheduling},\n pages = {590-598},\n title = {Joint Inference of Reward Machines and Policies for Reinforcement Learning},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2006.14795v1",
    "title": "Q-Learning with Differential Entropy of Q-Tables",
    "published": "2020-06-26T04:37:10Z",
    "updated": "2020-06-26T04:37:10Z",
    "authors": [
      "Tung D. Nguyen",
      "Kathryn E. Kasmarik",
      "Hussein A. Abbass"
    ],
    "summary": "It is well-known that information loss can occur in the classic and simple\nQ-learning algorithm. Entropy-based policy search methods were introduced to\nreplace Q-learning and to design algorithms that are more robust against\ninformation loss. We conjecture that the reduction in performance during\nprolonged training sessions of Q-learning is caused by a loss of information,\nwhich is non-transparent when only examining the cumulative reward without\nchanging the Q-learning algorithm itself. We introduce Differential Entropy of\nQ-tables (DE-QT) as an external information loss detector to the Q-learning\nalgorithm. The behaviour of DE-QT over training episodes is analyzed to find an\nappropriate stopping criterion during training. The results reveal that DE-QT\ncan detect the most appropriate stopping point, where a balance between a high\nsuccess rate and a high efficiency is met for classic Q-Learning algorithm.",
    "pdf_url": "http://arxiv.org/pdf/2006.14795v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Nguyen2020QLearningWD,\n author = {Tung D. Nguyen and Kathryn E. Kasmarik and H. Abbass},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Q-Learning with Differential Entropy of Q-Tables},\n volume = {abs/2006.14795},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2312.05787v1",
    "title": "Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a\n  High Replay Ratio and Regularization",
    "published": "2023-12-10T06:30:19Z",
    "updated": "2023-12-10T06:30:19Z",
    "authors": [
      "Takuya Hiraoka"
    ],
    "summary": "Reinforcement learning (RL) methods with a high replay ratio (RR) and\nregularization have gained interest due to their superior sample efficiency.\nHowever, these methods have mainly been developed for dense-reward tasks. In\nthis paper, we aim to extend these RL methods to sparse-reward goal-conditioned\ntasks. We use Randomized Ensemble Double Q-learning (REDQ) (Chen et al., 2021),\nan RL method with a high RR and regularization. To apply REDQ to sparse-reward\ngoal-conditioned tasks, we make the following modifications to it: (i) using\nhindsight experience replay and (ii) bounding target Q-values. We evaluate REDQ\nwith these modifications on 12 sparse-reward goal-conditioned tasks of Robotics\n(Plappert et al., 2018), and show that it achieves about $2 \\times$ better\nsample efficiency than previous state-of-the-art (SoTA) RL methods.\nFurthermore, we reconsider the necessity of specific components of REDQ and\nsimplify it by removing unnecessary ones. The simplified REDQ with our\nmodifications achieves $\\sim 8 \\times$ better sample efficiency than the SoTA\nmethods in 4 Fetch tasks of Robotics.",
    "pdf_url": "http://arxiv.org/pdf/2312.05787v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Source code:\n  https://github.com/TakuyaHiraoka/Efficient-SRGC-RL-with-a-High-RR-and-Regularization\n  Demo video:\n  https://drive.google.com/file/d/1UHd7JVPCwFLNFhy1QcycQfwU_nll_yII/view?usp=drive_link",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Hiraoka2023EfficientSG,\n author = {Takuya Hiraoka},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization},\n volume = {abs/2312.05787},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2507.10174v1",
    "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement\n  Learning?",
    "published": "2025-07-14T11:36:31Z",
    "updated": "2025-07-14T11:36:31Z",
    "authors": [
      "Yumi Omori",
      "Zixuan Dong",
      "Keith Ross"
    ],
    "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?",
    "pdf_url": "http://arxiv.org/pdf/2507.10174v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Omori2025ShouldWE,\n author = {Yumi Omori and Zixuan Dong and Keith Ross},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?},\n volume = {abs/2507.10174},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2106.08863v1",
    "title": "Unbiased Methods for Multi-Goal Reinforcement Learning",
    "published": "2021-06-16T15:31:51Z",
    "updated": "2021-06-16T15:31:51Z",
    "authors": [
      "Léonard Blier",
      "Yann Ollivier"
    ],
    "summary": "In multi-goal reinforcement learning (RL) settings, the reward for each goal\nis sparse, and located in a small neighborhood of the goal. In large dimension,\nthe probability of reaching a reward vanishes and the agent receives little\nlearning signal. Methods such as Hindsight Experience Replay (HER) tackle this\nissue by also learning from realized but unplanned-for goals. But HER is known\nto introduce bias, and can converge to low-return policies by overestimating\nchancy outcomes. First, we vindicate HER by proving that it is actually\nunbiased in deterministic environments, such as many optimal control settings.\nNext, for stochastic environments in continuous spaces, we tackle sparse\nrewards by directly taking the infinitely sparse reward limit. We fully\nformalize the problem of multi-goal RL with infinitely sparse Dirac rewards at\neach goal. We introduce unbiased deep Q-learning and actor-critic algorithms\nthat can handle such infinitely sparse rewards, and test them in toy\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2106.08863v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Inproceedings{Blier2021UnbiasedMF,\n author = {Léonard Blier and Y. Ollivier},\n title = {Unbiased Methods for Multi-Goal Reinforcement Learning},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2410.06648v5",
    "title": "Q-WSL: Optimizing Goal-Conditioned RL with Weighted Supervised Learning\n  via Dynamic Programming",
    "published": "2024-10-09T08:00:12Z",
    "updated": "2025-06-07T02:05:57Z",
    "authors": [
      "Xing Lei",
      "Xuetao Zhang",
      "Zifeng Zhuang",
      "Donglin Wang"
    ],
    "summary": "A novel class of advanced algorithms, termed Goal-Conditioned Weighted\nSupervised Learning (GCWSL), has recently emerged to tackle the challenges\nposed by sparse rewards in goal-conditioned reinforcement learning (RL). GCWSL\nconsistently delivers strong performance across a diverse set of goal-reaching\ntasks due to its simplicity, effectiveness, and stability. However, GCWSL\nmethods lack a crucial capability known as trajectory stitching, which is\nessential for learning optimal policies when faced with unseen skills during\ntesting. This limitation becomes particularly pronounced when the replay buffer\nis predominantly filled with sub-optimal trajectories. In contrast, traditional\nTD-based RL methods, such as Q-learning, which utilize Dynamic Programming, do\nnot face this issue but often experience instability due to the inherent\ndifficulties in value function approximation. In this paper, we propose\nQ-learning Weighted Supervised Learning (Q-WSL), a novel framework designed to\novercome the limitations of GCWSL by incorporating the strengths of Dynamic\nProgramming found in Q-learning. Q-WSL leverages Dynamic Programming results to\noutput the optimal action of (state, goal) pairs across different trajectories\nwithin the replay buffer. This approach synergizes the strengths of both\nQ-learning and GCWSL, effectively mitigating their respective weaknesses and\nenhancing overall performance. Empirical evaluations on challenging\ngoal-reaching tasks demonstrate that Q-WSL surpasses other goal-conditioned\napproaches in terms of both performance and sample efficiency. Additionally,\nQ-WSL exhibits notable robustness in environments characterized by binary\nreward structures and environmental stochasticity.",
    "pdf_url": "http://arxiv.org/pdf/2410.06648v5",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Lei2024QWSLOG,\n author = {Xing Lei and Xuetao Zhang and Zifeng Zhuang and Donglin Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Q-WSL: Optimizing Goal-Conditioned RL with Weighted Supervised Learning via Dynamic Programming},\n volume = {abs/2410.06648},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1911.04868v1",
    "title": "Challenging On Car Racing Problem from OpenAI gym",
    "published": "2019-11-02T20:14:55Z",
    "updated": "2019-11-02T20:14:55Z",
    "authors": [
      "Changmao Li"
    ],
    "summary": "This project challenges the car racing problem from OpenAI gym environment.\nThe problem is very challenging since it requires computer to finish the\ncontinuous control task by learning from pixels. To tackle this challenging\nproblem, we explored two approaches including evolutionary algorithm based\ngenetic multi-layer perceptron and double deep Q-learning network. The result\nshows that the genetic multi-layer perceptron can converge fast but when\ntraining many episodes, double deep Q-learning can get better score. We analyze\nthe result and draw a conclusion that for limited hardware resources, using\ngenetic multi-layer perceptron sometimes can be more efficient.",
    "pdf_url": "http://arxiv.org/pdf/1911.04868v1",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Li2019ChallengingOC,\n author = {Changmao Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Challenging On Car Racing Problem from OpenAI gym},\n volume = {abs/1911.04868},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1905.13406v1",
    "title": "RSS-Based Q-Learning for Indoor UAV Navigation",
    "published": "2019-05-31T04:08:21Z",
    "updated": "2019-05-31T04:08:21Z",
    "authors": [
      "Md Moin Uddin Chowdhury",
      "Fatih Erden",
      "Ismail Guvenc"
    ],
    "summary": "In this paper, we focus on the potential use of unmanned aerial vehicles\n(UAVs) for search and rescue (SAR) missions in GPS-denied indoor environments.\nWe consider the problem of navigating a UAV to a wireless signal source, e.g.,\na smartphone or watch owned by a victim. We assume that the source periodically\ntransmits RF signals to nearby wireless access points. Received signal strength\n(RSS) at the UAV, which is a function of the UAV and source positions, is fed\nto a Q-learning algorithm and the UAV is navigated to the vicinity of the\nsource. Unlike the traditional location-based Q-learning approach that uses the\nGPS coordinates of the agent, our method uses the RSS to define the states and\nrewards of the algorithm. It does not require any a priori information about\nthe environment. These, in turn, make it possible to use the UAVs in indoor SAR\noperations. Two indoor scenarios with different dimensions are created using a\nray tracing software. Then, the corresponding heat maps that show the RSS at\neach possible UAV location are extracted for more realistic analysis.\nPerformance of the RSS-based Q-learning algorithm is compared with the baseline\n(location-based) Q-learning algorithm in terms of convergence speed, average\nnumber of steps per episode, and the total length of the final trajectory. Our\nresults show that the RSS-based Q-learning provides competitive performance\nwith the location-based Q-learning.",
    "pdf_url": "http://arxiv.org/pdf/1905.13406v1",
    "doi": null,
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP",
    "comment": null,
    "journal_ref": null,
    "citation_count": 41,
    "bibtex": "@Article{Chowdhury2019RSSBasedQF,\n author = {Md Moin Uddin Chowdhury and F. Erden and Ismail Güvenç},\n booktitle = {IEEE Military Communications Conference},\n journal = {MILCOM 2019 - 2019 IEEE Military Communications Conference (MILCOM)},\n pages = {121-126},\n title = {RSS-Based Q-Learning for Indoor UAV Navigation},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2403.01112v2",
    "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent\n  Reinforcement Learning",
    "published": "2024-03-02T07:37:05Z",
    "updated": "2024-03-07T13:40:04Z",
    "authors": [
      "Hyungho Na",
      "Yunkyeong Seo",
      "Il-chul Moon"
    ],
    "summary": "In cooperative multi-agent reinforcement learning (MARL), agents aim to\nachieve a common goal, such as defeating enemies or scoring a goal. Existing\nMARL algorithms are effective but still require significant learning time and\noften get trapped in local optima by complex tasks, subsequently failing to\ndiscover a goal-reaching policy. To address this, we introduce Efficient\nepisodic Memory Utilization (EMU) for MARL, with two primary objectives: (a)\naccelerating reinforcement learning by leveraging semantically coherent memory\nfrom an episodic buffer and (b) selectively promoting desirable transitions to\nprevent local convergence. To achieve (a), EMU incorporates a trainable\nencoder/decoder structure alongside MARL, creating coherent memory embeddings\nthat facilitate exploratory memory recall. To achieve (b), EMU introduces a\nnovel reward structure called episodic incentive based on the desirability of\nstates. This reward improves the TD target in Q-learning and acts as an\nadditional incentive for desirable transitions. We provide theoretical support\nfor the proposed incentive and demonstrate the effectiveness of EMU compared to\nconventional episodic control. The proposed method is evaluated in StarCraft II\nand Google Research Football, and empirical results indicate further\nperformance improvement over state-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2403.01112v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2024",
    "journal_ref": null,
    "citation_count": 9,
    "bibtex": "@Article{Na2024EfficientEM,\n author = {Hyungho Na and Yunkyeong Seo and Il-Chul Moon},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning},\n volume = {abs/2403.01112},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1711.05715v2",
    "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for\n  Task-Oriented Dialogue Systems",
    "published": "2017-11-15T18:23:48Z",
    "updated": "2017-11-20T04:22:45Z",
    "authors": [
      "Zachary Lipton",
      "Xiujun Li",
      "Jianfeng Gao",
      "Lihong Li",
      "Faisal Ahmed",
      "Li Deng"
    ],
    "summary": "We present a new algorithm that significantly improves the efficiency of\nexploration for deep Q-learning agents in dialogue systems. Our agents explore\nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop\nneural network. Our algorithm learns much faster than common exploration\nstrategies such as \\epsilon-greedy, Boltzmann, bootstrapping, and\nintrinsic-reward-based ones. Additionally, we show that spiking the replay\nbuffer with experiences from just a few successful episodes can make Q-learning\nfeasible when it might otherwise fail.",
    "pdf_url": "http://arxiv.org/pdf/1711.05715v2",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Duplicate of article already in the arXiv: arXiv:1608.05081",
    "journal_ref": null,
    "citation_count": 173,
    "bibtex": "@Article{Lipton2016BBQNetworksEE,\n author = {Zachary Chase Lipton and Xiujun Li and Jianfeng Gao and Lihong Li and Faisal Ahmed and L. Deng},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {5237-5244},\n title = {BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for Task-Oriented Dialogue Systems},\n year = {2016}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2405.15194v2",
    "title": "Extracting Heuristics from Large Language Models for Reward Shaping in\n  Reinforcement Learning",
    "published": "2024-05-24T03:53:57Z",
    "updated": "2024-10-07T19:33:34Z",
    "authors": [
      "Siddhant Bhambri",
      "Amrita Bhattacharjee",
      "Durgesh Kalwar",
      "Lin Guan",
      "Huan Liu",
      "Subbarao Kambhampati"
    ],
    "summary": "Reinforcement Learning (RL) suffers from sample inefficiency in sparse reward\ndomains, and the problem is further pronounced in case of stochastic\ntransitions. To improve the sample efficiency, reward shaping is a well-studied\napproach to introduce intrinsic rewards that can help the RL agent converge to\nan optimal policy faster. However, designing a useful reward shaping function\nfor all desirable states in the Markov Decision Process (MDP) is challenging,\neven for domain experts. Given that Large Language Models (LLMs) have\ndemonstrated impressive performance across a magnitude of natural language\ntasks, we aim to answer the following question: `Can we obtain heuristics using\nLLMs for constructing a reward shaping function that can boost an RL agent's\nsample efficiency?' To this end, we aim to leverage off-the-shelf LLMs to\ngenerate a plan for an abstraction of the underlying MDP. We further use this\nLLM-generated plan as a heuristic to construct the reward shaping signal for\nthe downstream RL agent. By characterizing the type of abstraction based on the\nMDP horizon length, we analyze the quality of heuristics when generated using\nan LLM, with and without a verifier in the loop. Our experiments across\nmultiple domains with varying horizon length and number of sub-goals from the\nBabyAI environment suite, Household, Mario, and, Minecraft domain, show 1) the\nadvantages and limitations of querying LLMs with and without a verifier to\ngenerate a reward shaping heuristic, and, 2) a significant improvement in the\nsample efficiency of PPO, A2C, and Q-learning when guided by the LLM-generated\nheuristics.",
    "pdf_url": "http://arxiv.org/pdf/2405.15194v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Bhambri2024ExtractingHF,\n author = {Siddhant Bhambri and Amrita Bhattacharjee and Huan Liu and Subbarao Kambhampati},\n title = {Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1009.2566v1",
    "title": "Reinforcement Learning by Comparing Immediate Reward",
    "published": "2010-09-14T03:53:11Z",
    "updated": "2010-09-14T03:53:11Z",
    "authors": [
      "Punit Pandey",
      "Deepshikha Pandey",
      "Shishir Kumar"
    ],
    "summary": "This paper introduces an approach to Reinforcement Learning Algorithm by\ncomparing their immediate rewards using a variation of Q-Learning algorithm.\nUnlike the conventional Q-Learning, the proposed algorithm compares current\nreward with immediate reward of past move and work accordingly. Relative reward\nbased Q-learning is an approach towards interactive learning. Q-Learning is a\nmodel free reinforcement learning method that used to learn the agents. It is\nobserved that under normal circumstances algorithm take more episodes to reach\noptimal Q-value due to its normal reward or sometime negative reward. In this\nnew form of algorithm agents select only those actions which have a higher\nimmediate reward signal in comparison to previous one. The contribution of this\narticle is the presentation of new Q-Learning Algorithm in order to maximize\nthe performance of algorithm and reduce the number of episode required to reach\noptimal Q-value. Effectiveness of proposed algorithm is simulated in a 20 x20\nGrid world deterministic environment and the result for the two forms of\nQ-Learning Algorithms is given.",
    "pdf_url": "http://arxiv.org/pdf/1009.2566v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 13,
    "bibtex": "@Article{Pandey2010ReinforcementLB,\n author = {Punit Pandey and Deepshikha Pandey and Shishir Kumar},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement Learning by Comparing Immediate Reward},\n volume = {abs/1009.2566},\n year = {2010}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2507.18867v1",
    "title": "Learning Individual Intrinsic Reward in Multi-Agent Reinforcement\n  Learning via Incorporating Generalized Human Expertise",
    "published": "2025-07-25T00:59:10Z",
    "updated": "2025-07-25T00:59:10Z",
    "authors": [
      "Xuefei Wu",
      "Xiao Yin",
      "Yuanyang Zhu",
      "Chunlin Chen"
    ],
    "summary": "Efficient exploration in multi-agent reinforcement learning (MARL) is a\nchallenging problem when receiving only a team reward, especially in\nenvironments with sparse rewards. A powerful method to mitigate this issue\ninvolves crafting dense individual rewards to guide the agents toward efficient\nexploration. However, individual rewards generally rely on manually engineered\nshaping-reward functions that lack high-order intelligence, thus it behaves\nineffectively than humans regarding learning and generalization in complex\nproblems. To tackle these issues, we combine the above two paradigms and\npropose a novel framework, LIGHT (Learning Individual Intrinsic reward via\nIncorporating Generalized Human experTise), which can integrate human knowledge\ninto MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid\nunnecessary exploration by considering both individual action distribution and\nhuman expertise preference distribution. Then, LIGHT designs individual\nintrinsic rewards for each agent based on actionable representational\ntransformation relevant to Q-learning so that the agents align their action\npreferences with the human expertise while maximizing the joint action value.\nExperimental results demonstrate the superiority of our method over\nrepresentative baselines regarding performance and better knowledge reusability\nacross different sparse-reward tasks on challenging scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2507.18867v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE International Conference on Systems, Man, and Cybernetics",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Wu2025LearningII,\n author = {Xuefei Wu and Xiao Yin and Yuanyang Zhu and Chunlin Chen},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise},\n volume = {abs/2507.18867},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1608.05081v4",
    "title": "BBQ-Networks: Efficient Exploration in Deep Reinforcement Learning for\n  Task-Oriented Dialogue Systems",
    "published": "2016-08-17T20:00:04Z",
    "updated": "2017-11-23T10:24:17Z",
    "authors": [
      "Zachary C. Lipton",
      "Xiujun Li",
      "Jianfeng Gao",
      "Lihong Li",
      "Faisal Ahmed",
      "Li Deng"
    ],
    "summary": "We present a new algorithm that significantly improves the efficiency of\nexploration for deep Q-learning agents in dialogue systems. Our agents explore\nvia Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop\nneural network. Our algorithm learns much faster than common exploration\nstrategies such as $\\epsilon$-greedy, Boltzmann, bootstrapping, and\nintrinsic-reward-based ones. Additionally, we show that spiking the replay\nbuffer with experiences from just a few successful episodes can make Q-learning\nfeasible when it might otherwise fail.",
    "pdf_url": "http://arxiv.org/pdf/1608.05081v4",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 9 figures",
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Inproceedings{Lipton2016EfficientDP,\n author = {Zachary Chase Lipton and Xiujun Li and Jianfeng Gao and Lihong Li and Faisal Ahmed and L. Deng},\n title = {Efficient Dialogue Policy Learning with BBQ-Networks},\n year = {2016}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2104.06163v1",
    "title": "Reward Shaping with Dynamic Trajectory Aggregation",
    "published": "2021-04-13T13:07:48Z",
    "updated": "2021-04-13T13:07:48Z",
    "authors": [
      "Takato Okudo",
      "Seiji Yamada"
    ],
    "summary": "Reinforcement learning, which acquires a policy maximizing long-term rewards,\nhas been actively studied. Unfortunately, this learning type is too slow and\ndifficult to use in practical situations because the state-action space becomes\nhuge in real environments. The essential factor for learning efficiency is\nrewards. Potential-based reward shaping is a basic method for enriching\nrewards. This method is required to define a specific real-value function\ncalled a potential function for every domain. It is often difficult to\nrepresent the potential function directly. SARSA-RS learns the potential\nfunction and acquires it. However, SARSA-RS can only be applied to the simple\nenvironment. The bottleneck of this method is the aggregation of states to make\nabstract states since it is almost impossible for designers to build an\naggregation function for all states. We propose a trajectory aggregation that\nuses subgoal series. This method dynamically aggregates states in an episode\nduring trial and error with only the subgoal series and subgoal identification\nfunction. It makes designer effort minimal and the application to environments\nwith high-dimensional observations possible. We obtained subgoal series from\nparticipants for experiments. We conducted the experiments in three domains,\nfour-rooms(discrete states and discrete actions), pinball(continuous and\ndiscrete), and picking(both continuous). We compared our method with a baseline\nreinforcement learning algorithm and other subgoal-based methods, including\nrandom subgoal and naive subgoal-based reward shaping. As a result, our reward\nshaping outperformed all other methods in learning efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2104.06163v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by The International Joint Conference on Neural\n  Networks(IJCNN2021)",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Okudo2021RewardSW,\n author = {Takato Okudo and S. Yamada},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {2021 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-9},\n title = {Reward Shaping with Dynamic Trajectory Aggregation},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2106.07704v4",
    "title": "Efficient (Soft) Q-Learning for Text Generation with Limited Good Data",
    "published": "2021-06-14T18:48:40Z",
    "updated": "2022-10-22T13:32:13Z",
    "authors": [
      "Han Guo",
      "Bowen Tan",
      "Zhengzhong Liu",
      "Eric P. Xing",
      "Zhiting Hu"
    ],
    "summary": "Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many emerging applications, such as generating\nadversarial attacks or generating prompts to control language models.\nReinforcement learning (RL) on the other hand offers a more flexible solution\nby allowing users to plug in arbitrary task metrics as reward. Yet previous RL\nalgorithms for text generation, such as policy gradient (on-policy RL) and\nQ-learning (off-policy RL), are often notoriously inefficient or unstable to\ntrain due to the large sequence space and the sparse reward received only at\nthe end of sequences. In this paper, we introduce a new RL formulation for text\ngeneration from the soft Q-learning (SQL) perspective. It enables us to draw\nfrom the latest RL advances, such as path consistency learning, to combine the\nbest of on-/off-policy updates, and learn effectively from sparse reward. We\napply the approach to a wide range of novel text generation tasks, including\nlearning from noisy/negative examples, adversarial attacks, and prompt\ngeneration. Experiments show our approach consistently outperforms both\ntask-specialized algorithms and the previous RL methods.",
    "pdf_url": "http://arxiv.org/pdf/2106.07704v4",
    "doi": null,
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code available at\n  https://github.com/HanGuo97/soft-Q-learning-for-text-generation",
    "journal_ref": null,
    "citation_count": 36,
    "bibtex": "@Article{Guo2021EfficientQ,\n author = {Han Guo and Bowen Tan and Zhengzhong Liu and Eric P. Xing and Zhiting Hu},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {6969-6991},\n title = {Efficient (Soft) Q-Learning for Text Generation with Limited Good Data},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2506.24005v1",
    "title": "Provably Efficient and Agile Randomized Q-Learning",
    "published": "2025-06-30T16:08:29Z",
    "updated": "2025-06-30T16:08:29Z",
    "authors": [
      "He Wang",
      "Xingyu Xu",
      "Yuejie Chi"
    ],
    "summary": "While Bayesian-based exploration often demonstrates superior empirical\nperformance compared to bonus-based methods in model-based reinforcement\nlearning (RL), its theoretical understanding remains limited for model-free\nsettings. Existing provable algorithms either suffer from computational\nintractability or rely on stage-wise policy updates which reduce responsiveness\nand slow down the learning process. In this paper, we propose a novel variant\nof Q-learning algorithm, refereed to as RandomizedQ, which integrates\nsampling-based exploration with agile, step-wise, policy updates, for episodic\ntabular RL. We establish an $\\widetilde{O}(\\sqrt{H^5SAT})$ regret bound, where\n$S$ is the number of states, $A$ is the number of actions, $H$ is the episode\nlength, and $T$ is the total number of episodes. In addition, we present a\nlogarithmic regret bound under a mild positive sub-optimality condition on the\noptimal Q-function. Empirically, RandomizedQ exhibits outstanding performance\ncompared to existing Q-learning variants with both bonus-based and\nBayesian-based exploration on standard benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2506.24005v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Wang2025ProvablyEA,\n author = {He Wang and Xingyu Xu and Yuejie Chi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Provably Efficient and Agile Randomized Q-Learning},\n volume = {abs/2506.24005},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1910.04281v2",
    "title": "Integrating Behavior Cloning and Reinforcement Learning for Improved\n  Performance in Dense and Sparse Reward Environments",
    "published": "2019-10-09T22:32:23Z",
    "updated": "2020-04-03T19:08:25Z",
    "authors": [
      "Vinicius G. Goecks",
      "Gregory M. Gremillion",
      "Vernon J. Lawhern",
      "John Valasek",
      "Nicholas R. Waytowich"
    ],
    "summary": "This paper investigates how to efficiently transition and update policies,\ntrained initially with demonstrations, using off-policy actor-critic\nreinforcement learning. It is well-known that techniques based on Learning from\nDemonstrations, for example behavior cloning, can lead to proficient policies\ngiven limited data. However, it is currently unclear how to efficiently update\nthat policy using reinforcement learning as these approaches are inherently\noptimizing different objective functions. Previous works have used loss\nfunctions, which combine behavior cloning losses with reinforcement learning\nlosses to enable this update. However, the components of these loss functions\nare often set anecdotally, and their individual contributions are not well\nunderstood. In this work, we propose the Cycle-of-Learning (CoL) framework that\nuses an actor-critic architecture with a loss function that combines behavior\ncloning and 1-step Q-learning losses with an off-policy pre-training step from\nhuman demonstrations. This enables transition from behavior cloning to\nreinforcement learning without performance degradation and improves\nreinforcement learning in terms of overall performance and training time.\nAdditionally, we carefully study the composition of these combined losses and\ntheir impact on overall policy learning. We show that our approach outperforms\nstate-of-the-art techniques for combining behavior cloning and reinforcement\nlearning for both dense and sparse reward scenarios. Our results also suggest\nthat directly including the behavior cloning loss on demonstration data helps\nto ensure stable learning and ground future policy updates.",
    "pdf_url": "http://arxiv.org/pdf/1910.04281v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 Figures. AAMAS 2020",
    "journal_ref": null,
    "citation_count": 33,
    "bibtex": "@Article{Goecks2019IntegratingBC,\n author = {Vinicius G. Goecks and Gregory M. Gremillion and Vernon J. Lawhern and J. Valasek and Nicholas R. Waytowich},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {465-473},\n title = {Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Sparse Reward Environments},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2007.06741v1",
    "title": "Single-partition adaptive Q-learning",
    "published": "2020-07-14T00:03:25Z",
    "updated": "2020-07-14T00:03:25Z",
    "authors": [
      "João Pedro Araújo",
      "Mário Figueiredo",
      "Miguel Ayala Botto"
    ],
    "summary": "This paper introduces single-partition adaptive Q-learning (SPAQL), an\nalgorithm for model-free episodic reinforcement learning (RL), which adaptively\npartitions the state-action space of a Markov decision process (MDP), while\nsimultaneously learning a time-invariant policy (i. e., the mapping from states\nto actions does not depend explicitly on the episode time step) for maximizing\nthe cumulative reward. The trade-off between exploration and exploitation is\nhandled by using a mixture of upper confidence bounds (UCB) and Boltzmann\nexploration during training, with a temperature parameter that is automatically\ntuned as training progresses. The algorithm is an improvement over adaptive\nQ-learning (AQL). It converges faster to the optimal solution, while also using\nfewer arms. Tests on episodes with a large number of time steps show that SPAQL\nhas no problems scaling, unlike AQL. Based on this empirical evidence, we claim\nthat SPAQL may have a higher sample efficiency than AQL, thus being a relevant\ncontribution to the field of efficient model-free RL methods.",
    "pdf_url": "http://arxiv.org/pdf/2007.06741v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML",
      "68T05",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages, 15 figures",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Ara'ujo2020SinglepartitionAQ,\n author = {J. Ara'ujo and Mário A. T. Figueiredo and M. Botto},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Single-partition adaptive Q-learning},\n volume = {abs/2007.06741},\n year = {2020}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2106.12534v2",
    "title": "Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic\n  Manipulation via Discretisation",
    "published": "2021-06-23T16:57:16Z",
    "updated": "2022-03-15T00:33:43Z",
    "authors": [
      "Stephen James",
      "Kentaro Wada",
      "Tristan Laidlow",
      "Andrew J. Davison"
    ],
    "summary": "We present a coarse-to-fine discretisation method that enables the use of\ndiscrete reinforcement learning approaches in place of unstable and\ndata-inefficient actor-critic methods in continuous robotics domains. This\napproach builds on the recently released ARM algorithm, which replaces the\ncontinuous next-best pose agent with a discrete one, with coarse-to-fine\nQ-attention. Given a voxelised scene, coarse-to-fine Q-attention learns what\npart of the scene to 'zoom' into. When this 'zooming' behaviour is applied\niteratively, it results in a near-lossless discretisation of the translation\nspace, and allows the use of a discrete action, deep Q-learning method. We show\nthat our new coarse-to-fine algorithm achieves state-of-the-art performance on\nseveral difficult sparsely rewarded RLBench vision-based robotics tasks, and\ncan train real-world policies, tabula rasa, in a matter of minutes, with as\nlittle as 3 demonstrations.",
    "pdf_url": "http://arxiv.org/pdf/2106.12534v2",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2022). Videos and code:\n  https://sites.google.com/view/c2f-q-attention",
    "journal_ref": null,
    "citation_count": 158,
    "bibtex": "@Article{James2021CoarsetoFineQE,\n author = {Stephen James and Kentaro Wada and Tristan Laidlow and A. Davison},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {13729-13738},\n title = {Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2409.09869v1",
    "title": "Critic as Lyapunov function (CALF): a model-free, stability-ensuring\n  agent",
    "published": "2024-09-15T21:27:44Z",
    "updated": "2024-09-15T21:27:44Z",
    "authors": [
      "Pavel Osinenko",
      "Grigory Yaremenko",
      "Roman Zashchitin",
      "Anton Bolychev",
      "Sinan Ibrahim",
      "Dmitrii Dobriborsci"
    ],
    "summary": "This work presents and showcases a novel reinforcement learning agent called\nCritic As Lyapunov Function (CALF) which is model-free and ensures online\nenvironment, in other words, dynamical system stabilization. Online means that\nin each learning episode, the said environment is stabilized. This, as\ndemonstrated in a case study with a mobile robot simulator, greatly improves\nthe overall learning performance. The base actor-critic scheme of CALF is\nanalogous to SARSA. The latter did not show any success in reaching the target\nin our studies. However, a modified version thereof, called SARSA-m here, did\nsucceed in some learning scenarios. Still, CALF greatly outperformed the said\napproach. CALF was also demonstrated to improve a nominal stabilizer provided\nto it. In summary, the presented agent may be considered a viable approach to\nfusing classical control with reinforcement learning. Its concurrent approaches\nare mostly either offline or model-based, like, for instance, those that fuse\nmodel-predictive control into the agent.",
    "pdf_url": "http://arxiv.org/pdf/2409.09869v1",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE Conference on Decision and Control. Accepted for publication in\n  proceedings of the conference",
    "journal_ref": null,
    "citation_count": 7,
    "bibtex": "@Article{Osinenko2024CriticAL,\n author = {Pavel Osinenko and Grigory Yaremenko and Roman Zashchitin and Anton Bolychev and Sinan Ibrahim and Dmitrii Dobriborsci},\n booktitle = {IEEE Conference on Decision and Control},\n journal = {2024 IEEE 63rd Conference on Decision and Control (CDC)},\n pages = {2517-2524},\n title = {Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1901.09311v2",
    "title": "Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon\n  MDP",
    "published": "2019-01-27T03:44:42Z",
    "updated": "2019-09-27T02:09:54Z",
    "authors": [
      "Kefan Dong",
      "Yuanhao Wang",
      "Xiaoyu Chen",
      "Liwei Wang"
    ],
    "summary": "A fundamental question in reinforcement learning is whether model-free\nalgorithms are sample efficient. Recently, Jin et al. \\cite{jin2018q} proposed\na Q-learning algorithm with UCB exploration policy, and proved it has nearly\noptimal regret bound for finite-horizon episodic MDP. In this paper, we adapt\nQ-learning with UCB-exploration bonus to infinite-horizon MDP with discounted\nrewards \\emph{without} accessing a generative model. We show that the\n\\textit{sample complexity of exploration} of our algorithm is bounded by\n$\\tilde{O}({\\frac{SA}{\\epsilon^2(1-\\gamma)^7}})$. This improves the previously\nbest known result of $\\tilde{O}({\\frac{SA}{\\epsilon^4(1-\\gamma)^8}})$ in this\nsetting achieved by delayed Q-learning \\cite{strehl2006pac}, and matches the\nlower bound in terms of $\\epsilon$ as well as $S$ and $A$ except for\nlogarithmic factors.",
    "pdf_url": "http://arxiv.org/pdf/1901.09311v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 103,
    "bibtex": "@Article{Dong2019QlearningWU,\n author = {Kefan Dong and Yuanhao Wang and Xiaoyu Chen and Liwei Wang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP},\n volume = {abs/1901.09311},\n year = {2019}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1902.02907v1",
    "title": "Source Traces for Temporal Difference Learning",
    "published": "2019-02-08T01:21:17Z",
    "updated": "2019-02-08T01:21:17Z",
    "authors": [
      "Silviu Pitis"
    ],
    "summary": "This paper motivates and develops source traces for temporal difference (TD)\nlearning in the tabular setting. Source traces are like eligibility traces, but\nmodel potential histories rather than immediate ones. This allows TD errors to\nbe propagated to potential causal states and leads to faster generalization.\nSource traces can be thought of as the model-based, backward view of successor\nrepresentations (SR), and share many of the same benefits. This view, however,\nsuggests several new ideas. First, a TD($\\lambda$)-like source learning\nalgorithm is proposed and its convergence is proven. Then, a novel algorithm\nfor learning the source map (or SR matrix) is developed and shown to outperform\nthe previous algorithm. Finally, various approaches to using the source/SR\nmodel are explored, and it is shown that source traces can be effectively\ncombined with other model-based methods like Dyna and experience replay.",
    "pdf_url": "http://arxiv.org/pdf/1902.02907v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages. In proceedings of AAAI 2018. Slides and bibtex available at\n  https://silviupitis.com/#source-traces-for-temporal-difference-learning",
    "journal_ref": null,
    "citation_count": 16,
    "bibtex": "@Article{Pitis2018SourceTF,\n author = {Silviu Pitis},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3952-3959},\n title = {Source Traces for Temporal Difference Learning},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2506.14598v1",
    "title": "Learning From the Past with Cascading Eligibility Traces",
    "published": "2025-06-17T15:03:34Z",
    "updated": "2025-06-17T15:03:34Z",
    "authors": [
      "Tokiniaina Raharison Ralambomihanta",
      "Ivan Anokhin",
      "Roman Pogodin",
      "Samira Ebrahimi Kahou",
      "Jonathan Cornford",
      "Blake Aaron Richards"
    ],
    "summary": "Animals often receive information about errors and rewards after a\nsignificant delay. For example, there is typically a delay of tens to hundreds\nof milliseconds between motor actions and visual feedback. The standard\napproach to handling delays in models of synaptic plasticity is to use\neligibility traces. However, standard eligibility traces that decay\nexponentially mix together any events that happen during the delay, presenting\na problem for any credit assignment signal that occurs with a significant\ndelay. Here, we show that eligibility traces formed by a state-space model,\ninspired by a cascade of biochemical reactions, can provide a temporally\nprecise memory for handling credit assignment at arbitrary delays. We\ndemonstrate that these cascading eligibility traces (CETs) work for credit\nassignment at behavioral time-scales, ranging from seconds to minutes. As well,\nwe can use CETs to handle extremely slow retrograde signals, as have been found\nin retrograde axonal signaling. These results demonstrate that CETs can provide\nan excellent basis for modeling synaptic plasticity.",
    "pdf_url": "http://arxiv.org/pdf/2506.14598v1",
    "doi": null,
    "categories": [
      "q-bio.NC"
    ],
    "primary_category": "q-bio.NC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Ralambomihanta2025LearningFT,\n author = {Tokiniaina Raharison Ralambomihanta and Ivan Anokhin and Roman Pogodin and S. E. Kahou and Jonathan Cornford and Blake Aaron Richards},\n title = {Learning From the Past with Cascading Eligibility Traces},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1810.09967v3",
    "title": "Reconciling $λ$-Returns with Experience Replay",
    "published": "2018-10-23T16:55:28Z",
    "updated": "2020-01-13T19:05:20Z",
    "authors": [
      "Brett Daley",
      "Christopher Amato"
    ],
    "summary": "Modern deep reinforcement learning methods have departed from the incremental\nlearning required for eligibility traces, rendering the implementation of the\n$\\lambda$-return difficult in this context. In particular, off-policy methods\nthat utilize experience replay remain problematic because their random sampling\nof minibatches is not conducive to the efficient calculation of\n$\\lambda$-returns. Yet replay-based methods are often the most sample\nefficient, and incorporating $\\lambda$-returns into them is a viable way to\nachieve new state-of-the-art performance. Towards this, we propose the first\nmethod to enable practical use of $\\lambda$-returns in arbitrary replay-based\nmethods without relying on other forms of decorrelation such as asynchronous\ngradient updates. By promoting short sequences of past transitions into a small\ncache within the replay memory, adjacent $\\lambda$-returns can be efficiently\nprecomputed by sharing Q-values. Computation is not wasted on experiences that\nare never sampled, and stored $\\lambda$-returns behave as stable\ntemporal-difference (TD) targets that replace the target network. Additionally,\nour method grants the unique ability to observe TD errors prior to sampling;\nfor the first time, transitions can be prioritized by their true significance\nrather than by a proxy to it. Furthermore, we propose the novel use of the TD\nerror to dynamically select $\\lambda$-values that facilitate faster learning.\nWe show that these innovations can enhance the performance of DQN when playing\nAtari 2600 games, even under partial observability. While our work specifically\nfocuses on $\\lambda$-returns, these ideas are applicable to any multi-step\nreturn estimator.",
    "pdf_url": "http://arxiv.org/pdf/1810.09967v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2019 (Camera-Ready) Code available:\n  https://github.com/brett-daley/dqn-lambda",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Daley2018ReconcilingW,\n author = {Brett Daley and Chris Amato},\n journal = {arXiv: Learning},\n title = {Reconciling $\\lambda$-Returns with Experience Replay},\n year = {2018}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2312.12972v1",
    "title": "From Past to Future: Rethinking Eligibility Traces",
    "published": "2023-12-20T12:23:30Z",
    "updated": "2023-12-20T12:23:30Z",
    "authors": [
      "Dhawal Gupta",
      "Scott M. Jordan",
      "Shreyas Chaudhari",
      "Bo Liu",
      "Philip S. Thomas",
      "Bruno Castro da Silva"
    ],
    "summary": "In this paper, we introduce a fresh perspective on the challenges of credit\nassignment and policy evaluation. First, we delve into the nuances of\neligibility traces and explore instances where their updates may result in\nunexpected credit assignment to preceding states. From this investigation\nemerges the concept of a novel value function, which we refer to as the\n\\emph{bidirectional value function}. Unlike traditional state value functions,\nbidirectional value functions account for both future expected returns (rewards\nanticipated from the current state onward) and past expected returns\n(cumulative rewards from the episode's start to the present). We derive\nprincipled update equations to learn this value function and, through\nexperimentation, demonstrate its efficacy in enhancing the process of policy\nevaluation. In particular, our results indicate that the proposed learning\napproach can, in certain challenging contexts, perform policy evaluation more\nrapidly than TD($\\lambda$) -- a method that learns forward value functions,\n$v^\\pi$, \\emph{directly}. Overall, our findings present a new perspective on\neligibility traces and potential advantages associated with the novel value\nfunction it inspires, especially for policy evaluation.",
    "pdf_url": "http://arxiv.org/pdf/2312.12972v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in The 38th Annual AAAI Conference on Artificial\n  Intelligence",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Gupta2023FromPT,\n author = {Dhawal Gupta and Emma Jordan and Shreyas Chaudhari and Bo Liu and Philip S. Thomas and B. C. Silva},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {12253-12260},\n title = {From Past to Future: Rethinking Eligibility Traces},\n year = {2023}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2202.09699v1",
    "title": "Selective Credit Assignment",
    "published": "2022-02-20T00:07:57Z",
    "updated": "2022-02-20T00:07:57Z",
    "authors": [
      "Veronica Chelu",
      "Diana Borsa",
      "Doina Precup",
      "Hado van Hasselt"
    ],
    "summary": "Efficient credit assignment is essential for reinforcement learning\nalgorithms in both prediction and control settings. We describe a unified view\non temporal-difference algorithms for selective credit assignment. These\nselective algorithms apply weightings to quantify the contribution of learning\nupdates. We present insights into applying weightings to value-based learning\nand planning algorithms, and describe their role in mediating the backward\ncredit distribution in prediction and control. Within this space, we identify\nsome existing online learning algorithms that can assign credit selectively as\nspecial cases, as well as add new algorithms that assign credit backward in\ntime counterfactually, allowing credit to be assigned off-trajectory and\noff-policy.",
    "pdf_url": "http://arxiv.org/pdf/2202.09699v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Chelu2022SelectiveCA,\n author = {Veronica Chelu and Diana Borsa and Doina Precup and Hado van Hasselt},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Selective Credit Assignment},\n volume = {abs/2202.09699},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2206.01338v4",
    "title": "Biologically-plausible backpropagation through arbitrary timespans via\n  local neuromodulators",
    "published": "2022-06-02T23:38:10Z",
    "updated": "2023-01-14T00:58:51Z",
    "authors": [
      "Yuhan Helena Liu",
      "Stephen Smith",
      "Stefan Mihalas",
      "Eric Shea-Brown",
      "Uygar Sümbül"
    ],
    "summary": "The spectacular successes of recurrent neural network models where key\nparameters are adjusted via backpropagation-based gradient descent have\ninspired much thought as to how biological neuronal networks might solve the\ncorresponding synaptic credit assignment problem. There is so far little\nagreement, however, as to how biological networks could implement the necessary\nbackpropagation through time, given widely recognized constraints of biological\nsynaptic network signaling architectures. Here, we propose that extra-synaptic\ndiffusion of local neuromodulators such as neuropeptides may afford an\neffective mode of backpropagation lying within the bounds of biological\nplausibility. Going beyond existing temporal truncation-based gradient\napproximations, our approximate gradient-based update rule, ModProp, propagates\ncredit information through arbitrary time steps. ModProp suggests that\nmodulatory signals can act on receiving cells by convolving their eligibility\ntraces via causal, time-invariant and synapse-type-specific filter taps. Our\nmathematical analysis of ModProp learning, together with simulation results on\nbenchmark temporal tasks, demonstrate the advantage of ModProp over existing\nbiologically-plausible temporal credit assignment rules. These results suggest\na potential neuronal mechanism for signaling credit information related to\nrecurrent interactions over a longer time horizon. Finally, we derive an\nin-silico implementation of ModProp that could serve as a low-complexity and\ncausal alternative to backpropagation through time.",
    "pdf_url": "http://arxiv.org/pdf/2206.01338v4",
    "doi": null,
    "categories": [
      "q-bio.NC",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "NeurIPS 2022 Camera Ready",
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Liu2022BiologicallyplausibleBT,\n author = {Yuhan Helena Liu and Stephen Smith and Stefan Mihalas and E. Shea-Brown and Uygar Sumbul},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Biologically-plausible backpropagation through arbitrary timespans via local neuromodulators},\n volume = {abs/2206.01338},\n year = {2022}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2503.23972v2",
    "title": "Noise-based reward-modulated learning",
    "published": "2025-03-31T11:35:23Z",
    "updated": "2025-08-26T10:31:11Z",
    "authors": [
      "Jesús García Fernández",
      "Nasir Ahmad",
      "Marcel van Gerven"
    ],
    "summary": "Biological neural systems efficiently learn from delayed rewards despite\nrelying on noisy synaptic transmission and lacking centralized optimization\nmechanisms. In contrast, artificial neural networks trained with reinforcement\nlearning typically rely on backpropagation (BP), which limits their use in\nresource-constrained systems or with non-differentiable components. While\nnoise-based alternatives, like reward-modulated Hebbian learning (RMHL),\nprovide a biologically grounded framework for credit assignment, they struggle\nwith temporal delays and hierarchical processing -key challenges in real-world\nlearning. In this work, we derive a novel noise-based learning rule to address\nthese challenges. Drawing inspiration from biological neural circuits, our\nmethod uses reward prediction errors as its optimization target to generate\nincreasingly advantageous behavior, and incorporates an eligibility trace to\nfacilitate retrospective credit assignment. Its formulation relies on local\ninformation, aligning with biological constraints and enabling neuromorphic\nimplementation. Experimental validation on reinforcement tasks (immediate and\ndelayed rewards) shows our approach significantly outperforms RMHL and achieves\nperformance comparable to BP, although with slower convergence due to its\nnoise-driven updates. While tested on simple architectures, the results\nhighlight the potential of noise-driven, brain-inspired learning for low-power\nadaptive systems, particularly in scenarios where energy efficiency and\nbiological plausibility are a priority. These findings also offer mechanistic\ninsights into how dopamine-like signals and synaptic stochasticity may jointly\nenable learning in biological networks, bridging computational models with\nneurobiological principles.",
    "pdf_url": "http://arxiv.org/pdf/2503.23972v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Fern'andez2025NoisebasedRL,\n author = {J. Fern'andez and Nasir Ahmad and M. Gerven},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Noise-based reward-modulated learning},\n volume = {abs/2503.23972},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2405.03878v2",
    "title": "Sequence Compression Speeds Up Credit Assignment in Reinforcement\n  Learning",
    "published": "2024-05-06T21:49:29Z",
    "updated": "2024-06-04T05:28:56Z",
    "authors": [
      "Aditya A. Ramesh",
      "Kenny Young",
      "Louis Kirsch",
      "Jürgen Schmidhuber"
    ],
    "summary": "Temporal credit assignment in reinforcement learning is challenging due to\ndelayed and stochastic outcomes. Monte Carlo targets can bridge long delays\nbetween action and consequence but lead to high-variance targets due to\nstochasticity. Temporal difference (TD) learning uses bootstrapping to overcome\nvariance but introduces a bias that can only be corrected through many\niterations. TD($\\lambda$) provides a mechanism to navigate this bias-variance\ntradeoff smoothly. Appropriately selecting $\\lambda$ can significantly improve\nperformance. Here, we propose Chunked-TD, which uses predicted probabilities of\ntransitions from a model for computing $\\lambda$-return targets. Unlike other\nmodel-based solutions to credit assignment, Chunked-TD is less vulnerable to\nmodel inaccuracies. Our approach is motivated by the principle of history\ncompression and 'chunks' trajectories for conventional TD learning. Chunking\nwith learned world models compresses near-deterministic regions of the\nenvironment-policy interaction to speed up credit assignment while still\nbootstrapping when necessary. We propose algorithms that can be implemented\nonline and show that they solve some problems much faster than conventional\nTD($\\lambda$).",
    "pdf_url": "http://arxiv.org/pdf/2405.03878v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024 version",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Ramesh2024SequenceCS,\n author = {Aditya A. Ramesh and Kenny Young and Louis Kirsch and Jürgen Schmidhuber},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Sequence Compression Speeds Up Credit Assignment in Reinforcement Learning},\n volume = {abs/2405.03878},\n year = {2024}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2506.18482v2",
    "title": "Reliability-Adjusted Prioritized Experience Replay",
    "published": "2025-06-23T10:35:36Z",
    "updated": "2025-07-03T09:55:57Z",
    "authors": [
      "Leonard S. Pleiss",
      "Tobias Sutter",
      "Maximilian Schiffer"
    ],
    "summary": "Experience replay enables data-efficient learning from past experiences in\nonline reinforcement learning agents. Traditionally, experiences were sampled\nuniformly from a replay buffer, regardless of differences in\nexperience-specific learning potential. In an effort to sample more\nefficiently, researchers introduced Prioritized Experience Replay (PER). In\nthis paper, we propose an extension to PER by introducing a novel measure of\ntemporal difference error reliability. We theoretically show that the resulting\ntransition selection algorithm, Reliability-adjusted Prioritized Experience\nReplay (ReaPER), enables more efficient learning than PER. We further present\nempirical results showing that ReaPER outperforms PER across various\nenvironment types, including the Atari-10 benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2506.18482v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Pleiss2025ReliabilityAdjustedPE,\n author = {Leonard S. Pleiss and Tobias Sutter and Maximilian Schiffer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reliability-Adjusted Prioritized Experience Replay},\n volume = {abs/2506.18482},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2102.07260v2",
    "title": "PCM-trace: Scalable Synaptic Eligibility Traces with Resistivity Drift\n  of Phase-Change Materials",
    "published": "2021-02-14T22:35:22Z",
    "updated": "2021-02-16T09:32:53Z",
    "authors": [
      "Yigit Demirag",
      "Filippo Moro",
      "Thomas Dalgaty",
      "Gabriele Navarro",
      "Charlotte Frenkel",
      "Giacomo Indiveri",
      "Elisa Vianello",
      "Melika Payvand"
    ],
    "summary": "Dedicated hardware implementations of spiking neural networks that combine\nthe advantages of mixed-signal neuromorphic circuits with those of emerging\nmemory technologies have the potential of enabling ultra-low power pervasive\nsensory processing. To endow these systems with additional flexibility and the\nability to learn to solve specific tasks, it is important to develop\nappropriate on-chip learning mechanisms.Recently, a new class of three-factor\nspike-based learning rules have been proposed that can solve the temporal\ncredit assignment problem and approximate the error back-propagation algorithm\non complex tasks. However, the efficient implementation of these rules on\nhybrid CMOS/memristive architectures is still an open challenge. Here we\npresent a new neuromorphic building block,called PCM-trace, which exploits the\ndrift behavior of phase-change materials to implement long lasting eligibility\ntraces, a critical ingredient of three-factor learning rules. We demonstrate\nhow the proposed approach improves the area efficiency by >10X compared to\nexisting solutions and demonstrates a techno-logically plausible learning\nalgorithm supported by experimental data from device measurements",
    "pdf_url": "http://arxiv.org/pdf/2102.07260v2",
    "doi": null,
    "categories": [
      "cs.ET"
    ],
    "primary_category": "cs.ET",
    "comment": "Typos are fixed",
    "journal_ref": null,
    "citation_count": 25,
    "bibtex": "@Article{Demirağ2021PCMTraceSS,\n author = {Yiğit Demirağ and Filippo Moro and Thomas Dalgaty and G. Navarro and C. Frenkel and G. Indiveri and E. Vianello and M. Payvand},\n booktitle = {International Symposium on Circuits and Systems},\n journal = {2021 IEEE International Symposium on Circuits and Systems (ISCAS)},\n pages = {1-5},\n title = {PCM-Trace: Scalable Synaptic Eligibility Traces with Resistivity Drift of Phase-Change Materials},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2106.14993v3",
    "title": "Modularity in Reinforcement Learning via Algorithmic Independence in\n  Credit Assignment",
    "published": "2021-06-28T21:29:13Z",
    "updated": "2021-07-21T17:07:10Z",
    "authors": [
      "Michael Chang",
      "Sidhant Kaushik",
      "Sergey Levine",
      "Thomas L. Griffiths"
    ],
    "summary": "Many transfer problems require re-using previously optimal decisions for\nsolving new tasks, which suggests the need for learning algorithms that can\nmodify the mechanisms for choosing certain actions independently of those for\nchoosing others. However, there is currently no formalism nor theory for how to\nachieve this kind of modular credit assignment. To answer this question, we\ndefine modular credit assignment as a constraint on minimizing the algorithmic\nmutual information among feedback signals for different decisions. We introduce\nwhat we call the modularity criterion for testing whether a learning algorithm\nsatisfies this constraint by performing causal analysis on the algorithm\nitself. We generalize the recently proposed societal decision-making framework\nas a more granular formalism than the Markov decision process to prove that for\ndecision sequences that do not contain cycles, certain single-step temporal\ndifference action-value methods meet this criterion while all policy-gradient\nmethods do not. Empirical evidence suggests that such action-value methods are\nmore sample efficient than policy-gradient methods on transfer problems that\nrequire only sparse changes to a sequence of previously optimal decisions.",
    "pdf_url": "http://arxiv.org/pdf/2106.14993v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "cs.NE",
      "math.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Long Presentation at the Thirty-eighth International Conference on\n  Machine Learning (ICML) 2021. 21 pages, 11 figures. v2: updated\n  acknowledgments. v3: clarified that the internal function nodes of the credit\n  assignment mechanism are not considered O(1)",
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Chang2021ModularityIR,\n author = {Michael Chang and Sid Kaushik and S. Levine and T. Griffiths},\n booktitle = {International Conference on Machine Learning},\n pages = {1452-1462},\n title = {Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2502.01837v1",
    "title": "TESS: A Scalable Temporally and Spatially Local Learning Rule for\n  Spiking Neural Networks",
    "published": "2025-02-03T21:23:15Z",
    "updated": "2025-02-03T21:23:15Z",
    "authors": [
      "Marco Paul E. Apolinario",
      "Kaushik Roy",
      "Charlotte Frenkel"
    ],
    "summary": "The demand for low-power inference and training of deep neural networks\n(DNNs) on edge devices has intensified the need for algorithms that are both\nscalable and energy-efficient. While spiking neural networks (SNNs) allow for\nefficient inference by processing complex spatio-temporal dynamics in an\nevent-driven fashion, training them on resource-constrained devices remains\nchallenging due to the high computational and memory demands of conventional\nerror backpropagation (BP)-based approaches. In this work, we draw inspiration\nfrom biological mechanisms such as eligibility traces, spike-timing-dependent\nplasticity, and neural activity synchronization to introduce TESS, a temporally\nand spatially local learning rule for training SNNs. Our approach addresses\nboth temporal and spatial credit assignments by relying solely on locally\navailable signals within each neuron, thereby allowing computational and memory\noverheads to scale linearly with the number of neurons, independently of the\nnumber of time steps. Despite relying on local mechanisms, we demonstrate\nperformance comparable to the backpropagation through time (BPTT) algorithm,\nwithin $\\sim1.4$ accuracy points on challenging computer vision scenarios\nrelevant at the edge, such as the IBM DVS Gesture dataset, CIFAR10-DVS, and\ntemporal versions of CIFAR10, and CIFAR100. Being able to produce comparable\nperformance to BPTT while keeping low time and memory complexity, TESS enables\nefficient and scalable on-device learning at the edge.",
    "pdf_url": "http://arxiv.org/pdf/2502.01837v1",
    "doi": null,
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "9 pages, 2 figures",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Apolinario2025TESSAS,\n author = {M. Apolinario and Kaushik Roy and Charlotte Frenkel},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {TESS: A Scalable Temporally and Spatially Local Learning Rule for Spiking Neural Networks},\n volume = {abs/2502.01837},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2502.00520v1",
    "title": "Variance Reduction via Resampling and Experience Replay",
    "published": "2025-02-01T18:46:08Z",
    "updated": "2025-02-01T18:46:08Z",
    "authors": [
      "Jiale Han",
      "Xiaowu Dai",
      "Yuhua Zhu"
    ],
    "summary": "Experience replay is a foundational technique in reinforcement learning that\nenhances learning stability by storing past experiences in a replay buffer and\nreusing them during training. Despite its practical success, its theoretical\nproperties remain underexplored. In this paper, we present a theoretical\nframework that models experience replay using resampled $U$- and\n$V$-statistics, providing rigorous variance reduction guarantees. We apply this\nframework to policy evaluation tasks using the Least-Squares Temporal\nDifference (LSTD) algorithm and a Partial Differential Equation (PDE)-based\nmodel-free algorithm, demonstrating significant improvements in stability and\nefficiency, particularly in data-scarce scenarios. Beyond policy evaluation, we\nextend the framework to kernel ridge regression, showing that the experience\nreplay-based method reduces the computational cost from the traditional\n$O(n^3)$ in time to as low as $O(n^2)$ in time while simultaneously reducing\nvariance. Extensive numerical experiments validate our theoretical findings,\ndemonstrating the broad applicability and effectiveness of experience replay in\ndiverse machine learning tasks.",
    "pdf_url": "http://arxiv.org/pdf/2502.00520v1",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Han2025VarianceRV,\n author = {Jiale Han and Xiaowu Dai and Yuhua Zhu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Variance Reduction via Resampling and Experience Replay},\n volume = {abs/2502.00520},\n year = {2025}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/1106.0221v1",
    "title": "Evolutionary Algorithms for Reinforcement Learning",
    "published": "2011-06-01T16:16:14Z",
    "updated": "2011-06-01T16:16:14Z",
    "authors": [
      "J. J. Grefenstette",
      "D. E. Moriarty",
      "A. C. Schultz"
    ],
    "summary": "There are two distinct approaches to solving reinforcement learning problems,\nnamely, searching in value function space and searching in policy space.\nTemporal difference methods and evolutionary algorithms are well-known examples\nof these approaches. Kaelbling, Littman and Moore recently provided an\ninformative survey of temporal difference methods. This article focuses on the\napplication of evolutionary algorithms to the reinforcement learning problem,\nemphasizing alternative policy representations, credit assignment methods, and\nproblem-specific genetic operators. Strengths and weaknesses of the\nevolutionary approach to reinforcement learning are presented, along with a\nsurvey of representative applications.",
    "pdf_url": "http://arxiv.org/pdf/1106.0221v1",
    "doi": "10.1613/jair.613",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  241-276, 1999",
    "citation_count": 415,
    "bibtex": "@Article{Moriarty1999EvolutionaryAF,\n author = {David E. Moriarty and A. Schultz and J. Grefenstette},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {241-276},\n title = {Evolutionary Algorithms for Reinforcement Learning},\n volume = {11},\n year = {1999}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2103.02696v2",
    "title": "On the Importance of Sampling in Training GCNs: Tighter Analysis and\n  Variance Reduction",
    "published": "2021-03-03T21:31:23Z",
    "updated": "2021-11-01T17:26:18Z",
    "authors": [
      "Weilin Cong",
      "Morteza Ramezani",
      "Mehrdad Mahdavi"
    ],
    "summary": "Graph Convolutional Networks (GCNs) have achieved impressive empirical\nadvancement across a wide variety of semi-supervised node classification tasks.\nDespite their great success, training GCNs on large graphs suffers from\ncomputational and memory issues. A potential path to circumvent these obstacles\nis sampling-based methods, where at each layer a subset of nodes is sampled.\nAlthough recent studies have empirically demonstrated the effectiveness of\nsampling-based methods, these works lack theoretical convergence guarantees\nunder realistic settings and cannot fully leverage the information of evolving\nparameters during optimization. In this paper, we describe and analyze a\ngeneral doubly variance reduction schema that can accelerate any sampling\nmethod under the memory budget. The motivating impetus for the proposed schema\nis a careful analysis of the variance of sampling methods where it is shown\nthat the induced variance can be decomposed into node embedding approximation\nvariance (zeroth-order variance) during forward propagation and\nlayerwise-gradient variance (first-order variance) during backward propagation.\nWe theoretically analyze the convergence of the proposed schema and show that\nit enjoys an $\\mathcal{O}(1/T)$ convergence rate. We complement our theoretical\nresults by integrating the proposed schema in different sampling methods and\napplying them to different large real-world graphs.",
    "pdf_url": "http://arxiv.org/pdf/2103.02696v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Inproceedings{Cong2021OnTI,\n author = {Weilin Cong and M. Ramezani and M. Mahdavi},\n title = {On the Importance of Sampling in Training GCNs: Tighter Analysis and Variance Reduction},\n year = {2021}\n}\n"
  },
  {
    "id": "http://arxiv.org/abs/2403.16933v3",
    "title": "Backpropagation through space, time, and the brain",
    "published": "2024-03-25T16:57:02Z",
    "updated": "2025-05-05T14:43:33Z",
    "authors": [
      "Benjamin Ellenberger",
      "Paul Haider",
      "Jakob Jordan",
      "Kevin Max",
      "Ismael Jaras",
      "Laura Kriener",
      "Federico Benitez",
      "Mihai A. Petrovici"
    ],
    "summary": "How physical networks of neurons, bound by spatio-temporal locality\nconstraints, can perform efficient credit assignment, remains, to a large\nextent, an open question. In machine learning, the answer is almost universally\ngiven by the error backpropagation algorithm, through both space and time.\nHowever, this algorithm is well-known to rely on biologically implausible\nassumptions, in particular with respect to spatio-temporal (non-)locality.\nAlternative forward-propagation models such as real-time recurrent learning\nonly partially solve the locality problem, but only at the cost of scaling, due\nto prohibitive storage requirements. We introduce Generalized Latent\nEquilibrium (GLE), a computational framework for fully local spatio-temporal\ncredit assignment in physical, dynamical networks of neurons. We start by\ndefining an energy based on neuron-local mismatches, from which we derive both\nneuronal dynamics via stationarity and parameter dynamics via gradient descent.\nThe resulting dynamics can be interpreted as a real-time, biologically\nplausible approximation of backpropagation through space and time in deep\ncortical networks with continuous-time neuronal dynamics and continuously\nactive, local synaptic plasticity. In particular, GLE exploits the morphology\nof dendritic trees to enable more complex information storage and processing in\nsingle neurons, as well as the ability of biological neurons to phase-shift\ntheir output rate with respect to their membrane potential, which is essential\nin both directions of information propagation. For the forward computation, it\nenables the mapping of time-continuous inputs to neuronal space, effectively\nperforming a spatio-temporal convolution. For the backward computation, it\npermits the temporal inversion of feedback signals, which consequently\napproximate the adjoint variables necessary for useful parameter updates.",
    "pdf_url": "http://arxiv.org/pdf/2403.16933v3",
    "doi": null,
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "eess.SP"
    ],
    "primary_category": "q-bio.NC",
    "comment": "First authorship shared by Benjamin Ellenberger and Paul Haider",
    "journal_ref": null,
    "citation_count": 9,
    "bibtex": "@Article{Ellenberger2024BackpropagationTS,\n author = {B. Ellenberger and Paul Haider and Jakob Jordan and Kevin Max and Ismael Jaras and Laura Kriener and Federico Benitez and Mihai A. Petrovici},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Backpropagation through space, time, and the brain},\n volume = {abs/2403.16933},\n year = {2024}\n}\n"
  }
]