[
  {
    "paper_id": "2206.09328v1",
    "title": "A Survey on Model-based Reinforcement Learning",
    "methods_used": [
      "Dyna-style methods",
      "M2AC",
      "MBPO"
    ],
    "test_setup": "simulation environments",
    "main_limitations": "Compounding errors in model rollouts and generalization error between learned environment models and real environments"
  },
  {
    "paper_id": "2408.03029v4",
    "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning",
    "methods_used": [
      "Kernel Density Estimation (KDE)",
      "Random Fourier Features (RFF)"
    ],
    "test_setup": "high-dimensional environments including MuJoCo tasks, robotic tasks, Atari games, and a physical simulation task with extremely sparse rewards",
    "main_limitations": "SASR is designed for sparse-reward environments and may be unnecessary in dense-reward settings; extending it to such scenarios is suggested as future work."
  },
  {
    "paper_id": "1805.12375v3",
    "title": "Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update",
    "methods_used": [
      "Episodic Backward Update (EBU)",
      "DQN",
      "dueling network architecture",
      "Neural episodic control",
      "model-free episodic control",
      "RUDDER",
      "Ephemeral Value Adjustments (EVA)",
      "Prioritized experience replay",
      "Q(λ)",
      "Q*[λ]",
      "Retrace(λ)",
      "Count-based exploration with intrinsic motivation",
      "Optimality Tightening"
    ],
    "test_setup": "Atari 2600 games (49 games), deterministic and stochastic environments, with sample efficiency evaluated using 10M and 20M frames",
    "main_limitations": ""
  },
  {
    "paper_id": "2203.15845v3",
    "title": "Topological Experience Replay",
    "methods_used": [
      "Topological Experience Replay (TER)",
      "Q-learning"
    ],
    "test_setup": "goal-reaching tasks with high-dimensional observational data such as images",
    "main_limitations": ""
  },
  {
    "paper_id": "2103.04529v3",
    "title": "Self-Supervised Online Reward Shaping in Sparse-Reward Environments",
    "methods_used": [
      "Self-Supervised Online Reward Shaping (SORS)"
    ],
    "test_setup": "delayed MuJoCo environments",
    "main_limitations": "Theoretical guarantees of preserving optimal policies are not strictly met in practice, and weaker assumptions for such guarantees need further investigation."
  },
  {
    "paper_id": "2409.08724v1",
    "title": "Quasimetric Value Functions with Dense Rewards",
    "methods_used": [
      "DDPG+HER"
    ],
    "test_setup": "12 standard benchmark environments in GCRL featuring challenging continuous control tasks",
    "main_limitations": "Dense rewards were previously thought to deteriorate GCRL performance, and existing negative results suggested they foreclose efficacy in GCRL; however, the paper identifies that this only holds when the triangle inequality is violated, and establishes a condition under which dense rewards preserve it."
  },
  {
    "paper_id": "1805.07603v1",
    "title": "Episodic Memory Deep Q-Networks",
    "methods_used": [
      "Episodic Memory Deep Q-Networks (EMDQN)",
      "DQN",
      "prioritized experience replay",
      "EC"
    ],
    "test_setup": "Atari 2600 games from the arcade learning environment",
    "main_limitations": "Using a fixed λ value may limit performance, and episodic memory could act as a regularizer in stochastic environments rather than providing optimal guidance; also, NEC's redundant state representations are addressed but not fully resolved."
  },
  {
    "paper_id": "1711.06006v3",
    "title": "Hindsight policy gradients",
    "methods_used": [
      "hindsight policy gradients",
      "GCPG",
      "DQN+HER",
      "hindsight experience replay"
    ],
    "test_setup": "diverse sparse-reward environments including bit flipping, empty room, four rooms, Ms. Pac-man, and FetchPush",
    "main_limitations": "Indiscriminately adding hindsight transitions to the replay buffer is problematic and has mostly been tackled by heuristics; original estimator requires a constant number of time steps T, which is often infeasible in the considered environments."
  },
  {
    "paper_id": "2111.02104v2",
    "title": "Model-Based Episodic Memory Induces Dynamic Hybrid Controls",
    "methods_used": [
      "DQN",
      "model-based episodic memory",
      "dynamic hybrid control"
    ],
    "test_setup": "stochastic and non-Markovian environments including stochastic classical control tasks",
    "main_limitations": "Large number of hyperparameters preventing full tuning of MBEC++"
  },
  {
    "paper_id": "2404.15822v1",
    "title": "Recursive Backwards Q-Learning in Deterministic Environments",
    "methods_used": [
      "Q-learning",
      "recursive backwards Q-learning (RBQL)"
    ],
    "test_setup": "finding the shortest path through a maze",
    "main_limitations": ""
  },
  {
    "paper_id": "2206.02902v5",
    "title": "Goal-Space Planning with Subgoal Models",
    "methods_used": [
      "Goal-Space Planning (GSP)",
      "Double DQN",
      "Dyna architecture"
    ],
    "test_setup": "simulation environments with varying state spaces and domains including PinBall",
    "main_limitations": "Learned models are inaccurate and generate invalid states when iterated over many steps, leading to poor performance compared to model-free methods despite higher computational cost."
  },
  {
    "paper_id": "2401.14226v1",
    "title": "Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks",
    "methods_used": [
      "Goal-conditioned reinforcement learning",
      "Hierarchical Reinforcement Learning",
      "HAM",
      "MAXQ",
      "h-DQN",
      "DeepSynth"
    ],
    "test_setup": "sparse-reward environments",
    "main_limitations": "Manually designed reward structures suffer from inaccuracy; automatically learned reward structures (e.g., exact automata models) are computationally intractable (NP-complete) and may be partial or inaccurate, leading to failure in learning optimal policies."
  },
  {
    "paper_id": "2312.05787v1",
    "title": "Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization",
    "methods_used": [
      "REDQ",
      "hindsight experience replay (HER)",
      "bounding target Q-values (BQ)"
    ],
    "test_setup": "12 sparse-reward goal-conditioned tasks of Robotics (Plappert et al., 2018), including 4 Fetch tasks",
    "main_limitations": "Did not significantly improve sampling efficiency in extremely hard tasks (e.g., HandManipulateBlockFull) and experiments were limited to simulated environments, not real-world settings."
  },
  {
    "paper_id": "2205.15824v1",
    "title": "Graph Backup: Data Efficient Backup Exploiting Markovian Transitions",
    "methods_used": [
      "Graph Backup",
      "n-step Q-Learning",
      "TD(λ)",
      "n-step SARSA",
      "eligibility traces",
      "expected eligibility traces",
      "DQN",
      "Data-Efficient Rainbow"
    ],
    "test_setup": "data-efficient RL benchmarks including MiniGrid, Minatar and Atari100K",
    "main_limitations": "Current methods fail to fully utilise the structure of trajectory data, and eligibility traces are not widely applied in off-policy RL with neural networks due to their design for linear function approximators and on-policy settings."
  },
  {
    "paper_id": "2007.07582v1",
    "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning",
    "methods_used": [
      "Q-learning",
      "QGraph-bounded Q-learning",
      "QG-DDPG"
    ],
    "test_setup": "simulation environments with continuous state and action spaces, evaluated using regular grids of states and multiple runs with different random seeds",
    "main_limitations": "Constraining change rates may limit the rate of agent improvement; incorrect empirical bounds can have adverse effects on learning; inter-episodic information is only exchanged for exact state matches (up to floating point precision), limiting generalization beyond identical states."
  },
  {
    "paper_id": "2501.17842v1",
    "title": "From Sparse to Dense: Toddler-inspired Reward Transition in Goal-Oriented Reinforcement Learning",
    "methods_used": [
      "potential-based dense rewards",
      "Cross-Density Visualizer"
    ],
    "test_setup": "dynamic robotic arm manipulation and egocentric 3D navigation tasks",
    "main_limitations": ""
  },
  {
    "paper_id": "2009.10396v1",
    "title": "Is Q-Learning Provably Efficient? An Extended Analysis",
    "methods_used": [
      "Q-learning",
      "DQN"
    ],
    "test_setup": "theoretical analysis",
    "main_limitations": "Model-free methods tend to be sample inefficient, requiring many experiences to train."
  },
  {
    "paper_id": "2009.02602v2",
    "title": "A Hybrid PAC Reinforcement Learning Algorithm",
    "methods_used": [
      "Dyna-Delayed Q-learning (DDQ)",
      "R-max",
      "Delayed Q-learning",
      "Dyna-Q"
    ],
    "test_setup": "numerical results and theoretical analysis",
    "main_limitations": "The provable worst-case sample complexity upper bound of DDQ is higher than the best known model-based and model-free algorithms, though it performs better in practice."
  },
  {
    "paper_id": "1901.01977v1",
    "title": "Accelerating Goal-Directed Reinforcement Learning by Model Characterization",
    "methods_used": [
      "Q-Learning",
      "DYNA",
      "Mean First Passage Time based Q-Learning (MFPT-Q)",
      "Mean First Passage Time based DYNA (MFPT-DYNA)",
      "prioritized sweeping"
    ],
    "test_setup": "simulation environment with obstacles, agent initial state, and goal state",
    "main_limitations": ""
  },
  {
    "paper_id": "1910.02919v3",
    "title": "Multi-step Greedy Reinforcement Learning Algorithms",
    "methods_used": [
      "κ-Policy Iteration (κ-PI)",
      "κ-Value Iteration (κ-VI)",
      "DQN",
      "TRPO"
    ],
    "test_setup": "Atari and MuJoCo benchmark tasks",
    "main_limitations": "Lowering the discount factor improves performance in short-horizon MuJoCo tasks but degrades it in non-short-horizon Atari tasks, indicating a domain-dependent sensitivity to γ."
  },
  {
    "paper_id": "1904.03535v1",
    "title": "Randomised Bayesian Least-Squares Policy Iteration",
    "methods_used": [
      "Bayesian Least-Squares Policy Iteration (BLSPI)",
      "Randomised Bayesian Least-Squares Policy Iteration (RBLSPI)",
      "Bayesian Least-Squares Temporal-Difference (BLSTD)"
    ],
    "test_setup": "four continuous state-space environments, including mountain car and a sparse-reward version of it",
    "main_limitations": ""
  },
  {
    "paper_id": "2408.03539v3",
    "title": "Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes",
    "methods_used": [
      "PPO",
      "SAC",
      "TD-MPC",
      "Dreamer",
      "A*",
      "MPPI"
    ],
    "test_setup": "real-world robotics",
    "main_limitations": "Complexity and cost of interacting with the physical world, need for stable and sample-efficient real-world RL paradigms, lack of standardized evaluation protocols and reproducible test environments, challenges in scaling evaluation to complex open-world tasks"
  },
  {
    "paper_id": "2508.18420v1",
    "title": "LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning",
    "methods_used": [
      "Variational State as Intrinsic Reward (VSIMR)",
      "Large Language Models (LLMs)",
      "Actor-Critic (A2C)"
    ],
    "test_setup": "MiniGrid DoorKey environment",
    "main_limitations": "Inconsistent performance of VAE-only configurations, particularly in exploration, and need for hyperparameter fine-tuning; also, reliance on prompt engineering for effective LLM guidance."
  },
  {
    "paper_id": "2411.18892v2",
    "title": "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges",
    "methods_used": [
      "Dynamic Programming",
      "Model-based Planning"
    ],
    "test_setup": "",
    "main_limitations": "Computational overhead, memory requirements, dependence on accurate environmental modeling, sensitivity to hyperparameter tuning (e.g., temperature term, reduction ratio), and limited scalability in real-time or resource-constrained environments"
  },
  {
    "paper_id": "2109.06668v6",
    "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain",
    "methods_used": [
      "Go-Explore",
      "DTSIL",
      "NGU",
      "IDS",
      "Bootstrapped DQN",
      "R2D2",
      "Agent57",
      "EC",
      "Model-assisted RL",
      "Planning to explore",
      "Dreamer-based planning",
      "Noise-Augmented RL",
      "Hallucinated UCRL",
      "Ready Policy One",
      "Dynamics Bottleneck"
    ],
    "test_setup": "Montezuma’s Revenge, Atari suite, Vizdoom, SMAC",
    "main_limitations": "Sample inefficiency due to exploration challenges in complex environments with sparse rewards, noisy distractions, long horizons, and non-stationary co-learners; high convergence time requiring billions of frames"
  },
  {
    "paper_id": "2407.13734v1",
    "title": "Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review",
    "methods_used": [
      "PPO",
      "differentiable optimization",
      "reward-weighted MLE",
      "value-weighted sampling",
      "path consistency learning"
    ],
    "test_setup": "",
    "main_limitations": "The resulting distribution may deviate too much from the pre-trained diffusion model, requiring regularization via KL penalty to maintain fidelity."
  },
  {
    "paper_id": "2506.01096v2",
    "title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning",
    "methods_used": [
      "SuperRL",
      "SFT",
      "RL"
    ],
    "test_setup": "diverse reasoning benchmarks",
    "main_limitations": "Limitations and future work are discussed in Appendix A."
  },
  {
    "paper_id": "2506.17518v1",
    "title": "A Survey of State Representation Learning for Deep Reinforcement Learning",
    "methods_used": [
      "Forward Dynamics Models",
      "Inverse Dynamics Models"
    ],
    "test_setup": "",
    "main_limitations": "Failure to learn meaningful representations due to lack of grounding objectives (e.g., reward prediction), leading to representation collapse under noisy or distracting observations."
  },
  {
    "paper_id": "2409.12799v3",
    "title": "The Central Role of the Loss Function in Reinforcement Learning",
    "methods_used": [
      "binary cross-entropy loss",
      "maximum likelihood loss",
      "distributional RL"
    ],
    "test_setup": "theoretical analysis",
    "main_limitations": ""
  },
  {
    "paper_id": "2502.09417v1",
    "title": "A Survey of Reinforcement Learning for Optimization in Automation",
    "methods_used": [
      "DQN",
      "Distributional RL",
      "DRL",
      "A2C",
      "PPO",
      "Cooperative MARL",
      "Multi-Agent Actor Critic",
      "Deep Q-learning",
      "Q-learning",
      "TRPO",
      "DDPG",
      "Dynamic Q-table",
      "apprenticeship learning"
    ],
    "test_setup": "",
    "main_limitations": "sample efficiency and scalability; safety and robustness; interpretability and trustworthiness; transfer learning and meta-learning; real-world deployment and integration"
  },
  {
    "paper_id": "2411.04832v2",
    "title": "Plasticity Loss in Deep Reinforcement Learning: A Survey",
    "methods_used": [
      "Hard resets",
      "Soft resets (Shrink and Perturb)",
      "regularizing neural network weights",
      "regularizing feature representation rank",
      "activation function selection",
      "categorical loss leveraging",
      "distillation-based approaches"
    ],
    "test_setup": "Atari games, non-pixel continuous control tasks, offline and online RL agents, toy experiments with MNIST sequences and target regression",
    "main_limitations": "Lack of broader evaluation, insufficient understanding of neural activity and behavior, target non-stationarity causing plasticity loss, catastrophic forgetting in moving targets, performance collapse from bootstrapping, and growing parameter norms inhibiting learning"
  },
  {
    "paper_id": "2508.08189v2",
    "title": "Reinforcement Learning in Vision: A Survey",
    "methods_used": [
      "RLHF",
      "Proximal Policy Optimization",
      "Group Relative Policy Optimization",
      "DPO",
      "InstructGPT",
      "VARD",
      "Inversion-DPO",
      "DDPO",
      "UniRL",
      "HermesFlow"
    ],
    "test_setup": "Set-level fidelity, sample-level preference, and state-level stability evaluations using benchmark datasets and RL-free external benchmarks",
    "main_limitations": "Sample efficiency, robust generalization across domains/viewpoints/embodiment settings, and lack of principled reward design for long-horizon open-world tasks risking reward hacking and unsafe behavior"
  },
  {
    "paper_id": "1912.10944v2",
    "title": "A Survey of Deep Reinforcement Learning in Video Games",
    "methods_used": [
      "DQN",
      "Double DQN",
      "DQV",
      "Rainbow",
      "RUDDER",
      "Ape-X DQfD",
      "Soft DQN",
      "Averaged-DQN",
      "UNREAL",
      "PAAC",
      "C51",
      "QR-DQN",
      "IQN",
      "Noisy DQN"
    ],
    "test_setup": "Atari games (ALE benchmark)",
    "main_limitations": "exploration-exploitation, sample efficiency, generalization and transfer, multi-agent learning, imperfect information, and delayed sparse rewards"
  },
  {
    "paper_id": "2009.13303v2",
    "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey",
    "methods_used": [
      "domain randomization",
      "domain adaptation",
      "imitation learning",
      "meta-learning",
      "knowledge distillation",
      "policy distillation",
      "system identification",
      "proximal policy optimization (PPO)",
      "trust region policy optimization (TRPO)",
      "maximum a-posteriori policy optimization (MPO)",
      "asynchronous actor critic (A3C)",
      "soft actor critic (SAC)",
      "deep deterministic policy gradient (DDPG)"
    ],
    "test_setup": "simulation environments",
    "main_limitations": "The gap between simulated and real worlds degrades policy performance, compounded by sample inefficiency and high cost of collecting real-world data, along with challenges in building realistic simulators (e.g., high-quality visual rendering and variability of physical parameters)."
  },
  {
    "paper_id": "2305.10504v1",
    "title": "Model-Free Robust Average-Reward Reinforcement Learning",
    "methods_used": [
      "robust relative value iteration (RVI) TD",
      "robust RVI Q-learning"
    ],
    "test_setup": "numerical verification and additional experiments in Appendix G",
    "main_limitations": "The robust average-reward Bellman operator lacks a simple contraction property like in discounted settings, the average-reward depends on the limiting behavior of the MDP, and the Bellman function involves two variables (average-reward and relative value function), making the problem more intricate; prior model-free methods were limited to specific uncertainty sets and could not be generalized."
  },
  {
    "paper_id": "1606.04460v1",
    "title": "Model-Free Episodic Control",
    "methods_used": [
      "episodic control"
    ],
    "test_setup": "Atari games and a first-person 3D environment called Labyrinth",
    "main_limitations": "The episodic controller may be overtaken by parametric function approximators at later stages of training, and its effectiveness depends on state repetition or similarity, though it still performs well even in 3D environments where exact states are never re-visited."
  },
  {
    "paper_id": "2505.19769v2",
    "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning",
    "methods_used": [
      "TeViR"
    ],
    "test_setup": "simulated environment reflecting complex and sparse real-world robotic manipulation scenarios, using raw pixel observations without access to state or task rewards",
    "main_limitations": "poor text-to-video generation quality"
  },
  {
    "paper_id": "2210.04561v4",
    "title": "A Comprehensive Survey of Data Augmentation in Visual Reinforcement Learning",
    "methods_used": [
      "ROSIE",
      "GenAug",
      "SynthER",
      "MTDIFF"
    ],
    "test_setup": "Atari Games [165] and DeepMind Control Suite [10]",
    "main_limitations": "low sample efficiency and large generalization gaps in visual reinforcement learning"
  },
  {
    "paper_id": "2303.11191v1",
    "title": "A Survey of Demonstration Learning",
    "methods_used": [
      "Apprenticeship Learning",
      "GAIL",
      "Inverse RL",
      "Sequence Models",
      "Online RL",
      "Evolutionary Methods",
      "Transfer Learning",
      "Adaptive Methods",
      "Active Learning"
    ],
    "test_setup": "simulation environments and custom real-world task environments with demonstration datasets",
    "main_limitations": "Poor generalization to unseen scenarios due to demonstrations covering only a subset of the state distribution, leading to catastrophic consequences in real-world out-of-distribution cases; limited benchmark availability; difficulty in real-world evaluation due to hardware requirements, safety concerns, and need for custom environments; challenges in hyper-parameter specification"
  },
  {
    "paper_id": "2311.01450v2",
    "title": "DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing",
    "methods_used": [
      "DreamSmooth"
    ],
    "test_setup": "long-horizon sparse-reward tasks, Deepmind Control Suite, Atari benchmarks",
    "main_limitations": "Improved reward prediction can lead to more false positives, potentially harming task performance in some cases (e.g., Crafter), indicating a trade-off that requires further investigation."
  },
  {
    "paper_id": "2310.06253v2",
    "title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning",
    "methods_used": [
      "Variational Model-Based Policy Optimization (VMBPO)",
      "DAM"
    ],
    "test_setup": "Evaluation through agent performance metrics (final performance and learning speed), model misspecification experiments (varying model capacity, removing/distracting state features), and value-estimation bias measurements",
    "main_limitations": "Objective mismatch between accurate dynamics model learning and policy optimization, leading to poor correlation between model predictive accuracy and action quality; robustness issues with model misspecification and model-exploitation in online and offline RL."
  },
  {
    "paper_id": "2506.19686v2",
    "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers",
    "methods_used": [
      "meta-learning"
    ],
    "test_setup": "distribution of planning tasks inspired by rodent behavior",
    "main_limitations": ""
  },
  {
    "paper_id": "2410.10132v1",
    "title": "Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning",
    "methods_used": [
      "GRU",
      "FWP",
      "GPT-2",
      "S6",
      "mLSTM",
      "FFM",
      "SHM"
    ],
    "test_setup": "challenging partially observable benchmarks including meta-reinforcement learning, long-horizon credit assignment, and POPGym",
    "main_limitations": "Inefficient capture of relevant past information, poor adaptability to changing observations, and unstable updates over long episodes in partially observable reinforcement learning environments"
  },
  {
    "paper_id": "2403.01112v2",
    "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
    "methods_used": [
      "Q-learning",
      "EMU",
      "EMC",
      "QMIX",
      "QPLEX",
      "CDS"
    ],
    "test_setup": "StarCraft II and Google Research Football with SMAC and GRF benchmarks",
    "main_limitations": ""
  },
  {
    "paper_id": "2309.05105v1",
    "title": "Convex Q Learning in a Stochastic Environment: Extended Version",
    "methods_used": [
      "Q-learning",
      "CvxQ",
      "relative CvxQ"
    ],
    "test_setup": "theoretical analysis and application to a classical inventory control problem",
    "main_limitations": "Conditions on the function class are required for stability and convergence, even in linear function approximation settings; counterexamples exist without such conditions."
  },
  {
    "paper_id": "2304.12090v2",
    "title": "Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey",
    "methods_used": [
      "Q-learning",
      "MLN",
      "Situation Calculus",
      "Temporal Logics (TLs)",
      "LTL",
      "FSA",
      "autoregressive RNN"
    ],
    "test_setup": "simulation environments (e.g., RoboCup simulated-soccer), empirical evaluation on specific learning tasks or domains",
    "main_limitations": "Poor system generalization, low sample efficiency, safety and interpretability concerns; scalability issues with MLNs; suboptimal performance with partial domain data; challenges in modeling probabilistic transition models with logic structures; limited application of KRR to MARL, MBRL, HRL, and partially observable settings; need for better integration of epistemic logic and situation calculus in RL."
  },
  {
    "paper_id": "2207.09071v2",
    "title": "Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks",
    "methods_used": [
      "action translator",
      "context encoder",
      "forward dynamics model",
      "context-based meta-RL algorithms"
    ],
    "test_setup": "simulation environments with sparse-reward tasks",
    "main_limitations": "Sparse rewards in long-horizon tasks exacerbate sample efficiency issues, and difficulty discrepancies among tasks may cause easy tasks to dominate learning, hindering adaptation to new tasks."
  },
  {
    "paper_id": "2005.02979v3",
    "title": "A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical Systems",
    "methods_used": [
      "Simulated Annealing",
      "Evolutionary Algorithms",
      "Genetic Algorithm",
      "RRT (Rapidly-exploring Random Tree)",
      "Multiple Shooting Methods",
      "Importance Sampling",
      "Reinforcement Learning",
      "Optimization",
      "Path Planning"
    ],
    "test_setup": "simulation environments",
    "main_limitations": "Lack of convergence guarantees for heuristic-based methods, computational expense for rare failure events, and scalability challenges with large state spaces"
  },
  {
    "paper_id": "2009.01719v4",
    "title": "Grounded Language Learning Fast and Slow",
    "methods_used": [
      "Model-Agnostic Meta Learning",
      "policy-gradient algorithms"
    ],
    "test_setup": "simulated 3D world with embodied agent and dual-coding external memory",
    "main_limitations": ""
  },
  {
    "paper_id": "2006.11901v5",
    "title": "Free-rider Attacks on Model Aggregation in Federated Learning",
    "methods_used": [
      "FedAvg",
      "FedProx"
    ],
    "test_setup": "experimental scenarios in both iid and non-iid settings, with theoretical analysis and benchmark datasets",
    "main_limitations": ""
  },
  {
    "paper_id": "1705.03104v2",
    "title": "Sharp phase transition for the random-cluster and Potts models via decision trees",
    "methods_used": [
      "OSSS inequality"
    ],
    "test_setup": "theoretical analysis",
    "main_limitations": ""
  }
]