[Submitted to Statistical Science](https://imstat.org/journals-and-publications/statistical-science/)

# **The Central Role of the Loss Function in** **Reinforcement Learning**


**Kaiwen Wang, Nathan Kallus and Wen Sun**


Abstract. This paper illustrates the central role of loss functions in datadriven decision making, providing a comprehensive survey on their influence in cost-sensitive classification (CSC) and reinforcement learning (RL).
We demonstrate how different regression loss functions affect the sample efficiency and adaptivity of value-based decision making algorithms. Across
multiple settings, we prove that algorithms using the binary cross-entropy
loss achieve first-order bounds scaling with the optimal policy’s cost and
are much more efficient than the commonly used squared loss. Moreover,
we prove that distributional algorithms using the maximum likelihood loss
achieve second-order bounds scaling with the policy variance and are even
sharper than first-order bounds. This in particular proves the benefits of distributional RL. We hope that this paper serves as a guide analyzing decision
making algorithms with varying loss functions, and can inspire the reader to
seek out better loss functions to improve any decision making algorithm.


Key words and phrases: First-Order (Small-Loss) and Second-Order (VarianceDependent) Bounds, RL with Function Approximation, Distributional RL.



**1. INTRODUCTION**


The value-based approach to reinforcement learning
(RL) reduces the decision making problem to regression:
first predict the expected rewards to go under the optimal
policy, given state and action, and then one can simply
choose the action that maximizes the prediction at every state. This regression, called Q-learning [70], combined with recent decades’ advances in deep learning,
plays a central role in the empirical successes of deep RL.
Groundbreaking examples are DeepMind’s use of deep
Q-networks to play Atari with no feature engineering [51]
and OpenAI’s use of deep reward models to align large
language models with human preferences via RL finetuning [54].
In prediction, we often say a good model is one with
low mean-squared error out of sample. Correspondingly,
regression is usually done by minimizing the average
squared loss between predictions and targets in the training data. However, low mean-squared error may translate loosely to high-quality, downstream decision making.
Thus, the natural question arises: is squared loss always
the best choice for learning Q-functions?
In this survey article, we highlight that the answer to
this question is a resounding “no!” We focus on the the

The authors are from Cornell University. Correspondence to
[Kaiwen Wang (https://kaiwenw.github.io).](https://kaiwenw.github.io/)



TABLE 1

The decision-making regret of value-based RL per loss function,
where n is the number of samples. In this paper, we’ll see that
squared loss cannot adapt to small-cost or small-variance settings
while binary-cross-entropy (bce) and maximum likelihood estimation
(mle) can. We remark that mle is used with distributional regression
and thus requires stronger realizability or completeness conditions.
Moreover, all three losses use slightly different eluder dimensions in
the online setting. We summarize these nuances in Table 2.


Loss \ Setting Worst-case Small cost Small variance


ℓsq Θ(1/ [√] n) Θ(1/ [√] n) Θ(1/ [√] n)


ℓbce Θ(1/ [√] n) O(1/n) Θ(1/ [√] n)


ℓmle Θ(1/ [√] n) O(1/n) O(1/n)


oretical question: when and how do alternative loss functions attain better guarantees for decision making? We
start with the simple setting of cost-sensitive classification
(CSC) and show that the binary cross-entropy (bce) loss
leads to an improved convergence guarantee that adapts
to problem instances with low optimal cost; such a result
is called a first-order bound. This result is due to Foster
and Krishnamurthy [24], who first observed that the bce
loss can have benefits over square loss when outcomes
are heteroskedastic and used this observation to derive
first-order regret bounds for CSC and contextual bandits
(CB). Then, we prove that the maximum likelihood (mle)
loss yields an even better convergence guarantee that additionally adapts to problem instances with low variance;



1




2


such a result is called a second-order bound and is strictly
stronger than first-order. This result is due to Wang et al.

[65], who first observed that the mle loss can have benefits over the bce loss when outcomes have low variance
and used this observation to derive second-order bounds

for CB and RL. We also provide lower bounds to show
that the separation between these loss functions is indeed
real, with one result new to this paper separating bce and
mle in their ability to attain second-order bounds. The decision making performance that each loss function attains
in each type of problem instance is outlined in Table 1.
We then turn to extending these observations and intuitions to RL, following the results of Wang et al. [64,
65], Ayoub et al. [7]. We systematically derive bounds
for both online and offline RL, revealing that the trends
in Table 1 continue to hold for the more challenging RL
settings. Finally, we discuss issues surrounding computational complexity and provide a solution in the hybrid RL
setting [58]. In summary, we provide a detailed survey on
the decision performance of different loss functions, with
the aim of elucidating their central role in RL and datadriven decision making generally. The technical material
is largely based on Foster and Krishnamurthy [24], Wang
et al. [64, 65], Ayoub et al. [7] with a couple new results
along the way.


**2. COST-SENSITIVE CLASSIFICATION**


To best illuminate the phenomenon, we start with the
simplest setting of contextual decision making: costsensitive classification (CSC), where learning is done offline, decisions have no impact on future contexts, and
full feedback is given for all actions. To make it the simplest CSC setting, we even assume that the action space
is finite (an assumption we shed in later sections). An
instance of the CSC problem is then characterized by
a context space X, a finite number of actions A, and
a distribution d on X × [0, 1] [A] . The value of a policy
π : X →{1,...,A} is its average cost under this distribution: V (π) = E[c(π(x))], where x,c(1),...,c(A) ∼ d.
The optimal value is V [⋆] = minπ:X→{1,...,A} V (π). We are
given n draws of xi,ci(1),...,ci(A) ∼ d, sampled independently and identically distributed (i.i.d.), based on
which we output a policy ˆπ with the aim of it having low
V (ˆπ).
Let C : X × A → ∆([0, 1]) map x,a to the conditional
distribution of c(a) given x under d. Here ∆([0, 1]) denotes the set of distributions on [0, 1] that are absolutely
continuous with respect to (w.r.t.) a base measure λ, such
as Lesbesgue measure for continuous distributions or a
counting measure for discrete distributions. We identify
such distributions by its density function w.r.t. λ and we
write C(y | x,a) for the density of C(x,a) at y. We assume that λ is common across x,a and is known. We can
then write value as an expectation w.r.t. x alone:

V (π) = E[ C [¯] (x,π(x))],



where the bar notation on a distribution denotes the mean:
p¯ = �y [yp][(][y][)d][λ][(][y][)][ for any][ p][ ∈] [∆([0][,] [1])][.]


**2.1 Solving CSC with Squared-Loss Regression**


A value-based approach to CSC is to learn a cost prediction f (x,a) ≈ C [¯] (x,a) by regressing costs on contexts and then use an induced greedy policy: πf (x) ∈
argmina f (x,a). A standard way to learn such a cost prediction is to minimize the squared error [5, 45, 30].
Define the squared loss and the excess squared-loss risk
for a prediction function f as:


ℓsq(ˆy,y) := (ˆy − y) [2],


Esq(f ) := [�] a [E][[][ℓ][sq][(][f] [(][x,a][)][,c][(][a][))][ −] [ℓ][sq][( ¯][C][(][x,a][)][,c][(][a][))]][.]


This can be used to expediently bound the sub-optimality
of the policy induced by f :


V (πf ) − V [⋆] = E[ C [¯] (x,πf (x)) − C [¯] (x,π [⋆] (x))]


≤ E[ C [¯] (x,πf (x)) − f (x,πf (x))


+ f (x,π [⋆] (x)) − C [¯] (x,π [⋆] (x))]


(1) ≲ ��a [E][(][f] [(][x,a][)][ −] [C][¯][(][x,a][))][2][�][1][/][2]

= (Esq(f )) [1][/][2],


where ≲ means ≤ up to a universal constant factor (e.g.,
above in Eq. (1), it is 2).
How do we learn a predictor with low excess squaredloss risk? We minimize the empirical squared-loss risk
over a hypothesis class F of functions X × A → [0, 1]:


fˆF [sq] [∈] [argmin][f] [∈F] �ni=1 �Aa=1 [ℓ][sq][(][f] [(][x][i][,a][)][,c][i][(][a][))][.]


This procedure is termed nonparametric least squares
(since F is general), and standard results control the excess risk of f [ˆ] F [sq][. Here we give a version for finite hypoth-]
esis classes, while for infinite classes the excess risk convergence depends on their complexity, such as given by
the critical radius [61].


ASSUMPTION 1 (Realizability). C¯ ∈F .


Under Asm. 1, for any δ ∈ (0, 1), with probability at
least (w.p.a.l.) 1 − δ,


Esq( f [ˆ] F [sq][)][ ≲] [A] [log(][A][|F|][/δ][)][/n.]


Together with Eq. (1), we obtain the following probably
approximately correct (PAC) bound:


THEOREM 1. Under Asm. 1, for any δ ∈ (0, 1), w.p.a.l.
1 − δ, plug-in squared loss regression enjoys



V (π ˆf sqF [)][ −] [V][ ⋆] [≲] �



A log(A|F|/δ)/n.




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 3



This PAC bound shrinks at a nice parametric rate of
O(n [−][1][/][2] ) as the number of samples n grows, but can we
do better? The step in Eq. (1), which translates error in
predicted means to excess risk in the squared loss, was
rather loose. Foster and Krishnamurthy [24] thus investigated the bce loss and showed that it can achieve a firstorder bound, which we present next after introducing an
important second-order lemma.


**2.2 The Second-Order Lemma**


We know that estimating a mean of a random variable
is easier when the random variable has smaller variance.

Our next result recovers this intuition as a completely deterministic statement about comparing bounded scalars:


LEMMA 1 (Second-Order Mean Comparison). Let
p,q be two densities on [0, 1] with respect to a common
measure λ [′] . Then


¯ ¯
|p − q| ≤ 6σ(p)h(p,q) + 8h [2] (p,q),


where the variance and the squared Hellinger distance
are defined as



Replacing the bound in Eq. (1) with Eq. (2) and using
Cauchy-Schwartz, we obtain


V (πf ) − V [⋆] ≲ �(V (πf ) + V [⋆] ) · δBer(f ) + δBer(f ),


where δBer(f ) := [�] a [E][[][h][2] Ber [( ¯][C][(][x,a][)][,f] [(][x,a][))]][.]


Applying the inequality of arithmetic and geometric
means (AM-GM), we see that this implies V (πf ) ≲
V [⋆] + δBer(f ). Plugging this implicit inequality back into
the above, we have that


(3) V (πf ) − V [⋆] ≲ �V [⋆] - δBer(f ) + δBer(f ).



Since V [⋆] ≤ 1, Eq. (3) also implies V (πf )−V [⋆] ≲ �



Since V [⋆] ≤ 1, Eq. (3) also implies V (πf )−V [⋆] ≲ �δBer(f ).

That is, if we learn a predictor with low �δBer(f ), then



σ [2] (p) = �



y [y][2][p][(][y][)d][λ][′][(][y][)][ −] [p][¯][2]



h [2] (p,q) = 2 [1]



y [(] �



2 [1] �



p(y) − �



q(y)) [2] dλ [′] (y).



Here, h [2] (p,q) is the squared Hellinger distance, which
is an f -divergence, and it is bounded in [0, 1]. This lemma
is equivalent to Lemma 4.3 of [65] and we provide a
simplified proof in Sec. 2.6. Interpreting the inequality,
which is a completely deterministic statement, in terms
of estimating means, it says that estimation error can be
bounded by two terms: one involves the standard deviation times a discrepancy and the other is a squared discrepancy. As variance shrinks, the first term vanishes and
the second term dominates, which as a squared term we
expect to decay quickly. We note that [26, 27] obtain related first-moment and second-moment bounds, which are
generally looser and do not directly imply the variance
bound in Lem. 1.


**2.3 Regression with the Binary-Cross-Entropy Loss:**
**First-Order PAC Bounds for CSC**


One way to instantiate Lem. 1 is to let λ [′] be the counting measure on {0, 1} and, given any f,g ∈ [0, 1], set
p,q as the Bernoulli distributions with means f,g, respectively. Bounding f (1 − f ) ≤ f, this leads to


(2) |f − g| ≤ 8�fhBer(f,g) + 20h [2] Ber [(][f,g][)][,]



That is, if we learn a predictor with low �δBer(f ), then

its induced policy has correspondingly low suboptimality.
However, Eq. (3) also crucially involves V [⋆] . Thus, if the
optimal policy incurs little expected costs so that the first
term in Eq. (3) is negligible, we get to square the rate of

convergence.
How do we learn a predictor with low δBer(f )? Since
δBer(f ) is an average divergence between Bernoulli distributions, we could try to fit Bernoullis to the costs. Define
the binary-cross-entropy (bce) loss as


ˆ
ℓbce(ˆy,y) := −y ln ˆy − (1 − y)ln(1 − y).


We adopt the convention that 0ln0 = 0. Then, δBer(f ) is
bounded by an exponentiated excess bce-loss risk [24].


LEMMA 2. For any f : X × A → [0, 1],


δBer(f ) ≤Ebce(f ),



where Ebce(f ) := − [�][A] a=1 [ln] [E][[exp(] 2 [1] [ℓ][bce][( ¯][C][(][x,a][)][,c][(][a][))][−]

1
2 [ℓ][bce][(][f] [(][x,a][)][,c][(][a][)))]][.]


PROOF. For each a, let z ∼ Ber(c(a)),




[1] 2 [ℓ][bce][( ¯][C][(][x,a][)][,c][(][a][))][ −] 2 [1]



− ln E[exp( [1] 2



2 [ℓ][bce][(][f] [(][x,a][)][,c][(][a][)))]]



¯

2 [1] [(][c][(][a][)ln][ f] C [(][x,a][)]



1− [−] C [¯] ( [x,a] x,a) [))]]



= − ln E[exp( 2 [1]



C¯ [(] ( [x,a] x,a [)] ) [+ (1][ −] [c][(][a][))ln][ 1] 1− [−] C [f][¯][(] ( [x,a] x,a [)]



(i)
≥− ln E[exp( [1] 2



C¯ [(] ( [x,a] x,a [)] ) [+ (1][ −] [z][)ln][ 1] 1− [−] C [f][¯][(] ( [x,a] x,a [)] )



¯

[1] 2 [(][z][ ln][ f] C [(][x,a][)]



1− [−] C [¯] ( [x,a] x,a) [))]]



= − ln E[�f (x,a) C [¯] (x,a) + �


(ii)
≥ 1 − E[�f (x,a) C [¯] (x,a) + �



f (x,a) C [¯] (x,a) + �



(1 − f (x,a))(1 − C [¯] (x,a))]



(ii)
≥ 1 − E[�



(1 − f (x,a))(1 − C [¯] (x,a))]



2 [(][√][f][ −√][g][)][2] [+][ 1] 2



where h [2]
Ber [(][f,g][) =][ 1] 2



2 [(][√][1][ −] [f][ −√][1][ −] [g][)][2]



(iii)
= h [2]
Ber [( ¯][C][(][x,a][)][,f] [(][x,a][))][.]


where (i) is by Jensen’s inequality, (ii) is by − ln x ≥ 1 −
x, (iii) is by completing the square.


To learn a predictor with low Ebce, we may consider
minimizing the empirical bce-loss risk, simply replacing
ℓsq by ℓbce in nonparametric least squares:

fˆF [bce] ∈ argminf ∈F �ni=1 �Aa=1 [ℓ][bce][(][f] [(][x][i][,a][)][,c][i][(][a][))][.]



is the squared Hellinger distance between Bernoullis with
means f and g. This recovers the key inequalities in

[24, 64, 7].




4


The bce loss ℓbce(ˆy,y) is exactly the negative loglikelihood of observing y from a Bernoulli distribution
with mean ˆy. Nevertheless, even if y is not binary, it can
be used as a general-purpose surrogate loss for regression
(sometimes under the moniker “log loss" [24, 7]). In particular, for any density p ∈ ∆([0, 1]), the mean ¯p minimizes expected bce loss:


Ey∼p[ℓbce(f,y) − ℓbce(¯p,y)] ≥ 2(f − p¯) [2] .


This inequality also means that we could use the excess
bce-loss risk to bound Eq. (1) in place of excess squaredloss risk. The point of using bce loss, however, is to do
better than Eq. (1) via Eq. (2).
For the final part of the proof, we need to show that
minimizing the empirical bce-loss risk gives good control
on Ebce( f [ˆ] F [bce][)][. This is implied by the following tail bound.]


LEMMA 3. Let Z1,...,Zn denote n i.i.d. random
variables. For any δ ∈ (0, 1), w.p.a.l. 1 − δ,


−n ln E[exp(−Z1)] ≤ [�][n] i=1 [Z][i][ + ln(1][/δ][)][.]


PROOF. We note E[exp( [�][n] i=1 [Z][i][)] = (][E][[exp(][Z][1][)])][n][.]
By Chernoff’s method, Pr( [�][n] i=1 [Z][i] [−][n] [ln] [E][Z][1] [exp(][Z][1][)][ ≥]
t) ≤ exp(−t) for all t > 0. Finally, set t = ln(1/δ).


Then, applying Lem. 3 with Zi = [1] 2 [ℓ][bce][(][f] [(][x][i][,a][)][,c][i][(][a][))][−]

1
2 [ℓ][bce][( ¯][C][(][x][i][,a][)][,c][i][(][a][))][ and a union bound over][ a][ and][ f] [,]
we have w.p.a.l. 1 − δ, for all f ∈F


n A

nEbce(f ) ≤ 2 [1] �i=1 �a=1 [ℓ][bce][(][f] [(][x][i][,a][)][,c][i][(][a][))]

− ℓbce( C [¯] (xi,a),ci(a)) + A ln(2A|F|/δ).


With Asm. 1, the empirical minimizer f [ˆ] F [bce] enjoys


Ebce( f [ˆ] F [bce][)][ ≤] [A][ ln(][A] n [|F|][/δ][)] .


Thus, together with Eq. (3) and Lem. 2, we have shown a
first-order PAC bound for bce-loss regression:


THEOREM 2. Under Asm. 1, for any δ ∈ (0, 1), w.p.a.l.
1 − δ, plug-in bce loss regression enjoys



**2.4 Maximum Likelihood Estimation: Second-Order**

**PAC Bounds for CSC**


Can we do even better than a first-order PAC bound
with the bce loss? In this section we show that a second
order, variance-adaptive bound is possible if we learn the
conditional cost distribution instead of only regressing the
mean. To learn the distribution, we use a hypothesis class
P of conditional distributions X × A → ∆([0, 1]) and
minimize the negative-log likelihood loss from maximum
likelihood estimation (mle): for a density ˆp ∈ ∆([0, 1])
and target y ∈ [0, 1], define


ℓmle(ˆp,y) := − ln ˆp(y).


Unlike the previous sections where the loss measured the
discrepancy of a point prediction, the mle loss measures
the discrepancy of a distributional prediction. Indeed,
if ˆp = Ber(ˆy) and p = Ber(y), then Ey∼p[ℓmle(ˆp,y)] =
ℓbce(ˆy,y) so the bce loss can be viewed as a Bernoulli
specialization of the general mle loss. This generality allows us to directly apply Lem. 1 in place of Eq. (1) to
obtain for any p ∈P:


(4)


V (πp¯) − V [⋆] ≲ (σ [2] (πp¯) + σ [2] (π [⋆] ))δdis(p) + δdis(p),
�

where δdis(p) := [�] a [E][[][h][2][(][C][(][x,a][)][,p][(][x,a][))]][ and][ σ][2][(][π][) :=]
σ [2] ( C [¯] (x,π(x))). As in the bce section, we then upper
bound δdis(f ) by an exponentiated excess mle-loss risk.


LEMMA 4. For any f : X × A → ∆([0, 1]),


δdis(p) ≤Emle(p),

where Emle(p) := − [�][A] a=1 [ln] [E][[exp(] [1] 2 [ℓ][mle][(][C][(][x,a][)][,c][(][a][))][−]

1
2 [ℓ][mle][(][p][(][x,a][)][,c][(][a][)))]][.]


The proof is almost identical to that of Lem. 2, and is
even simpler since the inequality marked “(i)" in the proof
is not needed. To learn a predictor with low Emle, we minimize the empirical negative log-likelihood risk:

pˆ [mle] P ∈ argminp∈P Lmle(p),


A
where Lmle(p) := [�][n] i=1 �a=1 [ℓ][mle][(][p][(][x][i][,a][)][,c][i][(][a][))][.]


We also posit realizability in the distribution class.



O�(
�



V [⋆] (1 − V [⋆] ) · [1]




[1]

n [+][ 1] n



O( V [⋆] (1 − V [⋆] ) · n [+][ 1] n [)][, where the leading term van-]

ishes also if V [⋆] ≈ 1. Bounds scaling with 1 − V [⋆] are
sometimes called ‘small-reward’ bounds [6], which are
generally easier to prove than ‘small-cost’ bounds [46,
64]. For example, if the bound scales with the minimum
achievable reward R [⋆], the case where R [⋆] = 0 is actually trivial since all policies are optimal. However, if the
bound scales with the minimum achievable cost L [⋆], the
case where L [⋆] = 0 is interesting because sub-optimal
policies may still have large cost. Thus, following Lykouris, Sridharan and Tardos [46], Wang et al. [64], we
focus on the cost minimizing setting in this paper.



V (π ˆf bceF [)][ −] [V][ ⋆] [≲] �



V [⋆] - [A][ ln(][A][|F|][/δ][)]



n .



n [|F|][/δ][)] + [A][ ln(][A] n [|F|][/δ][)]



This result was first observed in Theorem 3 of [24].
Notably, the bound is adaptive to the optimal expected
costs V [⋆] and converges at a fast n [−][1] rate when V [⋆] ≲ 1/n.
Under the cost minimization setup, first-order bounds are
also called ‘small-cost’ bounds since they converge at a
fast rate when the optimal cost V [⋆] is small.


REMARK 1. A refinement of Eq. (2) keeps the first
term as �f (1 − f )hBer instead of [√] fhBer. This would

imply a more refined first-order bound that scales as




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 5



ASSUMPTION 2 (Distributional Realizability). C ∈P.


While this assumption is more stringent than meanrealizability (Asm. 1 from before), it is required to prove
that MLE succeeds [20, 3]. Alternative algorithms such
as Scheffé tournament can be used to learn distributions
under misspecification, but they are computationally hard
to implement [2].
Finally, we apply Lem. 3 with Zi = [1] 2 [ℓ][mle][(][p][(][x][i][,a][)][,c][i][(][a][))][−]

1
2 [ℓ][mle][(][C][(][x][i][,a][)][,c][i][(][a][))][ with union bound over][ P][, to de-]
duce that w.p.a.l. 1 − δ, for all p ∈P:



We start by defining a subclass of near-optimal distributions w.r.t. the empirical mle loss


Pn := {p ∈P : Lmle(p) − Lmle(ˆp [mle] P [)][ ≤] [β][}][,]


where β is a parameter that will be set appropriately.
Then, a pessimistic distribution is learnt by selecting the
element with the lowest value. The following lemma defines this formally and proves that the learned distribution
(a) has low excess risk and (b) is nearly pessimistic.


LEMMA 5. Under Asm. 2, for any δ ∈ (0, 1), set β =
2A ln(A|P|/δ) and define,


(6) pˆ [pes] ∈ argmaxp∈Pn �ni=1 [min][a][ ¯][p][(][x][i][,a][)][.]



2 [1] [L][mle][(][p][)][ −] [1] 2



(5) nEmle(p) ≤ [1]



2 [L][mle][(][C][) +][ A] [ln(][A][|P|][/δ][)][.]



Together with Asm. 2, we have that


Emle(ˆp [mle] P [)][ ≤] [A][ ln(][A] n [|P|][/δ][)] .


Thus we have proven a second-order PAC bound for the
greedy policy ˆπ [mle] := πˆmle
pP [.]


THEOREM 3. Under Asm. 2, for any δ ∈ (0, 1), w.p.a.l.
1 − δ, plug-in mle enjoys



nEmle(p) ≤ [1] 2




[1] 2 [L][mle][(][p][)][ −] [1] 2



Then, w.p.a.l. 1 − δ, (a) Emle(ˆp [pes] ) ≲ [A][ ln(][A][|P|][/δ][)]



Then, w.p.a.l. 1 − δ, (a) Emle(ˆp [pes] ) ≲ n, and (b)

V (ˆπ [pes] ) − E[mina ˆp [pes] (x,a)] ≲ [ln(][A][|P|][/δ][)] .



n .



PROOF OF LEM. 5. For both claims, we condition on
Eq. (5) which holds w.p.a.l. 1 − δ. For Claim (a): for any
p ∈Pn (which includes ˆp [pes] ), we have




[1] 2 [L][mle][(ˆ][p] P [mle][) +][ A] [ln(][A][|P|][/δ][)]



V (ˆπ [mle] ) − V [⋆] ≲ �



(σ [2] (ˆπ [mle] ) + σ [2] (π [⋆] )) · [A][ ln(][A][|P|][/δ][)]



≤ [1] 2 [β][ +][ A] [ln(][A][|P|][/δ][)][ ≤] [2][A] [ln(][A][|P|][/δ][)][,]



n



+ [A][ ln(][A][|P|][/δ][)] .

n



Since costs are bounded in [0, 1], we observe that
σ [2] (π) ≤ V (π), and hence a second-order bound is tighter
than a first-order bound. However, our approach is distributional with bounds depending on ln |P| which can
be larger than ln |F|, and as mentioned earlier, realizability in distribution space is stricter than realizability in
the conditional mean. We remark that distributional approaches still achieve superior performance [10, 11, 64,
65] in practice, suggesting that MLE with modern function approximators can well-approximate complex distributions despite the stronger assumption in theory.


**2.5 Improved Second-Order PAC Bounds for CSC**
**with Pessimistic MLE**


We can derive even tighter bounds if the distribution is
learned in a pessimistic manner – that is, the mean of the
learned distribution upper bounds the true optimal mean
V [⋆] with high probability. [1] In this section, we introduce
how to achieve pessimism by optimizing over a subset
of the function class defined by empirical losses, an approach that is often termed ‘version space’ [25]. This is
also important warmup for the optimistic and pessimistic
RL algorithms that we consider in the sequel.


1Here, we say the learned mean is pessimistic if it upper bounds V ⋆

since we’re in the cost minimization setting. Under the reward maximization setting, pessimism would be to lower bound V [⋆] .



where the first inequality is by Eq. (5) and the fact that
pˆ [mle] P minimizes the empirical risk; and the second inequality is by the definition of Pn. To prove Claim
(b), we first show that C ∈Pn: by Eq. (5) and the
non-negativity of Emle, we have Lmle(C) − Lmle(p) ≤
2A ln(A|P|/δ) = β for all p ∈P (which includes ˆp [mle] P [).]
Thus, this shows that C satisfies the Pn condition, implying its membership in the set. To conclude Claim (b), we
have [�][n]
i=1 [p][ˆ][pes][(][x][i][,π][⋆][(][x][i][))][ ≥] [�][n] i=1 [min][a][ ˆ][p][pes][(][x][i][,a][)][ ≥]
�ni=1 [min][a][ ¯][C][(][x][i][,a][)][. Claim (b) then follows by multi-]
plicative Chernoff [75, Theorem 13.5].


With pessimism, the induced policy ˆπ [pes] := πpˆpes only
sufferes one of the terms before Eq. (1), and so


V (ˆπ [pes] ) − V [⋆] ≤ E[ˆp [pes] (x,π [⋆] (x)) − C [¯] (x,π [⋆] (x))]


(7) ≲ �σ [2] (π [⋆] ) · δdis(ˆp [pes] ) + δdis(ˆp [pes] )



≲
�



σ [2] (π [⋆] ) · [A][ ln(][A][|P|][/δ][)]



n .



n [|P|][/δ][)] + [A][ ln(][A] n [|P|][/δ][)]



Thus, we have proven an improved second-order PAC
bound for pessimistic mle.


THEOREM 4. Under Asm. 2, for any δ ∈ (0, 1), w.p.a.l.
1 − δ, pessimistic mle enjoys



V (ˆπ [pes] ) − V [⋆] ≲ �



σ [2] (π [⋆] ) · [A][ ln(][A][|P|][/δ][)]



n [|P|][/δ][)] + [A][ ln(][A] n [|P|][/δ][)]



n .




6



Notably, Eq. (7) is an improvement to Eq. (4) since it
only contains the variance of the optimal policy π [⋆], which
is a fixed quantity, and not that of the learned policy,
which is a random algorithm-dependent quantity. We remark that while pessimism is typically used to solve problems with poor coverage, e.g., offline RL, we see it also
plays a crucial role in obtaining finer second-order PAC
bounds in CSC, which has full coverage due to complete
feedback.

Pessimism could have also been applied with the bceloss, but there would have been no improvement to
the first-order bound. This is because V (ˆπ [bce] ) − V [⋆] ≤
�(V (ˆπ [bce] ) + V [⋆] ) · [C] n [+][ C] n [already implies][ V][ (ˆ][π][bce][)][ −]



(V (ˆπ [bce] ) + V [⋆] ) · [C]



n [already implies][ V][ (ˆ][π][bce][)][ −]




[C]

n [+][ C] n




- (·) is a symmetric f -divergence and is equivalent to the
squared Hellinger distance up to universal constants:

(9) 2h [2] (p,q) ≤ ∆(p,q) ≤ 4h [2] (p,q),


which is a simple consequence of Cauchy-Schwartz [64,
Lemma A.1]. Thus, we first prove the second-order
lemma using △(·), which is more natural, and then convert the bounds to h [2] (·) using Eq. (9).


LEMMA 6. Let p,q be densities on [0, 1] and let q be
the one with smaller variance. Then,

(10) σ [2] (p) − σ [2] (q) ≤ 2�σ [2] (q) · ∆(p,q) + ∆(p,q),


¯ ¯

(11) |p − q| ≤ 3�σ [2] (q) · ∆(p,q) + 2∆(p,q).


PROOF OF LEM. 6. We first prove Eq. (10):

σ [2] (p) − σ [2] (q)


i
≤ [�]

y [(][y][ −] [q][¯][)][2][(][p][(][y][)][ −] [q][(][y][))d][λ][′][(][y][)]


ii
≤
��y [(][y][ −] [q][¯][)][4][(][p][(][y][) +][ q][(][y][))d][λ][′][(][y][)][ · △][(][p,q][)]


iii
≤
��y [(][y][ −] [q][¯][)][2][(][p][(][y][) +][ q][(][y][))d][λ][′][(][y][)][ · △][(][p,q][)]



V [⋆] ≤
�



n [′] [+][ C] n [′]



V [⋆] - [C] [′]



n [′] [where][ C] C [′]



V [⋆] ≤ V [⋆] - n [+] n [where] C [is a universal constant,]

due to the AM-GM inequality as noted in the text preceding Eq. (3). However, this implicit inequality does not
hold for variance-based inequalities, and so pessimism is
crucial for removing the dependence on the learned policy’s variance.
Finally, we note that pessimistic mle requires more
computation than plug-in mle, since we have the extra
step of optimizing over Pn. For one-step settings like CSC
or contextual bandits, this can be feasibly implemented
with binary search [25] or width computation [41, 22].
However, in multi-step settings like RL as we will soon
see, this optimization problem is NP-hard [18].



iv
≤ [1]



y [(][y][ −] [q][¯][)][2][p][(][y][)d][λ][′][(][y][) +][ σ][2][(][q][)] � + [△][(] 2 [p,q][)]



2,




[1] 2

��



REMARK 2 (Another improved bound via optimism).
We could also consider optimistic mle where ˆp [op] ∈

n

ˆ
argminp∈Pn �i=1 [min][a][ ¯][p][(][x][i][,a][)][ and][ ˆ][π][op][ =][ π] p [op][. Then,]
the decomposition of Eq. (7) would look like:


ˆ
V (ˆπ [op] ) − V [⋆] ≤ E[ C [¯] (x, ˆπ [op] (x)) − p [op] (x, ˆπ [op] (x))]


�

≲ �σ [2] (π [op] ) · δdis(ˆp [op] ) + δdis(ˆp [op] )



where (i) is by the definition of variance, (ii) is by
Cauchy-Schwarz, (iii) is by the premise that p,q are densities on [0, 1], and (iv) is by AM-GM. Rearranging the
inequality that (i)≤(iv), we get
�y [(][y][ −] [q][¯][)][2][p][(][y][)d][λ][′][(][y][)][ ≤] [3][σ][2][(][q][) +][ △][(][p,q][)][.]



where (i) is by the definition of variance, (ii) is by
Cauchy-Schwarz, (iii) is by the premise that p,q are densities on [0, 1], and (iv) is by AM-GM. Rearranging the
inequality that (i)≤(iv), we get
�y [(][y][ −] [q][¯][)][2][p][(][y][)d][λ][′][(][y][)][ ≤] [3][σ][2][(][q][) +][ △][(][p,q][)][.]

Finally, plugging this back into (iii) implies Eq. (10).
Now we prove Eq. (11). Set c = [p][¯][+¯] 2 [q] [. First, consider the]

case that D△(p,q) ≤ 1:



2

|p¯ − q¯| [2] = ����y [(][p][(][y][)][ −] [q][(][y][))(][y][ −] [c][)d][λ][′][(][y][)] ���



Finally, plugging this back into (iii) implies Eq. (10).
Now we prove Eq. (11). Set c = [p][¯][+¯][q]



≲
�



n [|P|][/δ][)] + [A][ ln(][A] n [|P|][/δ][)]



�
σ [2] (π [op] ) · [A][ ln(][A][|P|][/δ][)]



n .



This is also an improved second-order PAC bound, which
depends only on the variance of the learned policy and not
that of π [⋆] . The bound in Thm. 4 may be preferred since
σ(π [⋆] ) is a fixed quantity; however, we note that σ [2] (ˆπ) and
σ [2] (π [⋆] ) are not comparable in general, so neither bound
dominates the other.


**2.6 Proof of the Second-Order Lemma**


The goal of this subsection is to prove the second-order
lemma (Lem. 1), a key tool to derive first- and secondorder PAC bounds. We prove the result in terms of another divergence called the triangular discrimination: for
any densities p,q on [0, 1] w.r.t. a common measure λ [′],
the triangular discrimination is defined as



This finishes the case of △(p,q) ≤ 1. Otherwise, we sim
¯ ¯
ply have |p − q| ≤ 1 < △(p,q).



i
≤ [�]

y [(][p][(][y][) +][ q][(][y][))(][y][ −] [c][)][2][d][λ][′][(][y][)][ · △][(][p,q][)]



=ii σ [2] (p) + σ [2] (q) + 2� p¯−2 q¯�2 [�] - (p,q)
�



iii
≤ �σ [2] (p) + σ [2] (q)�△(p,q) + [(¯][p][−] 2 [q][¯][)][2]



2

where (i) is by Cauchy-Schwarz, (ii) is by expanding the
variance σ [2] (f ) = � [(][f] [(][y][)][ −] [c][)][2][d][λ][(][y][)][ −] [( ¯][f][ −] [c][)][2][ which]



variance σ [2] (f ) = �y [(][f] [(][y][)][ −] [c][)][2][d][λ][(][y][)][ −] [( ¯][f][ −] [c][)][2][ which]

holds for any c ∈ R, and (iii) is by △(p,q) ≤ 1. Rearranging terms and using Eq. (10), we get



¯ ¯
|p − q| ≤ �


≤ �



2(σ [2] (p) + σ [2] (q))△(p,q)


2(3σ [2] (q) + 2△(p,q))△(p,q)



≤ 3�



σ [2] (q)△(p,q) + 2△(p,q).



(p(y)−q(y)) [2]
y p(y)+q(y)



(8) - (p,q) := �



p(yy)+−qq(yy) [d][λ][′][(][y][)][.]




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 7



**3. LOWER BOUNDS FOR CSC**


So far, we have seen that plug-in regression with the
squared loss, bce loss and mle loss have progressively
more adaptive PAC bounds for CSC. A natural question is
if the previous bounds were tight: is it necessary to change
the loss function if we want to achieve these sharper and
more adaptive bounds? In this section, we answer this in
the affirmative by exhibiting counterexamples.


**3.1 Plug-in Squared Loss Cannot Achieve First-Order**


First, we show that the policy induced by squared loss
regression cannot achieve first-order bounds. The following counterexample is due to [24], where we have simplified the presentation and improved constants.


THEOREM 5. For all n > 400, there exists a CSC
problem with |A| = |X| = 2 and a realizable function class with |F| = 2 such that: (a) V [⋆] ≤ n [1] [, but (b)]

1
V (π ˆf sqF [)][ −] [V][ ⋆] [≥] 32 [√] n [w.p.a.l.][ 0][.][1][.]


The intuition is that squared loss regression does not
adapt to context-dependent variance, a.k.a. heteroskedasticity; so the convergence of squared loss regression is
dominated by the worst context’s variance. In this counterexample, the second context x [2] occurs with tiny probability n [−][1] but has high variance; however, the empirical
squared loss is dominated by this unlikely context.


PROOF OF THM. 5. The structure of the proof is the
following: for any n > 400, we first construct the CSC
problem and realizable function class, and then show that
indeed V [⋆] ≤O( n [1] [)][. Next, we show that under a bad event]

which occurs with probability at least 0.1, the function
with the lowest empirical squared risk induces a policy
that suffers regret which is lower bounded by Ω( √ [1] n ).
Fix any n > 400. We begin by setting up the CSC problem: label the two states as x [1],x [2] and the two actions as
a [1],a [2] . Set the data generating distribution d as follows:
d(x [1] ) = 1 − n [−][1], d(x [2] ) = n [−][1] and


c(a [1] ) | x [1] ∼ Ber(µn), c(a [2] ) | x [1] = νn,



Now, we compute the empirical squared-loss risk and
show that f [ˆ] F [sq] [=][ �][f][ under a bad event. The empirical risk]
can be simplified by shedding shared terms to be:


� n(x [i] )
Lsq(f ) = [�] i∈[2] n [(][f] [(][x][i][,a][1][)][ −] [µ][�][(][x][i][,a][1][))][2][,]


where n(x) denotes the number of times x occurs in the
dataset and �µ(x,a) = n(1x) �i:xi=x [c][i][(][a][)][ is the empirical]
conditional mean. We split the bad event into two parts:
(E1) x [2] appears only once in the dataset (i.e., n(x [2] ) = 1)
and its observed cost at a [1] is 0 (i.e., �µ(x [2],a [1] ) = 0); and
(E2) �µ(x [1],a [1] ) ≤ 2µn + n−3 1 [. We lower bound][ Pr(][E][1][ ∩]
E2) ≥ 0.1 at the end.
We now show that f [�] F [sq] [=][ �][f][ under][ E][1][ ∩] [E][2][. Under][ E][1][,]
we lower bound L [�] sq(f [⋆] ) by:



n(x [2] )



x �

n (f [⋆] (x [2],a [1] ) − µ(x [2],a [1] )) [2] = 4 [1] n



4n [.]



fUnder�(x [2],a E [1] ) =1 ∩ �µE(x2, the [2],a [1] ) x, and so [2] term of �Lsq(L [�] f �sq) can be bounded by:(f [�] ) vanishes since


(f [�] (x [1],a [1] ) − µ�(x [1],a [1] )) [2] ≤ 2ε [2] n [+ 2(2][µ][n] [+] n−3 1 [)][2][ <][ 1] 4n [,]


where the last inequality holds due to n > 400. Thus,
squared loss regression selects f [ˆ] F [sq] [=][ �][f][ and the regret of]
the induced policy can be lower bounded by:



V (π ˆf sqF [)][ −] [V][ ⋆] [=][ n][−] n [1]




[−] n [1][(][ν][n][ −] [µ][n][)][ ≥] [1]



16 [1] [(][ 1] √n − n [1]



1
n [1] [)][ ≥] 32 [√] n [.]



2 [1] [)][,] c(a [2] ) | x [2] = [1] 2



c(a [1] ) | x [2] ∼ Ber( 2 [1]



2 [,]



where µn = 81n [,ν][n][ =] 8 [√] 1 n [. Our realizable function class]
F contains two elements: the true f [⋆] (x,a) = E[c(a) | x]
and another function f [�] defined as


� �
f (x [1],a [1] ) = εn, f (x [1],a [2] ) = νn,


f�(x [2],a [1] ) = 0, f�(x [2],a [2] ) = [1] 2 [,]


where εn = 4 [√] 1 n [. Note that][ µ][n][ < ν][n][ so][ π][⋆][(][x][1][) =][ a][1][ but]

εn > νn so π �f (x [1] ) = a [2], i.e., π �f makes a mistake on x [1] .
Also, V [⋆] = (1 − n [−][1] )µn + 2 [1] [≤] [1] [.]



Probability of the bad event. For E1, since n(x [2] ) ∼
Bin(n,n [−][1] ), thus Pr(n(x [2] ) = 1) = (1 − n [−][1] ) [n][−][1] ≥ e [−][1] .
Hence, Pr(E1) ≥ (2e) [−][1] . For E2, we apply the multiplicative Chernoff bound [75, Theorem 13.5], which
implies �µ(x [1],a [1] ) < 2µn + n−3 1 [w.p.a.l.][ 1][ −] [e][−][3][. Thus,]
Pr(E1 ∩ E2) ≥ 1 − (1 − (2e) [−][1] ) − e [−][3] ≥ 0.1.


**3.2 Plug-in BCE Loss Cannot Achieve Second-Order**


Next, we show that the bce-loss induced policy cannot
achieve second-order bounds. This is a new result that did

not appear before.


THEOREM 6. For all odd n ∈ N, there exists a CSC
problem where |A| = 2, |X| = 1 and a realizable function class with |F| = 2 such that: (a) σ [2] (π [⋆] ) = 0, but (b)
1
V (π ˆf bceF [)][ −] [V][ ⋆] [≥] 8 [√] n [w.p.a.l.][ 1] 4 [.]


PROOF OF THM. 6. The proof structure is similar as
before: for any odd n, we construct the CSC problem
and a realizable function class. We show that σ [2] (π [⋆] ) = 0
which is the second-order regime; we also sanity check
that V [⋆] is bounded away from 0 and 1, to ensure that
we’re not in the first-order regime. Next, we show that
under a bad event which occurs with constant probability,
the function with the lowest empirical bce risk induces
a policy that suffers regret which is lower bounded by
Ω( √ [1] n ).




[1]

2n [≤] n [1]



n [.]




8



Fix any odd n ∈ N. We first construct the CSC problem:
label the two actions as a [1],a [2] and drop the context notation since there is one context. Set the data generating distribution d such that: c(a [1] ) ∼ Ber( 2 [1] [+][ ε][n][)][ and][ c][(][a][2][) =][ 1] 2

w.p. 1. The true conditional means are f [⋆] (a [1] ) = [1] 2 [+][ ε][n]

and f [⋆] (a [2] ) = [1] 2 [. In addition to][ f][ ⋆][, the function class][ F]



2 [1] [+][ ε][n][)][ and][ c][(][a][2][) =][ 1] 2



and f [⋆] (a ) = 2 [. In addition to][ f][ ⋆][, the function class][ F]

only contains one other functionf�(a [2] ) = [1] [. Note that the optimal action is] f [�] defined as [ a][⋆] [=] f [�][ a] (a [2][1][ and] ) =



f (a [2] ) = 2 [1] [. Note that the optimal action is][ a][⋆] [=][ a][2][ and]

the regret of a [1] is f [⋆] (a [1] ) − f [⋆] (a [2] ) = εn. We also check
that V [⋆] = Θ(1), and so this is not the first-order regime.
Now, we compute the empirical bce-loss risk and show
that f [ˆ] F [bce] = f [�] under a bad event. Since all elements of F
have the same prediction for a [2], the empirical bce-loss
risk can be simplified to



�
Lbce(f ) = p · ℓbce(f (a [1] ), 0) + (1 − p) · ℓbce(f (a [1] ), 1)


= ℓbce(f (a [1] ), 1 − p),



where p is the fraction of times that c(a [1] ) = 0 in the
dataset. The above loss is convex and its minimizer is
1 − p. The bad event we consider is that p > 2 [1] [, under]

which we have 1 − p < f [�] (a [1] ) < f [⋆] (a [1] ); since the loss is
convex, f [�] indeed achieves lower loss than f [⋆] . Thus, we
have that f [�] F [bce] = f [�] and the regret of the induced policy is
1
V (π �f bceF [)][ −] [V][ ⋆] [=][ f][ ⋆][(][a][1][)][ −] [f][ ⋆][(][a][2][) =] 8 [√] n [.]
Probability of the bad event. Kontorovich [40] proved
tight lower and upper bounds for binomial small deviations and we will make use of the following result: for all
n ≥ 1 and γ ∈ [0, √1n ], let Pr(Bin(n, 2 [1] [−] [γ] 2 [)][ ≤] � n2 �) −




[γ] n

2 [)][ ≤] � 2



small cost (i.e., first-order) or has small variance (i.e.,
second-order), while the bce or mle losses, respectively,
can be used to achieve fast O(1/n) rates.
In the rest of this paper, we will see that these observations and insights generally transfer to more complex decision making setups, in particular reinforcement learning
(RL). Compared to the CSC setting, two new challenges
of RL are that (1) the learner receives feedback only for
the chosen action (a.k.a., partial or bandit feedback) and
(2) the learner sequentially interacts with the environment
over multiple time steps. As before, we focus on valuebased algorithms with function approximation and prove
bounds for problems with high-dimensional observations,
i.e., beyond the finite tabular setting.


**4.1 Problem Setup**


We formalize the RL environment as a Markov De
cision Process (MDP) which consists of an observation
space X, action space A, horizon H, transition kernels
{Ph : X × A → ∆(X )}h∈[H] and conditional cost distributions {Ch : X × A → ∆([0, 1])}h∈[H]. We formalize the policy as a tuple of mappings π = {πh : X →
∆(A)}h∈[H] that interacts (a.k.a. rolls-in) with the MDP
as follows: start from an initial state x1 and at each
step h = 1, 2,...,H, sample an action ah ∼ πh(xh), collect a cost ch ∼ Ch(xh,ah) and transit to the next state
xh+1 ∼ Ph(xh,ah). We use Z [π] = [�][H] h=1 [c][h][ to denote the]
cumulative cost, a random variable, from rolling in π; we
consider the general setup where Z [π] is normalized between [0, 1] almost surely which allows for sparse rewards

[34]. We use Zh [π][(][x][h][,a][h][) =][ �][H] t=h [c][t][ to denote the cumu-]
lative cost of rolling in π from xh,ah at step h. We use
Q [π] h [(][x][h][,a][h][) =][ E][[][Z] h [π][(][x][h][,a][h][)]][ and][ V][ π] h [(][x][h][) =][ Q][π] h [(][x][h][,π][)][ to]
denote the expected cumulative costs, where we use the
shorthand f (x,π) = Ea∼π(x)f (x,a) for any f . For simplicity, we assume the initial state x1 is fixed and known,
and we let V [π] := V1 [π][(][x][1][)][ denote the initial state value]
of π. Our results can be extended to the case when x1 is
stochastic from an unknown distribution, or, in the online
setting, the initial state at round k may even be chosen by
an adaptive adversary.
Online RL. The learner aims to compete against the
optimal policy denoted as π [⋆] = argminπ V1 [π][(][x][1][)][. We use]
Z [⋆],V [⋆],Q [⋆] to denote Z [π][⋆],V [π][⋆],Q [π][⋆], respectively. The
online RL problem iterates over K rounds: for each round
k = 1, 2,...,K, the learner selects a policy π [k] to roll-in
and collect data, and the goal is to minimize regret,


(12) RegRL(K) = [�][K] k=1 [V][ π][k][ −] [V][ ⋆][.]


We also consider PAC bounds where the learner outputs
π [k] at each round but may roll-in with other exploratory
policies to better collect data.
Offline RL. The learner is given a dataset of prior interactions with the MDP and, unlike online RL, cannot



2 [−] [γ] 2



n

2 �) −



Pr(Bin(n, [1] 2



n
2 [)][ ≤] � 2



Pr(Bin(n, [1] 2 [+][ γ] 2 [)][ ≤] � n2 �) ≤ [√] nγ. If n is odd, we have

that 12 [= Pr(Bin(][n,][ 1] 2 [)][ ≤] � n2 �) < Pr(Bin(n, [1] 2 [−] [γ] 2 [)][ ≤]



2 [+][ γ] 2



n2 �) < Pr(Bin(n, [1] 2



n
2 [)][ ≤] � 2



2 [−] [γ] 2



that 2 [= Pr(Bin(][n,][ 1] 2 [)][ ≤] � n2 �) < Pr(Bin(n, 2 [−] [γ] 2 [)][ ≤]

� n2 �). Thus, Pr(Bin(n, 2 [1] [+][ γ] 2 [)][ <][ n] 2 [)][ ≥] 2 [1] [−√][nγ][. Setting]



n2 �). Thus, Pr(Bin(n, 2 [1]



2 [+][ γ] 2



2 [)][ ≥] 2 [1]



� n2 �). Thus, Pr(Bin(n, 2 [+][ γ] 2 [)][ <][ n] 2 [)][ ≥] 2 [−√][nγ][. Setting]

1 1
γ = 4 [√] n [(corresponding to][ ε][n][ =] 8 [√] n [), we have shown]



2 [)][ <][ n] 2



that the bad event occurs with probability at least 4 [1] [.]


In the above proof, we used two key properties of
the empirical bce risk: (1) its minimizer is the empirical
mean, and (2) it is convex w.r.t. the prediction (i.e., the
first argument). Since squared loss also has these properties, the above result also applies to squared regression.
However, since the mle loss learns a distribution rather
than just the mean, the counterexample does not apply.
Finally, since CSC is the most basic decision making setting, the counterexamples in this section also apply to reinforcement learning via the online-to-batch conversion.


**4. SETUP FOR REINFORCEMENT LEARNING**


In the preceding sections, we saw how the loss function
plays a central role in the sample efficiency of algorithms
for CSC, the simplest decision making problem. The commonly used squared loss results in slow Θ(1/ [√] n) rates in
benign problem instances where the optimal policy has




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 9


TABLE 2

Summary of main RL results in this paper. Since the algorithms presented all use temporal difference learning, all results posit some variant of
Bellman Completeness. In online RL, each bound uses a slightly different eluder dimension based on the loss; in offline RL, all bounds use the
single policy coverage C [π][�] (defined in Eq. (21)). Finally, we remark that the online and offline RL algorithms employ optimism and pessimism over
a version space, so they are computationally inefficient [18]. For computational efficiency, we study the hybrid RL setting (Sec. 8) where we show
that fitted-Q iteration with access to online and offline data achieves a bound that is the sum of online and offline bounds.


Setting Loss Function Class Bellman Completeness Regret / PAC Bound



ℓsq F : X × A �→ [0, 1] T [⋆] (Asm. 3) O�H�Kdsq ln(H|F|/δ)� (Thm. 7)



ℓbce F : X × A �→ [0, 1] T [⋆] (Asm. 3) O�H�V [⋆] - Kdbce ln(H|F|/δ) + H [2] dbce ln(H|F|/δ)� (Thm. 8)


K
ℓmle P : X × A �→ ∆([0, 1]) T [D][,⋆] (Asm. 5) O�H��k=1 [σ][2][(][π][k][)][ ·][ d][mle][ ln(][H][|P|][/δ][) +][ H][2][.][5][d][mle][ ln(][H][|P|][/δ][)] � (Thm. 9)



ℓsq F : X × A �→ [0, 1] T [π], ∀π ∈ Π (Asm. 6) O�H�



C [π][�] ln(H|F|/δ)


n



(Thm. 10)
�



ℓbce F : X × A �→ [0, 1] T [π], ∀π ∈ Π (Asm. 6) O�H�




[H] n [|F|][/δ][)] + H [2][ C][ �][π][ ln(][H] n [|F|][/δ][)]



(Thm. 10)
�



V [π][�] - [C][ �][π][ ln(][H][|F|][/δ][)]



n



ℓmle P : X × A �→ ∆([0, 1]) T [D][,π], ∀π ∈ Π (Asm. 7) O�H�




[H] n [|P|][/δ][)] + H [2][.][5][ C][ �][π][ ln(][H] n [|P|][/δ][)]



(Thm. 11)
�



�
σ [2] (π) · [C][ �][π][ ln(][H][|P|][/δ][)]



n



gather more data by interacting with the environment.
The dataset takes the form D = (D1, D2,..., DH) where
each Dh contains n i.i.d. samples (xh,i,ah,i,ch,i,x [′] h,i [)]
where (xh,i,ah,i) ∼ νh, ch,i ∼ Ch(xh,i,ah,i) and x [′] h,i [∼]
Ph(xh,i,ah,i). We note that νh is simply the marginal
distribution over (xh,ah) induced by the data generating process, e.g., mixture of policies. We also recall the
(single-policy) coverage coefficient: given a comparator
policy �π, define C [π][�] = maxh∈[H] ∥dd [π] h [�][/][d][ν][h][∥][∞] [[][73][,][ 60][].]
The goal is to learn a policy �π with a PAC guarantee
against any comparator policy �π such that C [π][�] < ∞.
Hybrid RL. We also consider the hybrid setting where
the learner is given a dataset as in offline RL, and can
also gather more data by interacting with the environment
as in online RL [58, 8]. By combining the analyses from

                                  both online and offline settings, we prove that fitted Q
iteration (FQI) [53], a computationally efficient algorithm
that does not induce optimism or pessimism, can achieve
first- and second-order regret and PAC bounds.
Bellman equations. Define the Bellman operator T [π]

by Th [π][f] [(][x,a][) =][ E][c][∼][C] h [(][x,a][)][,x][′][∼][P] h [(][x,a][)][[][c][ +][ f] [(][x][′][,π][h][+1][)]]
for any function f and policy π The Bellman equations are fh = Th [π][f][h][+1][ for all][ h][, where][ f][h][ =][ Q][π] h [is the]
unique solution. Also, the Bellman optimality operator
T [⋆] is defined by Th [⋆][f] [(][x,a][) =][ E][c][∼][C] h [(][x,a][)][,x][′][∼][P] h [(][x,a][)][[][c][ +]
mina [′] f (x [′],a [′] )]. The Bellman optimality equations are
fh = Th [⋆][f][h][+1][ for all][ h][, where][ f][h][ =][ Q][⋆] h [is the unique so-]
lution.

Distributional Bellman equations. There are also distributional analogs to the above [11]. Let T [D][,π] denote
the distributional Bellman operator for policy π, defined
by Th [D][,π] p(x,a) = [D] c + p(x [′],a [′] ) where c ∼ Ch(x,a),x [′] ∼
Ph(x,a),a [′] ∼ πh+1(x [′] ) for any conditional distribution



p. Here = [D] denotes equality in distribution. The distributional Bellman equations are ph =D T Dh,πph+1 for all
h, where Zh [π] [is a solution. The distributional Bellman]

optimality operator T [D][,⋆] is defined by Th [D][,⋆] p(x,a) = [D]
c + p(x [′],a [′] ) where c ∼ Ch(x,a),x [′] ∼ Ph(x,a),a [′] =
argmina′ ¯p(x [′],a [′] ). The distributional Bellman optimal
ity equations are ph =D T Dh,⋆ph+1 for all h, where Zh [⋆] [is a]
solution.


**5. ONLINE RL WITH SQUARED-LOSS REGRESSION**


We begin our discussion of RL by solving online RL
with optimistic temporal-difference (TD) learning with
the squared loss for regression [35, 74], which can be
viewed as an abstraction for deep RL algorithms such as
DQN [51]. The algorithm is value-based, meaning that
it aims to learn the optimal Q-function Q [⋆], which then
induces the optimal policy via greedy action selection
πh [⋆][(][x][) = argmin] a [Q][⋆] h [(][x,a][)][. To learn the][ Q][-function, it]
uses a function class F that consists of function tuples
f = (f1,f2,...,fH) ∈F where fh : X × A → [0, 1] and
we use the convention that fH+1 = 0 for all functions f .
In the sequential RL setting, TD learning is a powerful
idea for regressing Q-functions where the function at step
h is regressed on the current cost plus a learned prediction
at the next step h + 1. This process is also known as bootstrapping. One can view this as an approximation to the
Bellman equations Qh = ThQh+1 where T is a Bellman
operator. For online RL, we use the Bellman optimality
operator T [⋆] to learn the optimal Q [⋆], while in offline RL
we use the policy-specific Bellman operator T [π] to learn
Q [π] for all policies π.




10


Algorithm 1 Policy Data Collection

1: Input: policy π, uniform exploration (UA) flag.
2: if UA flag is True then
3: for step h ∈ [H] do
4: Roll-in π for h steps to arrive at xh.
5: Then, randomly act ah ∼ Unif(A) and observe ch,x [′] h [.]
6: end for

7: else

8: Roll-in π for H steps and collect x1,a1,c1, ..., xH,aH,cH .
9: Label x [′] h [=][ x] h+1 [for all][ h][ ∈] [[][H][]][.]
10: end if
11: Output: dataset {(xh,ah,ch,x [′] h [)][}] h∈[H] [.]


Algorithm 2 Optimistic Online RL


1: Input: number of rounds K, function class F, loss function
ℓ(ˆy,y), threshold β, uniform exploration (UA) flag
2: for round k = 1, 2,...,K do
3: Denote Fk = Cβ [ℓ] [(][D][<k][)][ as the version space defined by:]


C [ℓ]
β [(][D][) =][ {][f][ ∈F][ :][ ∀][h][ ∈] [[][H][]][, L] h [ℓ] [(][f][h][,f][h][+1][,] [D][h][)]


(13) − mingh∈Fh Lh [ℓ] [(][g][h][,f][h][+1][,] [D][h][)][ ≤] [β][}][,]


where


L [ℓ]
h [(][f][h][,g,] [D][h][) =][ �] i [|D] =1 [h][|] [ℓ][(][f][h][(][x][h,i][,a][h,i][)][,τ] [⋆][(][g,c][h,i][,x][′] h,i [))]


and τ [⋆] (g,c,x [′] ) = c + mina′ g(x [′],a [′] ) is the regression target. In
the proofs, we use L [sq] if ℓ = ℓsq and L [bce] if ℓ = ℓbce.
4: Get optimistic f [k] ← arg minf ∈Fk mina f1(x1,a).

5: Let π [k] be greedy w.r.t. f [k] : πh [k][(][x][) = arg min][a][ f] h [k][(][x,a][)][,] [∀][h][.]

6: Gather data Dk ← Alg. 1(π [k], UA flag).
7: end for


To formalize TD learning, let (xh,ah,ch,x [′] h [)][ be a tran-]
sition tuple where ch,x [′] h [are sampled conditional on]
xh,ah. For a predictor fh+1 at step h + 1, the regression
targets at step h are:


τ [⋆] (fh+1,c,x [′] ) = c + mina [′] fh+1(x [′],a [′] ),


τ [π] (fh+1,c,x [′] ) = c + fh+1(x [′],πh+1),


where τ [⋆] is the target for learning Q [⋆] which we use for
online RL, and τ [π] is the target for learning Q [π] which
we use for offline RL. The targets are indeed unbiased
estimates of the Bellman backup since Thfh+1(x,a) =
E[τ (fh+1,c,x [′] )]. Then, we regress fh by minimizing
the loss ℓ(fh(x,a),τ (fh+1,c,x [′] )) averaged over the data,
where the loss function ℓ(ˆy,y) captures the discrepancy
between the prediction ˆy and target y. Note this takes the
same form as the regression loss from the CSC warmup.
In online RL, the algorithm we consider (Alg. 2) performs TD learning optimistically by maintaining a version space constructed with the TD loss. Specifically,
given a dataset D = (D1,..., DH) where each Dh =
{xh,i,ah,i,ch,i,x [′] h,i [}][i][∈][[][n][]] [is a set of transition tuples, the]
version space Cβ [ℓ] [(][D][)][ is defined in][ Eq. (13)][ of][ Alg. 2][. In-]
tuitively, the version space contains all functions f ∈F



which nearly minimize the empirical TD risk measured
by loss function ℓ, for all time steps h. This construction
is useful since it satisfies two properties with high probability. First, any function in Cβ [ℓ] [(][D][)][ has small population]
TD risk (a.k.a. Bellman error) w.r.t. ℓ, so we can be assured that choosing any function from the version space
is a good estimate of the desired Q [⋆] . Second, we have that
Q [⋆] is an element of the version space, which provides a
means to achieve optimism (or pessimism) by optimizing
over the version space. Indeed, by selecting the function
in the version space with the minimum initial state value,
we are guaranteed to select a function that lower bounds
the optimal policy’s cost V [⋆] .
We now summarize the online RL algorithm (Alg. 2),
which proceeds iteratively. At each round k = 1, 2,...,K,
the learner selects an optimistic function f [k] from the version space defined by previously collected data: f [k] ←
argminf ∈Fk mina f1(x1,a) where Fk = Cβ [⋆][(][D][<k][)][ and]
D<k denotes the previously collected data. Then, let π [k]

be the greedy policy w.r.t. f [k] : π [k] (x) = argmina f [k] (x,a).
Finally, roll-in with π [k] to collect data, as per Alg. 1.
The roll-in procedure (Alg. 1) has two variants depending on the uniform action (UA) flag. If UA is enabled, we roll-in H times with a slightly modified policy: for each h ∈ [H], we collect a datapoint from π [k] ◦h
unif(A), which denotes the policy that executes π [k] for
h − 1 steps and switches to uniform actions at step h.
If UA is disabled, we roll-in π [k] once and collect trajectory x1,k,a1,k,c1,k,...,xH,k,aH,k,cH,k. While UA requires H roll-ins per round, this more exploratory data
collection is useful for proving bounds with non-linear
MDPs. The collected data is then used to define the confidence set at the next round.
As a historical remark, this algorithm was first proposed
with the squared loss ℓsq under the name GOLF by [35]
and then extended with the mle loss ℓmle under the name
O-DISCO by [64]. In this section, we focus on the squared
loss case, recovering the results of [35]. In the subsequent
sections, we propose a new variant with the bce loss ℓbce,
and then finally disuss application of the mle loss, recovering the results of [65].
We now state the Bellman Completness (BC) assumption needed to ensure that Alg. 2 succeeds [15, 35, 73, 14].


ASSUMPTION 3 (T [⋆] -BC). Th [⋆][f][h][+1][ ∈F][h][ for all][ h][ ∈]

[H] and fh+1 ∈Fh+1.


BC ensures that the TD-style regression which bootstraps on the next prediction is realizable, playing the
same role as realizability (Asm. 1) in the CSC setting.
In fact, BC implies realizability in Q [⋆] : Q [⋆] h [∈F][h][ for all]
h, which can be verified by using the Bellman optimality
equations and induction from h = H → 1. While appealing, Q [⋆] -realizability is not sufficient for sample efficient




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 11



RL [62, 28, 71] and TD learning can diverge or converge
to bad points with realizability alone [59, 53, 39]. We note
that Q [⋆] -realizability becomes sufficient when combined
with low coverability and generative access to the MDP

[48], where the learner can reset to any previously observed states. We believe that the techniques in this paper
can lead to first- and second-order bounds with realizability plus generative access for example. However, we do
not pursue this direction here since exchanging BC for
other conditions is orthogonal to our study of loss functions.
We also define the eluder dimension, [2] a flexible structural measure that quantifies the complexity of exploration and representation learning [35].


DEFINITION 1 (Eluder Dimension). Fix any set S,
function class Ψ = {ψ : S → R}, distribution class
M = {µ : ∆(S)}, threshold ε0, and number q ∈ N. The
ℓq-eluder dimension EluDimq(Ψ, M,ε0) is defined as
the length of the longest sequence µ [(1)],...,µ [(][L][)] ⊂M
s.t. ∃ε ≥ ε0, ∀t ∈ [L], ∃ψ ∈ Ψ s.t. ��Eµ(t) [ψ]�� - ε but
�i<t��Eµ(i)[ψ]��q ≤ εq.


Taking S = X × A, we will instantiate the Ψ class to
be a set of TD errors measured by the regression loss
function. For example with squared loss, we set Ψ [sq] h [=]
{Eh [sq][(][·][;] [f] [) :][ f][ ∈F}][ where]


Eh [sq][(][x,a][;] [f] [) := (][f][h][(][x,a][)][ −T][ ⋆] h [f][h][+1][(][x,a][))][2][.]


The distribution class M will be the set of all visitation
distributions by any policy, i.e., Mh = {x,a �→ d [π] h [(][x,a][) :]
π ∈ Π} where d [π] h [(][x,a][)][ is the state-action visitation dis-]
tribution of π at time step h. If UA is enabled, then we
will have S = X and the “V-type” distribution class is
M [V] h [=][ {][x][ �→] [d][π] h [(][x][) :][ π][ ∈] [Π][}][ where][ d][π] h [(][x][)][ is the state vis-]
itation distribution of π at time step h. Moreover we let
Ψ [sq] h [,][V] = {Ea∼unif(A)[ψ(x,a)] : ψ ∈ Ψ [sq] h [}][ denote the “V-]
type” function class, where “V-type” refers to the fact that
the functions only depend on state and not action. Thus,
define the eluder dimension for squared loss:


dsq = maxh∈[H] EluDim2(Ψ [sq] h [,] [M][h][,] [1][/K][)][,]

d [V] sq [= max] h∈[H] [EluDim][2][(Ψ][sq] h [,][V], M [V] h [,] [1][/K][)][.]


We now state the guarantees for Alg. 2 with the squared
loss ℓsq, which recovers the main results of Jin, Liu and
Miryoosefi [35].


2Def. 1 is often called the distributional eluder dimension to distin
guish it from the classic eluder dimension of [57]. To not confuse with
distributional RL, we simply refer to it as the eluder dimension.



THEOREM 7. Under Asm. 3, for any δ ∈ (0, 1), w.p.a.l.
1−δ, Alg. 2 with the squared loss ℓsq and β = 2ln(H|F|/δ)
enjoys the following:
�Kk=1 [V][ π][k][ −] [V][ ⋆] [≤] [O][�][(][H][√][K][ ·][ dβ][)][,]

where d = dsq if UA is false and d = Ad [V] sq [if UA is true,]
where A is the number of actions.


This shows that Alg. 2 with the squared loss is guaranteed to learn a policy that converges to the optimal policy
at a Θ(K [−][1][/][2] ) rate, which is the minimax-optimal rate.
In Sec. 5.1 we show that the V-type dimension can be
bounded by the rank of the transition kernel in a low-rank
MDP [3, 4], a canonical model for RL with non-linear
function approximation.
Computational complexity of version space algorithms. The algorithms we present for online and offline
RL optimize over version spaces to establish optimism
and pessimism, and this optimization over version space
is computationally inefficient in general [18]. The computational hardness comes from the non-convex optimization, not necessarily from any loss function itself. However, we note that the version space optimization is oracleefficient in the one-step H = 1 setting, a.k.a. contextual
bandits [25, 22, 65]. In the RL setting, there are also approaches to mitigate the computational hardness of optimism. One common approach in practice is to use myopic
exploration strategies such as epsilon-greedy [51, 10],
which also has bounded regret when exploration is easy

[19, 76]. Another approach which we later present is to
work in the hybrid RL setting, where the learner can both
interact with the MDP in an online manner and has access
to offline data with good coverage [58]. In Sec. 8, we will
see that our results for all loss functions naturally carry
over to the hybrid setting, giving us both computational
and statistical efficiency.
We now prove the main theorem for squared loss
Thm. 7. We prove the nested lemmas in the Appendix.


PROOF OF THM. 7. We define the excess squared-loss
risk for f ∈F under the visitation distribution of π as


Eh [sq][(][π][;] [f] [) :=][ E][π][[][E] h [sq][(][x][h][,a][h][;] [f] [)]][,]

and also set Esq [RL] [=][ �][H] h=1 [E] h [sq][. We first establish an opti-]
mism lemma for f [k] and prove it in the appendix.


LEMMA 7. Let ℓ = ℓsq and Dh be a dataset where
the i-th datapoint is collected from π [i], and denote D =
(D1,..., DH). Then under BC (Asm. 3), for any δ ∈
(0, 1), let β = 2ln(H|F|/δ) and define

(14) fˆ [op] ∈ fargmin∈Cβ [sq][(][D][)] mina [f][1][(][x][1][,a][)][.]


W.p.a.l. 1 − δ, we have (a) [�][n] i=1 [E] sq [RL][(][π][i][; ˆ][f][ op][)][ ≤] [2][Hβ][,]
and (b) mina f [ˆ] 1 [op][(][x][1][,a][)][ ≤] [V][ ⋆][.]




12


By Lem. 7, we have [�][K] k=1 [V][ π][k][ −] [V][ ⋆] [≤] [�][K] k=1 [V][ π][k][ −]
mina f1 [k][(][x][1][,a][)][, which can be further decomposed by the]
performance difference lemma (PDL) [2, 37].


LEMMA 8 (PDL). ∀f = (f1,f2,...,fH) and π, we
have V [π] −f1(x1,π) = [�][H] h=1 [E][π][[(][T][ π] h [f][h][+1][ −] [f][h][)(][x][h][,a][h][)]][.]


By PDL and Cauchy-Schwarz, we have


(15)

�Kk=1 [V][ π][k][ −] [f][ k] 1 [(][x][1][,π][k][(][x][1][))]


H
= [�][K] k=1 �h=1 [E][π][k][[][T][h][f][ k] h+1 [(][x][h][,a][h][)][ −] [f][ k] h [(][x][h][,a][h][)]]



functions) are satisfied in low-rank MDPs [3], a class of
rich-observation MDPs where the transition kernel has an

unknown low-rank decomposition.



DEFINITION 2 (Low-Rank MDP). An MDP has rank
d if its transition kernel has a low-rank decomposition:
Ph(x [′] | x,a) = φ [⋆] h [(][x,a][)][⊤][µ][⋆] h [(][x][′][)][ where][ φ][⋆] h [,µ][⋆] h [∈] [R][d][ are]
unknown feature maps that satisfy ∥φ [⋆] h [(][x,a][)][∥][2][ ≤] [1][ and]
∥ [�] gdµ [⋆] h [(][x][′][)][∥][2][ ≤∥][g][∥][∞] √d for all x,a,x [′] and g : X → R.



∥ [�] gdµ [⋆] h [(][x][′][)][∥][2][ ≤∥][g][∥][∞] √d for all x,a,x [′] and g : X → R.

We also require that the expected cost is linear in the features: C [¯] h(x,a) = φ [⋆] h [(][x,a][)][⊤][v] h [⋆] [for some unknown vectors]
vh [⋆] [∈] [R][d][ that satisfy][ ∥][v] h [⋆][∥][2][ ≤] √d.



d.



H
≤ [�][K] k=1 �h=1
�



Eh [sq][(][f][ k][,π][k][)][ ≤] [�] k [K] =1
�



HEsq [RL][(][f][ k][,π][k][)]



≤ �HK [�][K] k=1 [E] sq [RL][(][f][ k][,π][k][)][.]


The final step is to bound [�][K] k=1 [E] sq [RL][(][f][ k][,π][k][)][. By][ Lem. 7][,]
we have that [�] i<k [E] sq [RL][(][f][ k][,π][i][)][ ≲] [Hβ][ for all][ k][, which is]
very similar except that the expectations are taken under
previous policies π [<k] instead of π [k] . It turns out that the
eluder dimension can establish a link between the two, via
the following “pigeonhole principle” lemma:


LEMMA 9 (Pigeonhole). Fix a number N ∈ N, a sequence of functions ψ [(1)],...,ψ [(][N] [)] ∈ Ψ, and distributions µ [(1)],...,µ [(][N] [)] ∈M. Suppose [�] i<j [|][E][µ][(][i][)] [[][ψ][(][j][)][]][|][q][ ≤]
β [q] for all j ∈ [N ]. Then we have [�][N] j=1 [|][E][µ][(][j][)] [[][ψ][(][j][)][]][| ≤]
2EluDimq(Ψ, M,N [−][1] ) · (E + β [q] ln(EN )), where E :=
supµ∈M,ψ∈Ψ|Eµ[ψ]| is the envelope.


If we interpret ψ [(][i][)] as the regression error at round i,
then Lem. 9 essentially states that ratio of (online) out-ofdistribution errors (i.e., ψ [(][i][)] measured under p [(][i][)] ) to the
(offline) in-distribution errors (i.e., ψ [(][i][)] measured under
p [(1)],...,p [(][i][−][1)] ) is bounded by the eluder dimension. This
generalizes similar results from [57, 35, 44, 64, 74].
Finally, going back to the regret decomposition in
Eq. (15), applying the pigeonhole lemma implies that
�Kk=1 [E] sq [RL][(][f][ k][,π][k][)][ ≤] [O][�][(][d][sq][Hβ][)][. Thus, we have shown]
that [�][K] k=1 [V][ π][k][ −][f][ k] 1 [(][x][1][,π][k][(][x][1][))][ ≤] [O][�][(][H][�] Kdsqβ), which

proves the desired regret bound.
Moreover, if the uniform action (UA) flag is set, we also
perform a change of measure so that: [�][K] k=1 [E] sq [RL][(][f][ k][,π][k][)][ ≤]
A [�][K] k=1 �Hh=1 [E][π][k][◦] h [unif(][A][)][((][f][ h][ −T][h][f][ k] h+1 [)(][x][h][,a][h][))][2][ ≲]
Ad [V]
sq [Hβ][. Plugging into][ Eq. (15)][ proves the desired PAC]



This model captures non-linear representation learning
since φ [⋆] and µ [⋆] are unknown and can be non-linear. The
low-rank MDP model also generalizes many other models such as linear MDPs (where φ [⋆] is known) [36], block
MDPs [50] and latent variable models [52].
To perform representation learning, we posit a feature
class Φ = Φ1 × ··· × ΦH where each φh : X × A → R [d] ∈
Φh is a candidate for the ground truth features φ [⋆] .


ASSUMPTION 4 (φ [⋆] -realizability). φ [⋆] h [∈] [Φ][h][ for all][ h][.]


Then, the following class of linear functions in Φ satisfies all the assumptions needed in Thm. 7 and Thm. 8, a
subsequent result with the bce loss.



have that ∥w∥2 ≤ d + d∥fh+1∥∞ ≤ 2 d, which im
plies Th [π][f][h][+1][(][x,a][)][ ∈F] h [lin][.]



Fh [lin] [:=][ {][clip(][⟨][φ][h][(][·][)][,w][⟩][,] [0][,] [1) :][ w][ ∈] [R][d][ s.t.][ ∥][w][∥][2] [≤] [2] √



d},



where clip(y,l,h) := max(min(y,h),l). This function
class is sensible because Bellman backups of any function
are linear in φ [⋆] h [; thus][ Q][-functions are Bellman backups]
via the Bellman equations, they are linear in φ [⋆] . The clipping is to ensure that the functions are bounded in [0, 1]
which is true for the desired Q [⋆] .
We now show that F [lin] satisfies BC (Asm. 3).


LEMMA 10. In a low-rank MDP, under Asm. 4, F [lin]

satisfies Bellman Completeness (Asms. 3 and 6).


PROOF OF LEM. 10. Fix any fh+1 ∈Fh [lin] +1 [and][ π][. We]
want to show Th [π][f][h][+1][ ∈F] h [lin][. First, we note][ T][ π] h [f][h][+1][(][x,a][)]
is equal to


(16) φ [⋆] h [(][x,a][)][⊤][(][v] h [⋆] [+][ �] x [′][ f][h][+1][(][x][′][,π][(][x][′][))d][µ] h [⋆] [(][x][′][))][.]



Setting w = vh [⋆] [+][ �]



Setting w = vh [⋆] [+][ �] x [′][ f][h][+1][(][x][′][,π][(][x][′][))d][µ] h [⋆][(][x][′][)][, we indeed]

have that ∥w∥2 ≤ √d + √d∥fh+1∥∞ ≤ 2√d, which im


d + √



d∥fh+1∥∞ ≤ 2√



bound of O [�] (H
�



AKd [V] sqβ). This finishes the proof.



**5.1 Verifying Assumptions for Low-Rank MDPs**


In this subsection, we show that the assumptions in
Thm. 7 (as well as subsequent theorems with other loss



Moreover, we can also show that the V-type eluder dimension is bounded by the rank d of the low-rank MDP,
as defined in Def. 2. We note this applies to both ℓ1 and
ℓ2 eluder dimensions.




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 13



LEMMA 11. In a low-rank MDP with rank d, we
have EluDim1(Ψ [V] h [,] [M][V] h [,ε][)][ ≤] [EluDim][2][(Ψ][V] h [,] [M][V] h [,ε][)][ ≤]
O(d ln(d/ε)) for all steps h ∈ [H] and function classes
Ψ [V] h [⊂X →] [R][.]


PROOF. EluDim2(Ψ [V] h [,] [M][V] h [,ε][)][ ≤O][(][d] [ln(][d/ε][))][ can be]
proved by applying a standard elliptical potential argument to the decomposition in Eq. (16); for example, see

[35] or Theorem G.4 of [64]. Then, EluDim1(Ψ [V] h [,] [M][V] h [,ε][)][ ≤]
EluDim2(Ψ [V] h [,] [M][V] h [,ε][)][ is a simple consequenceof the fact]
that ��i [x][2] i [≤] [�] i [|][x][i][|][ [][64][, Lemma 5.4].]


Since the above lemma holds for all values of Ψ [V] h [, this]
implies that d [V] sq [(and][ d][V] bce [,d][V] mle [to be defined in future]
theorems) are all bounded by O [�] (d) in low-rank MDPs.
Finally, one can also show that the bracketing entropy of
Fh [lin] is O [�] (d + log |Φ|). We note that our PAC bounds can
all be extended to allow for infinite classes such as F [lin]

via a standard bracketing argument, e.g., see [35, 64] for
detailed extensions. Thus, we have established that our
bounds hold in low-rank MDPs when the algorithm uses
the linear function class F [lin] .


**6. FIRST/SECOND-ORDER BOUNDS FOR ONLINE RL**


As we learned from the CSC warmup, algorithms with
the squared loss can be sub-optimal in small-cost or
small-variance problems. The intuition is that squared
loss regression bounds do not capture the underlying heteroskedastic variance; indeed, minimizing squared loss
implicitly assumes that the underlying distribution is a homoskedastic Gaussian. We also learned that simply swapping the loss function for the bce loss or mle loss can yield
first-order or second-order bounds that are more sample
efficient in small-loss or small-variance settings. We now
show that this observation smoothly extends to RL as
well. In particular, we will sharpen the Cauchy-Schwarz
step in the proof of Thm. 7 by leveraging Eq. (2) from the
CSC warmup.


**6.1 First-Order Bounds with BCE Regression**


In this subsection, we analyze Alg. 2 with the bce
loss ℓbce and derive improved first-order bounds. Before stating guarantees with the bce loss, we first define the eluder dimension which measures discrepancy
with the Bernoulli squared hellinger distance. Let Ψ [bce] h =
{δh [Ber] (·; f ) : f ∈F} where

δh [Ber] (x,a; f ) := h [2] Ber [(][f][h][(][x,a][)][,] [T][ ⋆] h [f][h][+1][(][x,a][))][2][,]

and Ψ [bce] h [,][V] = {Ea∼unif(A)[ψ(x,a)] : ψ ∈ Ψ [bce] h [}][. Then de-]
fine the eluder dimension for bce loss:

dbce = maxh∈[H] EluDim1(Ψ [bce] h [,] [M][h][,] [1][/K][)][,]

d [V] bce [= max][h][∈][[][H][]] [EluDim][1][(Ψ][bce] h [,][V], M [V] h [,] [1][/K][)][.]


The following guarantees for Alg. 2 with bce loss is new.



THEOREM 8. Under Asm. 3, for any δ ∈ (0, 1), w.p.a.l.
1 − δ, Alg. 2 with the bce loss ℓbce and β = 2ln(H|F|/δ)
enjoys the following:
�Kk=1 [(][V][ π][k][ −] [V][ ⋆][)][ ≤] [O][�][(][H][√][V][ ⋆][K][ ·][ dβ][ +][ H] [2][dβ][)][,]

where d = dbce if UA is false and d = Ad [V] bce [if UA is true.]


Compared to the non-adaptive bounds of squared loss
(Thm. 7), the above bce loss bounds are first-order and
shrinks with the optimal policy’s cost V [⋆] . This adaptive scaling with V [⋆] gives the bound a small-cost property: if V [⋆] ≤O(1/K) (i.e., if the optimal policy achieves
low cost), then the leading term vanishes and the bound
enjoys logarithmic-in-� K regret, i.e., [�][K] k=1 [V][ π][k][ −] [V][ ⋆] [≤]
O(H [2] dβ). In other words, by dividing both sides by K,
the sub-optimality gap of the best learned policy shrinks
at a fast O [�] (1/K) rate. Moreover, since V [⋆] ≤ 1, Thm. 8
is never worse than the O [�] (√K) rate from Thm. 7, and so

these two bounds match in the worst-case but bce loss is

strictly better in the small-cost regime.
We remark that Thm. 8 uses the same completeness
assumption as Thm. 7, although its bound contains a
slightly different eluder dimension dbce instead of dsq.
The eluder dimension for bce is different because the bce

loss naturally measures Bellman error via the Bernoulli
squared hellinger while the squared loss uses squared distance. The change to ℓ1 eluder is due to the generalization bound analysis for log-losses, although it is actually
sharper than ℓ2 eluder [44, 64]. However, this is not a
significant change for our MDP of interest: Lem. 11 ensures that both eluder dimensions are bounded by O [�] (d) in
low-rank MDPs with rank d. Indeed, this implies that the
first-order bound in Thm. 8 can be specialized for lowrank MDPs by the same argument as before, yielding the
first small-loss bound for low-rank MDPs in online RL
without distributional RL [64]. Characterizing the exact
differences between these eluder dimension variants is an

interesting question for future work.
In terms of related works, several other works also propose to use different losses than squared loss to evaluate the Bellman error. For example, Bas-Serrano et al. [9]
argued that the logistic loss is advantageous since it is

                                  convex, whereas the squared loss is not convex in the Q
function due to the max operator. Farebrother et al. [21]
is a more applied paper which shows that classification
losses such as cross-entropy scale much better with deep
networks than squared loss. Overall these other works
provide other reasons for why squared loss is sub-optimal,
while we focus on the improved sample efficiency aspect
of employing alternative losses. We now prove Thm. 8.


PROOF OF THM. 8. For the bce loss, we measure the
Bellman error of f ∈F under π using the squared
Hellinger distance of Bernoullis (as defined in Eq. (2)):

δh [Ber] (π; f ) := Eπ[δh [Ber] (xh,ah; f )],




14


We define the bce excess risk Eh [bce] (π; f ) as:


− ln Eπ[exp( [1] 2 [ℓ][bce][(][T][h][f][h][+1][(][x][h][,a][h][)][,τ][ ⋆][(][f][h][+1][,c][h][,x][h][+1][))]


− [1]

2 [ℓ][bce][(][f][h][(][x][h][,a][h][)][,τ][ ⋆][(][f][h][+1][,c][h][,x][h][+1][)))]][.]


Note that Th [⋆][f][h][+1][(][x][h][,a][h][) =][ E][[][τ][ ⋆][(][f][h][+1][,c][h][,x][h][+1][)][ |][ x][h][,a][h][]][,]
which is realizable by BC. Recall that δh [Ber] ≤Eh [bce] by
Lem. 2. We also write δ [RL] and E [RL]
Ber [=][ �][H] h=1 [δ] h [Ber] bce [=]
�Hh=1 [E] h [bce] . The following lemma establishes optimism
and is analogous to Lem. 7.


LEMMA 12. Let ℓ = ℓbce. Under the same setup as
Lem. 7 with f [ˆ][op] selected from Cβ [bce] instead of Cβ [sq][, w.p.a.l.]

1 − δ, we have (a) [�][n] i=1 [E] bce [RL][( ˆ][f][ op][,π][i][)][ ≤] [2][Hβ][, and (b)]
mina f [ˆ] 1 [op][(][x][1][,a][)][ ≤] [V][ ⋆][.]


By Lem. 12, we have [�][K] k=1 [(][V][ π][k][ −][V][ ⋆][)][ ≤] [�][K] k=1 [(][V][ π][k][ −]
mina f1 [k][(][x][1][,a][))][. Then, the proof follows similarly as the]
squared loss case from before, except that we apply the
finer Eq. (2) in place of Cauchy-Schwarz:


(17)

�Kk=1 [(][V][ π][k][ −] [f][ k] 1 [(][x][1][,π][k][(][x][1][)))]


H
= [�][K] k=1 �h=1 [E][π][k][[][T][ ⋆] h [f][ k] h+1 [(][x][h][,a][h][)][ −] [f][ k] h [(][x][h][,a][h][)]]


H
≤ [�][K] k=1 �h=1 �Eπk[fh [k][(][x][h][,a][h][)]][ ·][ δ] h [Ber] (f [k],π [k] )


+ δh [Ber] (f [k],π [k] ).



Algorithm 3 Optimistic Online Distributional RL


1: Input: number of rounds K, conditional distribution class P,
threshold β, uniform exploration (UA) flag
2: for round k = 1, 2,...,K do
3: Define confidence set Pk = Cβ [mle] (D<k) where we define:

Cβ [mle] (D) = {p ∈P : ∀h ∈ [H], L [mle] h (ph,ph+1, Dh)


(18) − mingh∈Ph Lh [mle] (gh,ph+1, Dh) ≤ β},


where

L [mle] h (ph,g, Dh) = [�][|D] i=1 [h][|] [ℓ][mle][(][p][h][(][x][h,i][,a][h,i][)][,τ] [D][,⋆][(][g,c][h,i][,x][′] h,i [))]


and τ [D][,⋆] (g,c,x [′] ) = c + Z, Z ∼ g(x [′],πg¯(x [′] )) be the mle target.
Note that if c,x [′] are sampled conditional on x,a, then the target a
sample of the random variable Th [D][,⋆] g(x,a).

4: Get optimistic p [k] ← arg minp∈Pk mina ¯p1(x1,a).

5: Let π [k] be greedy w.r.t. ¯p [k] : πh [k][(][x][) = arg min][a][ ¯][p][k] h [(][x,a][)][,] [∀][h][.]

6: Gather data Dk ← Alg. 1(π [k], UA flag).
7: end for


By Lem. 12 and the pigeonhole principle, the error terms
Ebce [RL] [can be bounded similarly as in the squared loss proof:]
we can bound [�][K]
k=1 [δ] Ber [RL] [(][f][ k][,π][k][)][ by][ �][O][(][d][bce][Hβ][)][ if UA is]
false, and by O [�] (Ad [V] bce [Hβ][)][ if UA is true.]
Thus, setting d = dbce if UA is false, and d = Ad [V] bce [if]
UA is true, we have proven

�Kk=1 [(][V][ π][k][ −] [V][ ⋆][)][ ≤] [H] ��Kk=1 [V][ π][k][ ·][ dβ][ +][ H] [2][dβ.]


Finally, we observe an implicit inequality where we can
replace [�][K] k=1 [V][ π][k][ by][ KV][ ⋆] [by collecting a factor of][ 3][, as]
shown by the following lemma.


K
LEMMA 14. If [�][K] k=1 [(][V][ π][k][ −] [V][ ⋆][)][ ≤] [c] ��k=1 [V][ π][k][ +]



≤ [�][K] k=1



H
��h=1 [E][π][k][[][f][ k] h [(][x][h][,a][h][)]][ ·][ δ] Ber [RL] [(][f][ k][,π][k][)]


+ δBer [RL] [(][f][ k][,π][k][)][.]



Now, we bound [�][H] h=1 [E][π][k] [[][f][ k] h [(][x][h][,a][h][)]][ by][ HV][ π][k][ plus]
some lower-order error terms, which we achieve with a
‘self-bounding’ lemma:


LEMMA 13. Define δh [Ber] that uses T [π] instead of T [⋆] :


δh [Ber] (f,π,xh,ah) := h [2] Ber [(][f][h][(][x][h][,a][h][)][,] [T][ π] h [f][h][+1][(][x][h][,a][h][))][.]


Then, for any f, π, xh,ah,


fh(xh,ah) ≤ eQ [π] h [(][x][h][,a][h][) + 77][Hδ] Ber [RL] [(][f,π][)][.]


This implies the corollary:


Eπ[fh(xh,ah)] ≲ V [π] + HδBer [RL] [(][f,π][)][.]


By Lem. 13, we can bound Eq. (17) by


≲ [�][K] k=1 HV [π][k] δBer [RL] [(][f][ k][,π][k][) +][ Hδ] Ber [RL] [(][f][ k][,π][k][)]
�

≤ �H [�][K] k=1 [V][ π][k][ ·][ �][K] k=1 [δ] Ber [RL] [(][f][ k][,π][k][)]


+ H [�][K] k=1 [δ] Ber [RL] [(][f][ k][,π][k][)][.]



c [2], then [�][K] k=1 [V][ π][k][ −] [V][ ⋆] [≤] [c] √



2KV [⋆] + 3c [2] .



This concludes the proof of Thm. 8.


**6.2 Second-Order Bounds with MLE**


A natural question is how can we achieve second-order
bounds in RL? In this section, we consider a distributional
variant of the online RL algorithm that uses the mle loss
to learn the cost-to-go distributions Z [⋆] . RL algorithms
that learn the cost-to-go distributions are often referred to
as distributional RL (DistRL) [11] and have resulted in a
plethora of empirical success [10, 17, 31, 12, 32, 21]. Distributional losses, such as the mle loss and quantile regression loss, were initially motivated by improve representation learning and multi-task learning, but a theoretically
rigorous explanation was an open question. Recently,

[64, 65] provided an answer to this mystery by proving
that DistRL automatically yields first- and second-order
bounds in RL, thus establishing the benefits of DistRL.




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 15



In this section, we review the results of [65], a refinement of [64] that introduced the mle-loss variant of the
optimistic online RL algorithm. To learn the optimal policy’s cost-to-go distributions Z [⋆], we posit a conditional
distribution class P that consists of conditional distribution tuples p = (p1,p2,...,pH) ∈P where ph : X × A →
∆([0, 1]). We use the convention that pH+1 is deterministic point-mass at 0 for all conditional distributions p.
Then, Alg. 3 takes exactly the same structure as Alg. 2
except that it performs a distributional variant of TD to
D
solve the distributional Bellman equation Zh [⋆] = T Dh [Z] h [⋆] +1 [.]
It uses mle to learn the cost-to-go distributions and acts
greedily with respect to the learned distribution’s mean.
To ensure that distributional TD learning succeeds, we
assume distributional BC (DistBC) [72, 64].


ASSUMPTION 5 (T [D][,⋆] -DistBC). Th [D][,⋆] ph+1 ∈Ph for
all h ∈ [H] and ph+1 ∈Ph+1.


Distributional BC posits that the distribution class is
closed under the distributional Bellman operator and is a
stronger condition than the standard completeness condition (Asm. 3). Nevertheless, in low-rank MDPs with discrete cost distributions, Asm. 5 can be satisfied with a linear distribution class [65, Section 5.1], which also has
bounded bracketing entropy of O [�] (dM + log |Φ|) where
M is the number of discretizations.
Next, we define the eluder dimension for mle loss. Let
Ψ [mle] h = {δh [dis][(][·][;] [p][) :][ p][ ∈P}][ where]


δh [dis][(][x,a][;] [p][) :=][ h][2][(][p][h][(][x,a][)][,] [T][ D][,⋆][p][h][(][x,a][))]


and Ψ [mle] h [,][V] = {Ea∼unif(A)[ψ(x,a)] : ψ ∈ Ψ [mle] h }. Define:


dmle = maxh∈[H] EluDim1(Ψ [mle] h, Mh, 1/K),

d [V] mle [= max][h][∈][[][H][]][ EluDim][1][(Ψ] h [mle][,][V], M [V] h [,] [1][/K][)][.]


The following is the main online RL result from [65].


THEOREM 9. Under Asm. 5, for any δ ∈ (0, 1), w.p.a.l.
1 − δ, Alg. 3 with the mle loss ℓmle and β = 2ln(H|P|/δ)
enjoys the following:
�Kk=1 [(][V][ π][k][ −][V][ ⋆][)][ ≤] [O][�][(][H] ��Kk=1 [σ][2][(][π][k][)][ ·][ dβ] [+][H] [2][.][5][dβ][)][,]


where d = dmle if UA is false and d = Ad [V] mle [if UA is true.]


The above mle loss bounds scales with the variances

of the policies selected by the algorithm, and are thus are
called second-order (a.k.a. variance dependent) bounds.
As we saw in the CSC setting, a second-order bound is
actually strictly sharper than the first-order bound and this
is also true in RL [65, Theorem 2.1]. The variance bound
can be much tighter in near-deterministic settings where
the optimal policy’s cost is far from zero.



One drawback of the DistRL approach is that it requires
modeling the entire conditional distributions which are
more complex than the conditional mean, i.e., P is generally larger and more complex than F from Thms. 7 and 8.
Also, DistRL requires completeness w.r.t. the distributional Bellman operator, which is stronger than standard
BC. However, it is worth noting that in practice DistRL is
often much more performant than non-distributional approaches, which suggests that these modeling conditions
(e.g., distributional completeness) are not so restrictive

[10, 17, 21]. Closing this gap between theory and practice is an interesting future direction.
We also highlight two recent works on second-order
bounds for contextual bandits, the one-step special case of
RL where DistBC simplifies to distributional realizability
(Asm. 2). In this setting, Pacchiano [55] proved a secondorder bound for contextual bandits with only mean realizability, which is weaker than distributional realizability,
by using thresholded least squares. Concurrently, Jia et al.

[33] obtained a similar bound as [55] for the strong adversary setting and a complementary bound for the weak
adversary setting. Jia et al. [33] also furnished a lower
bound proving that the eluder-based second-order bounds
in Thm. 9 and [65] are tight and unimprovable even if the
number of actions is less than the eluder dimension.

Computational considerations for bce and mle. We
remark that switching from squared loss to bce does not
incur any computational overhead and is a single line of
code in practice. The mle loss is used for distribution fitting and thus the function approximator should output a
distribution instead of a scalar. In practice, the distribution
can be modeled with histograms [10, 31, 21] or quantiles

[17] and is only a constant factor more to compute and
maintain than the non-distributional losses. Thus, the mle
loss and distributional RL more generally also do not incur any notable computational overhead in practice.


PROOF OF THM. 9. We measure the distributional Bell
man error of p ∈P under π with the squared Hellinger
distance:


δh [dis][(][π][;] [p][) :=][ E][π][[][δ] h [dis][(][x][h][,a][h][;] [p][)]][,]


We define the mle excess risk Eh [mle] (p,π) as:


− ln Eπ[exp( 2 [1] [ℓ][bce][(][T][ D] h [,⋆] ph+1(xh,ah),τ [D][,⋆] (p,ch,xh+1))


− [1]

2 [ℓ][bce][(][p][h][(][x][h][,a][h][)][,τ][ D][,⋆][(][p,c][h][,x][h][+1][)))]][,]


Recall we have that δh [dis] ≤Eh [mle] by Lem. 4. We also write
δdis [RL] [=][ �] h [H] =1 [δ] h [dis] and Emle [RL] [=][ �] h [H] =1 [E] h [mle] . We now establish the optimism lemma, which is analogous to Lem. 7
and Lem. 12.




16


LEMMA 15. Let ℓ = ℓmle and Dh be the same as in
Lems. 7 and 12. Then, under Asm. 5, for any δ ∈ (0, 1) let
β = 2ln(H|P|/δ) and define


pˆ [op] ∈ argmin min p¯1(x1,a)
p∈Cβ [mle] (D) a


W.p.a.l. 1 − δ, we have (a) [�][n] i=1 [E] mle [RL] [(ˆ][p][op][,π][i][)][ ≤] [2][Hβ]
and (b) mina ˆp [op] 1 [(][x][1][,a][)][ ≤] [V][ ⋆][.]


By Lem. 15, we have [�][K] k=1 [V][ π][k][ −] [V][ ⋆] [≤] [�][K] k=1 [V][ π][k][ −]
mina ¯p [k] 1 [(][x][1][,a][)][. Now we apply the second-order lemma:]


(19)

�Kk=1 [(][V][ π][k][ −] [p][¯] 1 [k][(][x][1][,π][k][(][x][1][)))]


H
= [�][K] k=1 �h=1 [E][π][k][[][T][ ⋆] h [p][¯][k] h+1 [(][x][h][+1][)][ −] [p][¯] h [k][(][x][h][,a][h][)]]


H
= [�][K] k=1 �h=1 [E][π][k][[][T][ D] h [,⋆] p [k] h+1 [(][x][h][+1][)][ −] [p][¯] h [k][(][x][h][,a][h][)]]



≤ [�][K] k=1



H
��h=1 [E][π][k][[][σ][2][(][p][k] h [(][x][h][,a][h][))]][ ·][ δ] dis [RL][(][p][k][,π][k][)]


+ δdis [RL][(][p][k][,π][k][)][.]



Now, we bound the variance term by [�][H] h=1 [E][π][k][[][σ][2][(][c][h][ +]
V [π][k]
h+1 [(][x][h][+1][))]][ plus some lower-order error terms. We]
achieve this with the following lemma, which can be
viewed as a variance analog of Lem. 13.


LEMMA 16. Define the state-action analog of δh [dis][:]


δh [dis][(][p,π,x][h][,a][h][) :=][ h][2][(][p][h][(][x][h][,a][h][)][,] [T][ D] h [,π] ph+1(xh,ah)),


where Th [D][,π] p(x,a) :=D C(x,a) + p(X [′],π(X [′] )) is the distributional Bellman backup of p under π. Then, for any p,
π, xh, ah, we have


σ [2] (ph(xh,ah)) ≤ 2eσ [2] (Zh [π][(][x][h][,a][h][)) +][ Hδ] dis [RL][(][p,π][)][,]


where σ [2] (Zh [π][(][x][h][,a][h][))][ denotes the variance of the ran-]
dom variable Z [π]
h [(][x][h][,a][h][)][. This also implies the corollary:]


Eπ[σ [2] (ph(xh,ah))] ≲ σ [2] (Z [π] ) + H [2] δdis [RL][(][p,π][)][.]


By Lem. 16, we can bound Eq. (19) by



≲ [�][K] k=1
�



Hσ [2] (Z [π][k] ) · δdis [RL][(][p][k][,π][k][) +][ H] [1][.][5][δ] dis [RL][(][p][k][,π][k][)]



Algorithm 4 Pessimistic Offline RL

1: Input: function class F, offline dataset D, loss function ℓ(ˆy,y),
threshold β.
2: for each policy π ∈ Π do
3: Denote Fπ = Cβ [ℓ] [(][D][;] [π][)][ as the version space defined by:]


C [ℓ]
β [(][D][;] [π][) =][ {][f][ ∈F][ :][ ∀][h][ ∈] [[][H][]][, L] h [ℓ] [(][f][h][,f][h][+1][,] [D][h][,π][)]


(20) − mingh∈Fh Lh [ℓ] [(][g][h][,f][h][+1][,] [D][h][,π][)][ ≤] [β][}][,]


where


L [ℓ]
h [(][f][h][,g,] [D][h][,π][) =][ �][|D] i=1 [h][|] [ℓ][(][f][h][(][x][h,i][,a][h,i][)][,τ] [π][(][g,c][h,i][,x][′] h,i [))]


and τ [π] (g,c,x [′] ) = c + g(x [′],π) is the regression target. In the
proofs, we use L [sq] if ℓ = ℓsq and L [bce] if ℓ = ℓbce.
4: Get pessimistic f [π] ← arg maxf ∈Fπ mina f1(x1,a).
5: end for
6: Return: �π = arg minπ∈Π mina f1 [π] [(][x] 1 [,a][)][.]


**7. OFFLINE RL VIA PESSIMISTIC REGRESSION**


In offline RL, we are given a dataset D of size N and
the goal is to learn a good policy in a purely offline manner, without any interactions with the environment. Since
we cannot explore in offline RL, a natural strategy is to
be cautious about any states and actions not covered by
the given dataset – that is, we should be conservative or
pessimistic about unseen parts of the environment where
we may make catastrophic errors [43, 56, 73]. Indeed, it
is intuitively clear that we can only hope to learn a good
policy on the support of the given data. This will be soon
formalized with the single-policy coverage coefficient.
We summarize the offline RL algorithm in Alg. 4. We
achieve pessimism by maximizing over the version space
defined in Eq. (20), which is an inversion of online RL
which minimizes over a similar version space. The only
other difference is the regression target:


τ [π] (fh+1,c,x [′] ) = c + fh+1(x [′],πh+1),


is an unbiased estimate of Th [π][f][h][+1][ in contrast to the on-]
line case where τ [⋆] was used to estimate Th [⋆][f][h][+1][. Thus,]
we instead use the policy-wise BC for offline RL:


ASSUMPTION 6 (T [π] -BC). Th [π][f][h][+1][ ∈F][h][ for all][ h][ ∈]

[H], fh+1 ∈Fh+1 and π ∈ Π.


Since we may take π = πf, this is technically stronger
than Asm. 3. Nevertheless, Asm. 6 is also satisfied in lowrank MDPs by the linear function class F [lin] and so changing from Asm. 3 to Asm. 6 does not change any conclusions we make. As a historical remark, Alg. 4 was first
proposed with the squared loss ℓsq under the name BCP
by [73] and then extended with the mle loss ℓmle under
the name P-DISCO by [64].
We introduce the single-policy coverage coefficient: for
any given comparator policy in the policy class �π ∈ Π, its



≤ �H [�][K] k=1 [σ][2][(][Z] [π][k] [)][ ·][ �][K] k=1 [δ] dis [RL][(][p][k][,π][k][)]

+ H [1][.][5][ �][K] k=1 [δ] dis [RL][(][p][k][,π][k][)][.]


By Lem. 15 and the pigeonhole principle, the error terms
Emle [RL] [can be bounded similarly as before: we can bound]
O��(KkAd=1 [V] mle [δ] dis [RL][Hβ][(][p][k][)][,π][ if UA is true. This finishes the proof of][k][)][ by][ �][O][(][d][mle][Hβ][)][ if UA is false, and by]
Thm. 9.




coverage coefficient is defined by:



CENTRAL ROLE OF LOSS FUNCTIONS IN RL 17


Algorithm 5 Pessimistic Offline Distributional RL



d [π] h [�] [(][x,a][)]
(21) C [π][�] := maxh∈[H] maxx,a νh(x,a) [.]


For simplicity, we set the policy class to all greedy policies induced by our function class ΠF = {πf : f ∈F}. [3]

In the following theorem, the squared loss case recovers
the results of [73] and the bce loss result is new.


THEOREM 10. Under Asm. 6, for any δ ∈ (0, 1),
w.p.a.l. 1 − δ, Alg. 4 with β = 2ln(H|F|/δ) has the following guarantees each loss function:


1. If ℓ = ℓsq, then for any comparator policy �π ∈ ΠF,



V [π][�] − V [π][�] ≤ O [�] (H
�



Cn [π][�] β [)][.]



2. If ℓ = ℓbce, then for any comparator policy �π ∈ ΠF,



1: Input: conditional distribution class P, offline dataset D, threshold β.
2: for each policy π ∈ Π do
3: Denote Pπ [mle] = Cβ [mle] (D; π) as the version space defined by:


Cβ [mle] (D; π) = {p ∈P : ∀h ∈ [H], L [mle] h (ph,ph+1, Dh,π)


(22) − mingh∈Fh Lh [mle] (gh,ph+1, Dh,π) ≤ β},


where L [mle] h (fh,g, Dh,π) is


|Dh|
�i=1 [ℓ][mle][(][f][h][(][x][h,i][,a][h,i][)][,τ] [D][,π][(][g,c][h,i][,x][′] h,i [))]


and τ [D][,π] (g,c,x [′] ) = c + Z,Z ∼ g(x [′],π(x [′] )) is the mle target.
Note that if c,x [′] are sampled conditional on x,a, then the target
is a sample of the random variable Th [D][,π] g(x,a).
4: Get pessimistic p [π] ← arg maxp∈Pπ mina ¯p1(x1,a).
5: end for
6: Return: �π = arg minπ∈Π mina ¯p [π] 1 [(][x][1][,a][)][.]


By Lem. 17, we have V [π][�] − V [π][�] ≤ mina f1 [π][�][(][x][1][,a][)][ −]
V [π][�] . Then, by definition of �π, we further bound this by
mina f1 [π][�][(][x][1][,a][)][ −] [V][ �][π][. Now, we decompose with PDL:]


mina f1 [π][�][(][x][1][,a][)][ −] [V][ �][π]


= [�][H]
h=1 [E][π][�][[][f][ �] h [π][(][x][h][,a][h][)][ −T][ �] h [π][f][ �] h [π] +1 [(][x][h][,a][h][)]]


H
≤ ��h=1 [E][π][�][[][f][ �] h [π][(][x][h][,a][h][)]][ ·][ δ] Ber [RL] [(][f][ �][π][,][ �][π][) +][ δ] Ber [RL] [(][f][ �][π][,][ �][π][)]


≲ HV [π][�] - δBer [RL] [(][f][ �][π][,][ �][π][) +][ Hδ] Ber [RL] [(][f][ �][π][,][ �][π][)]
�


By importance sampling and Lem. 17, the error terms can
be bounded by O [�] (C [π][�] - [Hβ] n [)][. This completes the proof of]

Thm. 10.


**7.1 Second-Order Bounds via Distributional RL**


We now show that DistRL with the mle loss can yield
second-order guarantees, recovering the results of [65].
We make a few minor changes to the pessimistic offline
RL algorithm. We consider the set of greedy policies w.r.t.
the means of the conditional distribution as our policy
class ΠP = {πp¯ : p ∈P}. Following Xie et al. [73], Wang
et al. [65], in offline RL, we posit the policy-wise distributional BC condition.


ASSUMPTION 7 (T [D][,π] -DistBC). Th [D][,π] ph+1 ∈Ph for
all h ∈ [H],ph+1 ∈Ph+1 and π ∈ Π.


THEOREM 11. Under Asm. 7, for any δ ∈ (0, 1),
w.p.a.l. 1 − δ, Alg. 5 with β = 2ln(H|P|/δ) has the following guarantee: for any comparator policy �π ∈ ΠP,



V [π][�] − V [π][�] ≤ O [�] (H
�




[ �][π][β]

+ H [2][ C][ �][π][β]
n n



V [π][�] - [C][ �][π][β]



n [)][.]



We see that the squared loss algorithm always converges at a slow O [�] (1/ [√] n) rate. Simply changing the
squared loss to the bce loss yields a first-order bound that
converges at a fast O [�] (1/n) rate in the small-cost regime
where V [π][�] ≲ 1/n, and is never worse than the squared loss
bound since V [π][�] ≤ 1. Again, the only change needed to
achieve the improved bound is to change the loss function
from squared loss to bce loss, which mirrors our observations from before. One difference with the first-order online RL bound is that small-cost term here is V [π][�] instead

of V [⋆] . Of course, we can set �π = π [⋆] to recover the same
small-cost term. However, this offline RL bound is more
general since it can be applied to any comparator policy

�
π with bounded coverage coefficient.


PROOF OF THM. 10. We only prove the bce case as
the squared loss case follows essentially the same structure. The key difference compared to online RL is that we
establish pessimism instead of optimism.


LEMMA 17 (Pessimism). Let ℓ = ℓbce. Under Asm. 3,
for any δ ∈ (0, 1), setting β = Θ(ln(H|F|/δ)). Then,
w.p.a.l. 1 − δ, for all π ∈ Π, (a) Ebce [RL][( ˆ][f][ π][,ν][)][ ≤] [2][Hβ] n [, and]

(b) mina f [ˆ] 1 [π][(][x][1][,a][)][ ≥] [V][ π][.]


PROOF OF LEM. 17. The proof is essentially identical
to that of Lem. 12 where we show that w.p.a.l. 1 − δ, (1)
all elements of the version space have low excess risk and
(2)ˆ Q [π] lies in the version space. The only difference is that
f [π] is defined as the argmax rather than argmin, so that we
have pessimism (greater than V [π] ) instead of optimism.


3The offline RL results can be extended for general, infinite policy
classes with log covering numbers [16] or entropy integrals [38].



V [π][�] − V [π][�] ≤ O [�] (H
�




[ �][π][β]

n + H [2][.][5][ C] n [ �][π][β]



�
σ [2] (π) · [C][ �][π][β]



n [)][.]




18


Algorithm 6 Fitted Q-Iteration for Hybrid RL

1: Input: number of rounds K, function class F, offline dataset
D [off], loss function ℓ(ˆy,y), uniform exploration (UA) flag
2: for episode k = 1, 2,...,K do
3: for each h = H,H − 1,..., 1 do
4: Recall the loss from Alg. 2 (Eq. (13)):

L [ℓ] h [(][f] h [,g,] [D] h [) =][ �] i [|D] =1 [h][|] [ℓ][(][f][h][(][x][h,i][,a][h,i][)][,τ] [⋆][(][g,c][h,i][,x][′] h,i [))]

5: Set fh [k] [= arg min] fh∈Fh [L] h [ℓ] [(][f] h [,f] h [k] +1 [,] [D] h [off] ∪D<k [on] [)][.]
6: end for
7: Let π [k] be greedy w.r.t. f [k] : πh [k][(][x][) = arg min][a][ f] h [k][(][x,a][)][.]

8: Gather data Dk [on] ← Alg. 1(π [k], UA flag).
9: end for


Since σ [2] (π�) ≤ V [π][�], this implies a first-order bound as
well. This variance bound can be much tighter in neardeterministic settings where the comparator’s variance is
near zero, but its cost is far from zero. However, as was the
case in online RL, DistRL still has the drawbacks of requiring a distributional class and DistBC. While these are
more stringent conditions in theory, DistRL has achieved
state-of-the-art in many offline RL tasks as well [47],
suggesting that the benefits of DistRL can outweight the
stronger modeling assumptions in practice. The proof of
Thm. 11, which we omit due to space, follows from the
same argument as the proof of Thm. 10, coupled with the
variance arguments from Thm. 9. The interested reader
may find the full proof in [65].


**8. COMPUTATIONAL EFFICIENCY VIA HYBRID RL**


While we have exhibited the central role of loss func
tions, achieving tight variance-adaptive bounds, in both
online and offline RL, one issue which we have not yet
addressed is computational efficiency. As mentioned earlier, optimizing over the version space is computationally
difficult (NP-hard) even in tabular MDPs [18].
In this section, we discuss a solution via the hybrid
RL setting where the learner can access an offline dataset
D [off] with good coverage and also interact with the environment. We show that Fitted-Q Iteration (FQI) [53], a
computationally efficient algorithm, can also enjoy firstand second-order guarantees by simply regressing with
the bce and mle losses. The FQI algorithm in the hybrid
setting was first proposed with the squared loss ℓsq by [58]
and our extensions to the bce and mle losses are novel.
Intuitively, the offline dataset mitigates the need for optimism, while the online interactions mitigate the need for
pessimism – together, they obviate the need for maintaining a version space. For the following guarantees, we use
C [π][�] to denote the coverage coefficient of the comparator
policy �π under the data generating distribution of D [off] .
We also assume the offline dataset to be as large as the
number of interactions, i.e., |D [off] | ≥ Ω(K) [58].



THEOREM 12. Under Asm. 3 and |D [off] | ≥ Ω(K), for
any δ ∈ (0, 1), w.p.a.l. 1 − δ, Alg. 6 has the following
guarantees for each loss function:


1. If ℓ = ℓsq, for any comparator policy �π ∈ ΠF,

K
�k=1 [(][V][ π][k][ −] [V][ �][π][)][ ≤] [O][�][(][H] �K · (d + C [π][�] )β),


where d = dsq if UA is false, and d = Ad [V] sq [if UA is]
true.
2. If ℓ = ℓbce, for any comparator policy �π ∈ ΠF,

K
�k=1 [(][V][ π][k][ −] [V][ �][π][)][ ≤] [O][�] �H�V [π][�] K · (d + C [π][�] )β


+ H [2] (d + C [π][�] )β�


where d = dbce if UA is false, and d = Ad [V] bce [if UA]
is true.


Importantly, we see that simply changing the loss from
ℓsq to ℓbce again leads to improved first-order bounds,
which again supports our earlier observations. Compared
with our prior results, the main advantage of Thm. 12 is
computational: it bounds the sub-optimality of a computationally efficient algorithm FQI, which much more
closely resembles deep RL algorithms such as DQN [51].
From a statistical perspective, the hybrid RL bound is actually worse than either pure online or offline bounds,
since it takes the form of:


online RL bound + offline RL bound.


Indeed, the hybrid RL bounds contain both the structural
condition such as eluder dimension and the coverage coefficient V [π][�] . This form will be made clear in the proof,
which simply combines the prior online and offline RL
results. We finally discuss some related works. [7] analyzed FQI with ℓbce in the pure offline setting and proved
a first-order bound that depends on the much larger global
coverage coefficient C [Π] = maxπ�∈Π C [π][�], which is needed
to analyze FQI in the pure offline setting [15]. Also, [49]
is able to achieve computationally efficient learning lowrank MDPs without requiring offline data with good partial coverage; however, their bounds are neither first nor
second-order. It would be interesting future work to adapt
the techniques in this paper to derive variance dependent
bounds for computationally efficient algorithms without
requiring good offline data.


PROOF OF THM. 12. For any comparator policy �π, we
decompose:

�Kk=1 [(][V][ π][k][ −] [V][ �][π][) =] [�][K] k=1 [E][[][V][ π][k][ −] [min][a][ f][ k] 1 [(][x][1][,a][)]]


+ E[mina f1 [k][(][x][1][,a][)][ −] [V][ �][π][]]


We see that the first term is exactly the same term in the
online RL proof after we apply optimism (e.g., Eq. (15));
thus the first term is bounded by the online RL results,




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 19



Algorithm 7 Distributional FQI for Hybrid RL


1: Input: number of rounds K, conditional distribution class P, offline dataset D [off], loss function ℓ(ˆy,y), uniform exploration (UA)
flag
2: for episode k = 1, 2,...,K do
3: for each h = H,H − 1,..., 1 do
4: Recall the loss from Alg. 3 (Eq. (18)):

L [mle] h (ph,g, Dh) = [�] i [|D] =1 [h][|] [ℓ][mle][(][p][h][(][x][h,i][,a][h,i][)][,τ] [D][,⋆][(][g,c][h,i][,x][′] h,i [))]

5: Set p [k] h [= arg min][p] h [∈P] h [L] h [mle] (ph,p [k] h+1 [,] [D] h [off] ∪D<k [on] [)][.]
6: end for
7: Let π [k] be greedy w.r.t. p [k] : πh [k][(][x][) = arg min][a][ ¯][p][k] h [(][x,a][)][.]

8: Gather data Dk [on] ← Alg. 1(π [k], UA flag).
9: end for


e.g., Thms. 7 and 8. We also see that the second term is
exactly the same term in the offline RL proof after apply
pessimism. Thus, we can bound the second term by the
offline RL results, e.g., Thm. 10. Since we posit the offline dataset has as many samples as the online dataset,
the offline bound matches the online one in terms of K.
This completes the proof and shows why the bound in hybrid RL is the sum of online and offline RL bounds.


Finally, to apply the mle loss to achieve second-order
bounds, we naturally extend FQI with DistRL which
closely resembles deep DistRL algorithms such as C51

[10]. This gives the following new second-order guarantees for hybrid RL.


THEOREM 13. Under Asm. 5 and |D [off] | ≥ Ω(K), for
any δ ∈ (0, 1), w.p.a.l. 1 − δ, Alg. 7 has the following
guarantee: for any comparator policy �π ∈ ΠP,

K
�k=1 [(][V][ π][k][ −] [V][ �][π][)][ ≤] [O][�] �H [2][.][5] (d + C [π][�] )β

+ H�(σ [2] (π�)K + [�][K] k=1 [σ][2][(][π][k][))][ ·][ (][d][ +][ C] [π][�][)][β] �,


where d = dmle if UA is false, and d = Ad [V] mle [if UA is true.]


The hybrid second-order bound, being the sum of
the second-order bounds for online and offline DistRL
(Thms. 9 and 11), contains both the variance of the played
policies as well as the variance of the comparator policy.
Nevertheless, the hybrid second-order bound still implies
a hybrid first-order bound by the same AM-GM argument
as in CSC. Thus, this again shows that DistRL yields a
notable benefit compared to other losses.


**9. DISCUSSION AND CONCLUSION**


From the one-step CSC to online, offline and hybrid
RL, we see time and time again that the loss function
plays a central role in the adaptivity and efficiency of decision making algorithms. The classical squared loss always
converges at a slow O [�] (1/ [√] n) rate and cannot adapt to



easier problem instances with heteroskedasticity. The bce
loss can serve as a drop-in improvement that yields firstorder bounds with a much faster O [�] (1/n) rate when the
optimal cost is small. Switching from conditional-mean
learning to conditional-distribution learning, the mle loss
can tighten the bounds further with a second-order guarantee, that is bounds that converge at a O [�] (1/n) rate
in near-deterministic settings even if the optimal cost is
large. Crucially, these gaps in performance are not merely
theoretical as they have been observed many times by the
deep RL community [21, 11, 31, 7, 47]. The theory outlined herein is very general and can be applied to a wide
range of settings including imitation learning [23], modelbased [26, 66], risk-sensitive RL [63, 67], and robust bandits [38], and RL [13]. Moreover, the principles herein
can improve algorithms for post-training large language
models [29, 1, 68, 77], learning query optimizers [42, 69]
and many more real-world applications. We hope to have
not only clearly demonstrated that the loss function choice
is important in RL, but also to inspire the reader to seek
out opportunities for better loss functions to improve their
decision-making algorithms.


**10. ACKNOWLEDGMENTS**


This material is based upon work supported by a
Google PhD Fellowship and grants NSF IIS-1846210,
NSF IIS-2154711, NSF CAREER 2339395 and DARPA
Learning Network Cyberagents (LANCER). We also
thank the editor and anonymous reviewers for useful discussions and feedback.


**REFERENCES**


[1] ADLER, B., AGARWAL, N., AITHAL, A., ANH, D. H., BHAT
TACHARYA, P., BRUNDYN, A., CASPER, J., CATANZARO, B.,
CLAY, S., COHEN, J. et al. (2024). Nemotron-4 340B Technical
Report. arXiv preprint arXiv:2406.11704.

[2] AGARWAL, A., JIANG, N., KAKADE, S. M. and SUN, W.
(2019). Reinforcement learning: Theory and algorithms. CS
Dept., UW Seattle, Seattle, WA, USA, Tech. Rep 32 96.

[3] AGARWAL, A., KAKADE, S., KRISHNAMURTHY, A. and
SUN, W. (2020). Flambe: Structural complexity and representation learning of low rank mdps. Advances in neural information
processing systems 33 20095–20107.

[4] AGARWAL, A., SONG, Y., SUN, W., WANG, K., WANG, M. and
ZHANG, X. (2023). Provable benefits of representational transfer
in reinforcement learning. In The Thirty Sixth Annual Conference
on Learning Theory 2114–2187. PMLR.

[5] AUDIBERT, J.-Y. and TSYBAKOV, A. B. (2007). Fast learning
rates for plug-in classifiers.

[6] AUER, P., CESA-BIANCHI, N., FREUND, Y. and
SCHAPIRE, R. E. (2002). The nonstochastic multiarmed
bandit problem. SIAM journal on computing 32 48–77.

[7] AYOUB, A., WANG, K., LIU, V., ROBERTSON, S., MCINER
NEY, J., LIANG, D., KALLUS, N. and SZEPESVARI, C. (2024).
Switching the Loss Reduces the Cost in Batch Reinforcement
Learning. In Forty-first International Conference on Machine
Learning.




20


[8] BALL, P. J., SMITH, L., KOSTRIKOV, I. and LEVINE, S. (2023).
Efficient online reinforcement learning with offline data. In International Conference on Machine Learning 1577–1594. PMLR.

[9] BAS-SERRANO, J., CURI, S., KRAUSE, A. and NEU, G. (2021).
Logistic Q-learning. In International conference on artificial intelligence and statistics 3610–3618. PMLR.

[10] BELLEMARE, M. G., DABNEY, W. and MUNOS, R. (2017). A
distributional perspective on reinforcement learning. In International conference on machine learning 449–458. PMLR.

[11] BELLEMARE, M. G., DABNEY, W. and ROWLAND, M.
(2023). Distributional Reinforcement Learning. MIT Press
[http://www.distributional-rl.org.](http://www.distributional-rl.org)

[12] BELLEMARE, M. G., CANDIDO, S., CASTRO, P. S., GONG, J.,
MACHADO, M. C., MOITRA, S., PONDA, S. S. and WANG, Z.
(2020). Autonomous navigation of stratospheric balloons using
reinforcement learning. Nature 588 77–82.

[13] BENNETT, A., KALLUS, N., OPRESCU, M., SUN, W. and
WANG, K. (2024). Efficient and Sharp Off-Policy Evaluation in
Robust Markov Decision Processes. Advances in Neural Information Processing Systems.

[14] CHANG, J., WANG, K., KALLUS, N. and SUN, W. (2022).
Learning bellman complete representations for offline policy
evaluation. In International Conference on Machine Learning
2938–2971. PMLR.

[15] CHEN, J. and JIANG, N. (2019). Information-theoretic considerations in batch reinforcement learning. In International Conference on Machine Learning 1042–1051. PMLR.

[16] CHENG, C.-A., XIE, T., JIANG, N. and AGARWAL, A. (2022).
Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning 3852–
3878. PMLR.

[17] DABNEY, W., OSTROVSKI, G., SILVER, D. and MUNOS, R.
(2018). Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning
1096–1105. PMLR.

[18] DANN, C., JIANG, N., KRISHNAMURTHY, A., AGARWAL, A.,
LANGFORD, J. and SCHAPIRE, R. E. (2018). On oracle-efficient
pac rl with rich observations. Advances in neural information
processing systems 31.

[19] DANN, C., MANSOUR, Y., MOHRI, M., SEKHARI, A. and
SRIDHARAN, K. (2022). Guarantees for epsilon-greedy reinforcement learning with function approximation. In International conference on machine learning 4666–4689. PMLR.

[20] DEVROYE, L. and LUGOSI, G. (2001). Combinatorial methods
in density estimation. Springer Science & Business Media.

[21] FAREBROTHER, J., ORBAY, J., VUONG, Q., TAIGA, A. A.,
CHEBOTAR, Y., XIAO, T., IRPAN, A., LEVINE, S., CAS
TRO, P. S., FAUST, A., KUMAR, A. and AGARWAL, R. (2024).
Stop Regressing: Training Value Functions via Classification for
Scalable Deep RL. In Forty-first International Conference on
Machine Learning.

[22] FENG, F., YIN, W., AGARWAL, A. and YANG, L. (2021). Provably correct optimization and exploration with non-linear policies. In International Conference on Machine Learning 3263–
3273. PMLR.

[23] FOSTER, D. J., BLOCK, A. and MISRA, D. (2024). Is Behavior Cloning All You Need? Understanding Horizon in Imitation
Learning. arXiv preprint arXiv:2407.15007.

[24] FOSTER, D. J. and KRISHNAMURTHY, A. (2021). Efficient firstorder contextual bandits: Prediction, allocation, and triangular
discrimination. Advances in Neural Information Processing Systems 34 18907–18919.

[25] FOSTER, D., AGARWAL, A., DUDÍK, M., LUO, H. and
SCHAPIRE, R. (2018). Practical contextual bandits with regres


sion oracles. In International Conference on Machine Learning
1539–1548. PMLR.

[26] FOSTER, D. J., KAKADE, S. M., QIAN, J. and RAKHLIN, A.
(2021). The statistical complexity of interactive decision making.
arXiv preprint arXiv:2112.13487.

[27] FOSTER, D. J., RAKHLIN, A., SEKHARI, A. and SRIDHA
RAN, K. (2022a). On the complexity of adversarial decision
making. Advances in Neural Information Processing Systems 35
35404–35417.

[28] FOSTER, D. J., KRISHNAMURTHY, A., SIMCHI-LEVI, D. and
XU, Y. (2022b). Offline Reinforcement Learning: Fundamental
Barriers for Value Function Approximation. In Conference on
Learning Theory 3489–3489. PMLR.

[29] GAO, Z., CHANG, J. D., ZHAN, W., OERTELL, O.,
SWAMY, G., BRANTLEY, K., JOACHIMS, T., BAGNELL, J. A.,
LEE, J. D. and SUN, W. (2024). Rebel: Reinforcement
learning via regressing relative rewards. arXiv preprint
arXiv:2404.16767.

[30] HU, Y., KALLUS, N. and MAO, X. (2022). Fast rates for contextual linear optimization. Management Science 68 4236–4245.

[31] IMANI, E. and WHITE, M. (2018). Improving regression performance with distributional losses. In International conference on
machine learning 2157–2166. PMLR.

[32] JASON MA, Y., JAYARAMAN, D. and BASTANI, O. (2022).
Conservative Offline Distributional Reinforcement Learning.
Advances in neural information processing systems.

[33] JIA, Z., QIAN, J., RAKHLIN, A. and WEI, C.-Y. (2024). How
Does Variance Shape the Regret in Contextual Bandits? Advances in Neural Information Processing Systems.

[34] JIANG, N. and AGARWAL, A. (2018). Open problem: The dependence of sample complexity lower bounds on planning horizon. In Conference On Learning Theory 3395–3398. PMLR.

[35] JIN, C., LIU, Q. and MIRYOOSEFI, S. (2021). Bellman eluder
dimension: New rich classes of rl problems, and sample-efficient
algorithms. Advances in neural information processing systems
34 13406–13418.

[36] JIN, C., YANG, Z., WANG, Z. and JORDAN, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In Conference on learning theory 2137–2143. PMLR.

[37] KAKADE, S. and LANGFORD, J. (2002). Approximately optimal approximate reinforcement learning. In Proceedings of the
Nineteenth International Conference on Machine Learning 267–
274.

[38] KALLUS, N., MAO, X., WANG, K. and ZHOU, Z. (2022). Doubly robust distributionally robust off-policy evaluation and learning. In International Conference on Machine Learning 10598–
10632. PMLR.

[39] KOLTER, J. (2011). The fixed points of off-policy TD. Advances
in neural information processing systems 24.

[40] KONTOROVICH, A. (2024). Binomial small deviations. Tweet.

[41] KRISHNAMURTHY, A., AGARWAL, A., HUANG, T.-K.,
DAUMÉ III, H. and LANGFORD, J. (2019). Active learning for
cost-sensitive classification. Journal of Machine Learning Research 20 1–50.

[42] KRISHNAN, S., YANG, Z., GOLDBERG, K., HELLERSTEIN, J.
and STOICA, I. (2018). Learning to optimize join queries with
deep reinforcement learning. arXiv preprint arXiv:1808.03196.

[43] KUMAR, A., ZHOU, A., TUCKER, G. and LEVINE, S. (2020).
Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems 33 1179–
1191.

[44] LIU, Q., CHUNG, A., SZEPESVÁRI, C. and JIN, C. (2022).
When is partially observable reinforcement learning not scary?
In Conference on Learning Theory 5175–5220. PMLR.




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 21




[45] LUEDTKE, A. and CHAMBAZ, A. (2017). Faster rates for policy
learning. arXiv preprint arXiv:1704.06431.

[46] LYKOURIS, T., SRIDHARAN, K. and TARDOS, É. (2018).
Small-loss bounds for online learning with partial information.
In Conference on Learning Theory 979–986. PMLR.

[47] MA, Y., JAYARAMAN, D. and BASTANI, O. (2021). Conservative offline distributional reinforcement learning. Advances in
neural information processing systems 34 19235–19247.

[48] MHAMMEDI, Z., FOSTER, D. J. and RAKHLIN, A. (2024).
The Power of Resets in Online Reinforcement Learning. arXiv
preprint arXiv:2404.15417.

[49] MHAMMEDI, Z., BLOCK, A., FOSTER, D. J. and RAKHLIN, A.
(2024). Efficient model-free exploration in low-rank mdps. Advances in Neural Information Processing Systems 36.

[50] MISRA, D., HENAFF, M., KRISHNAMURTHY, A. and LANGFORD, J. (2020). Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In International
conference on machine learning 6961–6971. PMLR.

[51] MNIH, V., KAVUKCUOGLU, K., SILVER, D., RUSU, A. A., VENESS, J., BELLEMARE, M. G., GRAVES, A., RIEDMILLER, M.,
FIDJELAND, A. K., OSTROVSKI, G. et al. (2015). Human-level
control through deep reinforcement learning. nature 518 529–
533.

[52] MODI, A., CHEN, J., KRISHNAMURTHY, A., JIANG, N. and
AGARWAL, A. (2024). Model-free representation learning and
exploration in low-rank mdps. Journal of Machine Learning Research 25 1–76.

[53] MUNOS, R. and SZEPESVÁRI, C. (2008). Finite-Time Bounds
for Fitted Value Iteration. Journal of Machine Learning Research
9.

[54] OUYANG, L., WU, J., JIANG, X., ALMEIDA, D., WAIN
WRIGHT, C., MISHKIN, P., ZHANG, C., AGARWAL, S.,
SLAMA, K., RAY, A. et al. (2022). Training language models
to follow instructions with human feedback. Advances in neural

information processing systems 35 27730–27744.

[55] PACCHIANO, A. (2024). Second Order Bounds for Contextual Bandits with Function Approximation. arXiv preprint
arXiv:2409.16197.

[56] RASHIDINEJAD, P., ZHU, B., MA, C., JIAO, J. and RUS
SELL, S. (2021). Bridging offline reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems 34 11702–11716.

[57] RUSSO, D. and VAN ROY, B. (2013). Eluder dimension and the
sample complexity of optimistic exploration. Advances in Neural
Information Processing Systems 26.

[58] SONG, Y., ZHOU, Y., SEKHARI, A., BAGNELL, D., KRISH
NAMURTHY, A. and SUN, W. (2023). Hybrid RL: Using both
offline and online data can make RL efficient. In The Eleventh
International Conference on Learning Representations.

[59] TSITSIKLIS, J. and VAN ROY, B. (1996). Analysis of temporaldiffference learning with function approximation. Advances in
neural information processing systems 9.

[60] UEHARA, M. and SUN, W. (2022). Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage. In International Conference on Learning Representations.

[61] WAINWRIGHT, M. J. (2019). High-dimensional statistics: A
non-asymptotic viewpoint 48. Cambridge university press.

[62] WANG, R., FOSTER, D. and KAKADE, S. M. (2021). What are
the Statistical Limits of Offline RL with Linear Function Approximation? In International Conference on Learning Representations.

[63] WANG, K., KALLUS, N. and SUN, W. (2023). Near-minimaxoptimal risk-sensitive reinforcement learning with cvar. In International Conference on Machine Learning 35864–35907.
PMLR.




[64] WANG, K., ZHOU, K., WU, R., KALLUS, N. and SUN, W.
(2023). The benefits of being distributional: Small-loss bounds
for reinforcement learning. Advances in Neural Information Processing Systems 36.

[65] WANG, K., OERTELL, O., AGARWAL, A., KALLUS, N. and
SUN, W. (2024a). More Benefits of Being Distributional:
Second-Order Bounds for Reinforcement Learning. International Conference on Machine Learning.

[66] WANG, Z., ZHOU, D., LUI, J. and SUN, W. (2024b). Modelbased RL as a Minimalist Approach to Horizon-Free and
Second-Order Bounds. arXiv preprint arXiv:2408.08994.

[67] WANG, K., LIANG, D., KALLUS, N. and SUN, W. (2024c). A
Reductions Approach to Risk-Sensitive Reinforcement Learning with Optimized Certainty Equivalents. arXiv preprint
arXiv:2403.06323.

[68] WANG, K., KIDAMBI, R., SULLIVAN, R., AGARWAL, A.,
DANN, C., MICHI, A., GELMI, M., LI, Y., GUPTA, R.,
DUBEY, A. et al. (2024d). Conditional Language Policy: A General Framework for Steerable Multi-Objective Finetuning. Findings of Empirical Methods in Natural Language Processing.

[69] WANG, J., WANG, K., LI, Y., KALLUS, N., TRUMMER, I. and
SUN, W. (2024e). JoinGym: An Efficient Join Order Selection
Environment. Reinforcement Learning Journal 1.

[70] WATKINS, C. J. and DAYAN, P. (1992). Q-learning. Machine
learning 8 279–292.

[71] WEISZ, G., AMORTILA, P. and SZEPESVÁRI, C. (2021). Exponential lower bounds for planning in mdps with linearlyrealizable optimal action-value functions. In Algorithmic Learning Theory 1237–1264. PMLR.

[72] WU, R., UEHARA, M. and SUN, W. (2023). Distributional
offline policy evaluation with predictive error guarantees. In
International Conference on Machine Learning 37685–37712.

PMLR.

[73] XIE, T., CHENG, C.-A., JIANG, N., MINEIRO, P. and AGAR
WAL, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing
systems 34 6683–6694.

[74] XIE, T., FOSTER, D. J., BAI, Y., JIANG, N. and
KAKADE, S. M. (2023). The Role of Coverage in Online
Reinforcement Learning. In The Eleventh International Conference on Learning Representations.

[75] ZHANG, T. (2023). Mathematical Analysis of Machine Learning Algorithms. Cambridge University Press.
[https://doi.org/10.1017/9781009093057](https://doi.org/10.1017/9781009093057)

[76] ZHANG, S., LI, H., WANG, M., LIU, M., CHEN, P.-Y., LU, S.,
LIU, S., MURUGESAN, K. and CHAUDHURY, S. (2023). On
the Convergence and Sample Complexity Analysis of Deep QNetworks with ǫ-Greedy Exploration. In Thirty-seventh Conference on Neural Information Processing Systems.

[77] ZHOU, J. P., WANG, K., CHANG, J., GAO, Z., KALLUS, N.,
WEINBERGER, K. Q., BRANTLEY, K. and SUN, W. (2025).
Q♯: Provably Optimal Distributional RL for LLM Post-Training.
arXiv preprint arXiv:2502.20548.




22


**11. APPENDIX**


**11.1 Proof of Lemmas in Sec. 5**


LEMMA 7. Let ℓ = ℓsq and Dh be a dataset where
the i-th datapoint is collected from π [i], and denote D =
(D1,..., DH). Then under BC (Asm. 3), for any δ ∈
(0, 1), let β = 2ln(H|F|/δ) and define

(14) fˆ [op] ∈ fargmin∈Cβ [sq][(][D][)] mina [f][1][(][x][1][,a][)][.]


W.p.a.l. 1 − δ, we have (a) [�][n] i=1 [E] sq [RL][(][π][i][; ˆ][f][ op][)][ ≤] [2][Hβ][,]
and (b) mina f [ˆ] 1 [op][(][x][1][,a][)][ ≤] [V][ ⋆][.]


PROOF OF LEM. 7. By standard martingale concentration via Freedman’s inequality, w.p.a.l. 1 − δ, for all
f,h, we have

�ni=1 [E] h [sq][(][f,π][i][)][ ≤] [ln(][H][|F|][/δ][) +][ L][sq] h [(][f][h][,f][h][+1][,] [D][h][)]


(23) − L [sq] h [(][T][h][f][h][+1][,f][h][+1][,] [D][h][)][.]


Let gh [f] [∈] [argmin][g] h [∈G] h [L] h [⋆] [(][g][h][,f][h][+1][,] [D][h][)][ denote the em-]
pirical risk minimizer, as used in the definition of Cβ [⋆][(][D][)]
(Eq. (13)). Under the BC premise,

�ni=1 [E] h [sq][(][f,π][i][)][ ≤] [ln(][H][|F|][/δ][) +][ L][sq] h [(][f][h][,f][h][+1][,] [D][h][)]

− L [sq] h [(][g] h [f] [,f][h][+1][,] [D][h][)][.]


Thus, any f ∈Cβ [sq][(][D][)][ satisfies][ �] i [n] =1 [E] h [sq][(][f,π][i][)][ ≤] [2][β][,]
which proves Claim (a). For Claim (b), we prove that
Q [⋆] ∈Cβ [sq][(][D][)][: by][ Eq. (23)][ and non-negativity of][ E] [sq][,]

we have L [sq] h [(][T][ f][h][+1][,f][h][+1][,] [D][h][)][ −] [L][sq] h [(][g] h [f] [,f][h][+1][,] [D][h][)][ ≤]
ln(H|F|/δ) = β. Then, setting f = Q [⋆] and applying
Q [⋆] h [=][ T][ ⋆] h [Q][⋆] h+1 [shows that][ Q][⋆] [satisfies the version space]
condition. Thus, Q [⋆] ∈Cβ [sq][(][D][)][ and Claim (b) follows by]

definition of f [ˆ][op] .


The following is a proof for a stronger version of the
pigeonhole lemma (Lem. 9). In partiular, Lem. 9 follows
when ε0 = N [1] [.]


LEMMA 18. Let E := supν∈M,ψ∈Ψ|Epψ|. Fix any
N ∈ N, ψ [(1)],...,ψ [(][N] [)] ∈ Ψ, and ν [(1)],...,ν [(][N] [)] ∈M.
Let β be a constant s.t. [�] i<j [|][E][ν] [(][i][)][[][ψ][(][j][)][]][|][q][ ≤] [β][q][ for]
all j ∈ [N ]. Then, [�][N] j=1 [|][E][ν] [(][j][)] [ψ][(][j][)][| ≤] [inf][ε] 0 [∈][(0][,][1)][{][Nε][0][ +]
EluDimq(Ψ, M,ε0) · (2E + β [q] ln(Eε [−] 0 [1][))][}][.]


PROOF OF LEM. 18. Fix any q ∈ N; the proof will be
for the ℓq eluder dimension. We say a distribution ν ∈M
is ε-independent of a subset Γ ⊂M if there exists ψ ∈ Ψ
s.t. |Eνψ| > ε but also [�][n] µ∈Γ [(][E][µ][[][ψ][])][q][ ≤] [ε][q][. Conversely,]
we say ν is ε-dependent on Γ if for all ψ ∈ Ψ, we have
|Eν [ψ]| ≤ ε or [�][n] µ∈Γ [(][E][µ][[][ψ][])][q][ > ε][q][. For any][ Γ][ ⊂M][ and]
ν ∈M, we let N (ν, Γ,ε0) denote the largest number of



disjoint subsets of Γ that ν is ε0-dependent on. We also
use the shorthand µ [(][<j][)] = {µ [(1)],µ [(2)],...,µ [(][j][−][1)] }.
Claim 1: If |Eµ(j) [ψ [(][j][)] ]| > ε, then N (µ [(][j][)],µ [(][<j][)],ε) ≤
β [q] ε [−][1] . By definition of N := N (µ [(][j][)],µ [(][<j][)],ε), there are
disjoint subsets S [(1)],...,S [(][N] [)] ⊂ µ [(][<j][)] s.t. each S [(][i][)] satisfies [�] µ∈S [(][i][)][|][E][µ][[][ψ][(][j][)][]][|][ > ε][ since][ |][E][µ][(][j][)] [[][ψ][(][j][)][]][|][ > ε][ by]
premise. Thus, summing over all such subsets gives Nε <
�i<j [|][E][µ][(][i][)] [[][ψ][(][j][)][]][|][q][ ≤] [β][q][, proving Claim 1.]
Claim 2 (Pigeonhole): For any ε0 and any sequence µ [(1)],...,µ [(][N] [)] ∈M, there exists j ≤ N such
(N −1)
that N (µ [(][j][)],µ [(][<j][)],ε0) ≥⌊ EluDimq(Ψ,M,ε0) [⌋][.][ Recall that]

if µ [(1)],...,µ [(][L][)] ⊂M satisfies for all j ∈ [L], µ [(][j][)] is
ε0-independent of p [(][<j][)], then L ≤ EluDimq(Ψ, M,ε0)
by definition. To prove the claim, we maintain J :=
(k−1)
⌊ EluDimq(Ψ,M,ε0) [⌋] [disjoint sequences][ S][(1)][,...,S][(][J][)][ ⊂]

µ [(][<k][)] s.t. each S [(][i][)] has the property that each element
is ε-independent of its precedessors. We initialize S [(1)] =

- ·· = S [(][J][)] = ∅ and iteratively add elements µ [(1)],...,µ [(][N] [)]

until µ [(][j][)] is ε0-dependent on all these disjoint subsequences, at which point the claim is proven. If there exists a subsequence which µ [(][j][)] is ε0-independent of, we
add µ [(][j][)] to that subsequence, which preserves the invariant condition. This process indeed terminates since otherwise one subsequence would have more elements than
EluDimq(Ψ, M,ε0), a contradiction.
Claim 3: For any ε, [�][N] j=1 [I][[][|][E][µ][(][j][)] [[][ψ][(][j][)][]][|][ > ε][]][ ≤]
(β [q] ε [−][1] + 1)EluDimq(Ψ, M,ε) + 1. Let κ denote the
left hand sum and so let i1,...,iκ be all the indices j
s.t. |Eµ(j) [ψ [(][j][)] ]| > ε. By Claim 2, there exists j ≤ κ s.t.
(κ−1)
⌊ EluDimq(Ψ,M,ε) [⌋≤] [L][(][µ][(][i][j] [)][,µ][(][<i][j][)][,ε][)][. Then by Claim]
1, this is further upper bounded by β [q] ε [−][1] . Rearranging
proves the claim.
Concluding the proof. For any ε0, we have
�Nj=1 [|][E][µ][(][j][)] [[][ψ][(][j][)][]][|][ =][ �][N] j=1 �0E [I][[][|][E][µ][(][j][)] [[][ψ][(][j][)][]][|][ > y][]d][y]

≤ Nε0 + [�][N] j=1 �εE0 [I][[][|][E][µ][(][j][)] [ψ][(][j][)][|][ > y][]d][y]


(i)
≤ Nε0 + [�] ε [E] 0 [{][(][β][q][y][−][1][ + 1)EluDim][q][(Ψ][,] [M][,y][) + 1][}][d][y]


(ii)
≤ Nε0 + [�] ε [E] 0 [{][(][β][q][y][−][1][ + 1)EluDim][q][(Ψ][,] [M][,ε][0][) + 1][}][d][y]


(iii)
≤ Nε0 + EluDimq(Ψ, M,ε0)(2E + β [q] ln(Eε [−] 0 [1][))][,]


where (i) is by Claim 3, (ii) is by monotonicity of the
E
eluder dimension, and (iii) is by �ε0 [y][−][1][ = ln(][Eε] 0 [−][1][)][.]


**11.2 Proofs for Lemmas in Sec. 6**


LEMMA 12. Let ℓ = ℓbce. Under the same setup as
Lem. 7 with f [ˆ][op] selected from Cβ [bce] instead of Cβ [sq][, w.p.a.l.]

1 − δ, we have (a) [�][n] i=1 [E] bce [RL][( ˆ][f][ op][,π][i][)][ ≤] [2][Hβ][, and (b)]
mina f [ˆ] 1 [op][(][x][1][,a][)][ ≤] [V][ ⋆][.]




CENTRAL ROLE OF LOSS FUNCTIONS IN RL 23



PROOF OF LEM. 12. By Lem. 3 extended on martingale sequences [3], w.p.a.l. 1 − δ, for all f ∈F, h ∈ [H],

�ni=1 [E] h [Ber] (f,π [i] ) ≤ ln(H|F|/δ) + 2 [1] [L] h [bce][(][f][h][,f][h][+1][,] [D][h][)]


(24) − 2 [1] [L] h [bce][(][T][ ⋆] h [f][h][+1][,f][h][+1][,] [D][h][)][.]


Let gh [f] [:= argmin][g] h [∈G] h [L][h][(][g][h][,f][h][+1][,] [D][h][)][ denote the em-]
pirical risk minimizer. Under the BC premise,

�ni=1 [E] h [Ber] (f,π [i] ) ≤ ln(H|F|/δ) + [1] 2 [L] h [bce][(][f][h][,f][h][+1][,] [D][h][)]

− 2 [1] [L] h [bce][(][g] h [f] [,f][h][+1][,] [D][h][)][.]


Thus, any f ∈Cβ(D) satisfies [�][n] i=1 [E] h [Ber] (f,π [i] ) ≤ [1] 2 [β][ +]

ln(H|F|/δ) ≤ 2β, which proves Claim (a). For Claim
(b), we prove that Q [⋆] ∈Cβ(D). By Eq. (24) and nonnegativity of E [Ber], we have L [bce] h [(][T][ ⋆] h [f][h][+1][,f][h][+1][,] [D][h][)][ −]
L [bce] h [(][g] h [f] [,f][h][+1][,] [D][h][)][ ≤] [2ln(][H][|F|][/δ][) =][ β][. Then, setting]
f = Q [⋆] and noting that Q [⋆] h [=][ T][ ⋆] h [Q][⋆] h+1 [shows that][ Q][⋆] [sat-]
isfies the confidence set condition. Thus, Q [⋆] ∈Cβ(D) and
Claim (b) follows by definition of f [ˆ][op] .


LEMMA 13. Define δh [Ber] that uses T [π] instead of T [⋆] :


δh [Ber] (f,π,xh,ah) := h [2] Ber [(][f][h][(][x][h][,a][h][)][,] [T][ π] h [f][h][+1][(][x][h][,a][h][))][.]


Then, for any f, π, xh,ah,


fh(xh,ah) ≤ eQ [π] h [(][x][h][,a][h][) + 77][Hδ] Ber [RL] [(][f,π][)][.]


This implies the corollary:


Eπ[fh(xh,ah)] ≲ V [π] + HδBer [RL] [(][f,π][)][.]


PROOF OF LEM. 13. Fix any f,π. We use the shorthand δt(x,a) = δt [Ber] (f,π,x,a) to simplify notation. The
corollary follows from the main claim via Eπ[Q [π] h [(][x][h][,a][h][)]][ ≤]
V [π], since costs are non-negative. To prove the main
claim, we establish the following claim by induction:

fh(xh,ah) ≤ [�][H] t=h [(1 +][ 1] H [)][t][−][h][E][π][[][c][t][ +]


(25) 28Hδt(xt,at) | xh,ah].


The base case of h = H +1 holds since fH+1 = 0. For the
induction step, fix any h ∈ [H] and suppose that Eq. (25)
is true for h + 1. By Eq. (2) and AM-GM, we have


fh(xh,ah) ≤ (1 + H [1] [)][T][ π] h [f][h][+1][(][x][h][,a][h][) + 28][Hδ][h][(][x][h][,a][h][)]

By definition, Th [π][f][h][+1][(][x][h][,a][h][) =][ E][π][[][c][h] [+][f][h][+1][(][x][h][+1][,a][h][+1][)][ |]
xh,ah], so we can apply induction hypothesis to fh+1.
This proves the inductive claim Eq. (25). Then, we prove
the main claim by using the fact (1+ H [1] [)][H][ ≤] [e][. The corol-]

lary then follows by Eπ[Q [π] h [(][x][h][,a][h][)]][ ≤] [V][ π][ which holds]
due to the non-negativity of costs.


K
LEMMA 14. If [�][K] k=1 [(][V][ π][k][ −] [V][ ⋆][)][ ≤] [c] ��k=1 [V][ π][k][ +]



PROOF OF LEM. 14. By AM-GM, the premise implies [�][K] k=1 [(][V][ π][k][ −] [V][ ⋆][)][ ≤] [1] 2 �Kk=1 [V][ π][k][ +][ 3] 2 [c][2] [, which sim-]



Thus, any p ∈Cβ [mle] (D) satisfies [�][n] i=1 [E] h [dis][(][p,π][i][)][ ≤] 2 [1] [β][ +]

ln(H|P|/δ) ≤ 2β, which proves Claim (a). For Claim
(b), we prove that Z [⋆] ∈Cβ [mle] (D). By Eq. (26) and non
negativity of E [dis], we have L [mle] h (Th [D][,⋆] ph+1,ph+1, Dh) −
L [mle] h [(][g] h [p][,p][h][+1][,] [D][h][)][ ≤] [2ln(][H][|P|][/δ][) =][ β][. Then, setting]
p = Z [⋆] and noting that Zh [⋆] [=][ T][ D] h [,⋆] Zh [⋆] +1 [shows that]
Z [⋆] satisfies the confidence set condition. Thus, Z [⋆] ∈
Cβ [mle] (D) and Claim (b) follows by definition of ˆp [op] .


LEMMA 16. Define the state-action analog of δh [dis][:]


δh [dis][(][p,π,x][h][,a][h][) :=][ h][2][(][p][h][(][x][h][,a][h][)][,] [T][ D] h [,π] ph+1(xh,ah)),


where Th [D][,π] p(x,a) :=D C(x,a) + p(X [′],π(X [′] )) is the distributional Bellman backup of p under π. Then, for any p,
π, xh, ah, we have


σ [2] (ph(xh,ah)) ≤ 2eσ [2] (Zh [π][(][x][h][,a][h][)) +][ Hδ] dis [RL][(][p,π][)][,]


where σ [2] (Zh [π][(][x][h][,a][h][))][ denotes the variance of the ran-]
dom variable Z [π]
h [(][x][h][,a][h][)][. This also implies the corollary:]


Eπ[σ [2] (ph(xh,ah))] ≲ σ [2] (Z [π] ) + H [2] δdis [RL][(][p,π][)][.]


PROOF OF LEM. 16. Fix any p,π. We use the shorthand δt(x,a) = δt [dis][(][p,π,x,a][)][ to simplify notation. First,]
note that the corollary follows from the main claim since




[1] 2 �Kk=1 [V][ π][k][ +][ 3] 2 [c][2]



plies [�] k=1 [(][V][ π] [ −] [V][ ⋆][)][ ≤] 2 �k=1 [V][ π] [ +] 2 [c] [, which sim-]

plies to [�][K] k=1 [V][ π][k][ ≤] [2][KV][ ⋆] [+ 3][c][2][. Hence, plugging this]
back into the premise yields the desired bound.



LEMMA 15. Let ℓ = ℓmle and Dh be the same as in
Lems. 7 and 12. Then, under Asm. 5, for any δ ∈ (0, 1) let
β = 2ln(H|P|/δ) and define


pˆ [op] ∈ argmin min p¯1(x1,a)
p∈Cβ [mle] (D) a


W.p.a.l. 1 − δ, we have (a) [�][n] i=1 [E] mle [RL] [(ˆ][p][op][,π][i][)][ ≤] [2][Hβ]
and (b) mina ˆp [op] 1 [(][x][1][,a][)][ ≤] [V][ ⋆][.]


PROOF OF LEM. 15. The proof follows similarly as
the bce case of Lem. 12. By Lem. 3 extended on martingale sequences [3], w.p.a.l. 1 − δ, for all p ∈P, h ∈ [H],

�ni=1 [E] h [dis][(][p,π][i][)][ ≤] [ln(][H][|P|][/δ][) +][ 1] 2 [L] h [mle] (ph,ph+1, Dh)

(26) − [1] 2 [L] h [mle] (Th [D][,⋆] ph+1,ph+1, Dh).


Let gh [p] [:= argmin][g] h [∈P] h [L] h [mle] (gh,ph+1, Dh) denote the
empirical maximum likelihood estimate. Under the distributional BC premise, we have

�ni=1 [E] h [dis][(][p,π][i][)][ ≤] [ln(][H][|P|][/δ][)]



2 [1] [L] h [mle] (ph,ph+1, Dh) − 2 [1]



+ [1]



2 [1] [L] h [mle][(][g] h [p][,p][h][+1][,] [D][h][)][.]



c [2], then [�][K] k=1 [V][ π][k][ −] [V][ ⋆] [≤] [c] √



2KV [⋆] + 3c [2] .




24


the law of total variance (LTV) implies E[σ [2] (Zh [π][(][x][h][,a][h][))]][ ≤]
σ [2] (Z [π] ), where recall the LTV states: for any random variable X,Y :


σ [2] (Y ) = E[σ [2] (Y | X)] + σ [2] (E[Y | X]).


We now establish the main claim.
Step 1. We first show the following claim by induction:
for all h,

σ [2] (ph(xh,ah)) ≤ [�][H] t=h [(1 +][ 1] H [)][t][−][h][E][π][[8][Hδ][t][(][x][t][,a][t][)]


(27) 2σ [2] (ct + ¯pt+1(xt+1,π(xt+1))) | xh,ah]


The base case h = H + 1 is true since σ [2] (pH+1) = 0. For
the induction step, fix any h ∈ [H] and suppose that the
induction hypothesis (IH; Eq. (27)) is true for h + 1.
By our second-order lemma for variance (Eq. (10)),

σ [2] (ph(xh,ah)) ≤(1 + H [1] [)][σ][2][(][T][ D] h [,π] ph+1(xh,ah))


+ 8Hδh(xh,ah).


Then, we use LTV to condition on ch,xh+1 (i.e., the outer
mean/variance are w.r.t. ch,xh+1, the inner mean/variance
are w.r.t. ph+1): σ [2] (Th [D][,π] ph+1(xh,ah)) is equal to

E[σ [2] (ph+1(xh+1,π(xh+1)) | ch,xh+1)]


+ σ [2] (ch + ¯ph+1(xh+1,π(xh+1))).

We bound the first term by the IH, which completes the
proof for Eq. (27).
Step 2. By the above claim and (1 + H [1] [)][H][ ≤] [e][, we have]

σ [2] (ph(xh,ah)) ≤ 8Hδdis [RL][(][p,π][)+]


2e [�][H] t=h [E][π][[][σ][2][(][c][t][ + ¯][p][t][+1][(][x][t][+1][,π][(][x][t][+1][)))][ |][ x][h][,a][h][]][.]

Step 3. Lastly, it suffices to convert the above variance
term to σ [2] (ct + Vt [π] +1 [(][x][t][+1][))][, since][ σ][2][(][Z] h [π][(][x][h][,a][h][)) =]
H
�t=h [E][π][[][σ][2][(][c][t][ +][ V] t [ π] +1 [(][x][t][+1][))][ |][ x][h][,a][h][]][ by LTV. To per-]
form this switch in variance, observe that:
(28)

¯
|ph(xh,π(xh)) − V [π] (xh)| ≲ [�][H] t=h [E][π][[] �δt(xt,at)],


by the PDL and the second-order lemma (Lem. 1). Also,
recall that σ [2] (X) ≤ 2σ [2] (Y ) + 2σ [2] (X − Y ). Thus, we
have


σ [2] (ct + ¯pt+1(xt+1,π(xt+1)))


≤ 2σ [2] (ct + Vt [π] +1 [(][x][t][+1][))]


+ 2σ [2] (¯pt+1(xt+1,π(xt+1)) − Vt [π] +1 [(][x][t][+1][))]

≤ 2σ [2] (ct + Vt [π] +1 [(][x][t][+1][)) +][ H][ �] t [H] =h [E][π][[][δ][t][(][x][t][,a][t][)]][,]


where the last inequality used Eq. (28) and CauchySchwarz. Thus we have shown that


σ [2] (ph(xh,ah)) ≲ H [2] δdis [RL][(][p,π][)+]


4e [�][H] t=h [E][π][[][σ][2][(][c][t][ + ¯][p][t][+1][(][x][t][+1][,π][(][x][t][+1][)))][ |][ x][h][,a][h][]][.]


