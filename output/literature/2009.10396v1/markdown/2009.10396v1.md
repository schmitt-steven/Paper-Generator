## **Is Q-Learning Provably Efficient?** **An Extended Analysis**



**Kushagra Rastogi** _[∗]_
University of California, Los Angeles
Los Angeles, CA 90095
```
   krastogi@g.ucla.edu

```

**Fabrice Harel-Canada** _[∗]_
University of California, Los Angeles
Los Angeles, CA 90095
```
  fabricehc@cs.ucla.edu

```


**Jonathan Lee** _[∗]_
University of California, Los Angeles
Los Angeles, CA 90095
```
   jlee916@g.ucla.edu

```

**Aditya Joglekar**
University of California, Los Angeles
Los Angeles, CA 90095
```
   adivj123@gmail.com

```


**Abstract**


This work extends the analysis of the theoretical results presented within the paper
_Is Q-Learning Provably Efficient?_ by Jin _et al._ [1]. We include a survey of related
research to contextualize the need for strengthening the theoretical guarantees
related to perhaps the most important threads of model-free reinforcement learning.
We also expound upon the reasoning used in the proofs to highlight the critical
steps leading to the main result showing that Q-learning with UCB exploration
achieves a sample efficiency that matches the optimal regret that can be achieved
by any model-based approach.


**Introduction**


State-of-the-art reinforcement learning (RL) has been dominated by model-free algorithms (like
Q-learning) because they are online, more expressive and need less space. However, empirical work
has shown that model-free algorithms have a higher sample complexity [2, 3], meaning that they
require many more samples in order to perform well on a given task. Can we make model-free
algorithms sample-efficient? This is one of the most fundamental questions in the reinforcement
learning community that has yet to be answered definitely. As seen in the setting of multi-armed
bandits, good sample efficiency is the result of aptly managing the exploration-exploitation trade-off.
In our project, we aim to elaborate on the proofs establishing that Q-learning with Upper Confidence
Bound (UCB) exploration, in an episodic MDP setting and without access to a “simulator”, matches
the information-theoretic regret optimum, up to a single _√H_ where _H_ is the number of steps per

episode. To do this, we will leverage our current understanding of Q-learning and survey existing
literature related to sample efficiency and complexity of both model-free and model-based RL
methods.


**Related Work**


This section reviews related work that compares Model-free (MF) and Model-based (MB) reinforcement learning (RL) in general before focusing on theoretical research into their respective sample
efficiencies and complexities.


_∗_ Indicates equal contribution.




**Model-free vs. Model-based RL**


The study of reinforcement learning has given rise to two primary approaches for maximizing
cumulative rewards while interacting with an unknown environment through time: model-based
and model-free algorithms. MB algorithms are the “planners” that either learn or use a model of
environmental dynamics to form a form a suitable control policy. On the other hand, MF algorithms
make no attempt to model state transitions explicitly, instead updating their state and action value
functions directly. Both fundamentally and in practice, the two approaches overlap substantially;
indeed MF methods act as important building blocks for MB methods [4].


Despite the similarities, MF methods like classical Q-learning [5], DQNs [6] and their variants [7, 8],
most policy gradient approaches [9, 10, 3], and many others dominate most of the progress in modern
RL [1]. Table 1 highlights some of the pros and cons of both approaches and highlights why MF
methods enjoy wide attention in the field. Of the cons, the most problematic is the tendency for MF
approaches to be sample inefficient as they require many “experiences” to train. The current work we
analyze by Jin _et al._ [1] establishes that this con does not apply to the entire class of MF algorithms
by showing that not only is it possible to design MF algorithms that are sample efficient, but also that
Q-learning with an upper confidence bound (UCB) exploration policy _is provably efficient_ . However,
before expanding on the illustrative process and proofs, we review other work related to sample
efficiency and complexity in the next subsection.


**Model-free (MF)** **Model-based (MB)**



_◦_ Tend to be more sample efficient [2, 11]
_◦_ More efficient handling of changing goals
because it does not need “personal experience”
with every state-action pair [11, 4]


_◦_ Suffer from model bias, i.e., they inherently
assume that the learned dynamics model sufficiently accurately resembles the real environment [2, 12, 13, 11]
_◦_ Computationally more complex than MF
methods - can be difficult to learn a good
model of state transitions / rewards [4]



**Pros**


**Cons**



_◦_ Computationally less complex than MB
methods, requiring no model of the environment to be effective (which can be a bottleneck for MB methods) [4]
_◦_ Capable of functioning online (as opposed
to working with batches) [1]
_◦_ Require less space (memory) [1]
_◦_ More expressive since specifying value
functions / policies are more flexible than
specifying a model for the environment [1]


_◦_ Requires (repeated) “personal experience”
with many state-action pairs in order to train,
makes exploration more costly [4]
_◦_ Tend to be less sample efficient [4, 2, 11]



Table 1: Pros & Cons of MF vs. MB RL Approaches


**Sample Efficiency & Complexity**


Within RL, _sample efficiency e_ ( _·_ ) measures the number of inputs an agent requires in order to achieve
a given level of performance [14] on a particular task. For example, for any two agents _A_ 1 and _A_ 2,
_e_ ( _A_ 1) _> e_ ( _A_ 2) if _A_ 1 requires fewer inputs to achieve the _same_ performance as _A_ 2 on a given task.
The related idea of _sample complexity_ measures the minimum number of inputs required to guarantee
a probably approximately correct (PAC) estimator [15]. Generally, the lower the sample complexity,
the more efficient the class of estimators / agents.


In the MF setting, several recent works provide empirical evidence that MF algorithms generally
require higher sample complexity [2, 3]. In these cases, the authors elected to measure the duration of
interactions between the agent and the environment rather than the more literal count of inputs since
there is a one-to-one correspondence between the two units of measure. As an illustrative example,
the authors of PILCO [2] measure their MB approach against six MF approaches [16, 17, 18, 19, 20,
21, 22] and achieved up to 5x orders of magnitude reduction in time required to succeed at the classic
_cart-pole_ task.


In the MB setting, several publications [23, 24, 25, 26, 27] have been able to demonstrate asymptotically optimal sample efficiency by importing ideas from the bandit literature, such as the UCB
variations that our selected paper also pairs with Q-learning to prove its efficiency. If the existence of
a simulator is assumed, MF methods like Speedy Q-Learning [28] can be _almost_ as efficient as the


2




best MB algorithms [29]. Unfortunately, the value of this work is undercut by the observation that
simulators generally do not do a good job of representing real-world environments where exploration
is significantly harder — i.e. using a uniformly random exploration policy is optimal for the simulator
in question [29]. The only theoretical result for MF without using a simulator is that of “delayed
Q-learning” by Strehl _et al._ [30], which achieves a total regret of _O_ ( _T_ [4] _[/]_ [5] ) — ignoring factors in _S_,
_A_, and _H_ — compared to _O_ ( _√T_ ) achieved by MB methods.


This general issue with MF methods suggests that it may be fruitful to combine key elements of MF
and MB approaches to increase sample efficiency. While there is presently no theoretical basis for the
benefits of this line of inquiry, several researchers have [31, 32] have demonstrated that there is at least
some empirical evidence supporting the utility of blending both approaches. Nagabandi _et al._ [31]
combine the expressiveness of deep neural networks with a model-based controller (MBC) to achieve
3 _−_ 5 _×_ efficiency improvement over MF baselines on the MuJoCo [33] locomotion benchmark.
Similarly, Pong _et al._ [32] proposed the ideas of temporal difference models (TDMs), which are a
family of goal-conditioned value functions trained with MF learning, but used for MB control. Their
experimental results show substantial improvements in efficiency relative to _both_ high performing
MF methods like DDPG [34] and HER [35] as well as MB methods on a range of RL tasks.


Table 2 summarizes the regret of various algorithms discussed above and illustrates the comparative
sample efficiency of the work done in our selected paper by Jin _et al._ [1].

|Col1|Algorithm|Regret|Time|Space|
|---|---|---|---|---|
|**MB**|UCRL2[25]|_≥O_(<br>~~_√_~~<br>_H_4_S_2_AT_)<br>|Ω(_TS_2_A_)|_O_(_S_2_AH_)|
|**MB**|Agrawal & Jia[23]|_≥O_(<br>~~_√_~~<br>_H_3_S_2_AT_)<br>|_≥O_(<br>~~_√_~~<br>_H_3_S_2_AT_)<br>|_≥O_(<br>~~_√_~~<br>_H_3_S_2_AT_)<br>|
|**MB**|UCBVI[24]|_O_(<br>~~_√_~~<br>_H_2_SAT_)<br>|_O_(_TS_2_A_)|_O_(_TS_2_A_)|
|**MB**|vUCQ [26]|_O_(<br>~~_√_~~<br>_H_2_SAT_)|_O_(<br>~~_√_~~<br>_H_2_SAT_)|_O_(<br>~~_√_~~<br>_H_2_SAT_)|
|**MF**|DelayedQ-learning [30]|_OS,A,H_(_T_ 4_/_5)<br>|_O_(_T_)|_O_(_SAH_)|
|**MF**|Q-learning (UCB-H) [1]|_O_(<br>~~_√_~~<br>_H_4_SAT_)<br>|_O_(<br>~~_√_~~<br>_H_4_SAT_)<br>|_O_(<br>~~_√_~~<br>_H_4_SAT_)<br>|
|**MF**|Q-learning (UCB-B) [1]|_O_(<br>~~_√_~~<br>_H_3_SAT_)|_O_(<br>~~_√_~~<br>_H_3_SAT_)|_O_(<br>~~_√_~~<br>_H_3_SAT_)|
||information theoretic lower bound[1]|Ω(<br>~~_√_~~<br>_H_2_SAT_)|—|—|



Table 2: Regret comparisons for RL methods on Episodic MDP where _T_ = _KH_ is the total number
of steps, _H_ is the steps per episode, _S_ is the number of states, and _A_ is the number of actions. NOTE:
this table is presented for _T ≥_ poly( _S, A, H_ ), and thus omits the lower order terms.


**Preliminary**


The notation used in this paper is mostly adapted from [1].We consider an episodic Markov Decision
Process (MDP) _M_ = ( _S, A, H,_ P _, r_ ), where _S_ is a finite set of states with _|S|_ = _S_, _A_ is a finite
set of actions with _|A|_ = _A_, _H_ is the number of steps in each episode, P is the transition matrix
where P _h_ ( _·|x, a_ ) is the distribution of states when action _a_ is taken at state _x_ at step _h ∈_ [ _H_ ] and
_rh_ : _S × A →_ [0 _,_ 1] is a deterministic reward function at step _h_ .


Each episode of the MDP begins with the agent at state _x_ 1. For each step _h ∈_ [ _H_ ], the agent observes
state _xh ∈S_, takes action _ah ∈A_, receives reward _rh_ ( _xh, ah_ ) and subsequently transitions to
the next state _xh_ +1 that is drawn from P _h_ ( _·|xh, ah_ ). The episode ends when the agent reaches the
terminal state _xH_ +1.


We define _Vh_ _[π]_ [:] _[ S →]_ [R][ as the agent’s state-value function at step] _[ h]_ [ under policy] _[ π]_ [. We define]
_Q_ _[π]_ _h_ [:] _[ S × A →]_ [R][ as the agent’s Q-value function at step] _[ h]_ [ under policy] _[ π]_ [.]


_H_
_Vh_ _[π]_ [(] _[x]_ [) =][ E] � _rh′_ ( _xh′, πh′_ ( _xh′_ )) _|xh_ = _x_
� _h_ _[′]_ = _h_ �


_H_
_Q_ _[π]_ _h_ [(] _[x, a]_ [) =] _[ r][h]_ [(] _[x, a]_ [) +][ E] � _rh′_ ( _xh′, πh′_ ( _xh′_ )) _|xh_ = _x, ah_ = _a_
� _h_ _[′]_ = _h_ +1 �


For finite state and action spaces, we define the optimal state-value function as _Vh_ _[∗]_ [(] _[x]_ [) = max] _[π][ V][ π]_ _h_
_∀x ∈S_ and _h ∈_ [ _H_ ] with optimal policy _π_ _[∗]_ . Let the total number of episodes be _K_, initial


3




state be _x_ _[k]_ 1 [for episode] _[ k]_ [ and policy be] _[ π][k]_ [for the] _[ k]_ [th episode. Then, the total expected regret is]
_Regret_ ( _K_ ) = [�] _[K]_ _k_ =1 [[] _[V][ ∗]_ 1 [(] _[x][k]_ 1 [)] _[ −]_ _[V][ π]_ 1 _[k]_ [(] _[x]_ 1 _[k]_ [)]][.]


**Main Results**


We combine Q-learning with a UCB exploration strategy which has the following Q-value update:
_Qh_ ( _x, a_ ) _←_ (1 _−_ _αt_ ) _Qh_ ( _x, a_ ) + _αt_ [ _rh_ ( _x, a_ ) + _Vh_ +1( _x_ _[′]_ ) + _bt_ ] where _t_ counts the number of times
the algorithm has visited state-action pair ( _x, a_ ) at step _h_, _x_ _[′]_ is the next state, _bt_ is the confidence
bonus and _αt_ = _[H]_ _H_ [+1] + _t_ [is the step-size (learning rate). This choice of] _[ α][t]_ [ scales as] _[ O]_ [(] _[H/t]_ [)][ which]

allows the regret to be sub-exponential in _H_, thus making Q-learning efficient.


**Q-learning with Hoeffding bonus.** Since _rh ∈_ [0 _,_ 1] and there are _H_ steps in each episode, the
Q-values are upper-bounded by _H_ . By the Azuma-Hoeffding inequality, the Q-values confidence
bound scales as _O_ (1 _/√t_ ) if the state-action pair ( _x, a_ ) is visited _t_ times. Thus, a simple bonus would



be _bt_ = _O_
� ~~�~~

bonus.



_H_ [3] _ι_


_t_



where _ι_ = log( _SAT/p_ ). We present Q-learning algorithm with UCB-Hoeffding
�



**Algorithm 1** Q-learning with UCB-Hoeffding

1: Initialize _Qh_ ( _x, a_ ) _←_ _H_, _Nh_ ( _x, a_ ) _←_ 0 _∀_ ( _x, a, h_ ) _∈S × A ×_ [ _H_ ]
2: **for** episode _k_ = 1 to _K_ **do**
3: get _x_ 1
4: **for** step _h_ = 1 to _H_ **do**
5: _ah ←_ _argmaxa′Qh_ ( _xh, a_ _[′]_ )
6: _t_ = _Nh_ ( _x, a_ ) _←_ _Nh_ ( _x, a_ ) + 1
7: _bt ←_ _c_ � _H_ [3] _ι/t_ where _c >_ 0 is a constant and _ι_ = log( _SAT/p_ )

8: _Qh_ ( _xh, ah_ ) _←_ (1 _−_ _αt_ ) _Qh_ ( _xh, ah_ ) + _αt_ [ _rh_ ( _xh, ah_ ) + _Vh_ +1( _xh_ +1) + _bt_ ]


9: _Vh_ ( _xh_ ) _←_ min _H,_ max _a′∈A Qh_ ( _xh, a_ _[′]_ )
� �

10: **end for**

11: **end for**



**Theorem 1** (Hoeffding). _If bt_ = _c_ ~~�~~



**Theorem 1** (Hoeffding). _If bt_ = _c_ ~~�~~ _H_ [3] _ι/t, then with probability_ 1 _−_ _p ∀p ∈_ (0 _,_ 1) _, the total regret_

_of Algorithm 1 is at most O_ ( _√H_ [4] _SATι_ ) _where c >_ 0 _is a constant and ι_ = log( _SAT/p_ ) _._


Algorithm 1 has a _√T_ regret without having access to a simulator which makes it very efficient and

comparable to model-based algorithms. As an online learning algorithm, Algorithm 1 only stores the
Q-value table and has superior time and space complexities when _|S|_ is large.



_H_ [4] _SATι_ ) _where c >_ 0 _is a constant and ι_ = log( _SAT/p_ ) _._



Algorithm 1 has a _√_



**Theorem 2** (Bernstein). _For a specified bt, with probability_ 1 _−_ _p ∀p ∈_ (0 _,_ 1) _, the total regret of_
_Q-learning with UCB-Bernstein exploration is at most O_ ( _√H_ [3] _SATι_ + _√H_ [9] _S_ [3] _A_ [3] _ι_ [4] ) _._



_H_ [3] _SATι_ + _√_



_H_ [9] _S_ [3] _A_ [3] _ι_ [4] ) _._



Q-learning with UCB-Bernstein exploration improves the total regret by a factor of _√_



Q-learning with UCB-Bernstein exploration improves the total regret by a factor of _H_ over Q
learning with UCB-Hoeffding exploration. Thus, the asymptotic regret of UCB-Bernstein is only a
_√H_ factor away from the optimal regret achieved by model-based algorithms. However, when _T_ is

small, total regret of UCB-Bernstein exploration is dominated by _O_ ( _√H_ [9] _S_ [3] _A_ [3] _ι_ [4] ).


**Theorem 3** (Information-theoretic lower bound). _The total regret for any algorithm in an episodic_
_MDP setting must be at least_ Ω( _√H_ [2] _SAT_ ) _._


Note that the upper bounds mentioned in Theorem 1 and 2 differ from the optimal regret by a factor
of _H_ and _√H_ respectively.


**Proofs for Algorithm 1**


**Notation.** We have ( _x_ _[k]_ _h_ _[, a][k]_ _h_ [) =][ the state-action pair observed and chosen at step] _[ h]_ [ of episode] _[ k]_ [.][ I][[] _[A]_ []]
is the indicator function for event _A_ . We use _Q_ _[k]_ _h_ _[, V]_ _h_ _[ k][, N]_ _h_ _[ k]_ [to represent the] _[ Q][h][, V][h][, N][h]_ [ functions at]


4



_H_ [9] _S_ [3] _A_ [3] _ι_ [4] ).




the beginning of episode _k_ . We get the following update rules for Algorithm 1:

_Vh_ _[k]_ [(] _[x]_ [)] _[ ←]_ [min] � _H,_ max _a_ _[′]_ _∈A_ _[Q]_ _h_ _[k]_ [(] _[x, a][′]_ [)] � _, ∀x ∈S_


_k_
_Q_ _[k]_ _h_ [+1] ( _x, a_ ) = � _Q_ (1 _[k]_ _h −_ [(] _[x, a]_ _αt_ ) [)] _Q_ _[,]_ _h_ [(] _[x, a]_ [) +] _[ α][t]_ [[] _[r][h]_ [(] _[x, a]_ [) +] _[ V][ k]_ _h_ +1 [(] _[x]_ _h_ _[k]_ +1 [) +] _[ b][t]_ []] _[,]_ otherwiseif( _x, a_ ) = ( _x_ _[k]_ _h_ _[, a][k]_ _h_ [)] (1)


We have [P _hVh_ +1]( _x, a_ ) = E _x′∼_ P _h_ ( _·|x,a_ ) _Vh_ +1( _x_ _[′]_ ) and its empirical counterpart of episode _k_ is

[P [ˆ] _[k]_ _h_ _[V][h]_ [+1][](] _[x, a]_ [) =] _[ V][h]_ [+1][(] _[x][k]_ _h_ +1 [)][ which is only defined for][ (] _[x, a]_ [) = (] _[x]_ _h_ _[k][, a][k]_ _h_ [)][.]


The learning rate is _αt_ = _[H]_ _H_ [+1] + _t_ [. Also, we present] _[ α]_ _t_ [0] [=][ �] _[t]_ _j_ =1 [1] _[−]_ _[α][j]_ [ and] _[ α]_ _t_ _[i]_ [=][ �] _[t]_ _j_ = _i_ +1 [1] _[−]_ _[α][j]_ [. Since]

empty products are equal to 1 and empty summations equal to 0, we get _αt_ [0] [= 1][ and][ �] _[t]_ _i_ =1 _[α]_ _t_ _[i]_ [= 0]
for _t_ = 0. For _t ≥_ 1, we get _αt_ [0] [=][ �] _[t]_ _j_ =1 _Hj−_ +1 _j_ [= 0][ and][ �] _i_ _[t]_ =1 _[α]_ _t_ _[i]_ [= 1][. Combining these equations]
with (1), we get:



_t_
� _αt_ _[i]_


_i_ =1



(2)
�



_Q_ _[k]_ _h_ [(] _[x, a]_ [) =] _[ α]_ _t_ [0] _[H]_ [ +]


**Lemma 1.1.** _Properties of αt_ _[i][:]_



_rh_ ( _x, a_ ) + _Vh_ _[k]_ +1 _[i]_ [(] _[x]_ _h_ _[k][i]_ +1 [) +] _[ b][i]_
�



_α_ _[i]_ _t_
_t_ _[≤]_ [�] _i_ _[t]_ =1 ~~_√_~~



**(a)** _For every t ≥_ 1 _,_ 1
~~_√_~~



_i_ _[≤]_ ~~_√_~~ 2



_t_ _[.]_




_[H]_

_t_ _[and]_ [ �] _i_ _[t]_ =1 [(] _[α]_ _t_ _[i]_ [)][2] _[ ≤]_ [2] _[H]_ _t_



**(b)** _For every t ≥_ 1 _,_ max _i∈_ [ _t_ ] _αt_ _[i]_ _[≤]_ [2] _[H]_ _t_



_t_ _[.]_



**(c)** _For every i ≥_ 1 _,_ [�] _[∞]_ _t_ = _i_ _[α]_ _t_ _[i]_ [= 1 +] _H_ [1] _[.]_



_Proof of Lemma 1.1._ Our choice of the learning rate is crucial for Q-learning to be efficient. Property
(c) is particularly important to bound the regret by a constant factor of (1 + _H_ [1] [)] _[H]_ [ for each step in]

each episode. We provide proofs for the properties.



**(a)** We use induction on _t_ . For the base case _t_ = 1, we get _α_ 1 [1] [= 1][. Note that] _[ α]_ _t_ _[i]_ [= (1] _[ −]_ _[α][t]_ [)] _[α]_ _t_ _[i]_ _−_ 1
for _i_ = 1 _, ..., t −_ 1 and _t ≥_ 2. This means [�] _[t]_ _i_ =1 ~~_√_~~ _α_ _[i]_ _ti_ [=] ~~_√_~~ _αtt_ [+ (1] _[ −]_ _[α][t]_ [)][ �] _[t]_ _i_ =1 _[−]_ [1] _α_ ~~_√_~~ _[i]_ _t−i_ 1 [. Recall]



_α_ _[i]_ _t−_ 1
_t_ [+ (1] _[ −]_ _[α][t]_ [)][ �] _[t]_ _i_ =1 _[−]_ [1] ~~_√_~~ _i_



_i_ [=] ~~_√_~~ _αt_



for _i_ = 1 _, ..., t −_ 1 and _t ≥_ 2. This means [�] _i_ =1 ~~_√_~~ _ti_ [=] ~~_√_~~ _αtt_ [+ (1] _[ −]_ _[α][t]_ [)][ �] _i_ =1 ~~_√_~~ _t−i_ 1 [. Recall]

that _H ≥_ 1 for Q-learning to be meaningful. Using induction on both sides, we can show that



_α_ _[i]_ _t−_ 1
_t_ [+ (1] _[ −]_ _[α][t]_ [)][ �] _[t]_ _i_ =1 _[−]_ [1] ~~_√_~~ _i_



_t_ [and] ~~_√_~~ _[α][t]_ _t_



_α_ _[i]_ _t−_ 1
_t_ [+ (1] _[ −]_ _[α][t]_ [)][ �] _i_ _[t]_ =1 _[−]_ [1] ~~_√_~~ _i_



_αt_
~~_√_~~



_−i_ 1 _[≥]_ ~~_√_~~ 1



_i_ 1 _[≤]_ ~~_√_~~ 2



_t_ [.]



. By rearranging, we get _αt_ _[i]_ [=] _[H]_ _H_ [+1] + _t_ � _ti_ =1 _Hi_ + _i_ [=]
�



**(b)** We have _αt_ _[i]_ [=] _[H]_ _H_ [+1] + _i_



_i_ _i_ +1 _[t][−]_ [1]
_H_ + _i_ +1 _H_ + _i_ +2 _[...]_ _H_ + _t_
�



max _i∈_ [ _t_ ] _αt_ _[i]_ [. Each term in the product resembles] _x_ + _x_ _y_ [with] _[ y][ ≥]_ [1][. Thus,] _x_ + _x_ _y_ _[≤]_ [1][ and hence] _[ α]_ _t_ _[i]_ _[≤]_



_H_ +1 _[H]_ [+1]
_H_ + _t_ [. Since,] _H_ + _t_




_[H]_ [+1]

_H_ + _t_ _[≤]_ _[H]_ _H_ [+] + _[H]_ _t_




_[H]_ _H_ [+] + _[H]_ _t_ _[≤]_ _[H]_ [+] _t_ _[H]_




_[H]_

_t_, then _αt_ _[i]_ _[≤]_ [2] _[H]_ _t_




_[H]_ _t_ [. Thus, we have shown that][ max] _[i][∈]_ [[] _[t]_ []] _[ α]_ _t_ _[i]_ _[≤]_ [2] _[H]_ _t_



_H_ + _t_ _H_ + _t_ _H_ + _t_ _t_ _t_ _t_ [.]

But [�] _[t]_ _i_ =1 [(] _[α]_ _t_ _[i]_ [)(] _[α]_ _t_ _[i]_ [)] _[ ≤]_ [�] _[t]_ _i_ =1 _[α]_ _t_ _[i]_ [(max] _i∈_ [ _t_ ] _[α]_ _t_ _[i]_ [)][ which implies][ �] _[t]_ _i_ =1 [(] _[α]_ _t_ _[i]_ [)][2] _[ ≤]_ [2] _[H]_ _t_ [.]



_t_ [.]



**(c)** We have


_∞_
� _αt_ _[i]_ [=]


_t_ =1



_∞_
� _αi_


_t_ =1



_t_
� (1 _−_ _αj_ )

_j_ = _i_ +1



_t_
� (1 _−_ _αj_ ) = _αi_

_j_ = _i_ +1



_∞_
�


_t_ =1



_t_
�



_i_ _i_ _i_ + 1

= _[H]_ [ + 1] 1 +

_H_ + _i_ � _H_ + _i_ + 1 [+] _H_ + _i_ + 1 _H_ + _i_ + 2 [+] _[ ...]_ �



To simplify the last equality, we conjecture the following identity and prove it by induction:



_n_
_k_ [= 1 +] _[ n]_ _n_ _[ −]_ + 1 _[k]_




_[ −]_ _[k]_

_n_ + 1 [+] _[ n]_ _n_ _[ −]_ + 1 _[k]_



_n −_ _k_ + 1

_[ −]_ _[k]_

_n_ + 1 _n_ + 2



+ _..._
_n_ + 2



where _n, k >_ 0 and _n ≥_ _k_ .



5




_[n]_ _k_ _[−]_ [�] _i_ _[t]_ =0 _[x][i]_ [ =] _[n][−]_ _k_ _[k]_



_−n_ + _i_ [. For the base case]



Note that this is equivalent to induction on _[n]_




_[−]_ _k_ _[k]_ � _ti_ =1 _n−n_ + _k_ + _i_ _i_



= _[n][−][k]_

_k_

�



_−k_ +1 = _[n][−][k]_

_n_ +1 _k_




_[n]_

_k_ _[−]_ [1] _[−]_ _[n]_ _n_ _[−]_ +1 _[k]_




_[−][k]_

_k_ _[−]_ _[n]_ _n_ _[−]_ +1 _[k]_




_[n]_ _n_ _[−]_ +1 _[k]_ [and] _[n][−]_ _k_ _[k]_




_[−][k]_ _n−k_ +1

_k_ _n_ +1



_k_




_[−][k]_

_k_ _[−]_ _[n]_ _n_ _[−]_ +1 _[k]_




_[n][−]_

_n_ +1 [. Assume]



_t_ = 1, we get _[n]_




_[n][−][k]_ _[n][−][k]_

_n_ +1 [=] _k_



1 _−_ _k_
_n_ +1
�




_[n]_ _k_ _[−]_ [�] _i_ _[m]_ =0 _[x][i]_ [ =] _[n][−]_ _k_ _[k]_



_−n_ + _i_ [. For] _[ t]_ [ =] _[ m]_ [ + 1][,]



the induction hypothesis holds for _t_ = _m_ so _[n]_




_[−]_ _k_ _[k]_ � _mi_ =1 _n−n_ + _k_ + _i_ _i_



_k_



_n −_ _k_ + _i_

_−_ _xm_ +1
_n_ + _i_



_n_
_k_ _[−]_



_m_
�



� _xi −_ _xm_ +1 = _[n][ −]_ _k_ _[k]_

_i_ =0



_m_ +1
�


_i_ =1



_n −_ _k_ + _i −_ 1


_n_ + _i_



= _[n][ −]_ _[k]_

_k_



_m_
�


_i_ =1


_m_
�


_i_ =1



_n −_ _k_ + _i_


_−_
_n_ + _i_



_n −_ _k_

=
� _k_


_n −_ _k_

=
� _k_



_m_
�


_i_ =1


_m_
�


_i_ =1



�

�



_n −_ _k_ + _i_


_n_ + _i_


_n −_ _k_ + _i_


_n_ + _i_



_k_
1 _−_
�� _n_ + _m_ + 1


_n −_ _k_ + _m_ + 1
�� _n_ + _m_ + 1



= _[n][ −]_ _[k]_

_k_



_m_ +1
�


_i_ =1



_n −_ _k_ + _i_


_n_ + _i_



This finishes the induction. By taking _n_ = _H_ + _i_ and _k_ = _H_, we get [�] _[∞]_ _t_ =1 _[α]_ _t_ _[i]_ [=] _[H]_ _H_ [+1] _i_




_[H]_ [+1] _H_ + _i_

_H_ + _i_ _H_



_n_ = = _t_ =1 _[α]_ _t_ [=] _H_ + _i_ _H_ =

_H_ +1 [1]



_H_ +1 = 1 + _H_ [1]



_H_ [. This concludes the proof of Lemma 1.1.]



**Lemma 1.2.** _For any_ ( _x, a, h_ ) _∈S × A ×_ [ _H_ ] _and episode k ∈_ [ _K_ ] _let t_ = _Nh_ _[k]_ [(] _[x, a]_ [)] _[ and suppose]_
( _x, a_ ) _was previously taken at step h of episodes k_ 1 _, k_ 2 _, ..., kt < k. Then:_



�



( _Q_ _[k]_ _h_ _[−][Q]_ _h_ _[∗]_ [)(] _[x, a]_ [) =] _[ α]_ _t_ [0][(] _[H][−][Q]_ _h_ _[∗]_ [(] _[x, a]_ [))+]



_t_
� _αt_ _[i]_


_i_ =1



( _Vh_ _[k]_ +1 _[i]_ _[−][V][ ∗]_ _h_ +1 [)(] _[x]_ _h_ _[k][i]_ +1 [)+[(ˆ][P] _h_ _[k][i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)+] _[b][i]_
�



_Proof of Lemma 1.2._ Recall that [�] _[t]_ _i_ =1 _[α]_ _t_ _[i]_ [= 1][ and][ [ˆ][P] _[k]_ _h_ _[i][V][h]_ [+1][](] _[x, a]_ [) =] _[ V][h]_ [+1][(] _[x][k]_ _h_ _[i]_ +1 [)][.] The
Bellman optimality equation is _Q_ _[∗]_ _h_ [(] _[x, a]_ [) = (] _[r][h]_ [ +][ P] _[h][V][ ∗]_ _h_ +1 [)(] _[x, a]_ [)][. Then,][ �] _i_ _[t]_ =1 _[α]_ _t_ _[i][r][h]_ [(] _[x, a]_ [) =]
_rh_ ( _x, a_ ) [�] _[t]_ _i_ =1 _[α]_ _t_ _[i]_ [=] _[ r][h]_ [(] _[x, a]_ [)][. Similarly,][ [][P] _[h][V][ ∗]_ _h_ +1 [](] _[x, a]_ [) = [][P] _[h][V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _[ −]_ [[ˆ][P] _h_ _[k][i][V][ ∗]_ _h_ +1 [](] _[x, a]_ [) +]
_Vh_ _[∗]_ +1 [(] _[x]_ _h_ _[k][i]_ +1 [)][ and the same trick with][ �] _i_ _[t]_ =1 _[α]_ _t_ _[i]_ [applies here too.] Furthermore, _Q_ _[∗]_ _h_ [(] _[x, a]_ [) =]

0 _,_ _t ≥_ 1
( _αt_ [0] _[Q][∗]_ _h_ [+] _[ r][h]_ [ +][ P] _[h][V][ ∗]_ _h_ +1 [)(] _[x, a]_ [)][ where] _[ α]_ _t_ [0] [=]
�1 _,_ _t_ = 0 [. This manipulation is valid since] _[ t]_ [ = 1]

represents the start of the episode so _Q_ _[∗]_ _h_ [(] _[x, a]_ [)][ is technically just defined as itself at] _[ t]_ [ = 0][. By]
consolidating everything we get:



_Q_ _[∗]_ _h_ [(] _[x, a]_ [) =] _[ α]_ _t_ [0] _[Q][∗]_ _h_ [(] _[x, a]_ [) +]



_t_
� _αt_ _[i]_


_i_ =1



_rh_ ( _x, a_ ) + (P _h −_ P [ˆ] _[k]_ _h_ _[i]_ [)] _[V][ ∗]_ _h_ +1 [(] _[x, a]_ [) +] _[ V][ ∗]_ _h_ +1 [(] _[x]_ _h_ _[k][i]_ +1 [)] (3)
� �



We attain Lemma 1.2 by _Q_ _[k]_ _h_ [(] _[x, a]_ [)] _[ −]_ _[Q][∗]_ _h_ [(] _[x, a]_ [)][ where] _[ Q][k]_ _h_ [(] _[x, a]_ [)][ comes from][ (2)][ and] _[ Q][∗]_ _h_ [(] _[x, a]_ [)][ comes]
from (3). This concludes the proof of Lemma 1.2.


**Lemma 1.3.** _There exists an absolute constant c >_ 0 _such that, for any p ∈_ (0 _,_ 1) _, letting bt_ =
_c_ � _H_ [3] _ι/t, we have βt_ = 2 [�] _[t]_ _i_ =1 _[α]_ _t_ _[i][b][i]_ _[≤]_ [4] _[c]_ ~~�~~ _H_ [3] _ι/t and, with probability at least_ 1 _−_ _p, the_



_H_ [3] _ι/t, we have βt_ = 2 [�] _[t]_ _i_ =1 _[α]_ _t_ _[i][b][i]_ _[≤]_ [4] _[c]_ ~~�~~



_c_ � _H_ [3] _ι/t, we have βt_ = 2 [�] _i_ =1 _[α]_ _t_ _[i][b][i]_ _[≤]_ [4] _[c]_ ~~�~~ _H_ [3] _ι/t and, with probability at least_ 1 _−_ _p, the_

_following holds simultaneously ∀_ ( _x, a, h, k_ ) _∈S × A ×_ [ _H_ ] _×_ [ _K_ ] :



0 _≤_ ( _Q_ _[k]_ _h_ _[−]_ _[Q][∗]_ _h_ [)(] _[x, a]_ [)] _[ ≤]_ _[α]_ _t_ [0] _[H]_ [ +]



_t_
� _αt_ _[i]_ [(] _[V][ k]_ _h_ +1 _[i]_ _[−]_ _[V][ ∗]_ _h_ +1 [)(] _[x]_ _h_ _[k][i]_ +1 [) +] _[ β][t]_


_i_ =1



**Notation.** The idea behind this lemma is to construct an upper confidence bound on the optimal
state-action values, _Q_ _[∗]_ _h_ _[∀]_ _[h][ ∈{]_ [1] _[,]_ [ 2] _[, . . ., H][}]_ [. Before going into the proof, we first define some]
notation.


For each state-action-step pair ( _x, a, h_ ) _∈S ×A×_ [ _H_ ], we denote _ki_ as the episode in which ( _x, a, h_ )
occurs for the _i_ _[th]_ time. Otherwise, _ki_, _ki_ +1, _. . ._, _kK_ = _K_ + 1 if ( _x, a, h_ ) only occurs _i −_ 1 times


6




over the _K_ episodes. It is important to note that the _K_ episodes are indexed based on the ordering
in which they were observed, that is, _k_ = _j_ indicates the _j_ _[th]_ episode observed. Consequently, _ki_ is
denoted as


_k_
_ki_ = min( _k ∈_ [ _K_ ] _| {k > ki−_ 1 _∧_ ( _xh_ _[, a][k]_ _h_ [)] _[} ∪{][K]_ [ + 1] _[}]_ [)] _[,]_ _i ∈_ [ _K_ ]
�0 _,_ _i_ = 0


The aforementioned notation will be utilized for the proofs of this lemma and Theorem 1.


_Proof of Lemma 1.3._ For every fixed ( _x, a, h_ ) _∈S ×A×_ [ _H_ ], let _t_ = _Nh_ _[k]_ [(] _[x, a]_ [)][, indicating the number]
of occurrences of ( _x, a, h_ ) before the start of episode _k_ . Moreover, let _Fi_ be a _σ_ -field generated by all
random variables up to episode _ki_, step _h_ . In the context of a probability space (Ω _, F,_ P), _{Fi}_ _[K]_ _i_ =1 [is]
defined as a filtration over (Ω _, F,_ P) consisting of an increasing family of sub- _σ_ -fields [36] of the
event space _F_, where a _σ_ -field _Fj_ can be interpreted as the accumulative information or collection of
events generated from the observation of outcomes from the past episodes _k_ 1 _, k_ 2 _, . . . kj−_ 1 and the
current episode _kj_ . Note that we will only be concerned with episodes for which outcome ( _x, a, h_ )
occurs by use of an indicator function I[ _ki ≤_ _K_ ] in the latter half of the proof.


From the error [(P [ˆ] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)][ of the empirical data in Lemma 1.2 along with the predefined]
notion of _ki_ and filtration, we now construct the sequence


E [(P [ˆ] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _[ | F]_ [1] _[,][ F]_ [2] _[, . . .,][ F][i]_ = 0 _∀i ∈{_ 1 _,_ 2 _, . . ., K}_ (4)
� �


The result shown above stems from the fact that taking the expectation of P [ˆ] _[k]_ _h_ _[i][V][ ∗]_ _h_ +1 [(] _[x, a]_ [)][ condi-]
tioned on the past _σ_ -fields _F_ 1 _, F_ 2 _, . . ., Fi_ provides knowledge of the probability transition matrix
P _h_ ( _x′|x, a_ ), which implies the following:


E�[P [ˆ] _[k]_ _h_ _[i][V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _[ | F]_ [1] _[,][ F]_ [2] _[, . . .,][ F][i]_ � = � P _h_ ( _x_ _[′]_ _| x, a_ ) _· Vh_ _[∗]_ +1 [(] _[x][′]_ [)]

_x_ _[′]_ _∈S_


= E [P _hVh_ _[∗]_ +1 [](] _[x, a]_ [)] _[ | F]_ [1] _[,][ F]_ [2] _[, . . .,][ F][i]_
� �


Given that we assume the setting to be a tabular episodic finite-horizon MDP, _M_ = ( _S, A, H,_ P _, r_ ),
where _|S|, |A|_, and _H_ are finite with a finite amount of episodes _K_, then


E [(ˆP _khi_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _< ∞_ _∀i ∈{_ 1 _,_ 2 _, . . ., K}_ (5)
���� ��� �


Since (4) and (5) hold true, the sequence of empirical errors _{_ [(P [ˆ] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _[}]_ _i_ _[K]_ =1 [can be]
interpreted as a martingale difference sequence (MDS) with respect to the filtration _{F}_ _[K]_ _i_ =1 [[][37][].]
Therefore, we can use the Azuma-Hoeffding inequality to give a concentration result [38] for each
index in the MDS, i.e., to construct confidence bounds for _Q_ _[∗]_ _h_ _[∀]_ _[h][ ∈{]_ [1] _[,]_ [ 2] _[, . . ., H][}]_ [. Applying]
Azuma-Hoeffding and a union bound over all _K_ episodes gives the following:



~~�~~
~~�~~
�
�



�����



_τ_
�



_cH_
_≤_

2

�����



2



� _ατ_ _[i]_ _[·]_ [I][[] _[k][i]_ _[≤]_ _[K]_ []] _[·]_ [[(ˆ][P] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)]


_i_ =1



�



_H_ [3] _ι_

_∀τ ∈_ [ _K_ ] (6)
_τ_



_τ_
�



�( _ατ_ _[i]_ ) [2] _· ι ≤_ _c_


_i_ =1



for some absolute constant c, with probability at least 1 _−_ _SAHp_ [. Recall that][ I][[] _[k][i][ ≤]_ _[K]_ []][ is an indicator]
function that filters out episodes where ( _x, a_ ) _was not taken at step h_ . To prove the left inequality
in (6), we consider a previously stated fact that _rh ∈_ [0 _,_ 1], implying _Qh_ ( _x, a_ ) _≤_ _H_ and thus
_Vh_ ( _x_ ) _≤_ _H_ for any _x, a, h_ :

[(ˆP _khi_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _≤_ _H ≤_ _cH ≤_ _√_ 2 _cH ≤_ _√_ 2 _ατ_ _[i]_ _[cH]_ [ =] _[ c][i]_

(7)

����� �����



_≤_ _H ≤_ _cH ≤_ _√_
�����



2 _cH ≤_ _√_



2 _ατ_ _[i]_ _[cH]_ [ =] _[ c][i]_



(7)



_∀i ∈{_ 1 _,_ 2 _, . . ., K}, c >_ 0



7




Note that _ci_ is the symmetric bound on the martingale difference [(P [ˆ] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)][ and is used]
for the Azuma-Hoeffding inequality:



�



P



�����



_τ_
� _ατ_ _[i]_ _[·]_ [ I][(] _[k][i]_ _[≤]_ _[K]_ [)] _[ ·]_ [ [(ˆ][P] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _≥_ _ϵ_

_i_ =1 ����



2 _ϵ_ [2]

_−_
_≤_ 2 exp _K_
� ~~�~~ _i_ =1 _[c]_ _i_ [2]



(8)
�



whose complementary event is



�



(9)
�


(10)



P



�����



_τ_
� _ατ_ _[i]_ _[·]_ [ I][(] _[k][i]_ _[≤]_ _[K]_ [)] _[ ·]_ [ [(ˆ][P] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _≤_ _ϵ_

_i_ =1 ����



2 _ϵ_ [2]
_≥_ 1 _−_ 2 exp _−_ _K_
� ~~�~~ _i_ =1 _[c]_ _i_ [2]



To find the proper choice of _ϵ_, we revisit the bound on the martingale difference:


_τ_ _τ_
� _ατ_ _[i]_ _[·]_ [ I][(] _[k][i]_ _[≤]_ _[K]_ [)] _[ ·]_ [ [(ˆ][P] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _≤_ � _ατ_ _[i]_ _[·]_ [ I][(] _[k][i]_ _[≤]_ _[K]_ [)] _[ ·][ cH]_ ����

����� _i_ =1 ����� ����� _i_ =1 �



_τ_
�



_≤_
�����



����
�



� _ατ_ _[i]_ _[·]_ [ I][(] _[k][i]_ _[≤]_ _[K]_ [)] _[ ·]_ [ [(ˆ][P] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)]


_i_ =1



�����



_τ_
�



� _ατ_ _[i]_ _[·]_ [ I][(] _[k][i]_ _[≤]_ _[K]_ [)] _[ ·][ cH]_


_i_ =1



= _cH_


_≤_ _cH_



~~�~~
~~�~~
�
�


~~�~~
~~�~~
�
�



_τ_
�



�( _ατ_ _[i]_ ) [2] _·_ (I( _ki ≤_ _K_ )) [2]


_i_ =1



_τ_
�



�( _ατ_ _[i]_ ) [2] _≤_ _cH_


_i_ =1



~~�~~
~~�~~
�
�



_τ_
�



�( _ατ_ _[i]_ ) [2] _· ι_ = _ϵ_


_i_ =1



With _ci_ in (7) and _ϵ_ in (10), we can rewrite the right-hand side of the inequality in (9) as



1 _−_ 2 exp � _−_ ~~�~~ 2 _Ki_ =1 _ϵ_ [2] _[c]_ _i_ [2] � = 1 _−_ _SAH_ 2 _p_ (11)



Therefore, results from (9) and (11) indicate an upper bound on the left-hand side of (6) with a
probability of 1 _−_ _SAH_ 2 _p_ [. Rescaling] _[ p]_ [ to] _[p]_ 2 [finishes the proof of the left inequality of (6).]


To remove the notation of learning rate as shown on the right-hand side of (6), we apply property
(b) of Lemma 1.1, which gave an inclusive upper bound of [2] _[H]_ [for][ �] _i_ _[t]_ =1 [(] _[α]_ _t_ _[i]_ [)][2][,] _[ ∀][t][ ≥]_ [1][. Making the]



(b) of Lemma 1.1, which gave an inclusive upper bound of [2] _[H]_ _t_ [for][ �] _i_ _[t]_ =1 [(] _[α]_ _t_ _[i]_ [)][2][,] _[ ∀][t][ ≥]_ [1][. Making the]

substitution on the middle term of (6), that is, _[cH]_ 2 � ~~�~~ ~~_τ_~~ _i_ =1 [(] _[α]_ _τ_ _[i]_ [)][2] _[·][ ι]_ [, concludes the proof of (6).]


Because the inequality in (6) holds for all fixed _τ ∈_ [ _K_ ] uniformly, it also holds for _τ_ = _t_ =
_Nh_ _[k]_ [(] _[x, a]_ [)] _[ ≤]_ [[] _[K]_ []][. As a result, we can rewrite (6) in a way that removes the indicator function:]


_t_

_H_ [3] _ι_

� _αt_ _[i]_ _[·]_ [ [(ˆ][P] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)] _≤_ _c_ ~~�~~ _t_ _where t_ = _Nh_ _[k]_ [(] _[x, a]_ [)] (12)

����� _i_ =1 �����



~~_τ_~~
2 � ~~�~~ _i_ =1 [(] _[α]_ _τ_ _[i]_ [)][2] _[·][ ι]_ [, concludes the proof of (6).]



_t_
�



_≤_ _c_
�����



_where t_ = _Nh_ _[k]_ [(] _[x, a]_ [)] (12)
_t_



� _αt_ _[i]_ _[·]_ [ [(ˆ][P] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1 [](] _[x, a]_ [)]


_i_ =1



~~�~~



_H_ [3] _ι_



If we choose the Hoeffding-style bonus _bt_ to be _c_
�



_H_ [3] _ι_



If we choose the Hoeffding-style bonus _bt_ to be _c_ _t_ _ι_ from the equation above, then from property

(a) in Lemma 1.1,



_t_
� _αt_ _[i][b][i]_ [=]


_i_ =1



_t_
�



� _αt_ _[i]_ _[·][ c]_


_i_ =1



~~�~~



_t_ _[,]_ [ 2] _[c]_



_H_ [3] _ι_

_∈_
_t_

�



_c_

~~�~~



�



�



_H_ [3] _ι_



_H_ [3] _ι_


_t_



(13)



For notational convenience, we introduce _[β]_ 2 [=][ �] _i_ _[t]_ =1 _[α]_ _t_ _[i][b][i]_ [. The final step is putting everything]

together to yield an upper confidence bound for _Q_ _[∗]_ _h_ [:]



( _Q_ _[k]_ _h_ _[−]_ _[Q][∗]_ _h_ [)(] _[x, a]_ [)] _[ ≤]_ _[α]_ _t_ [0] _[·]_ [ (] _[H][ −]_ _[Q][∗]_ _h_ [(] _[x, a]_ [)) +]


_≤_ _αt_ [0] _[·]_ [ (] _[H][ −]_ _[Q][∗]_ _h_ [(] _[x, a]_ [)) +]



_t_
� _i_ =1 _αt_ _[i]_ _[·]_ �( _Vh_ _[k]_ +1 _[i]_ _[−]_ _[V][ ∗]_ _h_ +1 [)(] _[x]_ _h_ _[k][i]_ +1 [) +] �(P [ˆ] _[k]_ _h_ _[i]_ _[−]_ [P] _[h]_ [)] _[V][ ∗]_ _h_ +1�( _x_ _[k]_ _h_ _[i][, a][k]_ _h_ _[i]_ [) +] _[ b][i]_



_t_
� _αt_ _[i]_ _[·]_ ( _Vh_ _[k]_ +1 _[i]_ _[−]_ _[V][ ∗]_ _h_ +1 [)(] _[x]_ _h_ _[k][i]_ +1 [) +] _[ b][i]_

_i_ =1 �



+ _c_
� �



�



_H_ [3] _ι_


_t_



�



_t_

_≤_ _αt_ [0] _[·][ H]_ [ +] � _αt_ _[i]_ _[·]_ ( _Vh_ _[k]_ +1 _[i]_ _[−]_ _[V][ ∗]_ _h_ +1 [)(] _[x]_ _h_ _[k][i]_ +1 [)] + _βt_

_i_ =1 � �


8




where the first inequality stems immediately from Lemma 1.2. The right inequality in (6) is then
applied as an inclusive upper bound for the next step. Lastly, the definition of _β_ and the fact that
_t_
� _i_ =1 _[α]_ _t_ _[i]_ _[≤]_ [1][ are utilized to construct the final inequality, thus completing the proof of Lemma 1.3.]


**Proof of Theorem 1.**


The proof of Theorem 1 uses Lemma 1.3 and the Azuma-Hoeffding inequality to produce a recursive
formulation for the upper bound of the regret. Figure 1 illustrates the high-level flow of the proof to
follow.


Figure 1: Flowchart for the Proof of Theorem 1.


We define _δh_ _[k]_ [:= (] _[V][ k]_ _h_ _[−]_ _[V][ π]_ _h_ _[k]_ [)(] _[x]_ _h_ _[k]_ [)][ and] _[ φ]_ _h_ _[k]_ [:= (] _[V][ k]_ _h_ _[−]_ _[V][ ⋆]_ _h_ [)(] _[x][k]_ _h_ [)][. Using Lemma 1.3, the regret can be]
upper bounded as _Regret_ ( _K_ ) _≤_ [�] _[K]_ _k_ =1 _[δ]_ 1 _[k]_ [.]


The main idea is to upper bound [�] _[K]_ _k_ =1 _[δ]_ _h_ _[k]_ [by the next step][ �] _[K]_ _k_ =1 _[δ]_ _h_ _[k]_ +1 [which gives a recursive]
relation for the total regret. For any fixed ( _k, h_ ) _∈_ [ _K_ ] _×_ [ _H_ ], let _t_ = _Nh_ _[k]_ [(] _[x][k]_ _h_ _[, a][k]_ _h_ [)][ and suppose]
( _x_ _[k]_ _h_ _[, a][k]_ _h_ [)][ was previously taken at step] _[ h]_ [ of episodes] _[ k]_ [1] _[, k]_ [2] _[, ..., k][t][ < k]_ [. Then,]


_δh_ _[k]_ _[≤]_ [(] _[Q][k]_ _h_ _[−]_ _[Q][π]_ _h_ _[k]_ [)(] _[x]_ _h_ _[k][, a]_ _h_ _[k]_ [) = (] _[Q]_ _h_ _[k]_ _[−]_ _[Q][⋆]_ _h_ [)(] _[x]_ _h_ _[k][, a]_ _h_ _[k]_ [) + (] _[Q]_ _h_ _[⋆]_ _[−]_ _[Q][π]_ _h_ _[k]_ [)(] _[x]_ _h_ _[k][, a]_ _h_ _[k]_ [)]



![](images/2009.10396v1.pdf-8-0.png)

_≤_ _αt_ [0] _[H]_ [ +]


= _αt_ [0] _[H]_ [ +]



_t_
� _αt_ _[i][φ][k]_ _h_ _[i]_ +1 [+] _[ β][t]_ [ + [][P] _[h]_ [(] _[V][ ⋆]_ _h_ +1 _[−]_ _[V][ π]_ _h_ +1 _[k]_ [)](] _[x]_ _h_ _[k][, a]_ _h_ _[k]_ [)]


_i_ =1


_t_
� _αt_ _[i][φ][k]_ _h_ _[i]_ +1 [+] _[ β][t][ −]_ _[φ]_ _h_ _[k]_ +1 [+] _[ δ]_ _h_ _[k]_ +1 [+] _[ ϵ][k]_ _h_ +1


_i_ =1



(14)



where _βt_ = 2 [�] _αt_ _[i][b][i]_ _[≤O]_ [(1)] ~~�~~ _H_ [3] _ι/t_ and _ϵ_ _[k]_ _h_ +1 [= [(][P] _[h][ −]_ [P][ˆ] _[k]_ _h_ [)(] _[V][ ⋆]_ _h_ +1 _[−]_ _[V][ k]_ _h_ +1 [)](] _[x]_ _h_ _[k][, a][k]_ _h_ [)][ is a]

martingale difference sequence. Line 1 uses the definition of Q-value function and _Vh_ _[k]_ [(] _[x][k]_ _h_ [)] _[ ≤]_
max _a′∈A Q_ _[k]_ _h_ [(] _[x][k]_ _h_ _[, a][′]_ [) =] _[ Q][k]_ _h_ [(] _[x][k]_ _h_ _[, a][k]_ _h_ [)][.] Line 2 follows from Lemma 1.3, the Bellman equation
_Q_ _[π]_ _h_ [(] _[x, a]_ [) = (] _[r][h]_ [+][P] _[h][V][ π]_ _h_ +1 [)(] _[x, a]_ [)][ and Bellman optimality equation] _[ Q]_ _h_ _[∗]_ [(] _[x, a]_ [) = (] _[r][h]_ [+][P] _[h][V][ ∗]_ _h_ +1 [)(] _[x, a]_ [)][.]
Finally, Line 3 holds by definition of _δh_ _[k]_ +1 _[−]_ _[φ][k]_ _h_ +1 [= (] _[V][ ⋆]_ _h_ +1 _[−]_ _[V][ π]_ _h_ +1 _[k]_ [)(] _[x]_ _h_ _[k]_ +1 [)][.]


Now, we use (14) to compute [�] _[K]_ _k_ =1 _[δ]_ _h_ _[k]_ [. Hence, we get:]



_K_
� _δh_ _[k]_ +1 [+]


_k_ =1



_K_
� _δh_ _[k]_ _[≤]_


_k_ =1



_K_
� _αt_ [0] _[H]_ [ +]


_k_ =1



_K_
�


_k_ =1



_n_ _[k]_ _h_

_h_ _[,a][k]_ _h_ [)]

� _αn_ _[i]_ _[k]_ _h_ _[φ]_ _h_ _[k][i]_ +1 [(] _[x][k]_ +

_i_ =1



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)] (15)

_k_ =1



Let _n_ _[k]_ _h_ [=] _[ t]_ [ =] _[ N]_ _h_ _[ k]_ [(] _[x][k]_ _h_ _[, a][k]_ _h_ [)][. The first term of][ (15)][ is][ �] _[K]_ _k_ =1 _[α]_ _n_ [0] _[k]_ _h_ _[H]_ [ =][ �] _k_ _[K]_ =1 _[H][ ·]_ [ I][[] _[n]_ _h_ _[k]_ [= 0]] _[ ≤]_ _[SAH]_ [.]

0 _,_ _t ≥_ 1
The equality follows from _αt_ [0] [=] �1 _,_ _t_ = 0 [. The inequality stems from the fact that, in the worst]

case, _n_ _[k]_ _h_ [= 0][ for all state-action pairs][ (] _[x, a]_ [)][ which results in an upper bound of] _[ SAH]_ [.]


_n_ _[k]_ _h_ _h_ _[,a][k]_ _h_ [)]
Next, we bound the second term of (15): [�] _[K]_ _k_ =1 � _i_ =1 _[α]_ _n_ _[i]_ _[k]_ _h_ _[φ][k]_ _h_ _[i]_ +1 [(] _[x][k]_ where _ki_ ( _x_ _[k]_ _h_ _[, a][k]_ _h_ [)][ is the episode]

_h_ _[,a][k]_ _h_ [)]
in which ( _x_ _[k]_ _h_ _[, a][k]_ _h_ [)][ was taken at step] _[ h]_ [ for the] _[ i]_ [th time. We first reorder the] _[ α]_ _n_ _[i]_ _[k]_ _h_ [and] _[ φ]_ _h_ _[k][i]_ +1 [(] _[x][k]_
terms. Note that _n_ _[k]_ _h_ [=] _[ n][k]_ _h_ _[′]_ [+] _[ j]_ [ where] _[ j]_ [ = 1] _[,]_ [ 2] _[, ...]_ [ is the] _[ j]_ [th time] _[ φ][k]_ _h_ _[′]_ +1 [appears in the summand]


9




due to the fact that _∀_ _k_ _[′]_ _∈_ [ _K_ ], the term _φ_ _[k]_ _h_ _[′]_ +1 [appears in the summand with] _[ k > k][′]_ [ if and only if]
( _x_ _[k]_ _h_ _[, a][k]_ _h_ [) = (] _[x][k]_ _h_ _[′][, a][k]_ _h_ _[′]_ [)][.This results in the following simplification:]



� _φ_ _[k]_ _h_ _[′]_ +1 �

_k_ =1 _[k][′]_



_K_
�


_k_ =1



_n_ _[k]_ _h_

_h_ _[,a][k]_ _h_ [)]

� _αn_ _[i]_ _[k]_ _h_ _[φ][k]_ _h_ _[i]_ +1 [(] _[x][k]_ _≤_

_i_ =1



_K_
�



_h_

� _αt_ _[n][k][′]_ _≤_ �1 + _H_ [1]

_t_ = _n_ _[k]_ _h_ _[′]_ [+1]



_H_



_K_
� _φ_ _[k]_ _h_ +1
� _k_ =1



where the first inequality uses the reasoning above and the final inequality uses property (c) of Lemma
1.1. Plugging the above inequalities into (15) results in:



_K_
�



_H_



_K_
� _δh_ _[k]_ +1 [+]


_k_ =1



_K_
�



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)]

_k_ =1



_K_
�



_k_ �=1 _δh_ _[k]_ _[≤]_ _[SAH]_ [ +] �1 + _H_ [1]



_K_
� _φ_ _[k]_ _h_ +1 _[−]_
� _k_ =1



_K_
� _φ_ _[k]_ _h_ +1 [+]


_k_ =1



_K_
�



_K_
� _δh_ _[k]_ +1 [+]


_k_ =1



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)]

_k_ =1



= _SAH_ + [1]

_H_



_K_
� _φ_ _[k]_ _h_ +1 [+]


_k_ =1



_K_
�



(16)



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)]

_k_ =1



_≤_ _SAH_ + 1 + [1]
� _H_



_K_
� _δh_ _[k]_ +1 [+]
� _k_ =1



where the last inequality is true because _φ_ _[k]_ _h_ +1 _[≤]_ _[δ]_ _h_ _[k]_ +1 [since] _[ V][ ∗]_ _[≥]_ _[V][ π]_ _k_ [.]

Inequality (16) recursively upper bounds [�] _[K]_ _k_ =1 _[δ]_ _h_ _[k]_ [by][ �] _[K]_ _k_ =1 _[δ]_ _h_ _[k]_ +1 [. Applying recursion for steps] _[ h][ ∈]_
_{_ 1 _,_ 2 _, ...., H}_ and using _δH_ _[K]_ +1 [= 0][ (the algorithm reaches the terminal state so] _[ V][ K]_ _H_ +1 [=] _[ V][ π]_ _H_ _[K]_ +1 [= 0][)]
gives:



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)]

_k_ =1



_K_
�



_H_



_k_ �=1 _δ_ 1 _[k]_ _[≤]_ _[SAH]_ [ +] �1 + _H_ [1]



_K_
� _δ_ 2 _[k]_ [+]
� _k_ =1



_K_
�
�



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)]

_k_ =1



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)] +

_k_ =1 �



_≤_ _SAH_ + 1 + [1]
� _H_



_≤_ _SAH_ + 1 + [1]
� _H_


= _SAH_ + 1 + [1]
� _H_



_SAH_ + 1 + [1]
�� � _H_



_K_
� _δ_ 3 _[k]_ [+]
� _k_ =1



_H−_ 1

_SAH_
�



= _SAH_ + 1 + [1]
� _H_



_SAH_ + 1 + [1]
� � _H_



2
_SAH_ + _· · ·_ + 1 + [1]
� � _H_



_H_

+ _O_
�
� _h_ =1



_H_

+ _O_
�
�



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)]

_k_ =1 �



_K_
�



_K_
�( _βnkh_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)]

_k_ =1 �



= _O_ _H_ [2] _SA_ +
�



_H_
�


_h_ =1



(17)



Overall, we achieve [�] _[K]_ _k_ =1 _[δ]_ 1 _[k]_ _[≤O]_ _H_ [2] _SA_ + [�] _[H]_ _h_ =1 � _Kk_ =1 [(] _[β]_ _n_ _[k]_ _h_ [+] _[ ϵ]_ _h_ _[k]_ +1 [)] from (17).
� �



By definition of _β_, we have [�] _[K]_ _k_ =1 _[β]_ _n_ _[k]_ _h_ _[≤O]_ [(1)] _[ ·]_ [ �] _k_ _[K]_ =1 ~~�~~



_H_ [3] _ι_

_n_ _[k]_ _h_ [. Applying the pigeon-hole principle]



to the inequality would mean the following: Suppose we play 1 _/_ _[√]_ ~~_n_~~ at a state-action pair ( _x, a_ ). If
we visit ( _x, a_ ) again, then we only need to play 1 _/_ _[√]_ _n_ + 1 since we cannot include 1 _/_ _[√]_ ~~_n_~~ twice in



the summation for the same ( _x, a_ ). Thus for every ( _x, a_ ), we have [�] _[N]_ _n_ =1 _h_ _[ K]_ [(] _[x,a]_ [)]
~~�~~



1
_n_ [. Hence we get:]



_x,a_



_K_
� _βnkh_ _[≤O]_ [(1)] _[ ·]_

_k_ =1



_K_
�


_k_ =1



~~�~~



_H_ [3] _ι_



_n_ _[k]_ _h_ = _O_ (1) _·_ �



�



_H_ [3] _ι_


_n_



_Nh_ _[K]_ [(] _[x,a]_ [)]
�


_n_ =1



Note that [�] _x,a_ _[N]_ _h_ _[ K]_ [(] _[x, a]_ [) =] _[ K]_ [ because we are summing all occurrences of state-action pairs that]

occur at step _h_ over all episodes. Since there are _K_ episodes, there are _K_ occurrences of state-action
pairs occurring at step _h_ .


10




_H_ [3] _ι_ [�] _x,a_ � _Nn_ =1 _h_ _[K]_ ~~_√_~~ 1 ~~_n_~~ _≤_ _√_



Now, we have _√_



_x,a_ � _Nn_ =1 _h_ _[K]_ ~~_√_~~ 1 ~~_n_~~ _≤_ _√_



_H_ [3] _ι_ [�]

_x,a_ �



_Nh_ _[K]_ = _√_



_H_ [3] _ι_ **1** _[T]_ _v_ where _v_ =




[ _Nh_ _[K]_ [(] _[x]_ [1] _[, a]_ [1][)] _[, N]_ _h_ _[ K]_ [(] _[x]_ [2] _[, a]_ [2][)] _[, . . ., N]_ _h_ _[ K]_ [(] _[x][SA][, a][SA]_ [)]] _[T]_ [ . Using the Cauchy-Schwarz inequality, we get]
_√H_ [3] _ι_ **1** _[T]_ _v ≤_ _√H_ [3] _ι_ � _SA_ ~~[�]~~ _x,a_ _[N]_ _h_ _[ K]_ = _√H_ [3] _SAKι_ = _√H_ [2] _SATι_ by realizing that _T_ = _KH_ .

Consolidating everything in one place, we get the following:



_H_ [3] _ι_
�



_H_ [3] _ι_ **1** _[T]_ _v ≤_ _√_



_SA_ ~~[�]~~ _x,a_ _[N]_ _h_ _[ K]_ = _√_



_H_ [3] _SAKι_ = _√_



_x,a_



_K_
� _βnkh_ _[≤O]_ [(1)] _[ ·]_

_k_ =1



_K_
�


_k_ =1



~~�~~



_H_ [3] _ι_



_H_ [2] _SATι_ )


(18)



_n_ _[k]_ _h_ = _O_ (1) _·_ �



~~�~~



_H_ [3] _ι_

_≤O_ ( _H_ [3] _SAKι_ ) = _O_ ( _√_
_n_



_Nh_ _[K]_ [(] _[x,a]_ [)]
�


_n_ =1



By the Azuma-Hoeffding inequality, with probability 1 _−_ _p_, we get:


_H_ _K_ _H_ _K_
� � _ϵ_ _[k]_ _h_ +1 = � �[(P _h −_ P [ˆ] _[k]_ _h_ [)(] _[V][ ⋆]_ _h_ +1 [)] _[ −]_ _[V][ k]_ _h_ +1 [)](]

���� ���� ����



_H_
�


_h_ =1



_K_
�



=
���� ����



�[(P _h −_ P [ˆ] _[k]_ _h_ [)(] _[V][ ⋆]_ _h_ +1 [)] _[ −]_ _[V][ k]_ _h_ +1 [)](] _[x]_ _h_ _[k][, a][k]_ _h_ [)] _≤_ _cH√_

_k_ =1 ����



_K_
�



� _ϵ_ _[k]_ _h_ +1


_k_ =1



_H_
�


_h_ =1



_Tι_ (19)



Substituting (18) and (19) in (17) gives the following with probability 1 _−_ _p_ :



_K_
� _δ_ 1 _[k]_ _[≤O]_ [(] _[H]_ [2] _[SA]_ [ +] _[ H]_ _√_


_k_ =1



_H_ [2] _SATι_ + _cH√_



_Tι_ )



= _O_ ( _H_ [2] _SA_ + _√H_ [4] _SATι_ + _c√_

= _O_ ( _H_ [2] _SA_ + _√H_ [4] _SATι_ )



_H_ [2] _Tι_ )



where the final equality is valid since _c√H_ [2] _Tι_ is the smallest of the three terms. This concludes the

proof of Theorem 1.


**Conclusion**



In this paper, we showed that a subset of model-free reinforcement learning algorithms can be made
sample efficient. Specifically, we proved that, in an episodic setting, Q-learning with UCB-Hoeffding
exploration strategy achieves a regret of _O_ ( _√H_ [4] _SATι_ ). This is the first time a regret analysis

features a _√T_ factor for model-free algorithms that do not require access to a "simulator". Thus, the

key takeaways from the paper are:


_•_ Use UCB exploration over _ε_ -greedy in the model-free setting for better treatment of uncertainties in different states and actions.


_•_ Use dynamic learning rates _αt_ = _O_ ( _H/t_ ) such as _[H]_ _H_ [+1] + _t_ [instead of the commonly used][ 1] _[/t]_

for updates at time step _t_ . This applies more weight to more recent updates and is critical
for sample-efficiency guarantees.


We can build upon our current work by examining and unfolding the proof of Q-learning with the
more sophisticated UCB-Berstein exploration strategy. Lastly, we can attempt to apply the theoretical
framework used in this paper to analyze the pairing of Q-learning with another kind of exploration
strategy, such as optimistic initial values.


11




**References**


[1] Chi Jin, Zeyuan Allen-Zhu, Sébastien Bubeck, and Michael I. Jordan. Is q-learning provably
efficient? In _NeurIPS_, 2018.


[2] Marc Deisenroth and Carl Rasmussen. Pilco: A model-based and data-efficient approach to
policy search., 01 2011.


[3] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust
region policy optimization, 2015.


[4] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_ . The MIT
Press, second edition, 2018.


[5] Christopher John Cornish Hellaby Watkins. _Learning from Delayed Rewards_ . PhD thesis,
King’s College, Cambridge, UK, May 1989.


[6] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. _CoRR_,
abs/1312.5602, 2013.


[7] Hado V. Hasselt. Double q-learning. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S.
Zemel, and A. Culotta, editors, _Advances in Neural Information Processing Systems 23_, pages
2613–2621. Curran Associates, Inc., 2010.


[8] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay,
2015. cite arxiv:1511.05952Comment: Published at ICLR 2016.


[9] Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and
K. Müller, editors, _Advances in Neural Information Processing Systems 12_, pages 1057–1063.
MIT Press, 2000.


[10] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep
reinforcement learning. _CoRR_, abs/1602.01783, 2016.


[11] Christopher G. Atkeson and Juan Carlos Santamaria. A comparison of direct and modelbased reinforcement learning. In _IN INTERNATIONAL CONFERENCE ON ROBOTICS AND_
_AUTOMATION_, pages 3557–3564. IEEE Press, 1997.


[12] Jeff G. Schneider. Exploiting model uncertainty estimates for safe dynamic control learning. In
_Proceedings of the 9th International Conference on Neural Information Processing Systems_,
NIPS’96, page 1047–1053, Cambridge, MA, USA, 1996. MIT Press.


[13] Stefan Schaal. Learning from demonstration. In _Proceedings of the 9th International Conference_
_on Neural Information Processing Systems_, NIPS’96, page 1040–1046, Cambridge, MA, USA,
1996. MIT Press.


[14] Brian Everitt. _The Cambridge dictionary of statistics_ . Cambridge University Press, Cambridge,
UK; New York, 2002.


[15] Leslie Valiant. _Probably Approximately Correct: Nature’s Algorithms for Learning and Pros-_
_pering in a Complex World_ . Basic Books, Inc., USA, 2013.


[16] Hajime Kimura and Shigenobu Kobayashi. Efficient non-linear control by combining qlearning with local linear controllers. In _Proceedings of the Sixteenth International Conference_
_on Machine Learning_, ICML ’99, page 210–219, San Francisco, CA, USA, 1999. Morgan
Kaufmann Publishers Inc.


[17] Kenji Doya. Reinforcement learning in continuous time and space. _Neural Computation_,
12(1):219–245, 2000.


[18] Rémi Coulom. _Reinforcement Learning Using Neural Networks, with Applications to Motor_
_Control_ . PhD thesis, Institut National Polytechnique de Grenoble, 2002.


[19] Pawel Wawrzynski and Andrzej Pacut. Model-free off-policy reinforcement learning in continuous environment, 08 2004.


12




[20] Martin Riedmiller. Neural fitted q iteration – first experiences with a data efficient neural
reinforcement learning method. In _Proceedings of the 16th European Conference on Machine_
_Learning_, ECML’05, page 317–328, Berlin, Heidelberg, 2005. Springer-Verlag.


[21] Tapani Raiko and Matti Tornio. Variational bayesian learning of nonlinear hidden state-space
models for model predictive control. _Neurocomputing_, 72:3704–3712, 10 2009.


[22] Hado Philip van Hasselt. _Insights in Reinforcement Learning: formal analysis and empirical_
_evaluation of temporal-difference learning algorithms_ . PhD thesis, Universiteit Utrecht, January
2011.


[23] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning:
worst-case regret bounds. _ArXiv_, abs/1705.07041, 2017.


[24] Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for
reinforcement learning. In _ICML_, 2017.


[25] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. _J. Mach. Learn. Res._, 11:1563–1600, August 2010.


[26] Sham Kakade, Mengdi Wang, and Lin Yang. Variance reduction methods for sublinear reinforcement learning, 02 2018.


[27] Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized
value functions, 2016.


[28] Mohammad Gheshlaghi Azar, Remi Munos, Mohammad Ghavamzadeh, and Hilbert J. Kappen.
Speedy q-learning. In _Proceedings of the 24th International Conference on Neural Information_
_Processing Systems_, NIPS’11, page 2411–2419, Red Hook, NY, USA, 2011. Curran Associates
Inc.


[29] Mohammad Gheshlaghi Azar, Rémi Munos, and Bert Kappen. On the sample complexity of
reinforcement learning with a generative model . In _ICML_ . icml.cc / Omnipress, 2012.


[30] Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. Pac
model-free reinforcement learning. In _Proceedings of the 23rd International Conference on_
_Machine Learning_, ICML ’06, page 881–888, New York, NY, USA, 2006. Association for
Computing Machinery.


[31] Anusha Nagabandi, Gregory Kahn, Ronald Fearing, and Sergey Levine. Neural network
dynamics for model-based deep reinforcement learning with model-free fine-tuning, 05 2018.


[32] Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine. Temporal difference models:
Model-free deep rl for model-based control. _ArXiv_, abs/1802.09081, 2018.


[33] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In _IROS_, pages 5026–5033. IEEE, 2012.


[34] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom
Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement
learning. _CoRR_, abs/1509.02971, 2015.


[35] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,
Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience
replay. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, _Advances in Neural Information Processing Systems 30_, pages 5048–5058.
Curran Associates, Inc., 2017.


[36] Takis Konstantopoulos. _Conditional Expectation and Probability_ . Springer Berlin Heidelberg,
Berlin, Heidelberg, 2011.


[37] Ying-Xia Chen, Shui-Li Zhang, and Fu-Qiang Ma. On the complete convergence for martingale
difference sequence. _Communications in Statistics - Theory and Methods_, 46(15):7603–7611,
2017.


[38] David Williams. _Probability with Martingales_ . Cambridge University Press, 1991.


13


