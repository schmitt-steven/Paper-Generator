# Understanding Reinforcement Learning-Based
## Fine-Tuning of Diffusion Models: A Tutorial and Review

#### Masatoshi Uehara [*1], Yulai Zhao [†2], Tommaso Biancalani [1], and Sergey Levine [3]

1Genentech
2Princeton University
3University of California, Berkeley

#### July 19, 2024


**Abstract**


This tutorial provides a comprehensive survey of methods for fine-tuning diffusion models
to optimize downstream reward functions. While diffusion models are widely known to provide
excellent generative modeling capability, practical applications in domains such as biology
require generating samples that maximize some desired metric (e.g., translation efficiency in
RNA, docking score in molecules, stability in protein). In these cases, the diffusion model can
be optimized not only to generate realistic samples but also to maximize the measure of interest
explicitly. Such methods are based on concepts from reinforcement learning (RL). We explain
the application of various RL algorithms, including PPO, differentiable optimization, rewardweighted MLE, value-weighted sampling, and path consistency learning, tailored specifically for
fine-tuning diffusion models. We aim to explore fundamental aspects such as the strengths and
limitations of different RL-based fine-tuning algorithms across various scenarios, the benefits
of RL-based fine-tuning compared to non-RL-based approaches, and the formal objectives of
RL-based fine-tuning (target distributions). Additionally, we aim to examine their connections
with related topics such as classifier guidance, Gflownets, flow-based diffusion models, path
integral control theory, and sampling from unnormalized distributions such as MCMC. The
[code of this tutorial is available at https://github.com/masa-ue/RLfinetuning](https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq) ~~D~~ iffusion ~~B~~ ioseq.

### **Introduction**


Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) are widely recognized as powerful tools for generative modeling. They are able to accurately model complex
distributions by closely emulating the characteristics of the training data. There are many applications of diffusion models in various fields, including computer vision (Podell et al., 2023), natural
language processing (Austin et al., 2021), biology (Avdeyev et al., 2023; Stark et al., 2024; Li et al.,


*uehara.masatoshi@gene.com

  - yulaiz@princeton.edu. Equal contribution.


1




![](images/2407.13734v1.pdf-1-0.png)

![](images/2407.13734v1.pdf-1-1.png)

![](images/2407.13734v1.pdf-1-2.png)

![](images/2407.13734v1.pdf-1-3.png)

![](images/2407.13734v1.pdf-1-4.png)

Figure 1: Illustrative examples of RL-based fine-tuning, aimed at optimizing pre-trained diffusion
models to maximize downstream reward functions.


2023), chemistry (Jo et al., 2022; Xu et al., 2022; Hoogeboom et al., 2022), and biology (Avdeyev
et al., 2023; Stark et al., 2024; Campbell et al., 2024).
While diffusion models exhibit significant power in capturing the training data distribution,
there’s often a need to customize these models for particular downstream reward functions. For
instance, in computer vision, Stable Diffusion (Rombach et al., 2022) serves as a strong backbone
pre-trained model. However, we may want to fine-tune it further by optimizing downstream reward
functions such as aesthetic scores or human-alignment scores (Black et al., 2023; Fan et al., 2023).
Similarly, in fields such as biology and chemistry, various sophisticated diffusion models have
been developed for DNA, RNA, protein sequences, and molecules, effectively modeling biological
and chemical spaces. Nonetheless, biologists and chemists typically aim to optimize specific
downstream objectives such as cell-specific expression in DNA sequences (Gosai et al., 2023; Lal
et al., 2024; Sarkar et al., 2024), translational efficiency/stability of RNA sequences (Castillo-Hair
and Seelig, 2021; Agarwal and Kelley, 2022), stability/bioactivity of protein sequence (Frey et al.,
2023; Widatalla et al., 2024) or QED/SA scores of molecules (Zhou et al., 2019).
To achieve this goal, numerous algorithms have been proposed for fine-tuning diffusion models
via reinforcement learning (RL) (e.g., Black et al. (2023); Fan et al. (2023); Clark et al. (2023);
Prabhudesai et al. (2023); Uehara et al. (2024)), aiming to optimize downstream reward functions.
RL is a machine learning paradigm where agents learn to make sequential decisions to maximize
reward signals (Sutton and Barto, 2018; Agarwal et al., 2019). In our context, RL naturally emerges
as a suitable approach due to the sequential structure inherent in diffusion models, where each time
step involves a “decision” corresponding to how the sample is denoised at that step. This tutorial
aims to review recent works for readers interested in understanding the fundamentals of RL-based
fine-tuning from a holistic perspective, including the advantages of RL-based fine-tuning over
non-RL approaches, the pros and cons of different RL-based fine-tuning algorithms, the formalized
goal of RL-based fine-tuning, and its connections with related topics such as classifier guidance.


2




The content of this tutorial is primarily divided into three parts. In addition, as an implementation
example, we also release the code that employs RL-based fine-tuning for guided biological sequences
[(DNA/RNA) generation at https://github.com/masa-ue/RLfinetuning](https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq) ~~D~~ iffusion ~~B~~ ioseq.


1. We aim to provide a comprehensive overview of current algorithms. Notably, given the
sequential nature of diffusion models, we can naturally frame fine-tuning as a reinforcement
learning (RL) problem within Markov Decision Processes (MDPs), as detailed in Section 3
and 4. Therefore, we can employ any off-the-shelf RL algorithms such as PPO (Schulman
et al., 2017), differentiable optimization (direct reward backpropagation), weighted MLE
(Peters et al., 2010; Peng et al., 2019), value-weighted sampling (close to classifier guidance
in Dhariwal and Nichol (2021)), and path consistency learning (Nachum et al., 2017). We
discuss these algorithms in detail in Section 4.2 and 6. Instead of merely outlining each
algorithm, we aim to present both their advantages and disadvantages so readers can select
the most suitable algorithms for their specific purposes.

2. We categorize various fine-tuning scenarios based on how reward feedback is acquired in

Section 7. This distinction is pivotal for practical algorithm design. For example, if we can
access accurate reward functions, computational efficiency would become our primary focus.
However, in cases where reward functions are unknown, it is essential to learn them from
data with reward feedback, leading us to take feedback efficiency and distributional shift into
consideration as well. Specifically, when reward functions need to be learned from static
offline data without any online interactions, we must address the issue of overoptimization,
where fine-tuned models are misled by out-of-distribution samples, and generate samples
with low genuine rewards. This is crucial because, in an offline scenario, the coverage of
offline data distribution with feedback is limited; hence, the out-of-distribution region could
be extensive (Uehara et al., 2024).

3. We provide a detailed discussion on the relationship between RL-based fine-tuning methods
and closely related methods in the literature, such as classifier guidance (Dhariwal and Nichol,
2021) in Section 8, flow-based diffusion models (Liu et al., 2022; Lipman et al., 2023; Tong
et al., 2023) in Section 9, sampling from unnormalized distributions (Zhang and Chen, 2021)
in Section 10, Gflownets (Bengio et al., 2023) in Section 6.3, and path integral control theory
(Theodorou et al., 2010; Williams et al., 2017; Kazim et al., 2024) in Section 6.2.3. We
summarize the key messages as follows.


     - Section 6.3: The losses used in Gflownets are fundamentally equivalent to those derived
from a specific RL algorithm called path consistency learning.


     - Section 8: Classifier guidance employed in conditional generation is regarded as a
specific RL-based fine-tuning method, which we call value-weighted sampling. As
formalized in Zhao et al. (2024), this observation indicates that any off-the-shelf RLbased fine-tuning algorithms (e.g., PPO and differentiable optimization) can be applied
to conditional generation.


     - Section 10: Sampling from unnormalized distributions, often referred to as Gibbs
distributions, is an important and challenging problem in diverse domains. While
MCMC methods are traditionally used for this task, recognizing its similarity to the
objectives of RL-based fine-tuning suggests that off-the-shelf RL algorithms can also
effectively address the challenge of sampling from unnormalized distributions.


3




### **Contents**

**1** **Preliminaries** **5**

1.1 Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

1.1.1 Score-Based Diffusion Models (Optional) . . . . . . . . . . . . . . . . . 6
1.2 Fine-Tuning Diffusion Models with RL . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.1 Brief Overview: Fine-tuning with RL . . . . . . . . . . . . . . . . . . . . 8
1.2.2 Motivation for Using RL over Non-RL Alternatives . . . . . . . . . . . . . 9


**2** **Brief Overview of Entropy-Regularized MDPs** **10**
2.1 MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.2 Key Concepts: Soft Q-functions, Soft Bellman Equations. . . . . . . . . . . . . . 11


**3** **Fine-Tuning Diffusion Models with RL in Entropy Regularized MDPs** **12**


**4** **Theory of RL-Based Fine-Tuning** **14**
4.1 Key Concepts: Soft Value functions and Soft Bellman Equations. . . . . . . . . . . 14
4.2 Induced Distributions after Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . 15


**5** **RL-Based Fine-Tuning Algorithms 1: Non-Distribution-Constrained Approaches** **16**
5.1 Soft Proximal Policy Optimization (PPO) . . . . . . . . . . . . . . . . . . . . . . 16
5.2 Direct Reward Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . 17


**6** **RL-Based Fine-Tuning Algorithms 2: Distribution-Constrained Approaches** **18**
6.1 Reward-Weighted MLE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.1.1 Relation with Loss Functions for the Original Training Objective . . . . . . 21
6.2 Value-Weighted Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.2.1 Soft Q-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
6.2.2 Approximation using Tweedie’s formula . . . . . . . . . . . . . . . . . . 24
6.2.3 Zeroth-Order Guidance using Path Integral Control . . . . . . . . . . . . . 25
6.3 Path Consistency Learning (Losses Often Used in Gflownets) . . . . . . . . . . . . 25


**7** **Fine-Tuning Settings Taxonomy** **28**
7.1 Fine-Tuning with Known, Differentiable Reward Functions . . . . . . . . . . . . . 28
7.2 Fine-Tuning with Black-Box Reward Feedback . . . . . . . . . . . . . . . . . . . 28
7.3 Fine-Tuning with Unknown Rewards Functions . . . . . . . . . . . . . . . . . . . 29


**8** **Connection with Classifier Guidance** **30**
8.1 Classfier Guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
8.2 RL-Based Fine-Tuning for Conditional Generation . . . . . . . . . . . . . . . . . 31


**9** **Connection with Flow-Based Diffusion Models** **31**

9.1 Continuous Time Formulation of Score-Based diffusion models . . . . . . . . . . 32

9.2 Flow-Based (Bridge-Based) Diffusion Models . . . . . . . . . . . . . . . . . . . . 33
9.3 Connection with RL-Based Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . 34


4




**10 Connection with Sampling from Unnormalized Distributions** **35**
10.1 Markov Chain Monte Carlo (MCMC) . . . . . . . . . . . . . . . . . . . . . . . . 35
10.2 RL-Based Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36


**11 Closely Related Directions** **36**


**12 Summary** **37**

### **1 Preliminaries**


In this section, we outline the fundamentals of diffusion models and elucidate the objective of
fine-tuning them.

#### **1.1 Diffusion Models**


We present an overview of denoising diffusion probabilistic models (DDPM) (Ho et al., 2020).
For more details, refer to Yang et al. (2023); Cao et al. (2024); Chen et al. (2024); Tang and Zhao
(2024).
In diffusion models, the objective is to develop a deep generative model that accurately captures
the true data distribution. Specifically, denoting the data distribution by _p_ pre _∈_ ∆( _X_ ) where _X_ is an
input space, a DDPM aims to approximate _p_ pre using a parametric model structured as



_p_ ( _x_ 0; _θ_ ) = _p_ ( _x_ 0: _T_ ; _θ_ ) _dx_ 1: _T_ _,_ where _p_ ( _x_ 0: _T_ ; _θ_ ) = _pT_ +1( _xT_ ; _θ_ )
�



1
� _pt_ ( _xt−_ 1 _|xt_ ; _θ_ ) _._


_t_ = _T_



When _X_ is an Euclidean space (in R _[d]_ ), the forward process is modeled as the following dynamics:


_pT_ +1( _xT_ ) = _N_ (0 _, I_ ) _, pt_ ( _xt−_ 1 _|xt_ ; _θ_ ) = _N_ ( _ρ_ ( _xt, t_ ; _θ_ ) _, σ_ [2] ( _t_ ) _× I_ ) _,_


where _N_ ( _·, ·_ ) denotes a normal distribution, _I_ is an identity matrix and _ρ_ : R _[d]_ _×_ [0 _, T_ ] _→_ R _[d]_ . In
DDPMs, we aim to obtain a set of policies (i.e., denoising process) _{pt}_ [1] _t_ = _T_ +1 [,] _[ p][t]_ [ :] _[ X →]_ [∆(] _[X]_ [)][ such]
that _p_ ( _x_ 0; _θ_ ) _≈_ _p_ pre( _x_ 0). Indeed, by optimizing the variational bound on the negative log-likelihood,
we can derive such a set of policies. For more details, refer to Section 1.1.1.
Hereafter, we consider a situation where we have a pre-trained diffusion model that is already
trained on a large dataset, such that the model can accurately capture the underlying data distribution.
We refer to the pre-trained policies as _{p_ [pre] _t_ [(] _[·|·]_ [)] _[}]_ [1] _t_ = _T_ +1 [, and to the marginal distribution at] _[ t]_ [ = 0]
induced by the pre-trained diffusion model as _p_ pre. In other words,



_p_ [pre] ( _x_ 0) = _p_ [pre] _T_ +1 [(] _[x][T]_ [)]
�



1
� _p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)] _[dx]_ [1:] _[T]_ _[.]_


_t_ = _T_



**Remark 1** (Non-Euclidean space) **.** _For simplicity, we typically assume that the domain space is_
_Euclidean. However, we can easily extend most of the discussion to a more general space, such as_
_a Riemannian manifold (De Bortoli et al., 2022) or discrete space (Austin et al., 2021; Campbell_
_et al., 2022; Benton et al., 2024; Lou et al., 2023)._


5




**Remark 2** (Conditional generative models) **.** _Pre-trained models can be conditional diffusion models,_
_such as text-to-image diffusion models (Ramesh et al., 2022). The extension is straightforward:_
_augmenting the input spaces of policies with an additional space on which we want to condition._
_More specifically, by denoting that space by c ∈C, each policy becomes pt_ ( _xt|xt, c_ ; _θ_ ) : _X × C →_
∆( _X_ ) _._


**Remark 3** (Extension to Continuous-time diffusion models) **.** _In this tutorial, our discussion on_
_fine-tuning diffusion models will be primarily formulated on the discrete-time formulation, as we did_
_above Nonetheless, much of our discussion is also applicable to continuous-time diffusion models,_
_as formalized in Uehara et al. (2024)_


**1.1.1** **Score-Based Diffusion Models (Optional)**


We briefly discuss how to train diffusion models in a continuous-time framework (Song et al.,

2021). In our tutorial, this section is mainly used to discuss several algorithms (e.g., value-weighted
sampling in Section 6.2) and their relationship with flow-based diffusion models (Section 9) later.
Therefore, readers may skip it based on their individual needs.
The training process of diffusion models can be summarized as follows. Our objective is to train
a sequential mapping from a known noise distribution to a data distribution, formalized through
a stochastic differential equation (SDE). Firstly, we define a forward (fixed) SDE that maps from
the data distribution to the noise distribution. Then, a time-reversal SDE is expressed as an SDE,
which includes the (unknown) score function. Now, by learning this unknown score function from
the training data, the time-reversal SDE can be utilized as a generative model. Here are the details.


**Forward and time-reversal SDE.** Firstly, we introduce a forward (a.k.a., reference) Stochastic
differential equations (SDE) from 0 to _T_ . A common choice is a variance-preserving (VP) process:


_t ∈_ [0 _, T_ ]; _dxt_ = _−_ 0 _._ 5 _xtdt_ + _dwt, x_ 0 _∼_ _p_ [pre] ( _x_ ) _,_ (1)


where _dwt_ represents standard Brownian motion. Here are two crucial observations:


  - As _T_ approaches _∞_, the limiting distribution is _N_ (0 _,_ I).


  - The time-reversal SDE (Anderson, 1982), which preserves the marginal distribution, is
expressed as follows:


_t ∈_ [0 _, T_ ]; _dzt_ = [0 _._ 5 _zt_ + _∇_ log _qT_ _−t_ ( _zt_ )] _dt_ + _dwt._ (2)


Here, _qt ∈_ ∆(R _[d]_ ) denotes the marginal distribution at time _t_ induced by the reference SDE
(1). Notably, the marginal distribution of _zT_ _−t_ is the same as the one of _xt_ induced by the
reference SDE.


These observations suggest that with sufficiently large _T_, starting from _N_ (0 _,_ I) and following the
time-reversal SDE (2), we are able to sample from the data distribution (i.e., _p_ [pre] ) at the terminal
time point _T_ .


6




**Training score functions from the training data.** Now, the remaining question is how to estimate
the marginal score function _∇_ log _qt_ ( _·_ ) in (2). This can be computed using the following principles:


  - We have _∇_ log _qt_ ( _xt_ ) = E _x_ 0 _∼p_ pre[ _∇xt_ log _qt|_ 0( _xt | x_ 0)] _,_ where _qt|_ 0 represents the conditional
distribution of _xt_ given _x_ 0 induced by the reference SDE.


  - The conditional score function _∇xt_ log _qt|_ 0( _xt | x_ 0) can be analytically derived as


_µ_ _[⋄]_ _t_ _[x]_ [0] _[−]_ _[x][t]_

_, µ_ _[⋄]_ _t_ [= exp(] _[−]_ [0] _[.]_ [5] _[t]_ [)] _[x]_ [0] _[,][ {][σ]_ _t_ _[⋄][}]_ [2][ = 1] _[ −]_ [exp(] _[−]_ [0] _[.]_ [5] _[t]_ [)] _[.]_ (3)
_{σt_ _[⋄][}]_ [2]


Subsequently, by defining a parameterized model _sθ_ : R _[d]_ _×_ [0 _, T_ ] _→_ R _[d]_ and utilizing the following
weighted regression, the marginal score is estimated:


ˆ
_θ_ pre = argmin E _t∼_ Uni([0 _,T_ ]) _,xt∼qt|_ 0( _xt|x_ 0) _,x_ 0 _∼p_ pre[ _λ_ ( _t_ ) _∥∇xt_ log _qt|_ 0( _xt | x_ 0) _−_ _sθ_ ( _xt, t_ ) _∥_ [2] ] (4)
_θ_


where _λ_ : [0 _, T_ ] _→_ R is a weighted function.
Note we typically use another parametrization well. From (3), we can easily see that this score is
estimated by introducing networks _ϵθ_ : R _[d]_ _×_ [0 _, T_ ] _→_ R _[d]_, which aims to estimate the noise _[x][t][−]_ _σ_ _[µ]_ _t_ ~~_[⋄]_~~ _t_ _[⋄][x]_ [0] :


_θ_ ¯pre = argmin E _t∼_ Uni([0 _,T_ ]) _,xt∼µ_ _[⋄]_ _t_ _[x]_ [0][+] _[ϵσ]_ _t_ _[⋄][,ϵ][∼N]_ [(0] _[,]_ [I][)] _[,x]_ [0] _[∼][p]_ [pre][[] _[λ]_ [(] _[t]_ [)] _[/][{][σ]_ _t_ _[⋄][}]_ [2] _[∥][ϵ][ −]_ _[ϵ][θ]_ [(] _[x][t][, t]_ [)] _[∥]_ [2][]] _[.]_ (5)
_θ_


Then, by denoting the training data as _{x_ [(] 0 _[i]_ [)] _[}]_ [ and setting] _[ λ]_ [(] _[t]_ [) =] _[ {][σ]_ _t_ _[⋄][}]_ [2][, the actual loss function from]
the training data is



argmin
_θ_



� E _t∼_ Uni([0 _,T_ ]) _,x_ ( _ti_ ) _[∼][µ][⋄]_ _t_ _[x]_ [(] 0 _[i]_ [)][+] _[ϵσ]_ _t_ _[⋄][,ϵ][∼N]_ [(0] _[,]_ [I][)][[] _[∥][ϵ][ −]_ _[ϵ][θ]_ [(] _[x]_ _t_ [(] _[i]_ [)] _[, t]_ [)] _[∥]_ [2][]] _[.]_ (6)

_i_ =1



**Inference time.** Once this _θ_ [ˆ] pre (or _θ_ [¯] pre) is learned, we insert it into the time-reversal SDE (2) and
use it as a generative model. Ultimately, with standard discretization, we obtain:


_ρ_ ( _xt, t_ ; _θ_ [ˆ] pre) = _xt_ + [0 _._ 5 _xt_ + _sθ_ ˆpre( _xt, T −_ _t_ )]( _δt_ ) _,_ _{σt_ _[⋄]_ [(] _[t]_ [)] _[}]_ [2][ = (] _[δt]_ [)] _[.]_


Equivalently, this is


_ρ_ ( _xt, t_ ; _θ_ [ˆ] pre) = _xt_ + �0 _._ 5 _xt −_ 1 _/σt_ _[⋄]_ _[×][ ϵ]_ _θ_ [¯] pre [(] _[x][t][, T][ −]_ _[t]_ [)] � ( _δt_ ) _,_ (7)


where ( _δt_ ) denotes the discretization step.


**Equivalence to DDPM.** The objective function derived here is equivalent to the one formulated
based on variational inference in the discretized formulation, which is commonly referred to as
DDPMs (Ho et al., 2020). In DDPMs (Ho et al., 2020), we often see the following form:



1 1 _−_ _αt_ ¯
_ρ_ ( _xt, t_ ; _θ_ ) = _xt −_ ¯ _ϵθ_ ( _xt, T −_ _t_ ) _,_ _αt_ =
~~_√α_~~ _t_ ~~_√_~~ 1 _−_ _αt_ ~~_√α_~~ _t_



_t_


_αk._

�


_k_ =1



When _αt_ = 1 _−_ ( _δt_ ), the above is equivalent to (7) when _δt_ goes to 0 by noting 1 _/_ _[√]_ ~~_α_~~ _t_ _≈_ 1+0 _._ 5( _δt_ ).


7




#### **1.2 Fine-Tuning Diffusion Models with RL**

Importantly, our focus on RL-based fine-tuning distinguishes itself from the standard fine-tuning
methods. Standard fine-tuning typically involves scenarios where we have pre-trained models
(e.g., diffusion models) and new training data _{x_ [(] _[i]_ [)] _, y_ [(] _[i]_ [)] _}_ . In such cases, the common approach
for fine-tuning is to retrain diffusion models with the new training data using the same loss
function employed during pre-training. In sharp contrast, RL-based fine-tuning directly employs
the downstream reward functions as the primary optimization objectives, making the loss functions
different from those used in pre-training.
Hereafter, we start with a concise overview of RL-based fine-tuning. Then, before delving into
specifics, we discuss simpler non-RL alternatives to provide motivation for adopting RL-based
fine-tuning.


**1.2.1** **Brief Overview: Fine-tuning with RL**


In this article, we explore the fine-tuning of pre-trained diffusion models to optimize downstream
reward functions _r_ : R _[d]_ _→_ R. In domains such as images, these backbone diffusion models to be
fine-tuned include Stable Diffusion (Rombach et al., 2022), while the reward functions are aesthetic
scores and alignment scores (Clark et al., 2023; Black et al., 2023; Fan et al., 2023). More examples
are detailed in the introduction. These rewards are often unknown, necessitating learning from
data with feedback: _{x_ [(] _[i]_ [)] _, r_ ( _x_ [(] _[i]_ [)] ) _}_ . We will explore this aspect further in Section 7. Until then, we
assume _r_ is known.

Now, readers may wonder about the objectives we aim to achieve during the fine-tuning process.
A natural approach is to define the optimization problem:


argmax E _x∼q_ [ _r_ ( _x_ )] (8)
_q∈_ ∆( _X_ )


where _q_ is initialized with a pre-trained diffusion model _p_ [pre] _∈_ ∆( _X_ ). In this tutorial, we will detail
the procedure of solving (8) with RL in the upcoming sections. In essence, we leverage the fact
that diffusion models are formulated as a sequential decision-making problem, where each decision
corresponds to how samples are denoised.
Although the above objective function (8) is reasonable, the resulting distribution might deviate
too much from the pre-trained diffusion model. To circumvent this issue, a natural way is to add
penalization against pre-trained diffusion models. Then, the target distribution is defined as:


argmax E _x∼q_ [ _r_ ( _x_ )] _−_ _α_ KL( _q∥p_ [pre] ) _._ (9)
_q∈_ ∆( _X_ )


Notably, (9) reduces to the following distribution:


_pr_ ( _·_ ) := exp( _r_ ( _·_ ) _/α_ ) _p_ [pre] ( _·_ ) (10)
~~�~~ exp( _r_ ( _x_ ) _/α_ ) _p_ [pre] ( _x_ ) _dx_ _[.]_


Here, the first term in (9) corresponds to the mean reward, which we want to optimize in the
fine-tuning process. The second term in (10) serves as a penalty term, indicating the deviation of _q_
from the pre-trained model. The parameter _α_ controls the strength of this regularization term. The
proper choice of _α_ depends on the task we are interested in.


8




**1.2.2** **Motivation for Using RL over Non-RL Alternatives**


To achieve our goal of maximizing downstream reward functions with diffusion models, readers
may question whether alternative approaches can be employed apart from RL. Here, we investigate
these potential alternatives and explain why RL approaches may offer advantages over them.


**Rejection sampling.** One approach involves generating multiple samples from pre-trained diffusion models and selecting only those with high rewards. This method, called rejection sampling,
operates without needing fine-tuning. However, rejection sampling is effective primarily when the
pre-trained model already has a high probability of producing high-reward samples. It resembles
sampling from a prior distribution to obtain posterior samples (in this case, high-reward points).
This approach works efficiently when the posterior closely matches the prior but can become highly
inefficient otherwise. In contrast, by explicitly updating weight in diffusion models, RL-based finetuning allows us to obtain these high-reward samples, which are seldom generated by pre-trained
models.


**Conditional diffusion models (classifier-free guidance).** In conditional generative models, the
general goal is to sample from _p_ ( _x|c_ ), where _x_ is the output and _c_ denotes the conditioning variable.
For example, in text-to-language diffusion models, _c_ is a text, and _x_ is the generated image. Similarly,
in the context of protein engineering for addressing inverse folding problems, models often define _c_
as the protein backbone structure and _x_ as the corresponding amino acid sequence. Here, using the
training data _{c_ [(] _[i]_ [)] _, x_ [(] _[i]_ [)] _}_, the model is trained by using the loss function:



argmin
_θ_



� E _t∼_ Uni([0 _,T_ ]) _,x_ ( _ti_ ) _[∼][µ][⋄]_ _t_ _[x]_ [(] 0 _[i]_ [)][+] _[ϵσ]_ _t_ _[⋄][,,ϵ][∼N]_ [(0] _[,]_ [I][)][[] _[∥][ϵ][ −]_ _[ϵ][θ]_ [(] _[x]_ _t_ [(] _[i]_ [)] _[, c]_ [(] _[i]_ [)] _[, t]_ [)] _[∥]_ [2][]] _[,]_

_i_



where the denoising function _ϵθ_ additionally receives the conditioning information _c_ [(] _[i]_ [)] as input. In
practice, a variety of improvements such as classifier-free guidance (Ho and Salimans, 2022) can
further improve the model’s ability to learn the conditional distribution _p_ ( _x|c_ ).
These conditional generative models can be used to optimize down-stream rewards by conditioning on the reward values, then sampling _x_ conditioned on high reward values (Krishnamoorthy
et al., 2023; Yuan et al., 2023). While this method is, in principle, capable of generating plausible
_x_ values across a range of reward levels within the training data distribution, it is not the most
effective optimization strategy. This is primarily because high-reward inputs frequently reside
in the tails of the training distribution or even beyond it. Consequently, this method may not
effectively generate high-reward samples that lie outside the training data distribution. In contrast,
RL-based fine-tuning has the capability to generate samples with higher rewards beyond the training
data. This is achieved by explicitly maximizing reward models learned from the training data
and leveraging their extrapolative capabilities of reward models, as theoretically formalized and
empirically observed in Uehara et al. (2024).


**Reward-weighted training.** Another alternative approach is to use a reward-weighted version
of the standard training loss for diffusion models. Suppose that we have data _{x_ [(] _[i]_ [)] _, r_ ( _x_ [(] _[i]_ [)] ) _}_ . Then,
after learning a reward ˆ _r_ : _X →_ R with regression from the data, to achieve our goal, it looks


9




natural to use a reward-weighted version of the training loss for diffusion models (5), i.e.,



argmin
_θ_



� E _t∼_ Uni([0 _,T_ ]) _,x_ ( _ti_ ) _[∼][µ][⋄]_ _t_ _[x]_ [(] 0 _[i]_ [)][+] _[ϵσ]_ _t_ _[⋄][,ϵ][∼N]_ [(0] _[,]_ [I][)][[ˆ] _[r]_ [(] _[x]_ 0 [(] _[i]_ [)][)] _[∥][ϵ][ −]_ _[ϵ][θ]_ [(] _[x]_ _t_ [(] _[i]_ [)] _[, t]_ [)] _[∥]_ [2][]] _[.]_

_i_



There are two potential drawbacks to this approach. First, in practice, it may struggle to generate
samples with higher rewards beyond the training data. As we will explain later, many RL algorithms
are more directly focused on optimizing reward functions, which are expected to excel in obtaining
samples with high rewards not observed in the original data, as empirically observed in Black et al.
(2023). Second, when fine-tuning a conditional diffusion model _p_ ( _x|c_ ), the alternative approach
here requires a pair of _{c_ [(] _[i]_ [)] _, x_ [(] _[i]_ [)] _}_ during fine-tuning to ensure the validity of the loss function. When
we only have data _{x_ [(] _[i]_ [)] _, r_ ( _x_ [(] _[i]_ [)] ) _}_ but not _{c_ [(] _[i]_ [)] _, x_ [(] _[i]_ [)] _, r_ ( _x_ [(] _[i]_ [)] ) _}_, this implies that we might need to solve
an inverse problem from _x_ to _c_, which can often be challenging. In contrast, in these scenarios, RL
algorithms, which we will introduce later, can operate without needing such pairs _{c_ [(] _[i]_ [)] _, x_ [(] _[i]_ [)] _}_, as
long as we have learned reward functions ˆ _r_ .
Finally, it should be noted that reward-weighted training technically falls under the broader
category of RL methods. It shares a close connection with “reward-weighted MLE” introduced in
Section 6.1, as discussed later. Employing this reward-weighted MLE helps address the second
concern of “reward-weighted training” mentioned earlier.

### **2 Brief Overview of Entropy-Regularized MDPs**


In this tutorial, we explain how fine-tuning diffusion models can be naturally formulated as an RL
problem in entropy-regularized MDPs. This perspective is natural because RL involves sequential
decision-making, and a diffusion model is formulated as a sequential problem where each denoising
step is a decision-making process. To connect diffusion models with RL, we begin with a concise
overview of RL in standard entropy-regularized MDPs (Haarnoja et al., 2017; Neu et al., 2017;
Geist et al., 2019; Schulman et al., 2017).

#### **2.1 MDPs**


An MDP is defined as follows: _{S, A, {Pt_ [tra] _}_ _[T]_ _t_ =0 _[,][ {][r][t][}]_ _t_ _[T]_ =0 _[, p]_ [0] _[}]_ [ where] _[ S]_ [ is the state space,] _[ A]_ [ is the]
action space, _Pt_ [tra] is a transition dynamic mapping: _S × A →_ ∆( _S_ ), _rt_ : _S × A →_ R denotes
reward received at _t_ and _p_ 0 is an initial distribution over _S_ . A policy _πt_ : _S →_ ∆( _A_ ) is a map from
any state _s ∈S_ to the distribution over actions. The standard goal in RL is to solve



�



argmax E _{πt}_
_{πt}_



_T_
�
� _t_ =0



_rt_ ( _st, at_ )

_t_ =0



(11)



where E _{πt}_ [ _·_ ] is the expectation induced both policy _π_ and the transition dynamics as follows:
_s_ 0 _∼_ _p_ 0 _, a_ 0 _∼_ _π_ 0( _·|s_ 0) _, s_ 1 _∼_ _P_ 0 [tra][(] _[·|][s]_ [0] _[, a]_ [0][)] _[,][ · · ·]_ [ .] As we will soon detail in the next section
(Section 3), diffusion models can naturally be framed as MDPs as each policy corresponds to a
denoising process in diffusion models.


10




In entropy-regularized MDPs, we consider the following regularized objective instead:



�



_{πt_ _[⋆][}]_ [ = argmax] E _{πt}_
_{πt}_



_T_
�
� _t_ =0



_rt_ ( _st, at_ ) _−_ _α_ KL( _πt_ ( _·|st_ ) _, πt_ _[′]_ [(] _[·|][s][t]_ [))]

_t_ =0



(12)



where _π_ _[′]_ : _S →_ ∆( _A_ ) is a certain reference policy. The arg max solution is often called a set of soft
optimal policies. Compared to a standard objective (11), here we add KL terms against reference
policies. This addition aims to ensure that soft optimal policies closely align with the reference
policies. In the context of fine-tuning diffusion models, these reference policies correspond to
the pre-trained diffusion models, as we aim to maintain similarity between the fine-tuned and
pre-trained models.
This entropy-regularized objective in (12) has been widely employed in RL literature due to
several benefits (Levine, 2018). For instance, in online RL, it is known that these policies have
good exploration properties by setting reference policies as uniform policies (Fox et al., 2015;
Haarnoja et al., 2017). In offline RL, Wu et al. (2019) suggests using these policies as conservative
policies by setting reference policies close to behavior policies (policies used to collect offline data).
Additionally, in inverse RL, this soft optimal policy is used as an expert policy in scenarios where
rewards are unobservable, only trajectories from expert policies are available (typically referred to
as maximum entropy RL as Ziebart et al. (2008); Wulfmeier et al. (2015); Finn et al. (2016)).

#### **2.2 Key Concepts: Soft Q-functions, Soft Bellman Equations.**


The crucial question in RL is how to devise algorithms that effectively solve the optimization
problem (12). These algorithms are later used as fine-tuning algorithms of diffusion models. To see
these algorithms, we rely on several critical concepts in entropy-regularized MDPs. Specifically,
soft-optimal policies (i.e., solutions to (12)) can be expressed analytically as a blend of soft Qfunctions and reference policies. Furthermore, these soft Q-functions are defined as solutions to
equations known as soft Bellman equations. We elaborate on these foundational concepts below.


**Soft Q-functions and soft optimal policies.** Soft optimal policies are expressed as a blend of soft
Q-functions and reference policies. To see it, we define the soft Q-function as follows:



�



_qt_ ( _st, at_ ) = E _{πt_ _[⋆][}]_



_T_
� _rk_ ( _sk, ak_ ) _−_ _α_ KL( _πk_ _[⋆]_ +1 [(] _[·|][s][k]_ [+1][)] _[∥][π]_ _k_ _[′]_ +1 [(] _[·|][s][k]_ [+1][))] _[|][s][t][, a][t]_
� _k_ = _t_



_._ (13)



Then, by comparing (13) and (12), we clearly have


_πt_ _[⋆]_ [=] argmax E _at∼π_ ( _st_ )[ _qt_ ( _st, at_ ) _−_ _α_ KL( _π_ ( _·|st_ ) _∥πt_ _[′]_ [(] _[· |][ s][t]_ [)] _[|][s][t]_ []] _[.]_ (14)
_π∈_ [ _X→_ ∆( _X_ )]


Hence, by calculating the above explicitly, a soft optimal policy in (12) is described as follows:


exp( _qt_ ( _s, ·_ ) _/α_ ) _πt_ _[′]_ [(] _[·][|][s]_ [)]
_πt_ _[⋆]_ [(] _[·|][s]_ [)] _[ ∝]_ (15)
~~�~~ exp( _qt_ ( _s, a_ ) _/α_ ) _πt_ _[′]_ [(] _[a][|][s]_ [)d] _[a]_


11




**Soft Bellman equations.** We have already defined soft Q-functions in (13). However, this form
includes the soft optimal policies. Actually, without using soft optimal policies, the soft Q-function
satisfies the following recursive equation (a.k.a. soft Bellman equation):



_._ (16)
�



_qt_ ( _st, at_ ) = E _{πt_ _[⋆][}]_



_r_ ( _st, at_ ) + _α_ log exp( _qt_ +1( _st_ +1 _, a_ ) _/α_ ) _πt_ _[′]_ [(] _[a][|][s][t]_ [+1][)d] _[a]_ _| st, at_
� �� �



This is proven by noting we recursively have


_qt_ ( _st, at_ ) = E _{πt_ _[⋆][}]_ [[] _[r][t]_ [(] _[s][t][, a][t]_ [) +] _[ q][t]_ [+1][(] _[s][t]_ [+1] _[, a][t]_ [+1][)] _[ −]_ _[α]_ [KL(] _[π]_ _t_ _[⋆]_ +1 [(] _[·|][s][t]_ [+1][)] _[, π]_ _t_ _[′]_ +1 [(] _[·|][s][t]_ [+1][))] _[|][s][t][, a][t]_ []]


By substituting (15) into the above, we obtain the soft Bellman equation (16).


**Soft value functions.** So far, we have defined the soft Q-functions, which depend on both states
and actions. We can now introduce a related concept that depends solely on states, termed the soft
value function. The soft value function is defined as follows:



�



_vt_ ( _st_ ) = E _{πt_ _[⋆][}]_



_T_
� _rk_ ( _sk, ak_ ) _−_ _α_ KL( _πk_ _[⋆]_ [(] _[·|][s][k]_ [)] _[∥][π]_ _k_ _[′]_ [(] _[·|][s][k]_ [))] _[|][s][t]_
� _k_ = _t_



_._



Then, the soft optimal policy in (14) is also written as


_t_ [(] _[·][|][s]_ [)]
_πt_ _[⋆]_ [(] _[·|][s]_ [)] _[ ∝]_ [ex][p(] _[q][t]_ [(] _[s][,][ ·]_ [)] _[/][α]_ [)] _[π][′]_ (17)
exp( _vt_ ( _s_ ) _/α_ )


because we have



_πt_ _[′]_ [(] _[a][ |][ s]_ [)d] _[a.]_
�



_vt_ ( _s_ )
exp

_α_

�



_qt_ ( _s, a_ )
= exp
� � � _α_



Then, substituting the above in the soft Bellman equation (16), it is written as


_qt_ ( _st, at_ ) = E _{πt_ _[⋆][}]_ [[] _[r]_ [(] _[s][t][, a][t]_ [) +] _[ v][t]_ [+1][(] _[s][t]_ [+1][)] _[|][s][t][, a][t]_ []] _[.]_


**Algorithms in entropy-regularized MDPs.** As outlined in Levine (2018), to solve (12), various
well-known algorithms exist in the literature on RL. The abovementioned concepts are useful in
constructing these algorithms. These include policy gradients, which gradually optimize a policy
using a policy neural network; soft Q-learning algorithms, which utilize the soft-Bellman equation
and approximate the soft-value function with a value neural network; and soft actor-critic algorithms
that leverage both policy and value neural networks. We will explore how these algorithms can be
applied in the context of diffusion models shortly in Section 4.2 and 6.

### **3 Fine-Tuning Diffusion Models with RL in Entropy Regular-** **ized MDPs**


In this section, as done in Fan et al. (2023); Black et al. (2023); Uehara et al. (2024), we illustrate
how fine-tuning can be formulated as an RL problem in soft-entropy regularized MDPs, where each


12




![](images/2407.13734v1.pdf-12-0.png)

Figure 2: Formulating fine-tuning in diffusion models using MDPs.


denoising step of diffusion models corresponds to a policy in RL. Finally, we outline a specific RL
problem of interest in our context.
To cast fine-tuning diffusion models as an RL problem, we start with defining the following
MDP:


  - The state space _S_ and action space _A_ correspond to the input space _X_ .


  - The transition dynamics at time _t_ (i.e., _Pt_ ) is an identity map _δ_ ( _st_ +1 = _at_ ).


  - The reward at time _t ∈_ [0 _, · · ·, T_ ] (i.e., _rt_ ) is provided only at _T_ as _r_ (down-stream reward
function); but 0 at other time steps.


  - The policy at time _t_ (i.e, _πt_ ) corresponds to _pT_ +1 _−t_ : _X →_ ∆( _X_ ).


  - The initial distribution at time 0 corresponds to _pT_ +1 _∈_ ∆( _X_ ). With slight abuse of notation,
we often denote it by _pT_ +1( _·|·_ ), while this is just _pT_ +1( _·_ ).


  - The reference policy at _t_ (i.e., _πt_ _[′]_ [) corresponds to a denoising process in the pre-trained model]
_p_ [pre] _T_ +1 _−t_ [.]


We list several things to note.


  - We reverse the time-evolving process to adhere to the standard notation in diffusion models,
i.e., from _t_ = _T_ to _t_ = 0. Hence, _st_ in standard MDPs corresponds to _xT_ +1 _−t_ in diffusion
models.


  - In our context, unlike standard RL scenarios, the transition dynamics are known.


**Key RL Problem.** Now, by reformulating the original objective of standard RL into our contexts,
the objective function in (12) reduces to the following:



_{p_ _[⋆]_ _t_ _[}][t]_ [=] argmax E _{pt}_ [ _r_ ( _x_ 0)]
_{pt∈_ [R _[d]_ _→_ ∆(R _[d]_ )] _}_ [1] _t_ = _T_ +1 � ~~�~~ � ~~�~~
Reward



_−α_ Σ [1] _t_ = _T_ +1 [E] _[{][p]_ _t_ _[}]_ [[KL(] _[p][t]_ [(] _[·|][x][t]_ [)] _[∥][p]_ _t_ [pre][(] _[·|][x][t]_ [))]]
� ~~�~~ � ~~�~~
KL penalty



(18)



where the expectation E _{pt}_ [ _·_ ] is taken with respect to [�][1] _t_ = _T_ +1 _[p][t]_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)][, i.e.,] _[ x][T][ ∼]_ _[p][T]_ [+1][(] _[·]_ [)] _[, x][T]_ _[−]_ [1] _[ ∼]_
_pT_ _−_ 1( _· | xT_ _−_ 1) _, xT_ _−_ 2 _∼_ _pT_ _−_ 2( _· | xT_ _−_ 2) _, · · ·_ . In this article, we set this as an objective function in
fine-tuning diffusion models. This objective is natural as it seeks to optimize sequential denoising
processes to maximize downstream rewards while maintaining proximity to pre-trained models.
Subsequently, we investigate several algorithms to solve (18). Before discussing these algorithms,
we summarize several key theoretical properties that will aid their derivation.


13




### **4 Theory of RL-Based Fine-Tuning**

So far, we have introduced a certain RL problem (i.e., (18)) as a fine-tuning diffusion model. In
this section, we explain that solving this RL problem allows us to achieve the target distribution
discussed in Section 1.2.1. Additionally, we present several important theoretical properties, such
as the analytical form of marginal distributions and posterior distributions induced by fine-tuned
models. This formulation is also instrumental in introducing several algorithms (reward-weighted
MLE, value-weighted sampling, and path consistency learning in Section 6), and establishing
connections with related areas (classifier guidance in Section 8, and flow-based diffusion models in
Section 9). We start with several key concepts.

#### **4.1 Key Concepts: Soft Value functions and Soft Bellman Equations.**


Now, reflecting on how soft optimal policies are expressed using soft value functions in Section 2 in
the context of standard RL problems, we derive several important concepts applicable to fine-tuning
diffusion models. These concepts are later useful in constructing algorithms to solve our RL problem
(18).
Firstly, as we see in (15), soft-optimal policies are characterized as:


exp( _vt−_ 1( _·_ ) _/α_ ) _p_ [pre] _t_ [(] _[· ][|][ x][t]_ [)]
_p_ _[⋆]_ _t_ [(] _[·|][x][t]_ [) =] (19)
~~�~~ exp( _vt−_ 1( _xt−_ 1) _/α_ ) _p_ ~~[pre]~~ _t_ [(] _[x][t][−]_ [1] _[|][ x][t]_ [)] _[dx][t][−]_ [1]


where soft-value functions are defined as



_vt_ ( _xt_ ) = E _{p_ _[⋆]_ _t_ _[}]_ [[] _[r]_ [(] _[x]_ [0][)] _[ −]_ _[α]_


_qt_ ( _xt, xt−_ 1) = E _{p_ _[⋆]_ _t_ _[}]_ [[] _[r]_ [(] _[x]_ [0][)] _[ −]_ _[α]_



1
� KL( _pk_ ( _·|xk_ ) _∥p_ [pre] _k_ [(] _[·|][x][k]_ [))] _[|][x][t]_ []] _[,]_


_k_ = _t_


1
� KL( _pk_ ( _·|xk_ ) _∥p_ [pre] _k_ [(] _[·|][x][k]_ [))] _[|][x][t][, x][t][−]_ [1][] =] _[ v][t][−]_ [1][(] _[x][t][−]_ [1][)] _[.]_


_k_ = _t_ +1



Secondly, as we see in (16), the soft-value functions are also recursively defined by the soft
Bellman equations:

_vt_ ( _xt_ ) = _vt−_ 1( _xt−_ 1)
exp _α_ � exp _α_ _p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][ x][t]_ [)] _[dx][t][−]_ [1] [(] _[t]_ [ =] _[ T]_ [ + 1] _[,][ · · ·][,]_ [ 1)] _[,]_
� � � �

(20)

� _v_ 0( _x_ 0) = _r_ ( _x_ 0) _._



_αxt_ ) = � exp _vt−_ 1( _αxt−_ 1)
� �



_αxt−_ 1) _p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][ x][t]_ [)] _[dx][t][−]_ [1] [(] _[t]_ [ =] _[ T]_ [ + 1] _[,][ · · ·][,]_ [ 1)] _[,]_
�



_α_ _α_ (20)

_v_ 0( _x_ 0) = _r_ ( _x_ 0) _._



Now substituting the above in (19), we obtain


_t_ [(] _[· ][|][ x][t]_ [)]
_p_ _[⋆]_ _t_ [(] _[·|][x][t]_ [) = ex][p(] _[v][t][−]_ [1][(] _[·]_ [)] _[/][α]_ [)] _[p]_ [pre] _._
exp( _vt_ ( _xt_ ) _/α_ )


As mentioned earlier, these soft value functions and their recursive form will later serve as the
basis for constructing several concrete fine-tuning algorithms (such as reward-weighted MLE and
value-weighted sampling in Section 6).


14




#### **4.2 Induced Distributions after Fine-Tuning**

Now, with the above preparation, we can show that the induced distribution derived by this soft
optimal policy is actually equal to our target distribution.


**Theorem 1** (Theorem 1 in Uehara et al. (2024)) **.** _Let p_ _[⋆]_ ( _·_ ) _be an induced distribution at time_ 0
_from optimal policies {p_ _[⋆]_ _t_ _[}]_ [1] _t_ = _T_ +1 _[, i.e.,][ p][⋆]_ [(] _[x]_ [0][) =] � _{_ [�][1] _t_ = _T_ +1 _[p]_ _t_ _[⋆]_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)] _[}][dx]_ [1:] _[T]_ _[. The distribution][ p][⋆]_

_is equal to the target distribution_ (10) _, i.e.,_


_p_ _[⋆]_ ( _x_ ) = _pr_ ( _x_ ) _._


This theorem states that after solving (18) and obtaining soft optimal policies, we can sample
from the target distribution by sequentially running policies from _p_ _[⋆]_ _T_ +1 [to] _[ p]_ 1 _[⋆]_ [. Thus,][ (][18][)][ serves as a]
natural objective for fine-tuning. This fact is also useful in deriving a connection with classifier
guidance in Section 8.


**Marginal distributions.** We can derive the marginal distribution as follows.


**Theorem 2** (Theorem 2 in Uehara et al. (2024) ) **.** _Let p_ _[⋆]_ _t_ [(] _[x][t]_ [)] _[ be the marginal distributions at][ t]_
_induced by soft-optimal policies {p_ _[⋆]_ _t_ _[}]_ [1] _t_ = _T_ +1 _[, i.e.,][ p]_ _t_ _[⋆]_ [(] _[x][t]_ [) =] � _{_ [�] _[t]_ _k_ [+1] = _T_ +1 _[p]_ _k_ _[⋆]_ [(] _[x][k][−]_ [1] _[|][x][k]_ [)] _[}][dx][t]_ [+1:] _[T]_ _[. Then,]_


_t_ [(] _[x][t]_ [)]
_p_ _[⋆]_ _t_ [(] _[x][t]_ [) =] [ex][p(] _[v][t]_ [(] _[x][t]_ [)] _[/][α]_ [)] _[p]_ [pre] _,_
_C_


_where vt_ ( _·_ ) _is the soft-value function._


Interestingly, the normalizing constant is independent of _t_ in the above theorem.


**Posterior distributions.** We can derive the posterior distribution as follows.


**Theorem 3** (Theorem 3 in Uehara et al. (2024)) **.** _Denote the posterior distribution of xt given_
_xt−_ 1 _for the distribution induced by soft-optimal policies {p_ _[⋆]_ _t_ _[}]_ [1] _t_ = _T_ +1 _[by][ {][p][⋆][}][b]_ [(] _[· | ·]_ [)] _[. We define the]_
_analogous objective for a pre-trained policy and denote it by p_ [pre] _t_ [(] _[· | ·]_ [)] _[. Then, we get]_


_{p_ _[⋆]_ _}_ _[b]_ _t_ [(] _[x][t][|][x][t][−]_ [1][) =] _[ p]_ _t_ [pre][(] _[x][t][|][x][t][−]_ [1][)] _[.]_


This theorem indicates that after solving (18), the posterior distribution induced by pre-trained
models remains preserved. This property plays an important role in constructing PCL (path
consistency learning) in Section 6.3.


**Remark 4** (Continuous-time formulation) **.** _For simplicity, our explanation is generally based on_
_the discrete-time formulation. However, as training of diffusion models could be formulated in_
_the continuous-time formulation (Song et al., 2021), we can still extend most of our discussion of_
_fine-tuning in our tutorial in the continuous-time formulation. For example, the above Theorems are_
_extended to the continuous time formulation in Uehara et al. (2024)._


15




Table 1: Description of each RL algorithm for fine-tuning diffusion models (note that valueweighted sampling is technically not a fine-tuning algorithm.) Note (1) “Without learning value
functions” refers to the capability of algorithms to directly utilize non-differentiable black-box
reward feedback, bypassing the necessity to train differentiable reward functions, (2) “Distributionconstrained” indicates that the algorithms are designed to maintain proximity to pre-trained models.
Based on it, the practical recommendation of algorithms is summarized in Figure 3.


Memory Computational Without learning Distributionefficiency efficiency value functions constrained


Soft PPO ✓ ✓


Reward backpropagation ✓ ✓


Reward-weighted MLE ✓ ✓ ✓


Value-weighted sampling ✓ ✓


Path consistency learning ✓ ✓

### **5 RL-Based Fine-Tuning Algorithms 1: Non-Distribution-Constrained** **Approaches**


So far, we have explained how to frame fine-tuning diffusion models as the RL problem in
entropy-regularized MDPs. Moving forward, we summarize actual algorithms that can solve the RL
problem of interest described by Equation (18). In this section, we introduce two algorithms: PPO
and direct reward backpropagation.
These algorithms are originally designed to optimize reward functions directly, meaning they
operate effectively even without entropy regularization (i.e., _α_ = 0). Consequently, they are wellsuited for generating samples with high rewards that may not be in the original training dataset. More
distribution-constrained algorithms that align closely with pre-trained models will be discussed in
the subsequent section (Section 4.2). Therefore, we classify algorithms in this section (i.e., PPO and
direct reward backpropagation) as non-distribution-constrained approaches. The whole summary is
described in Table 1.

#### **5.1 Soft Proximal Policy Optimization (PPO)**


In order to solve Equation (18), Fan et al. (2023); Clark et al. (2023) propose using PPO (Schulman
et al., 2017). PPO has been widely used in RL, as well as, in the literature in fine-tuning LLMs,
due to its stability and simplicity. In the standard context of RL, this is especially preferred over
Q-learning when the action space is high-dimensional.
The PPO algorithm is described in Algorithm 1. This is an iterative procedure of updating
a parameter _θ_ . Each iteration comprises two steps: firstly, samples are generated by executing


16




**Algorithm 1** Soft PPO

1: **Require** : Pre-trained model _{N_ ( _ρ_ ( _xt, t_ ; _θ_ pre) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1 [, batch size] _[ m]_ [, parameter] _[ α][ ∈]_ [R][+][,]
learning rate _γ_
2: **Initialize** : _θ_ 1 = _θ_ pre
3: **for** _s ∈_ [1 _, · · ·, S_ ] **do**

4: Collect _m_ samples _{x_ [(] _t_ _[i]_ [)][(] _[θ]_ [)] _[}]_ [0] _t_ = _T_ [from a current diffusion model (i.e., generating by sequen-]
tially running polices _{N_ ( _ρ_ ( _xt, t_ ; _θ_ ) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1 [from] _[ t]_ [ =] _[ T]_ [ + 1][ to] _[ t]_ [ = 1][)]
5: Update as follows (several times if needed):



_−_ _,_ (21)

_p_ ( _x_ [(] _t−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][;] _[ θ][s]_ [)]



�



_θs_ +1 _←_ _θs −_ _[γ]_

_m_ _[∇][θ]_



1
�

_t_ = _T_ +1



_m_
�


_i_ =1



min



�



_r_ ˜ _t_ ( _x_ [(] 0 _[i]_ [)] _[, x]_ _t_ [(] _[i]_ [)][)] _[p]_ [(] _[x]_ _t_ [(] _−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][;] _[ θ]_ [)]



���



_r_ ˜ _t_ ( _x_ [(] 0 _[i]_ [)] _[, x]_ _t_ [(] _[i]_ [)][)] _[ ·]_ [ Clip]



_p_ ( _x_ [(] _t−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][;] _[ θ]_ [)]
� _p_ ( _x_ [(] _t−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][;] _[ θ][s]_



_−_ _,_ 1 _−_ _ϵ,_ 1 + _ϵ_

_p_ ( _x_ [(] _t−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][;] _[ θ][s]_ [)]



_|θ_ = _θs,_



where



˜
_rt_ ( _x_ 0 _, xt_ ) := _−r_ ( _x_ 0) + _α_ _[∥][ρ]_ [(] _[x][t][,][ t]_ [;] _[ θ]_ [)] _[ −]_ _[ρ]_ [(] _[x][t][,][ t]_ [;] _[ θ]_ [pre][)] _[∥]_ [2]

2 _σ_ [2] ( _t_ )
� ~~��~~ ~~�~~
KL term



_._



6: **end for**
7: **Output** : Policy _{pt_ ( _· | ·_ ; _θS_ ) _}_ [1] _t_ = _T_ +1


current policies to construct the loss function (inspired by policy gradient formulation); secondly,
the parameter _θ_ is updated by computing the gradient of the loss function.
PPO offers several advantages. The approach is known for its stability and relatively straightforward implementation. Stability comes from the conservative parameter updates. Indeed, PPO
builds upon TRPO (Schulman et al., 2015), where parameters are conservatively updated with a KL
penalty term (between _θs_ +1 and _θs_ ) to prevent significant deviation from the current parameter. This
gives us stability in the optimization landscape. Furthermore, in Algorithm 1, we do not necessarily
need to rely on value functions, although they could be useful for variance reduction. As discussed
in the subsequent subsection, this can be advantageous compared to other methods, especially since
learning value functions can be challenging in high-dimensional spaces, particularly within the
context of diffusion models.

#### **5.2 Direct Reward Backpropagation**


Another standard approach is a differentiable optimization (Clark et al., 2023; Prabhudesai et al.,
2023; Uehara et al., 2024), where gradients are directly propagated from reward functions to update
policies.
The entire algorithm is detailed in Algorithm 2. This reward backpropagation entails an iterative
process of updating a parameter _θ_ . Each iteration comprises two steps; firstly, samples are generated
by executing current policies to approximate the expectation in the loss function, which is directly
derived from (18); second, the current parameter _θ_ is updated by computing the gradient of the loss


17




**Algorithm 2** Reward backpropagation

1: **Require** : Pre-trained model _{N_ ( _ρ_ ( _xt, t_ ; _θ_ pre) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1 [, batch size] _[ m]_ [, parameter] _[ α][ ∈]_ [R][+][,]
learning rate _γ_
2: _Train a differentiable reward function (if reward feedback is not differentiable)_
3: **Initialize** : _θ_ 1 = _θ_ pre
4: **for** _s ∈_ [1 _, · · ·, S_ ] **do**

5: Collect _m_ samples _{x_ [(] _t_ _[i]_ [)][(] _[θ]_ [)] _[}]_ [0] _t_ = _T_ [from a current diffusion model (i.e., generating by sequen-]
tially running polices _{N_ ( _ρ_ ( _xt, t_ ; _θ_ ) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1 [from] _[ t]_ [ =] _[ T]_ [ + 1][ to] _[ t]_ [ = 1][)]
6: Update _θs_ to _θs_ +1:



��



_r_ ( _x_ [(] 0 _[i]_ [)][(] _[θ]_ [))] _[ −]_ _[α]_

�



1
�


_t_ = _T_ +1



_∥ρ_ ( _x_ [(] _t_ _[i]_ [)][(] _[θ]_ [)] _[,][ t]_ [;] _[ θ]_ [)] _[ −]_ _[ρ]_ [(] _[x]_ [(] _t_ _[i]_ [)][(] _[θ]_ [)] _[,][ t]_ [;] _[ θ]_ [p][re][)] _[∥]_ [2]

2 _σ_ [2] ( _t_ )



_θs_ +1 _←_ _θs_ + _γ_



1

_m_
�



_m_
�


_i_ =1



_|θ_ = _θs._



(22)


7: **end for**
8: **Output** : Policy _{pt_ ( _· | ·_ ; _θS_ ) _}_ [1] _t_ = _T_ +1


function.


**Advantages over PPO.** This approach offers further simplicity in implementation in a case where
we already have a pre-trained differentiable reward model. Furthermore, the training speed is much
faster since we are directly back-propagating from rewards.


**Potential disadvantages over PPO.** Reward backpropagation may face memory inefficiency
issues. However, there are strategies to mitigate this challenge. Firstly, implementing gradient
accumulation can help to keep a large batch size. Secondly, as proposed in DRaFT (Clark et al.,
2023), propagating rewards backward from time 0 to _k_ ( _k_ is an intermediate step smaller than _T_ )
and updating policies from _k_ to 0 can still yield high performance. Thirdly, drawing from insights in
the literature on neural SDE/ODE (Chen et al., 2018), more memory-efficient advanced techniques
such as adjoint methods could be helpful.
Another potential drawback is the requirement for “differentiable” reward functions. Often,
reward functions are obtained in a non-differentiable black-box way (e.g., computational feedback
derived from physical simulations). In such scenarios, using direct backpropagation necessitates
the learning of differentiable reward functions even if accurate reward feedback is available. This
learning step can pose challenges as it involves data collection and constructing suitable reward
models.

### **6 RL-Based Fine-Tuning Algorithms 2: Distribution-Constrained** **Approaches**


In this section, following Section 4.2, we present three additional algorithms (reward-weighted
MLE, value-weighted sampling, and path consistency learning) aimed at solving the RL problem


18




of interest defined by Equation (18). In the context of standard RL, these algorithms are tailored
to align closely with reference policies, specifically pre-trained diffusion models in our context.
Formally, indeed, all algorithms in this section are not well-defined when _α_ = 0 (i.e., without
entropy regularization). Hence, we categorize these three algorithms as distribution-constrained
approaches.
The algorithms in this section excel in preserving the characteristics of pre-trained diffusion
models. Practically, this property becomes especially crucial when reward functions are learned from
training data, and we want to avoid being fooled by distribution samples (a.k.a. overoptimization as
detailed in Section 7.3). However, as a caveat, this property might also pose challenges in effectively
generating high-reward samples beyond the training data. This implies that these approaches may
not be suitable when accurate reward feedback is readily available without learning. Hence, we
generally recommend using them when reward functions are unknown.

#### **6.1 Reward-Weighted MLE**


Here, we elucidate an approach based on reward-weighted MLE (Peters et al., 2010), a technique
commonly employed in offline RL (Peng et al., 2019). While Fan et al. (2023, Algorithm 2) and
Zhang and Xu (2023) propose variations of reward-weighted MLE for diffusion models, the specific
formulation of reward-weighted MLE discussed here does not seem to have been explicitly detailed
previously. Therefore, unlike the previous section, we start by outlining the detailed rationale for
this approach. Subsequently, we provide a comprehensive explanation of the algorithm. Finally, we
delve into its connection with the original training loss of diffusion models.


**Motivation.** First, from Theorem 2, recall the form of the optimal policy _p_ _[⋆]_ _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)][:]


_t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)]
_p_ _[⋆]_ _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [) :=] [ex][p(] _[v][t][−]_ [1][(] _[x][t][−]_ [1][)] _[/][α]_ [)] _[p]_ [pre] _._
exp( _vt_ ( _xt_ ) _/α_ )


Now, we have:


_p_ _[⋆]_ _t_ [= argmin] E _xt∼ut_ [KL( _p_ _[⋆]_ _t_ [(] _[·|][x][t]_ [)] _[∥][p][t]_ [(] _[·|][x][t]_ [))]]
_pt_ : _X→_ ∆( _X_ )


where _ut ∈_ ∆( _X_ ) is a roll-in distribution encompassing the entire space _X_ This can be reformulated
as value-weighted MLE as follows.


**Lemma 1** (Value-weighted MLE) **.** _When_ Π _t_ = [ _X →_ ∆( _X_ )] _, the policy p_ _[⋆]_ _t_ _[is equal to]_



log _pt_ ( _xt−_ 1 _|xt_ ) _._
� �



_p_ _[⋆]_ _t_ [(] _[·|·]_ [) = argmax] E _xt−_ 1 _∼p_ [pre] _t−_ 1 [(] _[·|][x][t]_ [)] _[,x][t][∼][u][t]_
_pt∈_ Π _t_



_vt−_ 1( _xt−_ 1)
exp

_α_

� �



This lemma illustrates that if _vt−_ 1 is known, _p_ _[⋆]_ _t_ [can be estimated using weighted maximum]
likelihood estimation (MLE). While this formulation is commonly used in standard RL (Peng et al.,
2019), in our context of fine-tuning diffusion models, learning a value function is often challenging.
Interestingly, this reward-weighted MLE can be performed without directly estimating the soft value
function after proper reformulation. To demonstrate this, let’s utilize the following lemma:


19




**Algorithm 3** Reward-weighed MLE

1: **Require** : Pre-trained model _{N_ ( _ρ_ ( _xt, t_ ; _θ_ pre) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1 [, batch size] _[ m]_ [, parameter] _[ α][ ∈]_ [R][+][,]
learning rate _γ_
2: **Initialize** : _θ_ 1 = _θ_ pre
3: **for** _s ∈_ [1 _, · · ·, S_ ] **do**
4: **for** _k ∈_ [ _T_ + 1 _, · · ·,_ 1] **do**

5: Collect _m_ samples _{x_ [(] _k_ _[i,t]_ [)] _}_ _[m,]_ _i_ =1 [0] _,k_ = _T_ [from]
a policy _pT_ +1( _· | ·_ ; _θs_ ) _, · · ·, pt_ +1( _·|·_ ; _θs_ ) _, p_ [pre] _t_ [(] _[·|·]_ [)] _[,][ · · ·][, p]_ [pre] 1 [(] _[·|·]_ [)][.]
6: **end for**

7: Update _θs_ to _θs_ +1 as follows:



�



�



_θs_ +1 _←_ _θs −_ _γ∇θ_



1
�


_t_ = _T_ +1



_m_
�


_i_ =1



exp



_r_ ( _x_ [(] 0 _[i,t]_ [)] ) _∥x_ [(] _t−_ _[i,t]_ 1 [)] _[−]_ _[ρ]_ [(] _[x]_ _t_ [(] _[i,t]_ [)] _, t_ ; _θ_ ) _∥_ 2 [2]
_α_ _{σt_ _[⋄][}]_ [2]
� �



_|θ_ = _θs._ (24)



8: **end for**
9: **Output** : Policy _{pt_ ( _· | ·_ ; _θS_ ) _}_ [1] _t_ = _T_ +1


**Lemma 2** (Characterization of soft optimal value functions) **.**



_vt_ ( _xt_ )
exp _α_ = E _x_ 0 _∼p_ [pre] 1 ( _x_ 1) _,···,xt−_ 1 _∼p_ [pre] _t−_ 1 [(] _[x][t]_ [)]
� �



_r_ ( _x_ 0)
exp

_α_

� �



_|xt_
�



_._
�



_Recall_ E _{p_ [pre] _t_ _}_ [[] _[·|][x]_ _t_ []] _[ means]_ [ E] _x_ 0 _∼p_ [pre] 1 ( _x_ 1) _,···,xt−_ 1 _∼p_ [pre] _t−_ 1 [(] _[x][t]_ [)][[] _[·|][x][t]_ []] _[.]_


_Proof._ This is obtained by recursively using the soft-Bellman equation (20):



_r_ ( _x_ 0)
exp

_α_

� �



_|xt_
�



_._
�



_vt_ ( _xt_ )
exp

_α_

�



_vt−_ 1( _xt−_ 1)
=
� � exp � _α_ � _p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][ x][t]_ [)] _[dx][t][−]_ [1] [=] _[ · · ·]_ [ =][ E] _{p_ [pre] _t_ _}_



(23)


**Algorithm.** Now, we are ready to present the algorithm. By combining Lemma 1 and Lemma 2,
we obtain the following.


**Lemma 3** (Reward-weighted MLE) **.** _When_ Π _t_ = [ _X →_ ∆( _X_ )] _,_



log _pt_ ( _xt−_ 1 _|xt_ ) _,_ (25)
� �



_p_ _[⋆]_ _t_ [= argmax] E _x_ 0 _∼p_ [pre] 1 ( _x_ 1) _,···,xt−_ 1 _∼p_ [pre] _t−_ 1 [(] _[x][t]_ [)] _[,x][t][∼][u][t]_
_pt∈_ Π _t_


_Proof._ Using Lemma 2, we have



_r_ ( _x_ 0)
exp

_α_

� �



log _pt_ ( _xt−_ 1 _|xt_ )
� �



E _xt−_ 1 _∼p_ [pre] _t−_ 1 [(] _[·|][x][t]_ [)] _[,x][t][∼][u][t]_



_vt−_ 1( _xt−_ 1)
exp

_α_

� �



_r_ ( _x_ 0)
exp

_α_

� �



_|xt−_ 1
�



log _pt_ ( _xt−_ 1 _|xt_ )
� �



= E _xt−_ 1 _∼p_ [pre] _t−_ 1 [(] _[·|][x][t]_ [)] _[,x][t][∼][u][t]_



E
_{p_ [pre] _t_ _}_
�



log _pt_ ( _xt−_ 1 _|xt_ ) _._
� �



= E _x_ 0 _∼p_ [pre] 1 ( _x_ 1) _,···,xt−_ 1 _∼p_ [pre] _t−_ 1 [(] _[x][t]_ [)] _[,x][t][∼][u][t]_



_r_ ( _x_ 0)
exp

_α_

� �



20




The rest of the proof is obvious by using Lemma 1.


Then, after approximating the expectation in (25), by using a Gaussian policy class with the
mean parameterized by neural networks as a policy class Π _t_, we can estimate _p_ _[⋆]_ _t_ [. Finally, the entire]
algorithm is described in Algorithm 3.
Here, we give two remarks. This is an off-policy algorithm. Hence, we can use any roll-in
policies as _ut_ in Lemma 3. In Algorithm 3, we use the current policy as a roll-in policy. Additionally,
in Algorithm 3, the loss function (24) is derived by recalling that, up to constant, _−_ log _pt_ ( _xt−_ 1 _|xt_ )

2
is equal to _[∥][x][t][−]_ [1] _[−]_ _{σ_ _[ρ]_ _t_ ~~_[⋄]_~~ [(] _[x][}]_ [2] _[t][,][t]_ [;] _[θ]_ [)] _[∥]_ [2] _._


**Advantages and potential disadvantages.** Like PPO in Section 5.1, this approach is expected to
be memory efficient, and does not require learning differentiable reward functions, which can often
be challenging. However, compared to direct reward backpropagation Section 5.2, it might not be
as computationally efficient. Furthermore, unlike PPO, this algorithm is not effective when _α_ = 0,
potentially limiting its ability to generate samples with extremely high rewards.


**6.1.1** **Relation with Loss Functions for the Original Training Objective**


In this subsection, we explore the connection with the original loss function of pre-trained diffusion
models. To see that, as we see in Section 1.1.1, recall



_x_ [(] _t−_ _[i,t]_ 1 [)] [=] _[ x]_ _t_ [(] _[i,t]_ [)] + [0 _._ 5 _xt −_ 1 _/σt_ _[⋄][ϵ]_ _θ_ [¯] pre [(] _[x][t][, T][ −]_ _[t]_ [)](] _[δt]_ [)]
� ~~�~~ � ~~�~~

_ρ_ ( _x_ [(] _t_ _[i,t]_ [)] _,t_ ; _θ_ pre)



+ _ϵ_ [(] _t_ _[i,t]_ [)] _σt_ _[⋄][, ϵ]_ [(] _[i,t]_ [)] _[ ∼N]_ [(0] _[,]_ [ I][)] _[.]_



Then, the loss function (24) in reward-weighted MLE reduces to



�����



2



2



_ϵ_ ( _ti,t_ ) _−_ _[ϵ]_ [(] _[x]_ _t_ [(] _[i,t]_ [)] _, T −_ _t_ ; _θ_ [¯] pre) _−_ _ϵ_ ( _x_ [(] _t_ _[i,t]_ [)] _, T −_ _t_ ; _θ_ )
_{σt_ _[⋄][}]_ [2]
������





exp



_r_ ( _x_ [(] 0 _[i,t]_ [)] )

_α_
�



_θs −_ _γ∇θ_



1
�


_t_ = _T_ +1



_m_
�


_i_ =1



 _|θ_ = _θs._





(26)


This objective function closely resembles the reward-weighted version of the loss function (5) used
for training pre-trained diffusion models.

#### **6.2 Value-Weighted Sampling**


Thus far, we have discussed methods for fine-tuning pre-trained diffusion models. Now, let’s delve
into an alternative approach during inference that aims to sample from the target distribution _pr_
without explicitly fine-tuning the diffusion models. In essence, this approach involves incorporating
gradients of value functions during inference alongside the denoising process in pre-trained diffusion
models. Hence, we refer to this approach as value-weighted sampling. While it seems that this
method has not been explicitly formalized in previous literature, the value-weighted sampling
closely connects with classifier guidance, as discussed in Section 8.1.
Before delving into the algorithmic details, we outline the motivation. Subsequently, we present
the concrete algorithm and discuss its advantages—specifically, its capability to operate without the
necessity of fine-tuning diffusion models—and its disadvantage, which involves the need to obtain
differentiable value functions.


21




**Algorithm 4** Value-weighted sampling


1: **Require** : Pre-trained model _{p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)] _[}][t]_ [=] _[ {N]_ [(] _[ρ]_ [(] _[x][t][, t]_ [;] _[ θ]_ [pre][)] _[, σ]_ [2][(] _[t]_ [))] _[}][t]_ [.]
2: Estimate _v_ : _X ×_ [0 _, T_ ] _→_ R and denote it by ˆ _v_ ( _·, ·_ )


    - (1) Monte-Carlo approach in (28)


    - (2) Value iteration approach (Soft Q-learning) in (29) in Section 6.2.1


    - (3) Approximation using Tweedie’s formula in Section 6.2.2


3: **for** _t ∈_ [ _T_ + 1 _, · · ·,_ 1] **do**
4: Set


˜
_ρ_ ( _xt, t_ ) := _[σ]_ [2][(] _[t]_ [)] _[∇][x][v]_ [ˆ][(] _[x][,][ t]_ [)] _[|][x]_ [=] _[x][t]_ + _ρ_ ( _xt, t_ ; _θ_ pre) _._

_α_


5: **end for**
6: **Output** : _{N_ (˜ _ρ_ ( _xt, t_ ) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1


**Motivation.** Considering a Gaussian policy _xt−_ 1 _∼N_ (˜ _ρ_ ( _xt, t_ ) _, σ_ [2] ( _t_ )), we aim to determine
_ρ_ ˜( _xt, t_ ; _θ_ ) such that _N_ (˜ _ρ_ ( _xt, t_ ; _θ_ ) _, σ_ [2] ( _t_ )) closely approximates _p_ _[⋆]_ _t_ [(] _[·|][x][t]_ [)][. Here, typically, we have]
_ρ_ ( _xt, t_ ; _θ_ ) = _xt_ + ( _δt_ )¯ _g_ ( _xt, t_ ) and _σ_ [2] ( _t_ ) = ˜ _g_ ( _t_ )( _δt_ ) for certain function ¯ _g_ : _X ×_ [0 _, T_ ] _→_ R _[d]_

and ˜ _g_ : [0 _, T_ ] _→_ R, as we have explained in Section 1.1.1. Then, using _∝_ as equality up to the
normalizing constant,



_−_ _[∥][x][t][−]_ [1] _[ −]_ _[x][t][ −]_ [(] _[δt]_ [)] _[g]_ [¯][(] _[x][t][,][ t]_ [)] _[∥]_ [2]
_p_ _[⋆]_ _t_ [(] _[x][t][−]_ [1] _[|][ x][t]_ [)] _[ ∝]_ [exp(] _[v][t][−]_ [1][(] _[x][t][−]_ [1][)] _[/α]_ [) exp]
� 0 _._ 5˜ _g_ ( _t_ )( _δt_ )



�



_≈_ exp( _vt_ ( _xt_ ) + _∇vt_ ( _xt_ ) _/α · {xt−_ 1 _−_ _xt}_ ) exp _−_ _[∥][x][t][−]_ [1] _[ −]_ _[x][t][ −]_ [(] _[δt]_ [)] _[g]_ [¯][(] _[x][t][,][ t]_ [)] _[∥]_ [2]
� 0 _._ 5˜ _g_ ( _t_ )( _δt_ )



�



_≈_ exp _−_ _[∥][x][t][−]_ [1] _[ −]_ _[x][t][ −]_ [(] _[δt]_ [)] _[g]_ [˜][(] _[t]_ [)] _[∇][v][t]_ [(] _[x][t]_ [)] _[/][α][ −]_ [(] _[δt]_ [)] _[g]_ [¯][(] _[x][t][,][ t]_ [))] _[∥]_ [2]
� 0 _._ 5˜ _g_ ( _t_ )( _δt_ )


Hence, we can approximate:



�



_ρ_ ˜( _xt, t_ ) _≈_ _[σ]_ [2][(] _[t]_ [)] _[∇][v][t]_ [(] _[x][t]_ [)] + _ρ_ ( _xt, t_ ; _θ_ pre) _._ (27)

_α_


**Remark 5.** _Note while the above derivation is heuristic, it is formalized in Uehara et al. (2024,_
_Lemma 2) in the continuous-time formulation._


**Algorithm.** Utilizing (27), the entire algorithm of value-weighted sampling is outlined in Algorithm 4. This method doesn’t involve updating parameters in a diffusion model; hence, it’s not a
fine-tuning method.
Note that in this algorithm, we require a form of _vt_ ( _·_ ) to compute gradients. This value function
can be estimated through regression using the characterization described in Lemma 2. Here, inspired


22




by Lemma 2, we use the following loss function:



exp



�



2 []


_,_ (28)

��











�




ˆ
_vt_ ( _x_ ) = argmin
_h_ :[R _[d]_ _,_ [0 _,T_ ]] _→_ R



1
�


_t_ = _T_ +1



_m_
�


_i_ =1



_ht_ ( _x_ [(] _t_ _[i,t]_ [)] )

_α_
�



_−_ exp



_r_ ( _x_ [(] 0 _[i,t]_ [)] )

_α_
�



where the data is collected in an off-policy way. Later, we explain two alternative approaches in
Section 6.2.1 and Section 6.2.2.


**Advantages and potential disadvantages.** The value-weighted sampling is straightforward and
less memory-intensive since it avoids fine-tuning. Therefore, it presents an appealing simple option
if it performs well. Indeed, in various inverse problems, such as inpainting, super-resolution, and
colorization, setting _r_ ( _x_ ) as a likelihood of the measurement model _y_ = _g_ ( _x_ )+ _ϵ_ (where _y_ represents
actual measurements, _ϵ_ denotes noise, and _g_ defines the measurement function) has proven highly
successful (Chung et al., 2022; Bansal et al., 2023; Chung et al., 2022).
The potential drawback compared to fine-tuning algorithms so far (i.e., PPO and rewardweighted MLE) is the necessity to learn differetiable soft value functions like direct reward backpropagation. This learning process is often not straightforward as explained in Section 5.2. The
previously discussed fine-tuning algorithms (i.e., PPO, reward-weighted MLE) circumvent this
requirement by obtaining rewards to go via Monte Carlo approaches, thereby avoiding the need for
soft value functions.


**6.2.1** **Soft Q-learning**


We have elucidated that leveraging Lemma 2, we can estimate soft value functions _vt_ ( _·_ ) based
on Equation (28) in a Monte Carlo way. Subsequently, these soft value functions are used in
value-weighted sampling to sample from the target distribution _pr_ . Alternatively, there is another
method that involves using soft Bellman equations to estimate soft value functions _vt_ ( _·_ ). This
technique is commonly called soft Q-learning in the context of standard RL.
First, recalling the soft Bellman equations in (16), we have



_p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][ x][t]_ [)] _[dx][t][−]_ [1] _[.]_
�



_vt_ ( _xt_ )
exp

_α_

�



_vt−_ 1( _xt−_ 1)
= exp
� � � _α_



Taking the logarithm, we obtain


_vt−_ 1( _xt−_ 1)
_vt_ ( _xt_ ) = _α_ log exp
� � _α_


Hence,



_p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][ x][t]_ [)] _[dx][t][−]_ [1] _[.]_
�



_p_ pre( _xt−_ 1 _|xt_ ) _dxt−_ 1
�



2 [�]
�



_vt_ = argmin E _xt∼ut_
_h_ : _X→_ R



_h_ ( _xt_ )


_α_

��



_xt_ ) _−_ log exp _vt−_ 1( _xt−_ 1)

_α_ � � _α_



_α_



where _ut ∈_ ∆( _X_ ) is any roll-in distribution that covers the entire space _X_ . Using this relation and
replacing the expectation E _xt∼ut_ with empirical approximation, we are able to estimate soft value


23




functions _vt_ in a recursive manner:



�



_p_ pre( _xt−_ 1 _|x_ [(] _t_ _[i,t]_ [)] ) _dxt−_ 1


(29)









�



ˆ
_vt_ _[⟨]_ _−_ _[j][−]_ 1 [1] _[⟩]_ [(] _[x][t][−]_ [1][)]


_α_

�



ˆ
_{vt_ _[⟨][j][⟩]_ [(] _[x]_ [)] _[}][t]_ _[←]_ argmin
_h_ :[R _[d]_ _,_ [0 _,T_ ]] _→_ R



1 _m_
� �


_t_ = _T_ +1 _i_ =1



_ht_ ( _x_ [(] _t_ _[i,t]_ [)] ) _−_ log exp
�



2 []


_,_

�





where the data is collected in an off-policy way.


**Remark 6.** _Although soft Q-learning is widely used in standard RL (Schulman et al., 2017), it_
_cannot be directly applied to our fine-tuning context without resorting to value-weighted sampling_
_or value-weighted MLE. This is because, even if we estimate soft-value functions as_ ˆ _vt, substituting_
_vt with_ ˆ _vt in the soft-optimal policy results in an unnormalized policy._


**6.2.2** **Approximation using Tweedie’s formula**


So far, we have explained two approaches: a Monte Carlo approach and a value iteration approach to
estimate soft value functions. However, learning value functions in (28) can still be often challenging
in practice. Therefore, we can employ approximation strategies inspired by recent literature on
classifier guidance (e.g., reconstruction guidance (Ho et al., 2022), manifold constrained gradients
(Chung et al., 2022), universal guidance (Bansal et al., 2023), and diffusion posterior sampling
(Chung et al., 2022)).
Specifically, we adopt the following approximation:



_|xt_
�



_r_ ( _x_ 0)
= _α_ log exp

_α_

� �� �



_p_ [pre] ( _x_ 0 _|xt_ ) _dx_ 0
�



(30)
�



_vt_ ( _xt_ ) = _α_ log E _{p_ [pre] _t_ _}_



_r_ ( _x_ 0)
exp

_α_

� �



ˆ
_r_ ( _x_ 0( _xt_ ))
_≈_ _α_ log exp

_α_

� �

= _r_ (ˆ _x_ 0( _xt_ )) _._



_,_ _x_ ˆ0( _xt_ ) = E _{p_ [pre] _t_ _}_ [[] _[x]_ 0 _[|][ x]_ _t_ []] _[,]_ (31)
��



Here, we replace the integration in (30) with a Dirac delta distribution with the posterior mean.
Importantly, we can calculate ˆ _x_ 0( _xt_ ) = E _p_ [pre] _t_ [[] _[x]_ [0] _[ |][ x][t]_ []][ using the pre-trained (score-based) diffusion]
model based on Tweedie’s formula:

E _p_ [pre] _t_ [[] _[x]_ [0] _[ |][ x][t]_ [] =] _[ x][t]_ [ +] _[{][σ]_ _t_ _[⋄][}]_ [2] _µ_ _[∇][⋄]_ _t_ [lo][g] _[q][t]_ [(] _[x][t]_ [)] _._


Recall that the notation _µ_ _[⋄]_ _t_ _[, σ]_ _t_ _[⋄][, q][t]_ [are defined in][ (3)][. Finally, by recalling] _[ ∇]_ [log] _[ q][t]_ _[≈]_ _[s]_ _θ_ [ˆ] pre [(] _[x][t][, t]_ [)][ in]
score-based diffusion models, we can approximate _∇vt_ ( _x_ ) with



�



_∇xtr_



_xt_ + _{σt_ _[⋄][}]_ [2] _[∇][s]_ _θ_ [ˆ] pre [(] _[x][t][, t]_ [)]

_µ_ _[⋄]_ _t_

�



_,_



and plug it into Algorithm 4 (i.e., value-weighted sampling).
As mentioned earlier, this approach has been practically widely used in the context of classifier
guidance. Despite its simplicity, the approximation error can be significant, as it can not be
diminished even with a large sample size because the discrepancy from (30) to (31) is not merely a
statistical error that diminishes with increasing sample size.


24




**6.2.3** **Zeroth-Order Guidance using Path Integral Control**


In the previous subsection (Section 6.2.2), we discussed an approximation technique to bypass the
necessity of learning explicit value functions. Another approach to bypass this requirement is to use
control-based techniques for obtaining soft-optimal policies, a.k.a., path integral control (Theodorou
et al., 2010; Williams et al., 2017; Kazim et al., 2024).
Recall that we have _ρ_ ( _xt, t_ ; _θ_ ) = _xt_ + ( _δt_ )¯ _g_ ( _xt, t_ ) and _σ_ [2] ( _t_ ) = ˜ _g_ ( _t_ )( _δt_ ), the motivation behind
this approach is as follows. Initially, we have:


_∇xt_ exp( _v_ ( _xt_ ) _/α_ )
= E _{p_ [pre] _t_ _}_ [[exp(] _[v]_ _t−_ 1 [(] _[x]_ _t−_ 1 [)] _[/α]_ [)] _[ |][ x]_ _t_ []] (Soft Bellman equation)



_dxt−_ 1
�



�



= _∇xt_



exp( _vt−_ 1( _xt−_ 1 _/α_ )) exp _−_ [(] _[x][t][−]_ [1] _[ −]_ _[x][t][ −]_ [(] _[δt]_ [)] _[g]_ [¯][(] _[x][t][,][ t]_ [))][2]
�� � 0 _._ 5˜ _g_ ( _t_ )( _δt_ )



�



= E
_{p_ [pre] _t_ _}_



exp( _vt−_ 1( _xt−_ 1 _/α_ )) _{xt−_ 1 _−_ _xt −_ ( _δt_ )¯ _g_ ( _xt, t_ ) _}_ _[{]_ [1 + ][(] _[δt]_ ˜ [)] _[∇][x][t][g]_ [¯][(] _[x][t][,][ t]_ [)] _[}]_ _|xt_
� _g_ ( _t_ )( _δt_ )



1
_≈_ _g_ ˜( _t_ )( _δt_ ) [E] _[{][p]_ _t_ [pre] _}_ [[exp(] _[v]_ _t−_ 1 [(] _[x]_ _t−_ 1 _[/α]_ [))] _[{][x]_ _t−_ 1 _[−]_ _[x]_ _t_ _[−]_ [(] _[δt]_ [)¯] _[g]_ [(] _[x]_ _t_ _[, t]_ [)] _[}|][x]_ _t_ []]

1 1
= _g_ ˜( _t_ )( _δt_ ) [E] _[{][p]_ _t_ [pre] _}_ �E _{p_ [pre] _t_ _}_ [[exp(] _[r]_ [(] _[x]_ 0 [)] _[/α]_ [)] _[ |][ x]_ _t−_ 1 []] _[ ϵ]_ _t_ _[|][x]_ _t_ � = _g_ ˜( _t_ )( _δt_ ) [E] _[{][p]_ _t_ [pre] _}_ [[exp(] _[r]_ [(] _[x]_ 0 _[/α]_ [))] _[ϵ]_ _t_ _[|][x]_ _t_ []] _[ .]_


Now, we obtain:



_σ_ [2] ( _t_ ) _∇xtv_ ( _xt_ )



_t_ _}_ [[exp(] _[r]_ [(] _[x]_ 0 [)] _[/α]_ [))] _[ϵ]_ _t_ _[|][x]_ _t_ []]
_α_ [1] _[×]_ [ E] E _[{]_ _{_ _[p]_ _p_ [pre][pre] _t_ _}_ [[exp(] _[r]_ [(] _[x]_ 0 [)] _[/α]_ [))] _[|][x]_ _t_ []] _[.]_ (32)



_xtv_ ( _xt_ )

_≈_ [1]

_α_ _α_



Importantly, this approach does not require any differentiation. Therefore, by running policies
from pre-trained models and simply using Monte Carlo approximation in both the denominator
and numerator, we can estimate the above quantity (the right-hand side of (32)) without making
any models (classifiers), unlike classifier guidance. Note while this approach is widely used in the
control community, it may not be feasible for diffusion models due to the high dimensionality of
the input space.

#### **6.3 Path Consistency Learning (Losses Often Used in Gflownets)**


Now, we explain how to apply path consistency learning (PCL) (Nachum et al., 2017) to finetune diffusion models. In the Gflownets literature (Bengio et al., 2023), it seems that this variant is
utilized as either a detailed balance or a trajectory balance loss, as discussed in Mohammadpour
et al. (2023); Tiapkin et al. (2023); Deleu et al. (2024). However, to the best of our knowledge,
the precise formulation of path consistency learning in the context of fine-tuning diffusion models
has not been established. Therefore, we start by elucidating the rationale of PCL. Subsequently,
we provide a comprehensive explanation of the PCL. Finally, we discuss its connection with the
literature on Gflownets.


**Motivation.** Here, we present the fundamental principles of the PCL. To start with, we prove the
following lemma, which characterizes soft-value functions and soft-optimal policies recursively.


25




**Algorithm 5** Path Consistency Learning (Training with detailed balance loss)

1: **Require** : Diffusion-model _{N_ ( _ρ_ ( _xt, t_ ; _θ_ ) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1 [,] pre-trained model
_{N_ ( _ρ_ ( _xt, t_ ; _θ_ pre) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1 [, batch size] _[ m]_ [, parameter] _[ α][ ∈]_ [R][+][, learning rate] _[ γ]_
2: Set a model _{vt_ ( _·_ ; _θ_ ) _}_ to learn optimal soft value function, and a model _{pt_ ( _·|·_ ; _θ_ ) _}_ to learn
optimal polices.
3: **Initialize** : _θ_ 1 = _θ_ pre
4: **for** _s_ = _{_ 1 _, · · ·, S}_ **do**

5: Collect _m_ samples _{x_ [(] _t_ _[i]_ [)][(] _[θ]_ [)] _[}]_ [0] _t_ = _T_ +1 [from a current diffusion model (i.e., generating by sequen-]
tially running polices _{N_ ( _ρ_ ( _xt, t_ ; _θ_ ) _, σ_ [2] ( _t_ )) _}_ [1] _t_ = _T_ +1 [from] _[ t]_ [ =] _[ T]_ [ to] _[ t]_ [ = 0][)]
6: Set _v_ 0 = _r_



_|ϕs,_


_|θs,_



1
�

_t_ = _T_ +1


1
�

_t_ = _T_ +1



2

_vt_ ( _x_ [(] _t_ _[i]_ [)][;] _[ϕ]_ [)] + log _pt_ ( _x_ [(] _t−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][;] _[ θ][s]_ [)] _[ −]_ _[v][t][−]_ [1][(] _[x]_ _t_ [(] _−_ _[i]_ [)] 1 [;] _[ ϕ][s]_ [)] _−_ log _p_ [pre] _t_ ( _x_ [(] _t−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][)]
_α_ _α_
� �



_ϕs_ +1 _←_ _ϕs −_ _γ∇ϕ_


_θs_ +1 _←_ _θs −_ _γ∇θ_



_m_
�


_i_ =1



_m_
�


_i_ =1



_vt_ ( _x_ [(] _t_ _[i]_ [)][;] _[ϕ][s]_ [)]


_α_

�



�2

�2




_[i]_ [)]

[;] _[ϕ][s]_ [)] + log _pt_ ( _x_ [(] _t−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][;] _[ θ]_ [)] _[ −]_ _[v][t][−]_ [1][(] _[x]_ _t_ [(] _−_ _[i]_ [)] 1 [;] _[ ϕ][s]_ [)] _−_ log _p_ [pre] _t_ ( _x_ [(] _t−_ _[i]_ [)] 1 _[|][x]_ _t_ [(] _[i]_ [)][)]

_α_ _α_



7: **end for**
8: **Output** : _{pt_ ( _xt−_ 1 _|xt_ ; _θS_ ) _}t_


**Lemma 4** (1-step Consistency Equation) **.**



_vt_ ( _xt_ )


_α_

�



_vt−_ 1( _xt−_ 1)
+ log _p_ _[⋆]_ _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [) =]

_α_

� �



+ log _p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)] (33)
�



_Proof._ We consider the marginal distribution with respect to _xt_ and _xt−_ 1 induced by the softoptimal policy, and denote it _l_ ( _xt, xt−_ 1) _∈_ ∆( _X × X_ ). Then, it is clearly _l_ ( _xt_ ) _l_ ( _xt−_ 1 _| xt_ ) =
_l_ ( _xt−_ 1) _l_ ( _xt | xt−_ 1) _._ Now, from Theorem 2 that characterizes marginal distributions, and Theorem 3
that characterizes posterior distributions, this results in :



1 _vt_ ( _xt_ )

_p_ [pre] _t_ [(] _[x][t]_ [)]

_C_ [exp] � _α_ �

� ~~�~~ � ~~�~~
Marginal distribution at t



1 _vt_ ( _xt_ )
_C_ [exp] � _α_



_× p_ _[⋆]_ _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)]
� � ~~�~~ �
Optimal policy



_× p_ [pre] _t−_ 1 [(] _[x][t][|][x][t][−]_ [1][)]
~~�~~ ~~�~~ � ~~�~~
Posterior distribution



= [1]




[1] _vt−_ 1( _xt−_ 1) _p_ [pre] _t−_ 1 [(] _[x][t][−]_ [1][)]

_C_ [exp] � _α_ �

� ~~��~~ ~~�~~
Marginal distribution at t-1




[1] _vt−_ 1( _xt−_ 1)

_C_ [exp] � _α_



_α_



_α_



Rearranging yields:



1 _vt_ ( _xt_ )
_C_ [exp] � _α_



_× p_ _[⋆]_ _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [) =] [1]
� _C_




[1] _vt−_ 1( _xt−_ 1)

_C_ [exp] � _α_



_× p_ [pre] _t_ [(] _[x][t][−]_ [1] _[|][x][t]_ [)]
�



_α_



Taking the logarithm, the statement is concluded.


**Algorithm.** Being motivated by the relation in (33), after initializing _v_ 0 = _r_, we obtain the
recursive equation:



2 []
�









( _vt, p_ _[⋆]_ _t_ [) =] argmin E _xt∼ut_
_g_ [(1)] : _X→_ R _,g_ [(2)] : _X→_ ∆( _X_ )









_g_ [(1)] ( _xt_ )


_α_

�



( _xt_ )

+ log _g_ [(2)] ( _xt−_ 1 _|xt_ ) _−_ _[v][t][−]_ [1][(] _[x][t][−]_ [1][)]
_α_ _α_




_[−]_

_−_ log _p_ [pre] _t_ ( _xt−_ 1 _|xt_ )
_α_



(34)



26




where _ut ∈_ ∆( _X_ ) is any exploratory roll-in distribution. Based on this algorithm, we outline the
entire algorithm in Algorithm 5.
We make several important remarks regarding Algorithm 5. Firstly, while we use on-policy data
collection, technically, any policy can be used in this off-policy algorithm, like reward-weighted
MLE. Secondly, in practice, it might be preferable to utilize a sub-trajectory from _xt_ to _xt−k_ based
on the following expression:



_vt_ ( _xt_ )
log _p_ _[⋆]_ _t−k_ +1 [(] _[x][t][−][k][|][x][t][−][k]_ [+1][) +] _[ · · ·]_ [ + log] _[ p]_ _t_ _[⋆]_ [(] _[x][t][−]_ [1] _[|][x][t]_ [) +]

_α_

�



�



_vt−k_ ( _xt−k_ )
=


_α_

�



+ log _p_ [pre] _t−k_ +1 [(] _[x][t][−][k][|][x][t][−][k]_ [+1][) +] _[ · · ·]_ [ + log] _[ p]_ _t_ [pre][(] _[x][t][−]_ [1] _[|][x][t]_ [)] _[,]_
�



which is an extension of (33). The loss function based on the above _k_ -step consistency equation
could make training faster without learning value functions at every time point, as noted in the
literature in PCL. In the extreme case (i.e., when we recursively apply it with _t_ = _T_ ), we obtain the
following.


**Corollary 1** ( _T_ -step consistency) **.**


log _p_ _[⋆]_ _t_ [(] _[x]_ [0] _[|][x]_ [1][) +] _[ · · ·]_ [ + log] _[ p]_ _T_ _[⋆]_ [(] _[x][T]_ _[−]_ [1] _[|][x][T]_ [) + log] _[ p][⋆]_ _T_ [(] _[x][T]_ [)] (35)



_r_ ( _x_ 0)
=


_α_

�



+ log _p_ [pre] 1 [(] _[x]_ [0] _[|][x]_ [1][) +] _[ · · ·]_ [ + log] _[ p]_ [pre] _T_ [(] _[x][T]_ _[−]_ [1] _[|][x][T]_ [) + log] _[ p]_ [pre] _T_ [(] _[x][T]_ [)] _[.]_
�



_Proof._ We consider the marginal distribution with respect to _xT_ _, · · ·, x_ 0 induced by the soft-optimal
policy, and denote it _l_ ( _xT_ _, · · ·, · · ·, x_ 0) _∈X × · · · × X_ . We have


_l_ ( _xT_ ) _l_ ( _xT_ _−_ 1 _| xT_ ) _· · · l_ ( _x_ 0 _|x_ 1) = _l_ ( _x_ 0) _l_ ( _x_ 1 _| x_ 0) _· · · l_ ( _xT | xT_ _−_ 1) (36)


From Theorem 2 that characterized marginal distributions, and Theorem 3 that characterize posterior
distributions, the left hand side of (36) is equal to



_p_ _[⋆]_ _T_ [(] _[x][T]_ [)]
� ~~�~~ � ~~�~~
Marginal distribution at T



_× p_ _[⋆]_ _T_ _−_ 1 [(] _[x][T]_ _[|][x][T]_ _[−]_ [1][)]
� ~~�~~ � ~~�~~
Optimal policy at _T −_ 1



_× · · · ×_ _p_ _[⋆]_ 1 [(] _[x]_ [0] _[|][x]_ [1][)]
~~�~~ � ~~�~~ �
Optimal policy at 1



and the right-hand side in (36) is equal to



exp( _r_ ( _x_ 0) _/α_ )

_C_ _p_ [pre] 0 [(] _[x]_ [0][)]
� ~~�~~ � ~~�~~
Marginal distribution at 0



_× p_ [pre] 0 [(] _[x]_ [1] _[|][ x]_ [0][)]
� ~~�~~ � ~~�~~
Posterior distribution



_× · · · × p_ [pre] _T_ _−_ 1 [(] _[x][T][ |][ x][T]_ _[−]_ [1][)]
� ~~�~~ � ~~�~~
Posterior distribution



_._



By rearranging the term, we obtain (35).


**Comparison with Gflownets.** In the Gflownets literature, similar losses are used. For instance,
the loss derived from (33) or (35) is commonly known as a detailed balance loss (Bengio et al.,
2023) or a trajectory loss (Malkin et al., 2022), respectively.
Note in general, the literature in Gflownets primarily focuses on sampling from unnormalized
models (distributions proportional to exp( _r_ ( _x_ ))). Hence, reference policies (i.e, _{p_ [pre] _t_ _[}]_ [ ) or latent]
states (i.e., _xT_ :1 before _x_ 0) are introduced without relying on pre-trained diffusion models. In
contrast, in our context, we use policies derived from pre-trained diffusion models as reference
policies, leveraging them as our prior knowledge.


27




### **7 Fine-Tuning Settings Taxonomy**

So far, we implicitly assume we have access to reward functions. However, these functions are often
unknown and need to be learned from data. We classify several settings in terms of whether reward
functions are available or, if not, how they could be learned. This section is summarized in Figure 3.


Figure 3: Practical recommendation of RL-based algorithms to optimize downstream reward
functions.

#### **7.1 Fine-Tuning with Known, Differentiable Reward Functions**


When accurate differentiable reward functions are available (e.g., standard in computer vision tasks),
our primary focus is typically on computational and memory efficiency. In general, we advocate for
employing reward backpropagation (c.f. Algorithm 2) for its computational efficiency. Alternatively,
if memory efficiency is a significant concern, we suggest using PPO due to its stability.

#### **7.2 Fine-Tuning with Black-Box Reward Feedback**


Here, we explore scenarios where we have access to accurate but non-differentiable (black-box)
reward feedback, often found in scientific simulations. The emphasis still remains primarily on
computational and memory efficiency. In such scenarios, due to the non-differentiability of reward
feedback, we recommend using PPO or reward-weighted MLE for the following reasons.
In the preceding section, we advocated for the use of reward backpropagation due to its computational advantages. However, when dealing with non-differentiable feedback, the advantage
of reward backpropagation may diminish. This is because learning from such feedback requires
learning differentiable reward functions to make the algorithm work as we discuss in Section 5.2.
However, obtaining a differentiable reward function can be challenging in many domains. For
example, in molecular property prediction, molecular fingerprints are informative features, and
accurate reward functions can be derived by training simple neural networks on these features


28



![](images/2407.13734v1.pdf-27-0.png)


(Pattanaik and Coley, 2020). Yet, these mappings, grounded in prior scientific understanding, are
non-differentiable, thereby restricting their applicability.
In contrast, PPO and reward-weighted MLE allow for policy updates without the explicit
requirement to learn a differentiable reward function, offering a significant advantage over reward
backpropagation.

#### **7.3 Fine-Tuning with Unknown Rewards Functions**


When reward functions (or computational proxies in Section 7.2) are unavailable, we must learn
from data with reward feedback. In such cases, compared to the previous scenarios, two important
considerations arise:


  - Not only computational or memory efficiency but also feedback efficiency (i.e., sample
efficiency regarding reward feedback) is crucial.


  - Given that learned reward feedback may not generalize well outside the training data distributions, it is essential to constrain fine-tuned models to prevent significant divergence from
diffusion models. In these situations, not only soft PPO and direct backpropagation but also
methods discussed in Section 6, such as reward-weighted MLE, can be particularly effective.


Further, the scenario involving unknown reward functions can be broadly categorized into two cases,
which we will discuss in more detail.



**Offline scenario (Figure 4).** In many scenarios, even
in cases where reward functions are unknown, we often have access to offline data with reward feedback
_{x_ [(] _[i]_ [)] _, r_ ( _x_ [(] _[i]_ [)] ) _}_ . One straightforward approach is to
perform regression with neural networks and build a
learned regressor ˆ _r_ . However, this learned reward function ˆ _r_ might not generalize well beyond the distribution
of the offline data. In regions outside this distribution, ˆ _r_
could assign a high value even when the actual reward
_r_ is low due to high uncertainty. Consequently, we
could be easily misled by out-of-distribution samples
if we solely optimize ˆ _r_ .
One approach to alleviate this issue is to adopt a
pessimistic (i.e., conservative) strategy, as proposed in
Uehara et al. (2024). This technique has been widely
applied in the field of offline RL (Levine, 2018). The
main idea is to penalize ˆ _r_ outside the distributions using techniques such as adding an explicit penalty term
(Yu et al., 2020; Chang et al., 2021), bootstrapping,
or more sophisticated methods (Kumar et al., 2020;
Xie et al., 2021; Rigter et al., 2022; Uehara and Sun,
2021). By doing so, we can avoid being fooled by
out-to-distribution regions while still benefiting from
the extrapolation capabilities of reward models.


29



![](images/2407.13734v1.pdf-28-0.png)

Figure 4: Offline scenario


Figure 5: Online scenario



![](images/2407.13734v1.pdf-28-1.png)


**Online scenario (Figure 5).** Consider situations
where reward functions are unknown, but we can gather data online. These scenarios are often
referred to as a lab-in-the-loop setting.
In such scenarios, Uehara et al. (2024) proposes an iterative procedure comprising three steps:
(1) collecting feedback data by exploring new areas (i.e., obtaining _x_ from current diffusion models
and corresponding _r_ ( _x_ )), (2) learning the reward function from the collected feedback data, and
(3) fine-tuning current diffusion models by maximizing the learned reward function enhanced by
an optimistic bonus term, using algorithms discussed in Section 4.2. In contrast to standard online
PPO, where feedback data is directly used in the fine-tuning process, actual reward feedback is
only used in step (2) and not in step (3). An additional key aspect is the use of an optimistic
reward function that encourages exploration beyond the distributions of the current diffusion model.
This deliberate separation between reward learning and fine-tuning steps, coupled with the use of
optimism, significantly enhances feedback efficiency.

### **8 Connection with Classifier Guidance**


Classifier guidance (Dhariwal and Nichol, 2021) is commonly used for conditional generation in
diffusion models. Interestingly, RL-based methods share a close connection with classifier guidance.
More specifically, in this section, following Zhao et al. (2024), we elucidate how RL-based methods
can be applied to conditional generation. Furthermore, from this unified viewpoint, we highlight
that classifier guidance is seen as a value-weighted sampling in Section 4.2.

#### **8.1 Classfier Guidance**


In this section, we first introduce classifier guidance. Classifier guidance utilizes an unconditional
pre-trained model for conditional generation. More specifically, we consider a scenario where we
have a pre-trained diffusion model, which enables us to sample from _p_ [pre] ( _x_ ), and data with feedback
_{x, y}_, where _y_ represents the new label we want to condition on. Our objective here is to sample
from _p_ ( _·|y_ ) _∈_ [ _Y →_ ∆(R _[d]_ )],


_p_ ( _· | y_ ) _∝_ _py_ ( _y|·_ ) _p_ [pre] ( _·_ ) _._ (37)


where _py_ : _X →_ ∆( _Y_ ) denotes the conditional distribution of _y_ given _x_ .


**Motivation.** In classifier guidance, we consider the following policy:


_p_ [gui] _t_ [(] _[·|][x][t][, y]_ [) =] _[ N]_ � _ρ_ ( _xt, t_ ; _θ_ pre) + _σ_ [2] ( _t_ ) _∇x_ log _qy_ ( _xt, t_ ) _, σ_ [2] ( _t_ )� _,_
_qy_ ( _x, t_ ) := E _{p_ [pre] _t_ _},Y ∼py_ ( _·|x_ 0) [[I(] _[Y]_ [ =] _[ y]_ [)] _[|][x]_ _t_ [=] _[ x]_ []] _[.]_


Under the continuous-time formulation of diffusion models, applying Doob’s h-transform (Rogers
and Williams, 2000), it is shown that we can sample from _p_ ( _x|y_ ) by sequentially running _{p_ [gui] _t_ [(] _[·|][x][t][, y]_ [)] _[}]_ [1] _t_ = _T_ +1 [.]


**Algorithm.** Specifically, the algorithm comprises two steps. It’s important to note that this is not
a fine-tuning method but rather an inference-time technique as we freeze the pre-trained model.


30




1. Learning _qy_ ( _x, t_ ) through regression by gathering data containing _{xt, y}_ . Here, for each
( _x_ 0 _, y_ ), using a forward policy for the pre-training (from 0 to _T_ ), we obtain _xt_ starting from

_x_ 0.


2. During inference, sequentially execute _{p_ [gui] _t_ [(] _[·|][x][t][, y]_ [)] _[}]_ [1] _t_ = _T_ +1 [by leverage the fact that compared]
to the policy in pre-trained diffusion models, the mean of _{p_ [gui] _t_ [(] _[·|][x][t][, y]_ [)] _[}]_ [1] _t_ = _T_ +1 [is just shifted]
with an additional term _∇x_ log _qy_ ( _x, t_ ).


Notably, the method described above can be reduced to value-weighted sampling (cf. Section 6.2), when _r_ ( _x, y_ ) = log _p_ ( _y|x_ ). Inspired by such an observation, we apply an RL-based
fine-tuning method for conditional generation as below.

#### **8.2 RL-Based Fine-Tuning for Conditional Generation**


In this section, following Zhao et al. (2024), we explain an RL-based fine-tuning approach for
conditional generation. The core idea is that setting _r_ ( _x, y_ ) = log _p_ ( _y|x_ ) allows us to sample from
the target conditional distribution (37), drawing on the insights discussed in Section 4.2.
More precisely, introducing a distribution over _y_ as _q_ ( _·_ ), we formulate the following RL problem:


_{p_ _[⋆]_ _t_ _[}][t]_ [=] argmax E _{pt_ ( _·|·,y_ ) _},y∼_ Π[log _p_ ( _y|x_ 0) _−_ Σ [1] _t_ = _T_ +1 [KL(] _[p][t]_ [(] _[·|][x][t][, y]_ [)] _[∥][p]_ _t_ [pre][(] _[·|][x][t]_ [))]] _[.]_
_{pt∈_ [( _Y,X_ ) _→_ ∆( _X_ )] _}_ [1] _t_ = _T_ +1

(38)


Compared to the objective function (18) in Section 4.2, the main differences are: (1) policy space
is expanded to accommodate additional control (i.e., _y_ ): _pt_ ( _·|·, ·_ ) _⊂_ [R _[d]_ _× Y →_ ∆(R _[d]_ )], (2) the
fine-tuning objectives are set as log-likelihoods: log _p_ ( _y|x_ ). Then, following Theorem 1, we can
establish the following result.


**Corollary 2.** _The distribution induced by {p_ _[⋆]_ _t_ _[}]_ [1] _t_ = _T_ +1 _[(i.e.,]_ � _{_ [�][1] _t_ = _T_ +1 _[p]_ _t_ _[⋆]_ [(] _[x][t][−]_ [1] _[|][ x][t][, y]_ [)] _[}][dx]_ [1:] _[T]_ _[is the]_
_target conditional distribution_ (37) _._


The above corollary suggests that for conditional generation, we can utilize various off-the-shelf
algorithms discussed in Section 4.2 and 6, such as PPO, reward backpropagation, and rewardweighted MLE, as well as value-weighted sampling.

### **9 Connection with Flow-Based Diffusion Models**


We elucidate the close relationship between bridge matching and fine-tuning, as summarized in
Table 2. Bridge (flow) matching (Liu et al., 2022; Tong et al., 2023; Lipman et al., 2023; Liu et al.,
2022; Shi et al., 2024; Albergo and Vanden-Eijnden, 2022), has recently gained popularity as a
distinct type of diffusion model from score-matching-based ones. These works suggest that training
and inference speed will be accelerated due to the ability to define a more direct trajectory from
noise to data distribution than score-matching-based diffusion models.
The connection between bridge matching and fine-tuning becomes evident in the continuoustime formulation. To begin, we first provide a brief overview of the continuous-time formulation of
score-matching-based diffusion models.


31




Table 2: Comparison between training score/flow-based diffusion models and fine-tuning. We
optimize the parameter _θ_ in P _[θ]_, which represents the induced distribution over trajectories associated
with _θ_ .


Loss Note of Q


Score-based KL(Q [time] _∥_ P _[θ]_ ) Q [time] : Time-reversal of reference SDE (reference
training SDE is defined from 0 to _T_ where the initial distribute at 0 is data distribution)


Flow-based KL(Q [doob] _∥_ P _[θ]_ ) Q [doob] : Coupling between the reference SDE from
training _T_ to 0 conditioning at 0 (dist. over _xT_ :1 _|x_ 0) and
a data distribution at time 0 (dis. over _x_ 0). Note
that reference SDE is defined from _T_ to 0.


Fine-tuning KL(P _[θ]_ _∥_ Q [fine] ) Q [fine] : Coupling between the pre-trained SDE
from _T_ to 0 conditioned on 0 (dist. over _xT_ :1 _|x_ 0)
and a data distribution at time 0 (dis. over _x_ 0)

#### **9.1 Continuous Time Formulation of Score-Based diffusion models**


We provide a brief overview of the continuous-time formulation of diffusion models (Song et al.,

2021), further expanding Section 1.1.1. In this formulation, we initially define the forward reference
SDE, spanning from time 0 to _T_ (e.g., variance exploding (VE) process or variance preserving (VP)
process), by setting an initial distribution at 0 as the data distribution. Additionally, the reference
SDE is tailored to converge to an easy distribution at time _T_, such as the normal distribution.
Subsequently, we consider the time-reversal SDE (Anderson, 1982), progressing from _T_ to 0. If
we could learn this time-reversal SDE, it would enable us to sample from the data distribution by
starting with the easy initial distribution (at _T_ ) and following the time-reversal SDE from _T_ to 0 at
inference time.


**Training.** Now, the remaining question is how to learn this time-reversal SDE. Here, by representing the induced distribution over trajectories from _T_ to 0 with this time-reversal SDE as Q [time],
our goal is to train a new SDE, parameterized by _θ_ in the neural network, such that the induced
distribution P _[θ]_ over trajectories associated with _θ_ aligns with Q [time] . Specifically, the objective is to
minimize the following losses:


argmin KL(Q [time] _∥_ P _[θ]_ ) _._ (39)
_θ_


With some algebra, leveraging the observation that the time-reversal SDE comprises the original
forward SDE and the scoring term of the _marginal_ distribution (Anderson, 1982) as in (2), Song
et al. (2021) demonstrates that this KL loss (39) is approximated as


ˆ
_θ_ pre = argmin E _t∼_ Uni([0 _,T_ ]) _,xt∼qt|_ 0( _xt|x_ 0) _,x_ 0 _∼p_ pre[ _{σt_ _[⋄][}]_ [2] _[∥∇][x]_ _t_ [log] _[ q][t][|]_ [0][(] _[x][t]_ _[|][ x]_ [0][)] _[ −]_ _[s][θ]_ [(] _[x][t][, t]_ [)] _[∥]_ [2][]] _[,]_ (40)
_θ_


32




which is equal to (4) where the weight _λ_ ( _t_ ) is _{σt_ _[⋄][}]_ [2][ (for notation, refer to][ Section 1.1.1][).]

#### **9.2 Flow-Based (Bridge-Based) Diffusion Models**


We adopt the explanation outlined in Liu et al. (2022). In this framework, we begin by considering
a reference SDE (from _T_ to 0), _xt_ = _zT_ _−t_ where _zt_ is defined as Brownian motion:


_dzt_ = _dwt,_ _t ∈_ [0 _, T_ ] _._ (41)


Subsequently, we consider the coupling of the reference SDE conditioned on state at time 0 (i.e.,
distribution of _x_ 0: _T_ conditional on _x_ 0) and the data distribution at time 0, denoting the resulting
distribution as Q [doob] . Informally, this Q [doob] is characterized as follows.


**Informal characterization of** Q **[doob]** **.** When discretizing the time step, by denoting the distribution
induced by the reference SDE as _p_ [ref] ( _xT_ _, · · ·, x_ 0), we define


Q [doob] _[,T]_ ( _xT_ _, · · ·, x_ 0) := _p_ [ref] ( _xT_ _, · · ·, x_ 1 _| x_ 0) _p_ [pre] ( _x_ 0) _._


The distribution Q [doob] above is a limiting distribution obtained by making the discretization step
small.


**Training.** In bridge-based diffusion models, our objective is to train a new SDE, parametrized by
_θ_ (e.g., a neural network), to align the induced distribution P _[θ]_ with Q [doob] . Specifically, bridge-based
training aims to minimize


argmin KL(Q [doob] _∥_ P _[θ]_ ) _._ (42)
_θ_


While the method to minimize the above is currently unclear, we can derive a trainable objective
through algebraic manipulation. Initially, we observe that the reference SDE (41), conditioned on
state _x_ 0 at time 0 ( _zT_ = _b_ ), is expressed as:


_t_
_t ∈_ [0 _, T_ ]; _dzt_ _[b]_ [=] _[ g][b]_ [(] _[z]_ _t_ _[b][, t]_ [)] _[dt]_ [ +] _[ dw][t][,]_ _g_ _[b]_ ( _zt_ _[b][, t]_ [) =] _[b][ −]_ _[z][b]_ (43)
_T −_ _t_ _[,]_


using the seminal “Doob’s h-transform” (Rogers and Williams, 2000). This SDE is commonly
known as the Brownian bridge. Now, we consider a new SDE parameterized by _θ_ :


_t ∈_ [0 _, T_ ]; _dzt_ = _s_ ( _zt, t_ ; _θ_ ) _dt_ + _dwt._ (44)


Then, we can demonstrate that minimizing the KL loss in (42) is equivalent to


argmin _θ_ E _t∼_ Uni[0 _,T_ ] _,zt∼qt_ _[x]_ 0 _[,x]_ [0] _[∼][p]_ [pre] � _∥g_ _[x]_ [0] ( _zt, t_ ) _−_ _s_ ( _zt, t_ ; _θ_ ) _∥_ 2 [2] � (45)


where _qt_ _[x]_ [0] denotes the induced distribution at time _t_ following SDE (43) conditioned on _x_ 0, and
_s_ : _X ×_ [0 _, T_ ] _→X_ is the model we aim to optimize.


33




**Flow-based diffusion models.** The aforementioned formulation has been originally proposed as a
bridge-based SDE (Liu et al., 2022) because the conditional SDE is often referred to as a Brownian
bridge. Flow-based diffusion models proposed in Lipman et al. (2023); Tong et al. (2023) share a
close relationship with bridge-based diffusion models. These flow-based models use a target SDE
(43) and an SDE with a parameter (44) without stochastic terms (i.e., no _dwt_ ) and aim to minimize
the loss function (45).


**Comparison with score-based diffusion models.** Both score-based diffusion models and bridgebased diffusion models aim to minimize KL divergences. Despite its similarity, in contrast to
score-based diffusion models, bridge-based diffusion models target Q [doob] rather than the timereversal SDE Q [time] . The distribution Q [doob] is often considered preferable because it can impose an
SDE that navigates efficiently from 0 to _T_ (e.g., a Brownian bridge with minimal noise). This can
expedite the learning process since we lack direct control over the time-reversal SDE in score-based
diffusion models.

#### **9.3 Connection with RL-Based Fine-Tuning**


RL-based fine-tuning shares a close relationship with flow-based training. To illustrate this, consider
the reference SDE (from _T_ to 0) following the pre-trained diffusion model. Then, akin to flowbased training, we consider the coupling of the reference SDE conditioned on state at time 0
(i.e., distribution over _x_ 0: _T_ conditioned on _x_ 0) and the _target_ distribution at time 0 (i.e., _pr_ ( _·_ ) _∝_
exp( _r_ ( _·_ ) _/α_ ) _p_ [pre] ( _·_ )), denoting the induced distribution as Q [fine] .


**Informal characterization of** Q **[fine]** **.** Informally, Q [fine] is introduced as follows. When discretizing
the time step, by denoting the distribution induced by the reference SDE as _p_ [ref] ( _xT_ _, · · ·, x_ 0), we
define


Q [fine] _[,T]_ ( _xT_ _, · · ·, x_ 0) := _p_ [ref] ( _xT_ _, · · ·, x_ 1 _| x_ 0) _pr_ ( _x_ 0) _._


The distribution Q [fine] above is a limiting distribution obtained by making the discretization step
small.


**Training.** Now, we introduce a new SDE parameterized by _θ_ such as an neural network to align
the induced distribution P _[θ]_ with Q [fine] . Actually, in the continuous formulation, the RL-problem (18)
in Section 4.2 is equal to solving


argmin KL(P _[θ]_ _∥_ Q [fine] ) _._
_θ_


This formalization has been presented in Uehara et al. (2024). Intuitively, as we see in Theorem 3,
this is expected because we see that the posterior distribution induced by pre-trained models:


_p_ [ref] ( _xT_ _, · · ·, x_ 1 _| x_ 0) (= _p_ [ref] _T_ _−_ 1 [(] _[x][T]_ _[|][x][T]_ _[−]_ [1][)] _[p]_ [ref] _T_ _−_ 2 [(] _[x][T]_ _[−]_ [1] _[|][ x][T]_ _[−]_ [2][)] _[ · · ·][ p]_ [ref] 0 [(] _[x]_ [1] _[|][ x]_ [0][))] _[,]_


remains preserved after fine-tuning based on the RL-formulation (18).


34




**Comparison with flow-based training.** In contrast to flow-based training, RL-based fine-tuning
minimizes the inverse KL divergence rather than the KL divergence. Intuitively, this is because,
unlike Q [time] or Q [doob], we cannot sample from Q [fine] (recall that a marginal distribution at time 0 in
Q [fine], i.e., _pr_, is unnormalized); on the other hand, the marginal distributions at time 0 in both Q [time]

and Q [doob] are data distributions.


**Remark 7** (Extension to _f_ -divergence) **.** _Each of these methods can be technically extended by_
_incorporating more general divergences beyond KL divergence. For instance, in the context of_
_fine-tuning, please see Tang (2024)._

### **10 Connection with Sampling from Unnormalized Distributions**


Numerous studies delve into sampling from an unnormalized distribution proportional to exp( _r_ ( _x_ )),
commonly known as the Gibbs distribution, extensively discussed in the computational statistics
and statistical physics community (Robert, 2014). This issue is pertinent to our work, as during
fine-tuning, the target distribution is also formulated in the Gibbs distribution form in (10), which is
proportional to exp( _r_ ( _x_ )) _p_ [pre] ( _x_ ).
The literature employs two main approaches to tackle this challenge: Markov Chain Monte
Carlo (MCMC) and RL-based methods. In particular, the RL-based diffusion model fine-tuning
we have discussed so far is closely related to the latter approach. In this section, we explain and
compare these two approaches.

#### **10.1 Markov Chain Monte Carlo (MCMC)**


In MCMC, a Markov chain is constructed to approximate a target distribution such that the equilibrium distribution of the Markov chain converges to the target distribution. However, dealing with
high-dimensional domain spaces is challenging for MCMC. A common strategy involves leveraging
gradient information, such as the Metropolis-adjusted Langevin algorithm (MALA) (Besag, 1994)
or Hamiltonian Monte Carlo (HMC) (Neal et al., 2011; Girolami and Calderhead, 2011). MALA
shares similarities with classifier guidance and value-weighted sampling, as both methods depend
on the first-order information from reward (or value) functions.


**Can we use MCMC for fine-tuning diffusion models?** In the context of fine-tuning diffusion models, while it may seem intuitive to sample from the target distribution (10) (i.e.,
_pr ∝_ exp( _r_ ( _x_ )) _p_ [pre] ( _x_ )) using MCMC, it’s not straightforward for the following reasons:


  - We lack of an analytical form of _p_ [pre] ( _·_ ).


  - Additionally, even if we can estimate _p_ [pre] ( _·_ ) in an unbiased manner, as in Chen et al. (2018),
the mixing time for obtaining a single sample in MCMC might be lengthy.


Therefore, we explore creating a generative model (i.e., diffusion model) for this task using RLbased fine-tuning so that we can easily sample from _pr_ ( _·_ ) during inference (i.e., simulating the
time-reversal SDE (2) with policies from _t_ = _T_ + 1 to _t_ = 1), without relying on MCMC. This
addresses the concern of MCMC by avoiding the need to estimate _p_ [pre] ( _·_ ) and minimizing inference
time, albeit at the expense of increased training time.


35




#### **10.2 RL-Based Approaches**

While MCMC has historically been widely used for this purpose, recent works have proposed an
RL-based approach or its variant (e.g., Bernton et al. (2019); Heng et al. (2020); Huang et al. (2023);
Vargas et al. (2023)). For example, a seminal study (Zhang and Chen, 2021) introduced a method
very similar to reward backpropagation. In their work, they initially define Brownian motion as a
reference SDE (from _T_ to 0), similar to the pre-trained diffusion models in our context. Then, by
appropriately setting rewards, Zhang and Chen (2021) aims to learn a new SDE (from _T_ to 0) such
that we can sample from the target distribution at the endpoint 0.
These approaches offer advantages over MCMC due to their ability to minimize inference time
(amortization) by constructing a generative model comprising policies in the training time and solely
executing learned policies during inference. Unlike MCMC, these methods significantly reduce the
inference cost as we don’t need to be concerned about the mixing time. Additionally, RL-based
approaches have the potential to effectively handle high-dimensional, complex (i.e., multi-modal)
distributions by harnessing the considerable expressive power arising from latent states.
Despite these advantages, it is often unclear how to select a reference SDE in these works. In
contrast, in RL-based fine-tuning of diffusion models, we directly leverage pre-trained diffusion
models as the reference SDEs, i.e., as our prior knowledge.


**Key message.** Finally, it is worth noting that we can technically utilize any off-the-shelf RL algorithms for training. For example, Zhang and Chen (2021) employs reward backpropagation, while
works in Gflownets typically leverage PCL, as we mention in Section 6.3. However, additionally, it
is possible to use PPO or reward-weighted MLE mentioned in Section 4.2.

### **11 Closely Related Directions**


Lastly, we highlight several closely related directions we have not yet discussed.


**Aligning text-to-image models.** We recognize that current efforts in fine-tuning diffusion models
primarily revolve around aligning text-to-image models using human feedback (Lee et al., 2023;
Wallace et al., 2023; Wu et al., 2023; Yang et al., 2023). In this paper, our goal is to consider a more
general and fundamental formulation that is not tailored to any particular task.


**Diffusion models for RL.** Diffusion models are well-suited for specific reinforcement learning
applications due to their ability to model complex and multimodal distributions as polices (Janner
et al., 2022; Ajay et al., 2023; Wang et al., 2022; Hansen-Estruch et al., 2023; Du et al., 2024; Zhu
et al., 2023). For example, Wang et al. (2022) use conditional diffusion models as policies and
demonstrate their effectiveness in typical offline RL benchmarks. While RL-based fine-tuning also
aims to maximize certain reward functions, the primary focus of this tutorial covers methods that
leverage pre-trained diffusion models for this purpose.


**RL from human feedback (RLHF) for language models.** RLHF is widely discussed in the
context of language models, where many algorithms share similarities with those used in diffusion
models (Ouyang et al., 2022; Bai et al., 2022; Casper et al., 2023). For instance, both domains


36




commonly employ PPO as a standard approach, and reward-weighted training (or reward-weighted
MLE) often serves as a basic baseline approach. Despite these similarities, nuanced differences
exist mainly because diffusion models typically operate in a continuous input space rather than
a discrete one. Moreover, techniques like value-weighted sampling emerge uniquely within the
context of diffusion models.

### **12 Summary**


In this article, we comprehensively explain how fine-tuning diffusion models to maximize downstream reward functions can be formalized as a reinforcement learning (RL) problem in entropyregularized Markov decision processes (MDPs). Based on this viewpoint, we elaborate on the
application of various RL algorithms, such as PPO and reward-weighted MLE, specifically tailored
for fine-tuning diffusion models. Additionally, we categorize different scenarios based on how
reward feedback is obtained. While this categorization is not always explicitly mentioned in many
existing works, it is crucial when selecting appropriate algorithms. Finally, we discuss the relationships with several related topics, including classifier guidance, Gflownets, path integral control
theory, and sampling from unnormalized distributions.

### **References**


Agarwal, A., N. Jiang, S. M. Kakade, and W. Sun (2019). Reinforcement learning: Theory and
algorithms. _CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep_, 10–4.


Agarwal, V. and D. R. Kelley (2022). The genetic and biochemical determinants of mrna degradation
rates in mammals. _Genome biology 23_ (1), 245.


Ajay, A., Y. Du, A. Gupta, J. B. Tenenbaum, T. S. Jaakkola, and P. Agrawal (2023). Is conditional
generative modeling all you need for decision making? In _The Eleventh International Conference_
_on Learning Representations_ .


Albergo, M. S. and E. Vanden-Eijnden (2022). Building normalizing flows with stochastic interpolants. _arXiv preprint arXiv:2209.15571_ .


Anderson, B. D. (1982). Reverse-time diffusion equation models. _Stochastic Processes and their_
_Applications 12_ (3), 313–326.


Austin, J., D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg (2021). Structured denoising
diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems 34_,
17981–17993.


Avdeyev, P., C. Shi, Y. Tan, K. Dudnyk, and J. Zhou (2023). Dirichlet diffusion score model for
biological sequence generation. _arXiv preprint arXiv:2305.10699_ .


Bai, Y., S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,
C. McKinnon, et al. (2022). Constitutional ai: Harmlessness from ai feedback. _arXiv preprint_
_arXiv:2212.08073_ .


37




Bansal, A., H.-M. Chu, A. Schwarzschild, S. Sengupta, M. Goldblum, J. Geiping, and T. Goldstein
(2023). Universal guidance for diffusion models. In _Proceedings of the IEEE/CVF Conference_
_on Computer Vision and Pattern Recognition_, pp. 843–852.


Bengio, Y., S. Lahlou, T. Deleu, E. J. Hu, M. Tiwari, and E. Bengio (2023). Gflownet foundations.
_Journal of Machine Learning Research 24_ (210), 1–55.


Benton, J., Y. Shi, V. De Bortoli, G. Deligiannidis, and A. Doucet (2024). From denoising diffusions
to denoising markov models. _Journal of the Royal Statistical Society Series B: Statistical_
_Methodology 86_ (2), 286–301.


Bernton, E., J. Heng, A. Doucet, and P. E. Jacob (2019). Schr _\_ ” odinger bridge samplers. _arXiv_
_preprint arXiv:1912.13170_ .


Besag, J. (1994). Comments on “representations of knowledge in complex systems” by u. grenander
and mi miller. _J. Roy. Statist. Soc. Ser. B 56_ (591-592), 4.


Black, K., M. Janner, Y. Du, I. Kostrikov, and S. Levine (2023). Training diffusion models with
reinforcement learning. _arXiv preprint arXiv:2305.13301_ .


Campbell, A., J. Benton, V. De Bortoli, T. Rainforth, G. Deligiannidis, and A. Doucet (2022). A
continuous time framework for discrete denoising models. _Advances in Neural Information_
_Processing Systems 35_, 28266–28279.


Campbell, A., J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola (2024). Generative flows on
discrete state-spaces: Enabling multimodal flows with applications to protein co-design. _arXiv_
_preprint arXiv:2402.04997_ .


Cao, H., C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z. Li (2024). A survey on generative
diffusion models. _IEEE Transactions on Knowledge and Data Engineering_ .


Casper, S., X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando, R. Freedman, T. Korbak,
D. Lindner, P. Freire, et al. (2023). Open problems and fundamental limitations of reinforcement
learning from human feedback. _arXiv preprint arXiv:2307.15217_ .


Castillo-Hair, S. M. and G. Seelig (2021). Machine learning for designing next-generation mrna
therapeutics. _Accounts of Chemical Research 55_ (1), 24–34.


Chang, J. D., M. Uehara, D. Sreenivas, R. Kidambi, and W. Sun (2021). Mitigating covariate shift
in imitation learning via offline data without great coverage. _arXiv preprint arXiv:2106.03207_ .


Chen, M., S. Mei, J. Fan, and M. Wang (2024). An overview of diffusion models: Applications,
guided generation, statistical rates and optimization. _arXiv preprint arXiv:2404.07771_ .


Chen, R. T., Y. Rubanova, J. Bettencourt, and D. K. Duvenaud (2018). Neural ordinary differential
equations. _Advances in neural information processing systems 31_ .


Chung, H., J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye (2022). Diffusion posterior sampling
for general noisy inverse problems. _arXiv preprint arXiv:2209.14687_ .


38




Chung, H., B. Sim, D. Ryu, and J. C. Ye (2022). Improving diffusion models for inverse problems
using manifold constraints. _Advances in Neural Information Processing Systems 35_, 25683–
25696.


Clark, K., P. Vicol, K. Swersky, and D. J. Fleet (2023). Directly fine-tuning diffusion models on
differentiable rewards. _arXiv preprint arXiv:2309.17400_ .


De Bortoli, V., E. Mathieu, M. Hutchinson, J. Thornton, Y. W. Teh, and A. Doucet (2022). Riemannian score-based generative modelling. _Advances in Neural Information Processing Systems 35_,
2406–2422.


Deleu, T., P. Nouri, N. Malkin, D. Precup, and Y. Bengio (2024). Discrete probabilistic inference as
control in multi-path environments. _arXiv preprint arXiv:2402.10309_ .


Dhariwal, P. and A. Nichol (2021). Diffusion models beat gans on image synthesis. _Advances in_
_neural information processing systems 34_, 8780–8794.


Du, Y., S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel (2024).
Learning universal policies via text-guided video generation. _Advances in Neural Information_
_Processing Systems 36_ .


Fan, Y., O. Watkins, Y. Du, H. Liu, M. Ryu, C. Boutilier, P. Abbeel, M. Ghavamzadeh, K. Lee, and
K. Lee (2023). DPOK: Reinforcement learning for fine-tuning text-to-image diffusion models.
_arXiv preprint arXiv:2305.16381_ .


Finn, C., S. Levine, and P. Abbeel (2016). Guided cost learning: Deep inverse optimal control via
policy optimization. In _International conference on machine learning_, pp. 49–58. PMLR.


Fox, R., A. Pakman, and N. Tishby (2015). Taming the noise in reinforcement learning via soft
updates. _arXiv preprint arXiv:1512.08562_ .


Frey, N. C., D. Berenberg, K. Zadorozhny, J. Kleinhenz, J. Lafrance-Vanasse, I. Hotzel, Y. Wu,
S. Ra, R. Bonneau, K. Cho, et al. (2023). Protein discovery with discrete walk-jump sampling.
_arXiv preprint arXiv:2306.12360_ .


Geist, M., B. Scherrer, and O. Pietquin (2019). A theory of regularized markov decision processes.
In _International Conference on Machine Learning_, pp. 2160–2169. PMLR.


Girolami, M. and B. Calderhead (2011). Riemann manifold langevin and hamiltonian monte
carlo methods. _Journal of the Royal Statistical Society Series B: Statistical Methodology 73_ (2),
123–214.


Gosai, S. J., R. I. Castro, N. Fuentes, J. C. Butts, S. Kales, R. R. Noche, K. Mouri, P. C. Sabeti,
S. K. Reilly, and R. Tewhey (2023). Machine-guided design of synthetic cell type-specific
cis-regulatory elements. _bioRxiv_ .


Haarnoja, T., H. Tang, P. Abbeel, and S. Levine (2017). Reinforcement learning with deep energybased policies. In _International conference on machine learning_, pp. 1352–1361. PMLR.


39




Hansen-Estruch, P., I. Kostrikov, M. Janner, J. G. Kuba, and S. Levine (2023). Idql: Implicit
q-learning as an actor-critic method with diffusion policies. _arXiv preprint arXiv:2304.10573_ .


Heng, J., A. N. Bishop, G. Deligiannidis, and A. Doucet (2020). Controlled sequential monte carlo.


Ho, J., A. Jain, and P. Abbeel (2020). Denoising diffusion probabilistic models. _Advances in neural_
_information processing systems 33_, 6840–6851.


Ho, J. and T. Salimans (2022). Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_ .


Ho, J., T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet (2022). Video diffusion
models. _Advances in Neural Information Processing Systems 35_, 8633–8646.


Hoogeboom, E., V. G. Satorras, C. Vignac, and M. Welling (2022). Equivariant diffusion for
molecule generation in 3d. In _International conference on machine learning_, pp. 8867–8887.
PMLR.


Huang, X., H. Dong, H. Yifan, Y. Ma, and T. Zhang (2023). Reverse diffusion monte carlo. In _The_
_Twelfth International Conference on Learning Representations_ .


Janner, M., Y. Du, J. B. Tenenbaum, and S. Levine (2022). Planning with diffusion for flexible
behavior synthesis. _arXiv preprint arXiv:2205.09991_ .


Jo, J., S. Lee, and S. J. Hwang (2022). Score-based generative modeling of graphs via the system
of stochastic differential equations. In _International Conference on Machine Learning_, pp.
10362–10383. PMLR.


Kazim, M., J. Hong, M.-G. Kim, and K.-K. K. Kim (2024). Recent advances in path integral control
for trajectory optimization: An overview in theoretical and algorithmic perspectives. _Annual_
_Reviews in Control 57_, 100931.


Krishnamoorthy, S., S. M. Mashkaria, and A. Grover (2023). Diffusion models for black-box
optimization. _arXiv preprint arXiv:2306.07180_ .


Kumar, A., A. Zhou, G. Tucker, and S. Levine (2020). Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems 33_, 1179–1191.


Lal, A., D. Garfield, T. Biancalani, and G. Eraslan (2024). reglm: Designing realistic regulatory
dna with autoregressive language models. _bioRxiv_, 2024–02.


Lee, K., H. Liu, M. Ryu, O. Watkins, Y. Du, C. Boutilier, P. Abbeel, M. Ghavamzadeh, and S. S. Gu
(2023). Aligning text-to-image models using human feedback. _arXiv preprint arXiv:2302.12192_ .


Levine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and
review. _arXiv preprint arXiv:1805.00909_ .


Li, Z., Y. Ni, T. A. B. Huygelen, A. Das, G. Xia, G.-B. Stan, and Y. Zhao (2023). Latent diffusion
model for dna sequence generation. _arXiv preprint arXiv:2310.06150_ .


40




Lipman, Y., R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le (2023). Flow matching for generative
modeling. _ICLR 2023_ .


Liu, X., C. Gong, and Q. Liu (2022). Flow straight and fast: Learning to generate and transfer data
with rectified flow. _arXiv preprint arXiv:2209.03003_ .


Liu, X., L. Wu, M. Ye, and Q. Liu (2022). Let us build bridges: Understanding and extending
diffusion generative models. _arXiv preprint arXiv:2208.14699_ .


Lou, A., C. Meng, and S. Ermon (2023). Discrete diffusion language modeling by estimating the
ratios of the data distribution. _arXiv preprint arXiv:2310.16834_ .


Malkin, N., M. Jain, E. Bengio, C. Sun, and Y. Bengio (2022). Trajectory balance: Improved credit
assignment in gflownets. _Advances in Neural Information Processing Systems 35_, 5955–5967.


Mohammadpour, S., E. Bengio, E. Frejinger, and P.-L. Bacon (2023). Maximum entropy gflownets
with soft q-learning. _arXiv preprint arXiv:2312.14331_ .


Nachum, O., M. Norouzi, K. Xu, and D. Schuurmans (2017). Bridging the gap between value and
policy based reinforcement learning. _Advances in neural information processing systems 30_ .


Neal, R. M. et al. (2011). Mcmc using hamiltonian dynamics. _Handbook of markov chain monte_
_carlo 2_ (11), 2.


Neu, G., A. Jonsson, and V. Gomez (2017). A unified view of entropy-regularized markov decision´
processes. _arXiv preprint arXiv:1705.07798_ .


Ouyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
K. Slama, A. Ray, et al. (2022). Training language models to follow instructions with human
feedback. _Advances in Neural Information Processing Systems 35_, 27730–27744.


Pattanaik, L. and C. W. Coley (2020). Molecular representation: going long on fingerprints.
_Chem 6_ (6), 1204–1207.


Peng, X. B., A. Kumar, G. Zhang, and S. Levine (2019). Advantage-weighted regression: Simple
and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_ .


Peters, J., K. Mulling, and Y. Altun (2010). Relative entropy policy search. In _Proceedings of the_
_AAAI Conference on Artificial Intelligence_, Volume 24, pp. 1607–1612.


Podell, D., Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Muller, J. Penna, and R. Rombach¨
(2023). Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv_
_preprint arXiv:2307.01952_ .


Prabhudesai, M., A. Goyal, D. Pathak, and K. Fragkiadaki (2023). Aligning text-to-image diffusion
models with reward backpropagation. _arXiv preprint arXiv:2310.03739_ .


Ramesh, A., P. Dhariwal, A. Nichol, C. Chu, and M. Chen (2022). Hierarchical text-conditional
image generation with clip latents. _arXiv preprint arXiv:2204.06125 1_ (2), 3.


41




Rigter, M., B. Lacerda, and N. Hawes (2022). Rambo-rl: Robust adversarial model-based offline
reinforcement learning. _Advances in neural information processing systems 35_, 16082–16097.


Robert, C. (2014). Statistical modeling and computation.


Rogers, L. C. G. and D. Williams (2000). _Diffusions, Markov processes and martingales: Volume 2,_
_Itˆo calculus_, Volume 2. Cambridge university press.


Rombach, R., A. Blattmann, D. Lorenz, P. Esser, and B. Ommer (2022, June). High-resolution
image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on_
_Computer Vision and Pattern Recognition (CVPR)_, pp. 10684–10695.


Sarkar, A., Z. Tang, C. Zhao, and P. Koo (2024). Designing dna with tunable regulatory activity
using discrete diffusion. _bioRxiv_, 2024–05.


Schulman, J., X. Chen, and P. Abbeel (2017). Equivalence between policy gradients and soft
q-learning. _arXiv preprint arXiv:1704.06440_ .


Schulman, J., S. Levine, P. Abbeel, M. Jordan, and P. Moritz (2015). Trust region policy optimization.
In _International conference on machine learning_, pp. 1889–1897. PMLR.


Schulman, J., F. Wolski, P. Dhariwal, A. Radford, and O. Klimov (2017). Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_ .


Shi, Y., V. De Bortoli, A. Campbell, and A. Doucet (2024). Diffusion schrodinger bridge matching.¨
_Advances in Neural Information Processing Systems 36_ .


Sohl-Dickstein, J., E. Weiss, N. Maheswaranathan, and S. Ganguli (2015). Deep unsupervised
learning using nonequilibrium thermodynamics. In _International conference on machine learning_,
pp. 2256–2265. PMLR.


Song, J., C. Meng, and S. Ermon (2020). Denoising diffusion implicit models. _arXiv preprint_
_arXiv:2010.02502_ .


Song, Y., C. Durkan, I. Murray, and S. Ermon (2021). Maximum likelihood training of score-based
diffusion models. _Advances in neural information processing systems 34_, 1415–1428.


Stark, H., B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola (2024). Dirichlet
flow matching with applications to dna sequence design. _arXiv preprint arXiv:2402.05841_ .


Sutton, R. S. and A. G. Barto (2018). _Reinforcement learning: An introduction_ . MIT press.


Tang, W. (2024). Fine-tuning of diffusion models via stochastic control: entropy regularization and
beyond. _arXiv preprint arXiv:2403.06279_ .


Tang, W. and H. Zhao (2024). Score-based diffusion models via stochastic differential equations–a
technical tutorial. _arXiv preprint arXiv:2402.07487_ .


Theodorou, E., J. Buchli, and S. Schaal (2010). A generalized path integral control approach to
reinforcement learning. _The Journal of Machine Learning Research 11_, 3137–3181.


42




Tiapkin, D., N. Morozov, A. Naumov, and D. Vetrov (2023). Generative flow networks as entropyregularized rl. _arXiv preprint arXiv:2310.12934_ .


Tong, A., N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, K. Fatras, G. Wolf, and Y. Bengio
(2023). Conditional flow matching: Simulation-free dynamic optimal transport. _arXiv preprint_
_arXiv:2302.00482_ .


Uehara, M. and W. Sun (2021). Pessimistic model-based offline reinforcement learning under
partial coverage. _arXiv preprint arXiv:2107.06226_ .


Uehara, M., Y. Zhao, K. Black, E. Hajiramezanali, G. Scalia, N. L. Diamant, A. M. Tseng,
T. Biancalani, and S. Levine (2024). Fine-tuning of continuous-time diffusion models as entropyregularized control. _arXiv preprint arXiv:2402.15194_ .


Uehara, M., Y. Zhao, K. Black, E. Hajiramezanali, G. Scalia, N. L. Diamant, A. M. Tseng, S. Levine,
and T. Biancalani (2024). Feedback efficient online fine-tuning of diffusion models. _arXiv preprint_
_arXiv:2402.16359_ .


Uehara, M., Y. Zhao, E. Hajiramezanali, G. Scalia, G. Eraslan, A. Lal, S. Levine, and T. Biancalani
(2024). Bridging model-based optimization and generative modeling via conservative fine-tuning
of diffusion models. _arXiv preprint arXiv:2405.19673_ .


Vargas, F., W. Grathwohl, and A. Doucet (2023). Denoising diffusion samplers. _arXiv preprint_
_arXiv:2302.13834_ .


Wallace, B., M. Dang, R. Rafailov, L. Zhou, A. Lou, S. Purushwalkam, S. Ermon, C. Xiong, S. Joty,
and N. Naik (2023). Diffusion model alignment using direct preference optimization. _arXiv_
_preprint arXiv:2311.12908_ .


Wang, Z., J. J. Hunt, and M. Zhou (2022). Diffusion policies as an expressive policy class for offline
reinforcement learning. _arXiv preprint arXiv:2208.06193_ .


Widatalla, T., R. Rafailov, and B. Hie (2024). Aligning protein generative models with experimental
fitness via direct preference optimization. _bioRxiv_, 2024–05.


Williams, G., A. Aldrich, and E. A. Theodorou (2017). Model predictive path integral control: From
theory to parallel computation. _Journal of Guidance, Control, and Dynamics 40_ (2), 344–357.


Wu, X., K. Sun, F. Zhu, R. Zhao, and H. Li (2023). Better aligning text-to-image models with
human preference. _arXiv preprint arXiv:2303.14420_ .


Wu, Y., G. Tucker, and O. Nachum (2019). Behavior regularized offline reinforcement learning.
_arXiv preprint arXiv:1911.11361_ .


Wulfmeier, M., P. Ondruska, and I. Posner (2015). Maximum entropy deep inverse reinforcement
learning. _arXiv preprint arXiv:1507.04888_ .


Xie, T., C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal (2021). Bellman-consistent pessimism
for offline reinforcement learning. _Advances in neural information processing systems 34_ .


43




Xu, M., L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang (2022). Geodiff: A geometric diffusion
model for molecular conformation generation. _arXiv preprint arXiv:2203.02923_ .


Yang, L., Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang (2023).
Diffusion models: A comprehensive survey of methods and applications. _ACM Computing_
_Surveys 56_ (4), 1–39.


Yu, T., G. Thomas, L. Yu, S. Ermon, J. Y. Zou, S. Levine, C. Finn, and T. Ma (2020). Mopo:
Model-based offline policy optimization. _Advances in Neural Information Processing Systems 33_,
14129–14142.


Yuan, H., K. Huang, C. Ni, M. Chen, and M. Wang (2023). Reward-directed conditional diffusion:
Provable distribution estimation and reward improvement. In _Thirty-seventh Conference on_
_Neural Information Processing Systems_ .


Zhang, H. and T. Xu (2023). Towards controllable diffusion models via reward-guided exploration.
_arXiv preprint arXiv:2304.07132_ .


Zhang, Q. and Y. Chen (2021). Path integral sampler: a stochastic control approach for sampling.
_arXiv preprint arXiv:2111.15141_ .


Zhao, Y., M. Uehara, G. Scalia, T. Biancalani, S. Levine, and E. Hajiramezanali (2024).
Adding conditional control to diffusion models with reinforcement learning. _arXiv preprint_
_arXiv:2406.12120_ .


Zhou, Z., S. Kearnes, L. Li, R. N. Zare, and P. Riley (2019). Optimization of molecules via deep
reinforcement learning. _Scientific reports 9_ (1), 10752.


Zhu, Z., H. Zhao, H. He, Y. Zhong, S. Zhang, Y. Yu, and W. Zhang (2023). Diffusion models for
reinforcement learning: A survey. _arXiv preprint arXiv:2311.01223_ .


Ziebart, B. D., A. L. Maas, J. A. Bagnell, A. K. Dey, et al. (2008). Maximum entropy inverse
reinforcement learning. In _Aaai_, Volume 8, pp. 1433–1438. Chicago, IL, USA.


44


