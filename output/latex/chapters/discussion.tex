\section{Discussion}
\label{sec:discussion}

Recursive Backwards Q-\ac{RBQL} successfully validates the hypothesis that persistent transition memory combined with backward BFS propagation significantly accelerates convergence in deterministic, sparse-reward environments. The experimental results demonstrate that RBQL achieves optimal policy convergence in an average of 4.8 episodes ($\pm$0.7), outperforming both standard Q-learning with $\alpha=0.5$ (11.7$\pm$2.5 episodes) and even Q-learning with $\alpha=1.0$---a baseline designed to match RBQL's update strength---by 1.37-fold (6.6$\pm$2.5 episodes). This performance gain is not attributable to aggressive learning rates, as the $\alpha=1.0$ comparison isolates the structural benefit of backward propagation: RBQL's batch value iteration over a cumulative transition graph ensures that all visited states receive updated Q-values informed by the most recent terminal reward, thereby eliminating the staleness of future-state estimates inherent in online updates \cite{29}. The low variance in convergence episodes further underscores RBQL's robustness, contrasting sharply with the high variability and frequent failure to converge within the episode limit observed in Q-learning variants. These findings confirm that RBQL's core innovation---recursively propagating rewards backward through an episodically built transition graph---enables dynamic programming-like value iteration without requiring explicit knowledge of the environment's transition dynamics \cite{12}.

The mechanism underlying RBQL's efficiency stems from its ability to enforce topological ordering of state updates via BFS traversal from the terminal state. In deterministic environments, this guarantees that each state's successor values are fully converged before its own Q-value is updated, ensuring the validity of Bellman backups and eliminating the need for repeated traversals to propagate reward information \cite{16}. This contrasts with standard Q-learning, which updates values sequentially during episodes using outdated estimates of future states \cite{29}, and with \ac{EBU}, which performs backward propagation only within a single episode and lacks cross-episode memory to accumulate reward signals \cite{13}. RBQL's persistent graph allows previously encountered terminal rewards to influence the value estimates of all prior states across multiple episodes, a feature absent in both EBU and RETRACE \cite{8}. Moreover, unlike Dyna-Q, which requires learning and simulating an internal model of transitions---introducing potential model bias and computational overhead \cite{19}---RBQL operates purely on observed transitions, making it both simpler and more reliable in domains where accurate modeling is infeasible. The scalability of this approach is further supported by results on larger mazes, where RBQL achieves up to 60-fold reductions in step efficiency compared to Q-learning, with performance gains growing disproportionately as problem size increases \cite{8}. This aligns with the theoretical claim that RBQL reduces sample complexity from $O(S^2)$ to $O(D)$, where $D$ is the path length \cite{1}, and suggests that its advantages become more pronounced in complex, high-dimensional deterministic tasks such as robotic path planning or strategic game \ac{AI} \cite{6}.

Despite its efficacy in deterministic settings, RBQL's current formulation is fundamentally limited by its assumption of environmental determinism. In stochastic environments---where transitions or rewards are subject to noise---the backward propagation of a single observed transition cannot reliably represent the true expected value, leading to biased updates. While model-based approaches such as Dyna-Q or MBEC mitigate this through learned transition models \cite{19,29}, RBQL's reliance on direct observation precludes such adaptation. Future extensions could incorporate uncertainty estimates into the transition graph, for instance by maintaining distributions over successor states or applying Bayesian updates to Q-values during backward propagation. Another promising direction involves integrating state abstraction techniques to compress the transition graph in large or continuous state spaces, a strategy that has shown success in reducing computational load while preserving value propagation fidelity \cite{8}. Additionally, the current implementation relies on optimistic initialization and $\varepsilon$-greedy exploration; future work could explore more sophisticated exploration policies, such as those informed by visitation counts or intrinsic motivation derived from transition graph novelty \cite{35}, to further accelerate the growth of the memory structure.

The performance advantages demonstrated here position RBQL as a bridge between model-free and dynamic programming paradigms, offering a novel pathway to sample-efficient learning without the need for explicit model acquisition. Its success in grid-based environments mirrors findings from Graph Backup and Topological Experience Replay, which leverage transition graph structures to improve credit assignment \cite{22,36}, but RBQL uniquely combines persistent memory with full Bellman backups over the entire observed state space. This distinguishes it from episodic control methods that store and replay high-reward trajectories without iterative value refinement \cite{37}, and from model-based approaches that incur simulation overhead \cite{19}. The empirical validation on a 15-state grid, corroborated by results from larger mazes \cite{8}, provides strong evidence for RBQL's potential in real-world applications where data collection is costly---such as robotic manipulation, autonomous navigation, or medical robotics \cite{6}. Future work should extend RBQL to partially observable Markov decision processes by integrating state estimation, and evaluate its performance on benchmark domains such as MiniGrid or MuJoCo with sparse rewards \cite{24}, to assess its generalizability beyond grid worlds. By formalizing the relationship between persistent memory and value iteration, RBQL opens a new avenue for designing sample-efficient RL algorithms that exploit structural properties of deterministic environments without relying on explicit modeling.