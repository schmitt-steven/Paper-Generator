\section{Discussion}
\label{sec:discussion}

\ac{RBQL} successfully validates the hypothesis that persistent transition memory combined with backward BFS propagation significantly reduces sample complexity in deterministic, sparse-reward environments. As demonstrated across 50 trials on a 15-state grid world, \ac{RBQL} achieved optimal policy convergence in an average of $4.8 \pm 0.7$ episodes, outperforming both standard Q-learning with $\alpha = 0.5$ ($11.7 \pm 2.5$ episodes) and the direct-learning baseline with $\alpha = 1.0$ ($6.6 \pm 2.5$ episodes). The $1.37\times$ improvement over $\alpha = 1.0$ Q-learning—despite identical exploration policies, initialization schemes, and effective learning rates—confirms that the performance gain stems not from aggressive updates but from the structural reorganization of credit assignment: by propagating terminal rewards backward through a persistent transition graph, \ac{RBQL} ensures that early-state values are updated using the most current estimates of future rewards, thereby eliminating the temporal delay inherent in sequential, online updates \cite{Diekhoff2024RecursiveBQ}. This mechanism effectively transforms model-free RL into a form of batch value iteration over the observed portion of the MDP, achieving dynamic programming-like convergence without requiring explicit transition models \cite{Diekhoff2024RecursiveBQ}.

The efficacy of \ac{RBQL} arises from its unique integration of three critical components: persistent transition storage, topologically ordered backward propagation via BFS, and deterministic Bellman updates with $\alpha = 1$. The persistent graph enables cross-episode reward accumulation, allowing a terminal reward from one episode to inform value estimates in subsequent episodes—a capability absent in single-trajectory backward methods such as \ac{EBU}, which updates values only within the episode in which they occur \cite{Lee2018SampleEfficientDR}. Unlike \ac{EBU}, \ac{RBQL} does not rely on a diffusion factor to mitigate overestimation; instead, it leverages the deterministic nature of the environment and complete backward propagation to ensure exact value convergence on first visit to any state \cite{Diekhoff2024RecursiveBQ}. Furthermore, \ac{RBQL}’s use of actual observed transitions distinguishes it from Dyna-Q and its variants, which rely on learned transition models to generate hypothetical experiences, thereby introducing model bias and computational overhead \cite{Ghasemi2024ACS}. While Dyna-Q accelerates learning through simulation, \ac{RBQL} achieves similar or superior sample efficiency without any model estimation step, making it more robust and computationally lightweight.

The convergence trajectories in Figure 1 reveal that \ac{RBQL} stabilizes within the first few episodes, with minimal variance across trials, whereas Q-learning exhibits erratic, gradual improvement over many episodes. This low variance is particularly advantageous in real-world applications where predictable sample requirements are essential for deployment planning \cite{Diekhoff2024RecursiveBQ}. The performance gap widens with problem complexity: in larger mazes (e.g., $50\times50$), \ac{RBQL} reduces average steps by a factor of over 60 compared to Q-learning, demonstrating that its efficiency scales favorably with state space size \cite{Diekhoff2024RecursiveBQ}. This is because \ac{RBQL}’s backward propagation bypasses the quadratic growth in non-optimal state visits that plague standard Q-learning, instead propagating rewards along linear paths of optimal transitions \cite{Diekhoff2024RecursiveBQ}. In contrast, methods like Graph Backup leverage transition graphs for credit assignment but do not guarantee full backward propagation from terminal states, limiting their ability to achieve optimal value assignments on first visit \cite{Jiang2022GraphBD}.

Despite its empirical success, \ac{RBQL} has critical limitations. First, it is inherently episodic and requires a terminal state to initiate backward propagation, rendering it inapplicable to continuous or non-episodic tasks. Second, the method assumes deterministic dynamics; in stochastic environments with noisy transitions or rewards, backward propagation may amplify estimation errors due to inconsistent state-action outcomes. While \ac{EBU} introduces a diffusion factor $\beta$ to stabilize updates in stochastic MDPs \cite{Lee2018SampleEfficientDR}, \ac{RBQL} currently lacks such a mechanism. Third, although the persistent graph avoids model learning, it incurs memory overhead proportional to the number of unique transitions observed—a trade-off that may become prohibitive in high-dimensional state spaces. Finally, the method does not incorporate reward shaping or intrinsic motivation techniques that could further accelerate exploration in sparse-reward domains \cite{Memarian2021SelfSupervisedOR,Park2025FromST}, though its performance remains robust without them, confirming that the core innovation lies in update structure rather than auxiliary enhancements.

Compared to related work, \ac{RBQL} occupies a distinct position. Unlike model-based approaches such as Dyna-Q or Goal-Space Planning, which rely on learned models for planning \cite{Ghasemi2024ACS,Lo2022GoalSpacePW}, \ac{RBQL} operates purely on observed transitions. Unlike \ac{EMDQN} or Model-Free Episodic Control, which store and replay high-reward trajectories \cite{lin2018,blundell2016}, \ac{RBQL} propagates values backward through the entire transition graph, enabling systemic updates rather than trajectory-based retrieval. Unlike SORS, which infers dense rewards from trajectory rankings \cite{Memarian2021SelfSupervisedOR}, \ac{RBQL} preserves the original sparse reward structure while accelerating its propagation. This positions \ac{RBQL} as a principled, model-free alternative to dynamic programming for deterministic environments—a rare combination that bridges the gap between online learning and offline value iteration \cite{Diekhoff2024RecursiveBQ}.

Future work should address \ac{RBQL}’s limitations through three concrete directions. First, extend the algorithm to stochastic environments by integrating a probabilistic transition model that estimates transition probabilities from observed frequencies, then apply weighted backward propagation using likelihood-based weighting—a hybrid approach analogous to Dyna-Delayed Q-learning but without full model learning \cite{Zehfroosh2020AHP}. Second, develop a state-space compression mechanism to reduce memory footprint in high-dimensional environments by clustering similar states using embedding-based representations, inspired by quasimetric learning for goal-reaching tasks \cite{Valieva2024QuasimetricVF}. Third, integrate \ac{RBQL} with intrinsic motivation frameworks—such as LLM-driven curiosity \cite{Quadros2025LLMDrivenIM} or toddler-inspired reward transitions \cite{Park2025FromST}—to enhance exploration in large, sparse-reward environments without altering the core backward propagation mechanism. These extensions would preserve \ac{RBQL}’s sample efficiency while expanding its applicability beyond deterministic, episodic domains.