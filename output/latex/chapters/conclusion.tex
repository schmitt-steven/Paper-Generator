\section{Conclusion}
\label{sec:conclusion}

\ac{RBQL} demonstrates that persistent transition memory combined with backward BFS propagation can significantly accelerate convergence in deterministic, episodic environments with sparse rewards. By deferring value updates until episode termination and propagating terminal rewards through a structured, graph-based representation of observed transitions, \ac{RBQL} eliminates the propagation delays inherent in standard Q-learning, enabling value iteration-like updates without requiring explicit transition models. Across 50 trials on a 15-state grid world, \ac{RBQL} achieved optimal policy convergence in an average of 4.8 $\pm$ 0.7 episodes, outperforming both standard Q-learning with $\alpha = 0.5$ (11.7 $\pm$ 2.5 episodes) and the direct-update baseline with $\alpha = 1.0$ (6.6 $\pm$ 2.5 episodes). The 1.37$\times$ improvement over $\alpha = 1.0$ Q-learning confirms that the benefit stems from the structural reorganization of credit assignment—not merely aggressive learning rates—and aligns with findings that backward propagation through historical transitions enables more accurate and stable value estimation \cite{Diekhoff2024RecursiveBQ}. The narrow interquartile range of \ac{RBQL}’s convergence times further indicates high consistency, a critical advantage in deployment-constrained applications where sample efficiency and predictability are paramount.

The method’s efficacy arises from its integration of three core components: persistent transition storage, topologically ordered backward updates via BFS, and deterministic Bellman corrections with $\alpha = 1$. This structure ensures that each state’s value is updated only after all its successors have been finalized, preventing the use of stale estimates that plague online methods. Unlike Episodic Backward Update \cite{Lee2018SampleEfficientDR}, which operates within single episodes and requires diffusion factors to stabilize learning, \ac{RBQL} aggregates transitions across episodes, enabling reward signals from one episode to inform subsequent value estimates. Similarly, while Graph Backup \cite{Jiang2022GraphBD} exploits transition graphs for counterfactual credit assignment, it does not guarantee full backward propagation from terminal states, whereas \ac{RBQL}’s BFS-based ordering ensures exact value convergence upon first visit to any state. Furthermore, unlike Dyna-Q \cite{Ghasemi2024ACS}, \ac{RBQL} requires no learned transition model, eliminating the risk of model bias and computational overhead associated with simulation.

Despite its empirical success, \ac{RBQL} remains limited to deterministic, episodic settings. The absence of mechanisms to handle stochastic transitions or reward noise leaves it vulnerable to estimation errors in uncertain environments. Additionally, the persistent transition graph incurs memory costs proportional to the number of unique transitions observed—a trade-off that may become prohibitive in high-dimensional state spaces. Future work should extend \ac{RBQL} to stochastic domains by incorporating probabilistic transition estimates derived from observed frequencies, enabling weighted backward propagation that accounts for outcome uncertainty \cite{Zehfroosh2020AHP}. This hybrid approach would preserve \ac{RBQL}’s sample efficiency while broadening its applicability beyond deterministic systems.