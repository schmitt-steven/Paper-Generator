\section{Conclusion}
\label{sec:conclusion}

Recursive Backwards Q-\ac{RBQL} demonstrates that persistent transition memory and backward BFS propagation can significantly accelerate convergence in deterministic, sparse-reward environments by transforming model-free reinforcement learning into a dynamic programming-like process without requiring explicit transition models. Across 50 trials on a 15-state grid, RBQL achieved optimal policy convergence in an average of 4.8 episodes (±0.7), outperforming standard Q-learning with $\alpha=1.0$—its most direct comparable baseline—by 1.37-fold (6.6±2.5 episodes), and standard Q-learning with $\alpha=0.5$ by 2.45-fold (11.7±2.5 episodes). The low variance in RBQL’s convergence times, contrasted with the high variability and frequent failure to converge within the episode limit in Q-learning variants, confirms that backward propagation eliminates the staleness of future-state estimates inherent in online updates. This structural advantage enables RBQL to propagate terminal rewards across all previously observed transitions in a single batch update, ensuring that every state receives an updated value informed by the most recent reward signal. The method’s efficiency scales with path length rather than state space size, consistent with theoretical expectations that such approaches reduce sample complexity from $O(S^2)$ to $O(D)$. While RBQL is currently restricted to deterministic environments due to its reliance on single-successor transitions, its architecture provides a clear pathway for extension—such as incorporating uncertainty estimates or state abstraction—to address stochasticity and scalability in larger domains. The results validate that structured use of observed transitions, rather than learned models or complex exploration heuristics, is sufficient to achieve value iteration-like efficiency. A natural next step is to integrate RBQL with state abstraction techniques to extend its applicability to high-dimensional deterministic systems, such as robotic path planning in structured environments, where computational efficiency and sample reduction are critical for deployment.