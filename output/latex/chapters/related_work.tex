\section{Related Work}
\label{sec:related_work}

Recent advances in sample-efficient reinforcement learning have sought to address the inefficiencies of standard model-free methods in deterministic, sparse-reward environments by leveraging structured value propagation and episodic memory. While traditional Q-learning updates state-action values incrementally during episode execution, its sequential update rule leads to delayed and inaccurate propagation of terminal rewards—particularly in long-horizon tasks where early states rely on stale estimates of future values, resulting in O(S\textsuperscript{2}) sample complexity for convergence in large state spaces \cite{diekhoff2024}. This limitation has motivated a suite of approaches that decouple value updates from online interaction, instead exploiting the structure of observed trajectories to accelerate learning.

A prominent line of work in this direction is \ac{EBU}, which propagates rewards backward through entire episodes in reverse chronological order, aligning value updates with causal reward dependencies and mimicking human-like backward reasoning \cite{lee2018}. EBU theoretically guarantees convergence in deterministic MDPs and demonstrates substantial sample efficiency gains on Atari games, achieving DQN-level performance with only 5--10\% of the required samples \cite{lee2018,lee2018}. Similarly, Graph Backup extends this idea by modeling transition data as a directed graph and performing counterfactual backups across multiple trajectories, enabling reward propagation beyond single-episode boundaries \cite{jiang2022}. By aggregating information from subgraphs and averaging over multiple next-state transitions, Graph Backup reduces variance and improves value estimation stability in discrete environments such as MiniGrid and Minatar \cite{jiang2022,jiang2022}. However, both EBU and Graph Backup remain constrained to single-episode or trajectory-local updates; they do not maintain a persistent, cross-episode transition graph that enables global value iteration over all known states after each episode. Consequently, their backward propagation is confined to the current trajectory and cannot leverage historical transitions to refine earlier state estimates in subsequent episodes.

Model-based approaches such as Dyna-Q \cite{sutton1990} and Goal-\ac{GSP} \cite{lo2022} attempt to overcome sample inefficiency by learning an internal model of the environment. Dyna-Q simulates future transitions using a learned transition model, allowing hypothetical rollouts to supplement real experience; however, this introduces model bias and computational overhead from generating potentially inaccurate or invalid state predictions \cite{luo2022,lo2022}. GSP circumvents full dynamics modeling by learning local, subgoal-conditioned transition models and using potential-based reward shaping to propagate value estimates between abstract subgoals \cite{lo2022,lo2022}. While GSP demonstrates robustness to model inaccuracies and integrates seamlessly with standard algorithms like Sarsa(\textlambda), it requires manual or automated subgoal discovery and does not perform full Bellman updates over the entire state space. In contrast, RBQL operates without any learned transition model—relying solely on observed transitions—and performs exact, batch Bellman updates over the entire persistent transition graph after each episode, achieving dynamic programming-like convergence without model estimation.

Another class of methods leverages episodic memory to replay high-reward trajectories, drawing inspiration from biological systems. Model-Free Episodic Control \cite{blundell2016} and Episodic Memory Deep Q-\ac{EMDQN} \cite{lin2018} store successful episodes and directly reuse their action-value estimates to accelerate learning, particularly in early stages. Model-\ac{MBEC} further refines this by combining episodic memory with a learned dynamics model to compute action-values via trajectory simulation, using TD error to refine stored values \cite{le2021}. While these approaches demonstrate impressive sample efficiency, they rely on direct memory retrieval and do not propagate values backward through the graph structure of transitions. Their updates are local to stored trajectories and lack the systematic, topological reordering necessary for global value convergence.

\ac{TER} \cite{hong2022} introduces a related but distinct perspective by reordering transitions in the replay buffer according to state dependencies encoded in a transition graph, thereby improving value function learning through structured update sequences. However, TER operates on a static replay buffer and does not perform backward propagation or recursive Bellman updates—it merely optimizes the order of existing samples. In contrast, RBQL dynamically constructs a persistent transition graph across episodes and performs full backward BFS-based value iteration after each episode, ensuring that every state-action pair is updated using the most recently propagated terminal rewards. This eliminates the need for repeated exposure to the same trajectory and enables true value iteration in model-free settings.

Recent theoretical work has shown that Q-learning with UCB exploration can achieve near-optimal sample complexity in episodic MDPs, matching model-based regret bounds up to a \(\sqrt{H}\) factor \cite{rastogi2020}, and that PAC guarantees can be established for hybrid algorithms like DDQ \cite{zehfroosh2020}. Yet these approaches still rely on incremental, online updates and do not exploit the structure of observed transitions to enable batch value iteration. Similarly, \ac{MFPT}-based methods accelerate goal-directed learning by estimating reachability between states, but they require explicit computation of transition probabilities and are computationally prohibitive in large or high-dimensional state spaces \cite{debnath2019,debnath2019}. RBQL bypasses these limitations by directly using observed transitions as the basis for value updates, avoiding model estimation entirely while still achieving dynamic programming-level convergence.

Finally, methods such as Self-\ac{SORS} \cite{memarian2021} and \ac{HPG} \cite{rauber2017} address sparse rewards by inferring dense reward signals from trajectories, thereby improving gradient-based learning. However, these methods rely on auxiliary objectives and reward function learning, which may alter the original MDP’s policy structure or require careful tuning. RBQL requires no reward engineering, preserves the original sparse reward structure, and directly modifies value updates to resolve the temporal credit assignment problem at its root.

In summary, while prior work has made significant strides in improving sample efficiency through episodic memory \cite{blundell2016,lin2018}, trajectory reordering \cite{hong2022}, model-based simulation \cite{sutton1990,lo2022}, and backward value propagation \cite{lee2018,jiang2022}, none combine the three core innovations of RBQL: (1) persistent storage of all observed transitions across episodes, (2) backward BFS propagation to enforce topological ordering for Bellman updates, and (3) batch value iteration using \(\alpha=1\) to achieve exact convergence in deterministic environments. RBQL thus establishes a new paradigm: model-free reinforcement learning that emulates dynamic programming through structured, cross-episode memory—enabling true value iteration without explicit transition models.