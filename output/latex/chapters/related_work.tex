\section{Related Work}
\label{sec:related_work}

Standard Q-learning \cite{Diekhoff2024RecursiveBQ} operates as a model-free temporal difference method that updates Q-values incrementally via single-step Bellman backups with a learning rate $\alpha < 1$, requiring multiple visits to the same state-action pair to propagate reward signals from terminal states. In deterministic, episodic environments with sparse rewards—such as grid worlds or Pong-like tasks—this leads to severe sample inefficiency, as the backward propagation of value information is slow and fragmented across episodes \cite{Diekhoff2024RecursiveBQ}. Each transition update contributes only a fraction of the true return, and convergence relies on repeated exploration to average over stochastic-like estimation errors, even when the environment is fully deterministic. This fundamental limitation stems from the algorithm’s inability to exploit the structural property that, in a deterministic MDP, a single successful episode contains sufficient information to compute exact optimal Q-values for all visited states—information that is discarded after each update.

\ac{MBRL} attempts to mitigate this inefficiency by learning an internal model of the environment’s dynamics and using it to simulate future transitions for planning \cite{Kaelbling1996ReinforcementLA}. Dyna-Q \cite{Kaelbling1996ReinforcementLA} exemplifies this approach: after each real interaction, it performs multiple simulated backups using the learned model to update Q-values via value iteration. However, Dyna-Q’s updates remain forward-looking and iterative—simulating possible next states from the current state—and require repeated planning steps to propagate reward information backward over long horizons. Crucially, Dyna-Q does not perform exact Bellman updates with $\alpha=1$ nor leverage backward traversal; instead, it approximates value iteration through simulated rollouts, introducing additional variance and computational overhead without guaranteeing single-episode convergence \cite{Kaelbling1996ReinforcementLA}. In contrast, \ac{RBQL} eliminates simulation entirely: upon reaching a terminal state, it performs an exact, one-time backward propagation over the \textit{actual} experienced transitions using $\alpha=1$, ensuring immediate and precise value assignment without averaging or iteration.

Dynamic programming methods such as \ac{VI} \cite{Kaelbling1996ReinforcementLA} offer optimal convergence guarantees in known MDPs by iteratively updating all state-action pairs using the full Bellman optimality equation. However, \ac{VI} requires complete knowledge of the transition and reward functions—a strong assumption that renders it inapplicable to unknown environments. \ac{RBQL}, by contrast, operates without prior model knowledge; it incrementally constructs a transition model through interaction and performs backward Bellman updates \textit{only} over visited states, making it applicable to online learning in partially unknown deterministic MDPs. While some variants of “online \ac{VI}” have been proposed \cite{Kaelbling1996ReinforcementLA}, none combine episodic model building, backward BFS propagation, and $\alpha=1$ updates in a single framework. \ac{RBQL} is the first to demonstrate that such an approach can achieve exact optimal Q-values in finite episodes without requiring full state-space knowledge or iterative sweeps.

Monte Carlo methods \cite{Kaelbling1996ReinforcementLA} compute value estimates by averaging returns over complete episodes, which eliminates bootstrapping bias but introduces high variance and requires multiple episode completions to converge. In deterministic environments, this averaging is redundant: the return from a given state-action pair under a fixed policy is deterministic and identical across episodes. \ac{RBQL} exploits this determinism by computing the exact return in a single pass via backward propagation, eliminating variance entirely and achieving convergence after the first successful episode. Unlike Monte Carlo, \ac{RBQL} does not rely on episode averaging or require multiple trajectories to reduce noise—it leverages the structure of deterministic dynamics to perform a single, exact Bellman backup per episode.

Recent work on expected eligibility traces \cite{Hasselt2020ExpectedET} extends credit assignment by considering counterfactual trajectories, but remains fundamentally model-free and focused on distributing reward across \textit{recent} states using trace decay mechanisms. These methods still rely on incremental updates with $\alpha < 1$ and do not enable exact, one-shot value propagation from terminal states. Similarly, backpropagation in deep RL \cite{Arulkumaran2017DeepRL} refers to gradient computation through neural networks—not backward value propagation over transition graphs—and is irrelevant in tabular deterministic settings where \ac{RBQL} operates.

To our knowledge, no prior algorithm has combined three critical elements: (1) persistent storage of episodic transitions to form a complete backward graph, (2) breadth-first search over this graph to determine update order by distance from the terminal state, and (3) exact Bellman updates with $\alpha=1$ applied in a single backward pass. While some works have explored reverse \ac{TD} learning or backward induction in planning contexts \cite{Kaelbling1996ReinforcementLA}, none apply these mechanisms to online, model-based RL with episodic growth and deterministic dynamics. \ac{RBQL}’s backward propagation is not a form of planning or simulation—it is an exact, deterministic value update procedure enabled by the structure of the environment and the persistence of experience. This distinction is fundamental: Dyna-Q simulates \textit{future} transitions; Monte Carlo averages \textit{past} returns; \ac{VI} iterates over the \textit{entire} state space; \ac{RBQL} propagates \textit{past rewards backward} through an episodically built model to update visited states exactly once. The result is a paradigm shift: in deterministic episodic MDPs, optimal Q-values need not be learned through repeated trials—they can be computed in a single sweep after the first success. This theoretical insight, empirically validated by our results showing up to 60-fold reductions in episodes to convergence \cite{Diekhoff2024RecursiveBQ}, establishes \ac{RBQL} as the first algorithm to formally exploit deterministic structure in this manner.