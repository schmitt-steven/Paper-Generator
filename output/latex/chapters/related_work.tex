\section{Related Work}
\label{sec:related_work}

Recent advances in sample-efficient reinforcement learning have explored diverse strategies to accelerate value propagation in deterministic, sparse-reward environments. Among these, methods that leverage episodic structure and historical transition data have emerged as particularly promising avenues for reducing sample complexity. We situate \ac{RBQL} within three thematic clusters: episodic backward update mechanisms, graph-based value backup frameworks, and model-free dynamic programming hybrids.

Episodic backward update techniques have demonstrated significant gains in sample efficiency by reordering value updates to propagate terminal rewards from end to start of an episode. Lee et al. \cite{Lee2018SampleEfficientDR} introduced \ac{EBU}, a model-free method that processes transitions in reverse chronological order within each episode, ensuring that rewards are directly linked to their causal predecessors. This approach eliminates the delay inherent in standard Q-learning’s forward updates and theoretically guarantees convergence while reducing sample requirements by up to 90\% on Atari benchmarks \cite{Lee2018SampleEfficientDR}. However, \ac{EBU} operates strictly within the confines of a single episode and does not retain or reuse transitions across episodes. In contrast, \ac{RBQL} extends this principle by maintaining a persistent transition graph that accumulates state-action-reward observations across multiple episodes, enabling backward propagation of terminal rewards not just within a trajectory but throughout the entire known state space. This cross-episode consolidation allows \ac{RBQL} to leverage previously discovered paths to accelerate convergence in subsequent episodes—a capability \ac{EBU} lacks. Furthermore, while \ac{EBU} employs adaptive diffusion factors to stabilize value propagation \cite{Lee2018SampleEfficientDR}, \ac{RBQL} achieves exact convergence in deterministic settings through a single backward pass with $\alpha=1$, eliminating the need for hyperparameter tuning of propagation weights.

Another line of work focuses on exploiting graph structures to enhance data efficiency in value estimation. Jiang et al. \cite{Jiang2022GraphBD} proposed Graph Backup, a method that represents MDP transitions as a directed graph and aggregates counterfactual updates across multiple trajectories to compute weighted value targets. By leveraging the topology of observed transitions—particularly in repetitive or low-degree environments—Graph Backup reduces variance and improves credit assignment beyond one-step or multi-step backups. However, Graph Backup retains a model-free bootstrapping structure and does not perform full Bellman updates over the entire graph; instead, it computes weighted averages of next-state values based on visitation counts. \ac{RBQL} diverges fundamentally by treating the persistent transition graph as a dynamic model of the environment and applying exact value iteration over all known states after each episode. This transforms \ac{RBQL} into a form of model-free dynamic programming: it does not require learned transition probabilities like \ac{DYNA-Q} \cite{sutton1990} or subgoal models \cite{Lo2022GoalSpacePW}, yet achieves value iteration--like updates by recursively applying the Bellman optimality equation to all observed transitions. Unlike Graph Backup, which is designed for stochastic environments and uses visitation-based weighting \cite{Jiang2022GraphBD}, \ac{RBQL} exploits determinism to ensure exact, non-stochastic propagation of terminal rewards without averaging or approximation.

The closest conceptual relatives to \ac{RBQL} are hybrid methods that blend episodic memory with model-based planning. Le et al. \cite{Le2021ModelBasedEM} developed \ac{MBEC}, which encodes trajectories as compressed representations and retrieves them for value estimation via nearest-neighbor search. \ac{MBEC} dynamically fuses episodic recall with parametric Q-networks, enabling fast adaptation to new goals. However, \ac{MBEC} relies on learned transition models for encoding and requires careful memory management to avoid noise amplification \cite{Le2021ModelBasedEM}. In contrast, \ac{RBQL} requires no parameterized model or encoding function—it directly uses raw observed transitions to construct a transition graph and performs deterministic value iteration. Similarly, \ac{GSP} \cite{Lo2022GoalSpacePW} accelerates learning by propagating values over abstract subgoals rather than full states, avoiding the need to model complete transition dynamics. Yet \ac{GSP} still requires learning local subgoal-conditioned models and relies on value iteration over a reduced goal space. \ac{RBQL} operates at the level of raw states, eliminating the need for abstraction or subgoal discovery while still achieving value iteration--like convergence. Moreover, \ac{RBQL}’s backward BFS propagation ensures topological correctness in deterministic environments without requiring prior knowledge of subgoals or transition structure.

Dynamic programming methods such as value iteration \cite{Ghasemi2024ACS} provide the theoretical foundation for \ac{RBQL}’s update rule, but they are inapplicable to large-scale problems due to their requirement of complete knowledge of the transition and reward models. \ac{RBQL} circumvents this limitation by constructing an empirical model from observed transitions alone—effectively performing tabular value iteration without explicit environmental modeling. This positions \ac{RBQL} as a bridge between model-free and model-based paradigms: it retains the sample efficiency of dynamic programming while preserving the practicality of model-free methods. Unlike \ac{DYNA-Q} \cite{sutton1990}, which simulates hypothetical transitions from a learned model and incurs model bias, \ac{RBQL} uses only real observed transitions. Unlike R-MAX \cite{brafman2003} or PAC-MDP algorithms \cite{Zehfroosh2020AHP}, which rely on explicit exploration bonuses or PAC guarantees to bound sample complexity, \ac{RBQL} achieves convergence through structural exploitation of determinism and backward propagation. The algorithm’s reliance on terminal states for initiation of updates limits its applicability to episodic tasks \cite{Diekhoff2024RecursiveBQ}, but this constraint is precisely what enables its efficiency in domains like robotic path planning and strategic game AI, where episodes are naturally bounded.

In summary, while prior work has advanced sample efficiency through backward update ordering \cite{Lee2018SampleEfficientDR}, graph-based value aggregation \cite{Jiang2022GraphBD}, or abstract planning with subgoals \cite{Lo2022GoalSpacePW}, none combine persistent transition memory, full-state Bellman updates, and backward propagation in a model-free framework. \ac{RBQL} fills this critical gap by enabling true dynamic programming-style convergence without requiring transition models, thereby offering a novel and theoretically grounded approach to sample-efficient reinforcement learning in deterministic environments.