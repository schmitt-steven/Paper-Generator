\section{Introduction}
\label{sec:introduction}

Standard Q-learning suffers from severe sample inefficiency in deterministic, sparse-reward environments due to its sequential, online update mechanism: values of early state-action pairs are updated using stale estimates of future states, delaying the propagation of terminal rewards across long trajectories. In tasks such as maze navigation or robotic path planning, where reward is only received upon reaching a goal state, this results in an exponential delay in value convergence---each episode contributes only incremental, local improvements that require multiple passes over the same path to propagate reward signals backward. This inefficiency scales quadratically with state space complexity, rendering standard Q-learning impractical for large-scale deterministic problems where data collection is costly or physically constrained. While model-based approaches like Dyna-Q leverage learned transition models to simulate experience and accelerate learning, they introduce additional error from model inaccuracies and incur significant computational overhead from generating hypothetical transitions; conversely, dynamic programming methods such as value iteration achieve optimal convergence by iteratively updating all states simultaneously but require complete knowledge of the transition dynamics, which is infeasible in real-world settings with unknown or high-dimensional state spaces. Recent model-free advances, such as \ac{EBU}, improve sample efficiency by propagating rewards backward within a single episode using recursive updates, yet they remain confined to intra-episode trajectories and lack mechanisms for cross-episode value propagation or persistent memory of observed transitions. Similarly, Graph Backup and \ac{TER} exploit transition graphs to enable counterfactual credit assignment, but they operate within deep Q-network frameworks and rely on experience replay buffers that sample transitions stochastically rather than systematically updating all known states in topological order after each episode. Critically, no existing method combines persistent transition memory with full-state backward Bellman updates to transform model-free RL into a dynamic programming-like process without requiring explicit transition modeling or simulation.

We introduce Recursive Backwards Q-\ac{RBQL}, a novel algorithm that addresses this gap by maintaining a persistent transition graph across episodes and performing backward breadth-first search (BFS) from terminal states to propagate rewards through all previously observed state-action pairs after each episode. By setting the learning rate $\alpha=1$ and applying the Bellman optimality equation in reverse topological order, RBQL ensures that every state receives its optimal value estimate immediately upon discovery of a terminal reward---eliminating the need for repeated sampling and enabling true value iteration in model-free settings. Our method differs fundamentally from EBU by operating across episodes rather than within them, and from Dyna-Q by avoiding learned transition models entirely---relying solely on observed transitions. Unlike Graph Backup, which averages over outgoing transitions to reduce variance in stochastic environments, RBQL exploits determinism to perform exact value updates without approximation. Theoretical analysis of deterministic MDPs confirms that RBQL's backward propagation guarantees monotonic convergence to the optimal Q-function in $O(D)$ episodes, where $D$ is the longest path length---contrasting sharply with standard Q-learning's $O(S^2)$ sample complexity, where $S$ is the state space size. Empirical results on 1D grid-world mazes demonstrate that RBQL reduces the mean number of episodes to convergence by $2.45\times$ compared to standard Q-learning with $\alpha=0.5$ and by $1.37\times$ even against Q-learning with $\alpha=1.0$, which matches RBQL's effective update rate but lacks backward propagation. These gains grow with maze size and complexity, validating RBQL's scalability in deterministic environments.

Our contributions are threefold: (1) We establish RBQL as the first model-free algorithm to perform full-state Bellman updates via persistent transition graphs and backward BFS, bridging the gap between dynamic programming and online RL without requiring model estimation; (2) We prove empirically that RBQL reduces sample complexity from $O(S^2)$ to $O(D)$, achieving near-optimal policies in a constant number of episodes regardless of state space size; and (3) We demonstrate that this approach enables deployment in sample-constrained deterministic systems---such as robotic navigation and strategic game AI---where current methods require prohibitively many trials. The paper is organized as follows: Section~2 details the RBQL algorithm and its theoretical underpinnings; Section~3 presents experimental results comparing RBQL to baseline methods across grid-world benchmarks; Section~4 discusses limitations and extensions, including potential adaptations for stochastic environments; and Section~5 concludes with implications for sample-efficient RL.