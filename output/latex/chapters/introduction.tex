\section{Introduction}
\label{sec:introduction}

In deterministic reinforcement learning environments with sparse rewards, standard Q-learning suffers from severe sample inefficiency due to its sequential, online update mechanism. When a terminal reward is received only at the end of an episode—such as in maze navigation or robotic path planning—the value signal must propagate backward through the entire trajectory to inform earlier state-action pairs. However, because Q-learning updates values incrementally during episode execution using outdated estimates of future states, early transitions are corrected only after multiple passes through the same path \cite{Diekhoff2024RecursiveBQ}. This results in a convergence rate that scales poorly with path length and state space size, often requiring O(S²) episodes to stabilize in large environments \cite{Diekhoff2024RecursiveBQ}. The inefficiency is exacerbated in real-world applications such as robotics, where each physical trial consumes time, energy, and hardware resources, making sample complexity a critical bottleneck for deployment \cite{Diekhoff2024RecursiveBQ}. While model-based methods like Dyna-Q \cite{sutton1990} and Goal-Space Planning \cite{Lo2022GoalSpacePW} attempt to accelerate learning by simulating transitions or leveraging subgoal models, they rely on learned environmental dynamics that introduce bias and computational overhead. Meanwhile, dynamic programming approaches such as value iteration \cite{Ghasemi2024ACS} offer optimal convergence guarantees but require complete knowledge of transition dynamics—making them infeasible for large-scale or unknown environments.

Recent advances in sample-efficient RL have explored alternative update mechanisms to accelerate reward propagation. \ac{EBU} \cite{Lee2018SampleEfficientDR} and \ac{TER} \cite{Hong2022TopologicalER} both exploit backward value propagation from terminal states within a single episode, demonstrating significant improvements over standard Q-learning by enforcing topological ordering of updates. Similarly, Graph Backup \cite{Jiang2022GraphBD} treats transition data as a directed graph to enable counterfactual credit assignment across trajectories, improving stability and convergence in sparse-reward settings. However, these methods remain confined to intra-episode updates or replay-buffer-based reordering without persistent cross-episode memory. \ac{TER}, for instance, constructs graphs from the replay buffer but does not retain or accumulate transition structures across episodes \cite{Hong2022TopologicalER}, limiting its ability to propagate rewards from distant past successes. Dyna-Q \cite{sutton1990} and QGRAPH-bounded Q-learning \cite{Hoppe2019QgraphboundedQS} leverage memory structures but depend on learned transition models or subgraph extraction, introducing model error and additional complexity. Crucially, no existing method combines persistent transition memory with full-state backward Bellman updates across episodes to achieve dynamic programming-like convergence without requiring explicit environmental modeling.

We introduce \ac{RBQL}, a model-free algorithm that resolves this gap by maintaining a persistent transition graph across episodes and performing batch backward value iteration via breadth-first search (BFS) upon episode termination. Unlike prior approaches, \ac{RBQL} does not rely on learned models or synthetic transitions; instead, it directly reuses observed state-action-reward triples to recursively update all known states using the Bellman optimality equation with $\alpha=1$, ensuring optimal value propagation from terminal rewards \cite{Diekhoff2024RecursiveBQ}. This approach transforms online Q-learning into a batch value iteration process over the accumulated transition graph, enabling true cross-episode reward diffusion. We demonstrate that \ac{RBQL} reduces sample complexity from O(S²) to O(D), where D is the longest path length in deterministic environments—dramatically accelerating convergence. In a 15-state grid world, \ac{RBQL} achieves optimal policy in 4.8 ± 0.7 episodes on average, compared to 6.6 ± 2.5 for Q-learning with $\alpha=1.0$ and 11.7 ± 2.5 for standard Q-learning with $\alpha=0.5$, representing a 1.37× and 2.45× speedup respectively. This performance gain is not due to enhanced exploration but to fundamentally improved value propagation mechanics.

Our contribution is threefold: (1) We formalize \ac{RBQL} as a novel model-free algorithm that leverages persistent transition graphs to enable backward BFS propagation of terminal rewards, bridging the gap between dynamic programming and online RL; (2) We empirically validate that \ac{RBQL} achieves near-optimal convergence in deterministic sparse-reward environments with orders-of-magnitude fewer episodes than standard Q-learning, even when compared to $\alpha=1.0$ updates; and (3) We establish a practical framework for sample-efficient RL that requires no model learning, no reward shaping, and no synthetic data generation. The paper is structured as follows: Section 2 details the \ac{RBQL} algorithm and its theoretical underpinnings; Section 3 presents experimental results on grid-world benchmarks demonstrating its sample efficiency gains; and Section 4 discusses implications, limitations, and future extensions to stochastic environments.