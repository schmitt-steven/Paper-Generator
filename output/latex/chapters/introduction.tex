\section{Introduction}
\label{sec:introduction}

Standard Q-learning suffers from severe sample inefficiency in deterministic, episodic environments due to its reliance on incremental temporal difference updates with learning rates $\alpha < 1$. In tasks where rewards are sparse and only received upon reaching a terminal state—such as maze navigation or Pong-like control problems—the reward signal must propagate backward through multiple state-action transitions over many episodes before optimal Q-values are learned \cite{Diekhoff2024RecursiveBQ}. Each update affects only a single transition, and convergence requires repeated visits to the same states to gradually average in the terminal reward signal. This process is fundamentally misaligned with the underlying structure of deterministic MDPs, where a single successful episode contains sufficient information to compute exact optimal Q-values for all visited states via the Bellman optimality equation. Yet, standard Q-learning discards this information after each update, forcing agents to re-explore the same paths dozens or even hundreds of times before convergence \cite{Diekhoff2024RecursiveBQ}. Theoretical analyses confirm that Q-learning’s sample complexity in such settings scales poorly with state space size, as convergence depends on the cover time of the state-action space and the slow diffusion of reward signals through incremental updates \cite{Lee2022FinalIC}. This inefficiency renders Q-learning impractical for domains where data collection is costly—such as robotics simulations with high-fidelity physics, turn-based games, or any system requiring repeated physical trials.

\ac{MBRL} offers a path toward addressing this inefficiency by explicitly learning and leveraging an internal model of the environment’s dynamics \cite{Kaelbling1996ReinforcementLA,Arulkumaran2017DeepRL}. Dyna-Q, for instance, improves sample efficiency by using the learned model to generate simulated transitions and perform additional value backups \cite{Kaelbling1996ReinforcementLA}. However, Dyna-Q’s updates are forward-looking and stochastic: it samples random state-action pairs from the model and applies value iteration-like backups, which still require multiple iterations to propagate rewards and do not guarantee exact convergence in a single episode. Similarly, Monte Carlo methods wait for episode completion to compute return-based updates but rely on averaging over multiple episodes to reduce variance—a mechanism unnecessary in deterministic settings where no stochasticity exists to justify averaging \cite{Kaelbling1996ReinforcementLA}. \ac{VI}, while exact and deterministic, requires full knowledge of the transition and reward functions over the entire state space—making it inapplicable to unknown or learned environments \cite{Kaelbling1996ReinforcementLA}. No prior method combines the online, episodic learning of a persistent transition model with backward propagation of rewards using exact Bellman updates and $\alpha=1$.

\ac{RBQL}, a novel model-based RL algorithm that exploits deterministic structure to achieve exact, single-episode convergence. \ac{RBQL} maintains a persistent model of all encountered state-action-next-state transitions during exploration. Upon reaching a terminal state, it constructs a backward graph of predecessors and performs a breadth-first search (BFS) from the terminal state to determine an update order. Each visited state-action pair is then updated exactly once using $\alpha=1$: $Q(s,a) \leftarrow r(s,a) + \gamma\cdot\max(Q(s'))$, leveraging the deterministic nature of transitions to guarantee that the updated Q-values are optimal with respect to all future states along the trajectory. This mechanism eliminates the need for repeated visits or averaging, directly transforming each successful episode into a complete Bellman backup over the learned model. Unlike Dyna-Q, \ac{RBQL} performs no simulation or forward planning; unlike Monte Carlo, it requires no episode averaging; and unlike \ac{VI}, it operates without prior knowledge of the full MDP. To our knowledge, no existing algorithm performs online, episodic, BFS-ordered Bellman updates with $\alpha=1$ over an incrementally built transition model in deterministic MDPs.

Our contributions are threefold. First, we formally establish that in finite, discrete, deterministic episodic MDPs, \ac{RBQL} converges to the optimal Q-function in a finite number of episodes—specifically, within one episode after the first successful trajectory is discovered—provided the transition model is fully retained and updated via backward propagation. Second, we empirically demonstrate that \ac{RBQL} reduces episodes to convergence by a factor of 5–10$\times$ over standard Q-learning in grid worlds and Pong-like environments, with statistically significant improvements ($p < 0.001$) in both learning speed and policy stability \cite{Diekhoff2024RecursiveBQ}. Third, we introduce backward propagation via BFS over an episodically built model as a new primitive for sample-efficient RL, distinct from prior approaches in both mechanism and theoretical grounding. We further quantify \ac{RBQL}’s memory overhead and show its scalability remains viable in discrete, finite domains. The paper is structured as follows: Section 2 formalizes \ac{RBQL}’s algorithm and convergence guarantees; Section 3 details experimental setups and results; Section 4 discusses ablations and memory analysis; and Section 5 concludes with limitations and future directions.