\section{Methods}
\label{sec:methods}

Recursive Backwards Q-Learning (\ac{RBQL}) is a model-based reinforcement learning algorithm designed for deterministic, episodic Markov Decision Processes (MDPs) with discrete states and actions. Unlike standard Q-learning, which updates \ac{TD} learning with a small learning rate $\alpha < 1$, \ac{RBQL} exploits the deterministic structure of the environment to perform exact Bellman backups over all visited state-action pairs in a single backward pass after each episode. This is achieved through a persistent transition model that records every observed $(s, a) \rightarrow (s', r)$ transition throughout the learning process. Upon reaching a terminal state, \ac{RBQL} constructs a backward graph by inverting the transition model: for each state $s'$ reached via action $a$, it identifies all predecessor states $s$ such that $(s, a) \rightarrow (s', r)$ exists. A breadth-first search (BFS) is then initiated from the terminal state, traversing this backward graph to determine a topological update order based on distance from the terminal. Q-values are updated in this reverse order using the Bellman optimality equation with a learning rate of $\alpha = 1$:  
$$
Q(s, a) \leftarrow r(s, a) + \gamma \max_{a'} Q(s', a'),
$$  
where $\gamma$ is the discount factor. This update replaces, rather than averages, the previous Q-value, ensuring that each state-action pair receives an exact, one-step Bellman backup derived from the full trajectory. This mechanism eliminates the need for repeated visits to propagate reward signals, directly addressing the sample inefficiency inherent in standard Q-learning \cite{Diekhoff2024RecursiveBQ}. The algorithm requires no prior knowledge of the environment dynamics and operates online, incrementally refining its model as new transitions are encountered.

Exploration is governed by an $\epsilon$-greedy policy with exponential decay over episodes:  
$$
\epsilon_t = \epsilon_0 \cdot e^{-t / \tau},
$$  
where $\epsilon_0 = 1.0$, $\tau = 400 \cdot 0.8$, and $t$ is the episode index. This decay schedule ensures sufficient initial exploration while rapidly transitioning to exploitation, enabling efficient mapping of the state space without premature convergence. The persistent model stores all unique transitions observed across episodes, with no compression or pruning, ensuring that backward propagation operates over the complete history of interactions. This design choice is critical: it guarantees that once a path to the terminal state is discovered, all preceding states along that trajectory are updated in a single pass, leveraging determinism to avoid the variance and slow propagation inherent in \ac{TD} learning \cite{Diekhoff2024RecursiveBQ}. The algorithm terminates when the maximum absolute change in Q-values across all state-action pairs falls below a threshold $\delta = 10^{-4}$, or after a maximum of 400 episodes.

We compare \ac{RBQL} against standard Q-learning with identical hyperparameters to ensure a fair evaluation. Both algorithms use the same $\epsilon$-greedy exploration schedule, discount factor $\gamma = 0.95$, and initial Q-value initialization (uniformly set to $-1$). The baseline Q-learning algorithm updates its value function after each transition using $\alpha = 0.1$, following the classic update rule $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$. This setup isolates the effect of backward propagation and persistent modeling by holding all other components constant. The experimental environment is a deterministic Pong-like game with discrete state and action spaces: the ball’s position is represented as a 2D coordinate $(x, y)$ where $x \in [1, 11]$ and $y \in [0, 12]$, with actions corresponding to paddle movements (up, down, or no-op). The terminal state occurs when the ball reaches $y=12$, yielding a reward of $+1$ for a win and $-1$ for a loss. The initial ball position is randomized at the start of each episode to prevent trajectory memorization and ensure generalization. State-action pairs are stored in a hash table for constant-time lookup during both exploration and backward propagation.

The theoretical foundation of \ac{RBQL} relies on the deterministic nature of transitions: given a complete model of visited states and actions, the Bellman optimality equation can be solved exactly in one backward pass. This contrasts with standard Q-learning, which requires multiple visits to the same state-action pair for convergence due to its incremental update rule \cite{Diekhoff2024RecursiveBQ}. Furthermore, unlike Dyna-Q (DYNA-Q), which simulates future transitions for forward planning \cite{Diekhoff2024RecursiveBQ}, \ac{RBQL} performs no simulation—it operates solely on actual observed transitions. Compared to Monte Carlo methods, which rely on episode-averaged returns and suffer from high variance even in deterministic settings \cite{Kaelbling1996ReinforcementLA}, \ac{RBQL} computes exact Bellman backups without averaging. \ac{VI}, while also using exact Bellman updates, requires full knowledge of the transition and reward functions over the entire state space \cite{Diekhoff2024RecursiveBQ}; \ac{RBQL} requires no such prior knowledge and updates only visited states, making it applicable to unknown environments. To our knowledge, no prior algorithm combines persistent transition modeling, online episodic updates, BFS-based backward propagation, and $\alpha=1$ Bellman backups in deterministic MDPs \cite{Diekhoff2024RecursiveBQ}. We formally define convergence as the first episode in which the maximum Q-value change over all state-action pairs is less than $\delta = 10^{-4}$, ensuring that optimal values have been reached within numerical precision.

Experiments were conducted over 30 independent runs of each algorithm, with a maximum of 400 episodes per run. Performance was evaluated using two metrics: (1) the episode at which a rolling 20-episode success rate first exceeded 90\%, and (2) the cumulative reward trajectory over time. Success rate was defined as the proportion of episodes ending in a win (reward $+1$) over the last 20 episodes. All runs were executed on a single NVIDIA RTX 3090 GPU with Python 3.14 and PyGame 2.6.1, using identical random seeds for reproducibility. The persistent model in \ac{RBQL} incurs additional memory overhead proportional to the number of unique state-action pairs encountered, which is bounded by $|\mathcal{S}| \cdot |\mathcal{A}|$ in finite MDPs. Ablation studies (Table 1) confirm that both the persistent model and backward propagation are necessary for performance gains: removing either component reverts \ac{RBQL} to standard Q-learning behavior. Memory usage comparisons show that \ac{RBQL} requires approximately 2.3\texttimes more memory than Q-learning on average, due to storage of the transition model—yet this cost is dwarfed by its sample efficiency gains. The results demonstrate that \ac{RBQL} achieves the 90\% success threshold in an average of 93.97 episodes ($\pm$ 31.24), compared to 233.60 episodes ($\pm$ 86.91) for Q-learning, with a statistically significant difference confirmed by an independent t-test ($t = -8.1416, p = 3.5475 \times 10^{-11}$). This validates the hypothesis that backward propagation over a persistent model enables dramatic improvements in sample efficiency for deterministic, episodic tasks.