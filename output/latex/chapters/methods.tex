\section{Methods}
\label{sec:methods}

Recursive Backwards Q-\ac{RBQL} is a model-free reinforcement learning algorithm designed to accelerate convergence in deterministic, episodic environments with sparse rewards by leveraging persistent transition memory and backward value propagation. Unlike standard Q-learning, which updates state-action values incrementally after each transition using outdated estimates of future rewards, RBQL defers all value updates until the end of an episode and propagates terminal rewards backward through a dynamically constructed transition graph. This approach ensures that every visited state receives an updated Q-value based on the most recent terminal reward, eliminating the delay in credit assignment inherent in online updates and directly addressing the sample inefficiency of traditional methods in sparse-reward settings \cite{30}.

The core innovation of RBQL lies in its persistent transition graph, which accumulates all observed state-action-next-state-reward tuples across episodes. Upon episode termination, the algorithm constructs a backward transition graph by inverting the recorded transitions: for each state-action pair $(s, a)$ leading to $s'$, an edge is added from $s'$ to $s$. This enables breadth-first search (BFS) to be initiated from the terminal state, generating a topological ordering of all visited states by their distance to the goal. This ordering guarantees that when Q-values are updated, each state’s successor has already been assigned its most up-to-date value—a critical requirement for correct Bellman backup in deterministic environments \cite{16}. The Q-value update is then applied iteratively to all explored state-action pairs using the full Bellman optimality equation with a learning rate $\alpha = 1$:

$$
Q(s, a) \leftarrow r(s, a) + \gamma \cdot \max_{a'} Q(s', a'),
$$

where $s'$ is the next state resulting from action $a$ in state $s$, and $\gamma \in [0, 1)$ is the discount factor. Updates proceed in reverse BFS order until convergence, defined as a maximum absolute change in Q-values below $10^{-6}$ across all state-action pairs. This batch update procedure effectively transforms the model-free Q-learning framework into a dynamic programming-like process, enabling full value iteration over the explored portion of the state space without requiring explicit knowledge of transition dynamics \cite{3}.

To ensure adequate exploration, an $\epsilon$-greedy policy is employed throughout episode generation, with $\epsilon = 0.3$. Exploration is further enhanced by prioritizing unvisited actions via A* search during episode rollouts, enabling systematic traversal of novel paths and accelerating the growth of the transition graph \cite{11}. This episodic exploration strategy ensures that each episode contributes maximally to the model’s understanding of the environment, while the backward propagation mechanism ensures that rewards are efficiently distributed across all relevant prior states in a single update phase. This contrasts sharply with Dyna-Q, which requires learning and simulating an explicit transition model to generate hypothetical transitions \cite{19}, and with \ac{EBU}, which performs backward propagation within a single episode but lacks persistent memory across episodes, thereby limiting its ability to propagate rewards from previously encountered terminal states \cite{5}. RBQL’s persistent graph enables cross-episode reward propagation, a feature absent in both EBU and RETRACE \cite{8}, allowing the algorithm to accumulate and refine value estimates over time without requiring a model of the environment.

The algorithm’s design explicitly exploits determinism: in stochastic environments, the assumption that a single next state follows each action would introduce bias into backward updates. While RBQL is currently restricted to deterministic settings, its architecture provides a foundation for future extensions—such as incorporating transition probabilities or uncertainty estimates—that could extend its applicability to partially observable or noisy domains \cite{34}. The use of optimistic initialization ($Q(s, a) = 1.0$ for all $(s, a)$ pairs) encourages exploration of unvisited state-action pairs and ensures that the backward propagation process begins with an upper bound on expected returns, aligning with theoretical guarantees for sample-efficient exploration in MDPs \cite{17}.

For evaluation, RBQL is compared against two baselines: standard Q-learning with $\alpha = 0.5$, representative of typical learning rates in the literature, and Q-learning with $\alpha = 1.0$, which matches RBQL’s effective update strength by applying full Bellman updates after each step. This controlled comparison isolates the benefit of backward propagation from that of aggressive learning rates, ensuring a fair assessment of RBQL’s structural innovation. The primary metric is the number of episodes required to converge to an optimal policy, defined as achieving a policy that matches the analytically computed optimal Q-values derived via backward induction from the terminal state. Convergence is verified by ensuring that the policy derived from learned Q-values (i.e., $\arg\max_a Q(s, a)$) is identical to the optimal policy for all reachable states. Secondary metrics include the mean and standard deviation of convergence episodes across 50 independent trials, providing statistical validation of sample efficiency gains. The environment used is a deterministic 1D grid world with sparse rewards (only +1 at the goal), chosen for its simplicity and direct interpretability, allowing precise analysis of reward propagation dynamics \cite{20}. All experiments are conducted using only NumPy and Matplotlib, ensuring reproducibility without reliance on complex frameworks.

By combining persistent memory with backward value iteration over observed transitions, RBQL bridges the gap between model-free and dynamic programming approaches, achieving convergence rates that scale with path length rather than state space size—a theoretical improvement over standard Q-learning’s $O(S^2)$ sample complexity \cite{1}. This enables deployment in resource-constrained deterministic systems such as robotic path planning, where each trial incurs significant cost \cite{6}. The method demonstrates that explicit model learning is not necessary to achieve value iteration-like efficiency; instead, structured use of observed transitions suffices to accelerate learning in deterministic environments.