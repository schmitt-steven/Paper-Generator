\section{Methods}
\label{sec:methods}

\ac{RBQL} is a model-free reinforcement learning algorithm designed to accelerate convergence in deterministic, episodic environments with sparse rewards by leveraging persistent transition memory and backward value propagation. Unlike standard Q-learning, which updates state-action values incrementally during episode execution using single-step temporal difference targets \cite{Diekhoff2024RecursiveBQ}, \ac{RBQL} defers all value updates until the end of each episode, enabling a holistic, backward propagation of terminal rewards across the entire history of observed transitions. This approach fundamentally alters the credit assignment mechanism by ensuring that early-state values are informed by the most up-to-date estimates of future rewards, thereby eliminating the propagation delays inherent in sequential updates.

The core mechanism of \ac{RBQL} is a persistent transition graph, maintained across episodes, that records all observed state-action-reward-next-state tuples. Upon episode termination, this graph is used to construct a backward reachability tree rooted at the terminal state. A breadth-first search (BFS) is then performed in reverse direction to establish a topological ordering of all visited states based on their distance from the goal. This ordering guarantees that when Q-values are updated, each state’s successor values have already been finalized—enabling a direct application of the Bellman optimality equation without bootstrapping from outdated estimates. The update rule is applied deterministically with a learning rate $\alpha = 1$:

$$
Q(s, a) \leftarrow r(s, a) + \gamma \cdot \max_{a'} Q(s', a'),
$$

where $s'$ is the next state resulting from action $a$ in state $s$, and $\gamma \in [0, 1)$ is the discount factor. This update is iterated over all explored state-action pairs until convergence, defined as a maximum change in Q-values below $10^{-6}$. This procedure effectively transforms \ac{RBQL} into a form of batch value iteration operating over the observed portion of the MDP, without requiring explicit knowledge of transition dynamics—a key distinction from classical dynamic programming methods \cite{Diekhoff2024RecursiveBQ}.

To ensure adequate exploration and prevent premature convergence, an $\varepsilon$-greedy policy with $\varepsilon = 0.3$ is employed during episode execution, coupled with optimistic initialization of all Q-values to 1.0. This encourages the agent to explore unvisited state-action pairs while maintaining stability during backward updates. The persistent transition graph allows \ac{RBQL} to accumulate and reuse transitions across episodes, enabling reward signals from one episode to inform value estimates in subsequent ones—a capability absent in single-trajectory backward methods such as \ac{EBU} \cite{Lee2018SampleEfficientDR}. Unlike Dyna-Q (\ac{DYNA-Q}), which relies on a learned transition model to generate hypothetical experiences \cite{Ghasemi2024ACS}, \ac{RBQL} operates purely on actual observed transitions, eliminating the risk of model bias and computational overhead associated with simulation. Furthermore, unlike \ac{GRAPHBD} \cite{Jiang2022GraphBD}, which propagates values through transition graphs but does not guarantee full backward propagation from terminal states, \ac{RBQL} explicitly structures updates via BFS to ensure optimal value assignment upon first visit to a state \cite{Diekhoff2024RecursiveBQ}.

We compare \ac{RBQL} against two baselines: standard Q-learning with $\alpha = 0.5$ (typical in literature) and an enhanced variant with $\alpha = 1.0$ to isolate the effect of batch versus online updates. The $\alpha = 1.0$ variant serves as a fair baseline, matching \ac{RBQL}’s effective learning rate while retaining online update dynamics. All algorithms are evaluated on a 15-state one-dimensional grid world with sparse $+1$ rewards at the terminal state (position 14), zero otherwise, and actions to move left or right. The discount factor is fixed at $\gamma = 0.9$, and episodes are capped at 300 steps to prevent infinite loops. Convergence is defined as the point at which the learned policy matches the analytically computed optimal policy, verified by comparing argmax actions across all states. We conduct 50 independent trials per algorithm to ensure statistical robustness.

The primary evaluation metric is the number of episodes required to achieve optimal policy convergence. Secondary metrics include mean and standard deviation of episode counts across trials, as well as the relative speedup of \ac{RBQL} over baselines. These metrics are chosen because they directly quantify sample efficiency—a critical concern in deterministic environments where each episode represents a non-renewable resource \cite{Diekhoff2024RecursiveBQ}. The experimental design is intentionally simplified to isolate the impact of backward propagation, avoiding confounding factors such as function approximation or reward shaping \cite{Memarian2021SelfSupervisedOR,Park2025FromST}. All implementations use only NumPy and are executed on standard CPU hardware, with no GPU acceleration.

By integrating persistent memory with backward value iteration, \ac{RBQL} bridges the gap between model-free and dynamic programming approaches. It achieves the sample efficiency of value iteration without requiring full knowledge of transition dynamics, a feat unattainable by prior model-free methods \cite{Diekhoff2024RecursiveBQ}. This framework establishes a new paradigm for sample-efficient RL in deterministic settings, where historical transitions are not discarded but actively restructured into a recurrent value propagation mechanism.