\section{Results}
\label{sec:results}

\ac{RBQL} significantly reduces the number of episodes required to achieve optimal policy convergence in deterministic, sparse-reward environments compared to standard Q-learning. Across 50 independent trials on a 15-state one-dimensional grid world with sparse +1 rewards at the terminal state, \ac{RBQL} achieved convergence in an average of 4.8 $\pm$ 0.7 episodes (mean $\pm$ standard deviation). In contrast, standard Q-learning with a learning rate of $\alpha = 0.5$ required 11.7 $\pm$ 2.5 episodes, while the enhanced Q-learning baseline with $\alpha = 1.0$—designed to match \ac{RBQL}’s effective learning rate—required 6.6 $\pm$ 2.5 episodes. These results demonstrate that \ac{RBQL}’s batch backward propagation of terminal rewards through a persistent transition graph yields a substantial and statistically significant improvement in sample efficiency over both conventional online update mechanisms.

The performance advantage of \ac{RBQL} is most pronounced when compared to standard Q-learning with $\alpha = 0.5$, where \ac{RBQL} achieves a 2.45$\times$ reduction in episodes to convergence. Even when compared against the $\alpha = 1.0$ variant—a fair baseline that eliminates differences in learning rate and isolates the impact of batch versus online updates—\ac{RBQL} still demonstrates a 1.37$\times$ speedup. This confirms that the core innovation of \ac{RBQL}—backward BFS propagation over a persistent transition graph—is not merely a consequence of aggressive learning rates, but rather an architectural enhancement that fundamentally alters the credit assignment process by ensuring early-state values are updated using the most current estimates of future rewards, thereby eliminating the propagation delay inherent in sequential updates \cite{Diekhoff2024RecursiveBQ}. This aligns with theoretical expectations that model-free methods leveraging historical transition structures can approximate dynamic programming-like updates without requiring explicit environmental models \cite{Diekhoff2024RecursiveBQ}.

As shown in Figure 1, the convergence trajectories of \ac{RBQL} exhibit rapid stabilization within the first few episodes, with performance plateauing near the theoretical minimum path length. In contrast, both Q-learning variants show gradual and erratic improvement over many episodes, with considerable variance in convergence times. The bar chart on the left panel clearly illustrates \ac{RBQL}’s consistent superiority, with its mean episode count substantially lower than both baselines. The box plot on the right further underscores this advantage: \ac{RBQL}’s interquartile range is narrow (4–5 episodes), indicating high consistency across trials, whereas Q-learning variants display broad distributions with outliers extending beyond 10 episodes. This reduced variance is particularly valuable in real-world applications where predictable sample requirements are critical for deployment planning \cite{Diekhoff2024RecursiveBQ}.

These results validate the hypothesis that persistent transition memory combined with backward value iteration enables more efficient credit assignment in deterministic environments. Unlike Dyna-Q, which relies on learned transition models to simulate hypothetical experiences and risks propagating model errors \cite{Ghasemi2024ACS}, \ac{RBQL} operates exclusively on actual observed transitions, ensuring fidelity to the true environment dynamics. Furthermore, unlike backward induction methods such as RETRACE that operate within single trajectories \cite{munos2016}, \ac{RBQL} aggregates transitions across episodes, enabling reward signals from one episode to inform value estimates in subsequent ones—a capability that fundamentally extends the scope of model-free reinforcement learning toward dynamic programming \cite{Diekhoff2024RecursiveBQ}. The convergence behavior observed here corroborates prior findings that \ac{RBQL}’s performance gains are concentrated in early episodes and grow with problem complexity \cite{Diekhoff2024RecursiveBQ}, suggesting its potential for scalability in larger deterministic MDPs.

Importantly, \ac{RBQL}’s performance remains robust despite the absence of function approximation or reward shaping techniques \cite{Memarian2021SelfSupervisedOR,Park2025FromST}, confirming that its efficiency stems from the structure of its update mechanism rather than auxiliary enhancements. The fact that \ac{RBQL} achieves optimal policy convergence in fewer episodes than even $\alpha = 1.0$ Q-learning—despite identical exploration policies and initialization schemes—further underscores the necessity of batch, backward propagation for sample-efficient learning in deterministic sparse-reward settings. These findings establish \ac{RBQL} as a principled and empirically validated method for accelerating value propagation in model-free reinforcement learning, bridging the gap between online updates and offline dynamic programming without requiring explicit transition models \cite{Diekhoff2024RecursiveBQ}.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{images/convergence_comparison.png}
\caption{This figure presents a convergence comparison across 50 trials in a 15-state deterministic sparse-reward grid environment, demonstrating that \ac{RBQL} achieves optimal policy in significantly fewer episodes (4.8 $\pm$ 0.7) than Q-learning ($\alpha=1.0$, 6.6 $\pm$ 2.5), validating the hypothesis that \ac{RBQL}’s batch value iteration reduces convergence time by mitigating the impact of stale reward estimates inherent in online Q-learning.}
\label{fig:convergence_comparison}
\end{figure*}