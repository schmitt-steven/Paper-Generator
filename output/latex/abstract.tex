Standard Q-learning suffers from severe sample inefficiency in deterministic, episodic environments due to its incremental, multi-visit update mechanism that fails to exploit the deterministic structure of transitions, leading to slow propagation of terminal rewards. \ac{RBQL}, a model-based algorithm that leverages a persistent transition model to perform exact, one-step Bellman backups with $\alpha=1$ via backward breadth-first search over all previously visited state-action pairs after each episode, eliminating the need for repeated exploration. In a deterministic Pong-like environment, \ac{RBQL} achieves 90\% of optimal performance in an average of 93.97 episodes, compared to 233.60 episodes for standard Q-learning—a reduction of over 60\%—with statistically significant gains ($p < 0.001$) and lower variance across runs. This demonstrates that backward propagation over an episodically built model enables exact value convergence in finite episodes, establishing a new paradigm for sample-efficient reinforcement learning in deterministic domains where data collection is costly.