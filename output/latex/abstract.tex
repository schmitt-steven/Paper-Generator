In deterministic reinforcement learning environments with sparse rewards, standard Q-learning suffers from severe sample inefficiency due to sequential updates that propagate terminal rewards slowly and inaccurately, as early state values are updated using outdated estimates of future returns. To address this gap, we introduce \ac{RBQL}, a model-free method that leverages a persistent transition graph to enable backward breadth-first search propagation of terminal rewards across episodes, transforming online updates into batch value iteration over observed transitions. By enforcing topological ordering of state updates via backward BFS and applying the Bellman optimality equation with unit learning rate, \ac{RBQL} ensures that each state’s value is informed by the most current estimates of its successors—eliminating propagation delays inherent in conventional methods. Across 50 trials on a 15-state grid world, \ac{RBQL} achieved optimal policy convergence in 4.8 $\pm$ 0.7 episodes, outperforming Q-learning with $\alpha=1.0$ (6.6 $\pm$ 2.5 episodes) and standard Q-learning with $\alpha=0.5$ (11.7 $\pm$ 2.5 episodes), demonstrating a 1.37$\times$ and 2.45$\times$ reduction in sample complexity, respectively. This structural advantage enables dynamic programming-like convergence without requiring explicit transition models, establishing \ac{RBQL} as a principled framework for sample-efficient learning in deterministic domains where data collection is costly.