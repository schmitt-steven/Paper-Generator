Standard Q-learning suffers from severe sample inefficiency in deterministic, sparse-reward environments due to its sequential update mechanism, which propagates terminal rewards slowly across long trajectories using outdated future-state estimates. This limitation impedes convergence in tasks such as robotic navigation or maze traversal, where each episode contributes only incremental value updates. We introduce Recursive Backwards Q-\ac{RBQL}, a model-free algorithm that eliminates this delay by maintaining a persistent transition graph across episodes and performing backward breadth-first search from terminal states to propagate rewards via exact Bellman updates with $\alpha=1$. Unlike prior methods that operate within single episodes or rely on learned transition models, RBQL enables dynamic programming-like value iteration over the entire observed state space without explicit environmental modeling. Experiments on a 15-state 1D grid demonstrate that RBQL reduces the mean episodes to convergence by $2.45\times$ compared to standard Q-learning ($\alpha=0.5$) and by $1.37\times$ even against Q-learning with $\alpha=1.0$, achieving optimal policy in $4.8\pm0.7$ episodes versus $6.6\pm2.5$ and $11.7\pm2.5$, respectively. The methodâ€™s low variance and consistent performance confirm that backward propagation through persistent memory fundamentally resolves temporal credit assignment delays. RBQL establishes a new paradigm for sample-efficient reinforcement learning by demonstrating that structured use of observed transitions, rather than model estimation or complex exploration heuristics, suffices to achieve value iteration-like convergence in deterministic settings.