[
  {
    "id": "user_2404.15822v1",
    "title": "Recursive Backwards Q-Learning in Deterministic Environments",
    "published": "2024-04-24",
    "authors": [
      "Jan Diekhoff",
      "Jorn Fischer"
    ],
    "summary": "Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.",
    "pdf_url": "",
    "doi": "10.48550/arXiv.2404.15822",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "arXiv.org",
    "citation_count": 0,
    "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n",
    "markdown_text": "## RECURSIVE BACKWARDS Q-LEARNING IN DETERMINISTIC ENVIRONMENTS\n\n\n\n**Jan Diekhoff**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: jan.diekhoff@web.de\n\n\n\n**Jörn Fischer**\nMannheim University\nof Applied Sciences\nPaul-Wittsack-Str. 10\n\n68163 Mannheim\nGermany\nEmail: j.fischer@hs-mannheim.de\n\n\n\n**ABSTRACT**\n\n\nReinforcement learning is a popular method of finding optimal solutions to complex problems.\nAlgorithms like Q-learning excel at learning to solve stochastic problems without a model of their\nenvironment. However, they take longer to solve deterministic problems than is necessary. Q-learning\ncan be improved to better solve deterministic problems by introducing such a model-based approach.\nThis paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds\na model of the environment. After reaching a terminal state, it recursively propagates its value\nbackwards through this model. This lets each state be evaluated to its optimal value without a lengthy\nlearning process. In the example of finding the shortest path through a maze, this agent greatly\noutperforms a regular Q-learning agent.\n\n\n_**Keywords**_ Q-learning _·_ deterministic _·_ recursive _·_ reinforcement learning\n\n\n**1** **Introduction**\n\n\nMachine learning and reinforcement learning are increasingly popular and important fields in the modern age. There are\nproblems that reinforcement learning agents can learn to solve more efficiently and consistently than any human when\ngiven enough time to practice. However, modern approaches like Q-learning run into issues when facing certain types\nof problems. Their approach to solving problems in combination with not using a model of the environment causes\nthem to take longer than is necessary to learn to solve problems that are deterministic in nature. By working without\nmodel of the environment, information that is available and help the learning process is ignored.\n\n\nThis paper introduces an adapted Q-learning agent called the _recursive backwards Q-Learning (RBQL) agent_ . It solves\nthese types of problems by building a model of its environment as it explores and recursively applying the Q-value\nupdate rule to find an optimal policy much quicker than a regular Q-learning agent. This agent is shown to work with\nthe example of finding the fastest path through a maze. Its results are compared to the results of a regular Q-learning\nagent.\n\n\n**2** **Reinforcement Learning**\n\n\nReinforcement learning is one of the main fields of machine learning. It is commonly used for optimizing solutions to\nproblems. At its most fundamental level, a reinforcement learning method is an implementation of an agent for solving\na Markov decision process [1] by interacting with an environment. Markov decision processes describe problems as\na set of states _S_, a set of actions _A_ and a set of rewards _R_ . For every time step _t_, the agent chooses an action _a ∈_ _A_\nand receives a new state _s ∈_ _S_ and a reward _r ∈_ _R_ for the action [2]. Rewards may be positive or negative, depending\non the outcome of the action, to encourage or discourage taking that action in the future [3]. The process of the agent\ninteracting with the environment is called an episode which ends when a terminal state is reached which resets the\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nenvironment and agent to their original configuration for the start of a new episode [3]. For the purposes of this paper,\nonly finite Markov decision processes are considered, meaning the environment has at least one terminal state.\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-1-0.png)\n\n\n_St_ +1\n\n\nFigure 1: Basic agent-environment relationship in a Markov decision process. The agent chooses an action _At_ and the\nenvironment returns a new state _St_ +1 and a reward _Rt_ +1. The dotted line represents the transition from step _t_ to step\n_t_ + 1 [3].\n\n\nReinforcement learning agents learn an optimal strategy for a given Markov decision process by estimating the value of\neither being in a state or taking a certain action in a certain state. They do this through a value function or action-value\nfunction respectively. The aim of the agent is to maximize the reward they receive in an episode [3]. To achieve this,\nvalue estimations do not only consider the immediate action the agent takes but also consider all future states and actions\nthat may occur when taking the original action. Agents follow so-called policies according to which they choose which\nactions to take. Through gaining knowledge, they continuously adapt this policy in order to eventually reach an optimal\npolicy - a policy which chooses the optimal action at every step. To explore, agents have to balance between exploration\nand exploitation [3]. Exploration is the act of following suboptimal actions to attempt to find an even better policy. On\nthe other hand, exploitation is following the actions that will yield the currently highest estimated value. An agent that\nonly exploits acts _greedily_ . To ensure continual exploration so that all actions get updated given enough time, agents\ncan choose policies that are mostly greedy but choose to explore sometimes [2]. To this end, an approach like _ϵ_ -greedy\nmay be used. Here, _ϵ_ is the probability of choosing a random action and 1 _−_ _ϵ_ is the probability of acting greedily.\n\n\nA widely used modern approach to RL is temporal difference learning [4], more specifically Q-learning [2]. Q-learning\nworks with the Q-learning update formula to update its policies:\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + _α ·_\n\n[ _Rt_ +1 + _γ ·_ max _Q_ ( _St_ +1 _, a_ ) _−_ _Q_ ( _St, At_ )] (1)\n_a_\n\n\n_Q_ ( _St, At_ ) is the estimated value for any given state-action pair. The equation shows how it is updated after taking\naction _At_ from state _St_ . _Rt_ +1 represents the reward gained, max _Q_ ( _St_ +1 _, a_ ) is the value estimation of the best action\n_a_\n_a ∈_ _At_ +1 that can be taken from _St_ +1 according to the current policy, the state resulting from action _At_ . _α_ is a step-size\nparameter, also known as the _learning rate_ . Its value lies between 0 and 1 and it determines how importantly the agent\nvalues new information against the current estimate it already has. A value of 0 completely ignores new information\nwhile a value of 1 completely overrides the preexisting value estimate. _γ_ is the discount factor, weighing future rewards\nless than immediate ones. It also lies between 0 and 1, where 1 weighs the best future action equally to the current one\nand 0 does not consider it at all.\n\n\n**3** **Recursive Backwards Q-Learning**\n\n\n**3.1** **Idea**\n\n\nQ-learning agents are very widespread in modern reinforcement learning. Working free of a model allows them to\nbe generally applicable to many problems. However, some Markov decision processes take longer to solve than is\nnecessary because the agent ignores readily available information. This is noticeable in deterministic, episodic tasks\nwhere a positive reward is only given when reaching a terminal state. Before this state is reached for the first time, the\nagent appears to be moving entirely at random. Looking at figure 2, the issue becomes apparent. Even when following\nthe optimal path at every step, it still takes multiple episodes for the reward of the terminal state to propagate back to the\nstarting state. In fact, the optimal paths value estimation gets worse before it gets better. If every step has a reward of\n\n\n2\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_−_ 1, values along the optimal path get worse if they do not lead to a state that has already been reached by the terminal\nstate’s positive reward as it travels backwards.\n\n\nIn this paper, grid worlds [3] are used as an example Markov decision process for the agent to solve. Grid worlds are a\ntwo-dimensional grid in which every tile represents a state and the actions are limited to walking up, down, left or right.\nGrid worlds are useful in that they are very simple to understand and to display, they have a limited set of actions and\ntheir set of states can be as small or large as is desired. Additionally, showing the value or optimal policy for each state\nis as easy as writing a number or drawing an arrow on the corresponding tile. Actions that would place the agent off of\nthe grid simply return the state the agent is already in, but may still give a reward. Special tiles can also be defined, such\nas walls that act like the grid edge or pits that are terminal fail states because the agent cannot leave them once it has\nfallen in. Every grid world tile gives a reward of _−_ 1 to punish taking unnecessary actions in favor of taking the fastest\npath to the goal.\n\n\n_Q_ greedy policy\nw.r.t. _Q_\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -0.8\n\n\n-1\n\n\n-1\n\n\n-1 0.52\n\n\n-1\n\n\n-1\n\n\n-1 1.99\n\n\n-1\n\n\n-1\n\n\n-1 3.27\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 0.78\n\n\n-1\n\n\n-1\n\n\n-1 3.15\n\n\n-1\n\n\n-1\n\n\n-1 4.96\n\n\n-1\n\n\n-1\n\n\n-1 6.17\n\n\n-1\n\n\n-1\n\n\n-1 6.93\n\n\n-1\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 4.5\n\n\n-1\n\n\n-1\n\n\n-1 7.25\n\n\n-1\n\n\n-1\n\n\n-1 8.63\n\n\n-1\n\n\n-1\n\n\n-1 9.32\n\n\n-1\n\n\n-1\n\n\n-1 9.66\n\n\n-1\n\n\n-1\n\n\n-1 9.83\n\n\n-1\n\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n0\n\n\n0 0\n\n\n0\n\n\n\nep. 0\n\n\nep. 1\n\n\nep. 2\n\n\nep. 3\n\n\nep. 4\n\n\nep. 5\n\n\nep. 6\n\n\n\n-1\n\n\n-1 -1\n\n\n-1\n\n\n-1\n\n\n-1 -1.5\n\n\n-1\n\n\n-1\n\n\n-1 -1.3\n\n\n-1\n\n\n-1\n\n\n-1 -1.6\n\n\n-1\n\n\n-1\n\n\n-1 -1.66\n\n\n-1\n\n\n-1\n\n\n-1 -1.1\n\n\n-1\n\n\n-1\n\n\n-1 -0.15\n\n\n-1\n\n\n\nFigure 2: Q-learning in a one-dimensional grid world. All Q-values are initialized as _−_ 1. Actions that lead to the\nterminal state reward 10. All other actions reward -1. The discount rate _γ_ is set to 0 _._ 9. The learning rate _α_ is set to 0 _._ 5.\nThe value of _ϵ_ is irrelevant as the only action the agent takes is _→_ .\n\n\nFigure 2 is a very simple grid world and it still takes six episodes to reach an optimal policy, even when taking the\noptimal action at every step. This problem will only grow worse and add noticeably more episodes of training for grid\nworlds that are not as trivial to solve, or even more complex tasks with more variables to consider. As stated, the issue\nis that the agent has no source of direction until it has randomly stumbled across the terminal state, its only source of\npositive rewards. The larger the state space, the longer it is blindly searching.\n\n\nReinforcement learning agents that work with a model of their environment are known as _model-based_ reinforcement\nlearning agents. They can either work with a preexisting model or, more commonly, build their own. The way they\nconstruct their models is important as having perfect knowledge of an environment is neither feasible nor sensible. In\nthe case of a grid world it is no problem, but imagining a more complex scenario like a self-driving car makes this fact\napparent. When trying to drive from one city to another, knowing every centimeter of the road with every possible place\nother cars might be on the route is resource intensive and unnecessary. Instead, an agent should attempt to simplify its\nmodel as much as possible. Instead of every bit of road, long stretches going straight can be clumped together. Similar\nsituations like a car in front slowing down can be treated the same wherever they occur.\n\n\nThe purpose of this paper is to introduce and evaluate a new type of model-based agent called the RBQL agent. The\nRBQL agent solves deterministic, episodic tasks that positively reward only the terminal state more efficiently than a\nregular Q-learning agent. It functions by building a model of its environment through exploration. When it reaches a\nterminal state, it recursively travels backwards through all previously explored states, applying a modified Q-learning\nupdate rule, the RBQL update rule. By setting the learning rate _α_ to 1, equation (1) can be simplified as such:\n\n\n3\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_Q_ ( _St, At_ ) _←_ _Q_ ( _St, At_ ) + 1 _·_ [ _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )]\n= _Q_ ( _St, At_ ) + _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n_−_ _Q_ ( _St, At_ )\n= _Rt_ +1 + _γ_ max _Q_ ( _St_ +1 _, a_ )\n_a_\n\n\n\n(2)\n\n\n\nAs can be seen in formula (2), the Q-value now exclusively depends on the reward and the discounted reward of the\nbest neighbor. Because the algorithm applies this formula starting with what is guaranteed to be the highest value of the\nenvironment and working its way away from it, the best possible neighbor for any given state is always the previously\nevaluated state.\n\n\nEvaluating all states at the end of the episode is reminiscent of dynamic programming [5] or Monte Carlo methods [3]\nand is a point of critique for those approaches. However, as will be shown in chapter 4, this evaluation method is so\neffective in RBQL that evaluating all known states in one go is still cost effective. RBQL also differs in comparison\nto dynamic programming and Monte Carlo in a few major ways. In contrast with dynamic programming, it does not\nstart out with a perfect model but has to build its own. It also propagates its reward throughout all states much more\nquickly and it uses an action-value function, not a state-value function. In contrast with Monte Carlo, it does not use\nexploring starts to guarantee exploration. It also does not only update the values that were seen in an episode. Instead,\nto facilitate exploration, it always prioritizes visiting unexplored actions, only following the greedy path when there\nare none. Because this mode of exploration still results in unexplored actions, the _ϵ_ -greedy approach is adapted for\nRBQL. Instead of exploring steps, the agent has exploration episodes. _ϵ_ serves the same purpose as before, marking\nthe probability of taking an exploration episode while 1 _−_ _ϵ_ is the probability of taking an exploitation episode. In an\nexploration episode, the agent randomly chooses an unexplored action anywhere in its model, navigates the model to\nput itself in a position to take that action and then continues to explore until it finds a known path again or the episode\nends.\n\n\nIn this paper, finding an optimal path through a randomly generated grid world maze is used as an example task for\nRBQL to solve. It is also used to compare the performance of RBQL to Q-learning.\n\n\n**3.2** **Implementation**\n\n\nTo implement RBQL [1], the Godot game engine v. 3.5 [2] was used. Godot is a free, open source engine used mainly for\nvideo game development. Its main language is GDScript, an internal language that is very similar in syntax to Python,\nthough it also supports C, C++, C# and VisualScript. Because Python is very popular for machine learning development,\nthe implementation is written in GDScript so that it is easily readable for interested parties. Godot uses a hierarchical\nstructure of objects called _nodes_ . In the implementation, there are two main nodes: the agent and the environment.\n\n\n**3.2.1** **Environment**\n\n\nThe environment is of the type `TileMap` [3] – a class designed for creating maps in grid-based environments like grid\nworlds. Before starting the first episode, the environment generates a maze given a width _w_ and a height _h_ using a\nrecursive backtracking algorithm [6]. The starting point for the agent is always (0 _,_ 0) and the goal it attempts to reach –\nthe only terminal state – is ( _w −_ 1 _, h −_ 1). To ensure that the agent has the ability to improve even after finding the goal\nin the first episode, a maze with multiple paths is needed. Because a maze generated with recursive backtracking only\nhas one path to the terminal state, a number of alternate paths are generated by taking _w · h/_ 4 random positions and a\ndirection for each position. If the position has a wall in that direction, it is removed. If not, nothing happens.\n\n\nThe environment has a function `step(state,action)` that serves as the only way for the agent to interact with it.\nThe possible moves are `UP`, `DOWN`, `LEFT` and `RIGHT` . The state is described as a coordinate of the current position. In\nGodot, the class `Vector2(x,y)` [4] is used for this purpose. `step()` checks if taking the given action from the given\nstate results in hitting a wall or not. If not, the agent moves to a new position. There are three different rewards: _−_ 1 for\nany normal tile, _−_ 5 for hitting a wall and 10 for reaching the terminal state. _−_ 1 is awarded at every step to discourage\nagents from taking unnecessary steps. Walls give _−_ 5 to quickly teach the agent to ignore them. After taking an action,\n\n\n1 The source code can be downloaded at `[https://github.com/JanDiekhoff/BackwardsLearner](https://github.com/JanDiekhoff/BackwardsLearner)`\n2 Godot v. 3.5 can be downloaded at `[https://godotengine.org/download/archive/3.5-stable/](https://godotengine.org/download/archive/3.5-stable/)`\n3 `[https://docs.godotengine.org/en/3.5/classes/class_tilemap.html](https://docs.godotengine.org/en/3.5/classes/class_tilemap.html)`\n4 `[https://docs.godotengine.org/en/3.5/classes/class_vector2.html](https://docs.godotengine.org/en/3.5/classes/class_vector2.html)`\n\n\n4\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nthe new state and reward are returned to the agent, as well as a notification if the episode has ended or not and if the\nagent has hit a wall or not.\n\n\nThe `TileMap` has a tile for each combination of having or not having a wall in each of the four directions, totaling 2 [4] or\n16 total possible tiles. Another option would be to just have a floor tile and a wall tile. However, that would make a\nmaze with an equivalent wall layout much larger, leading to a larger state set and longer solving times. To determine if a\nwall is in a certain direction, the id of each tile from 0 to 15 acts as a four-bit flag. Each direction is assigned one of the\nbits ( `UP` = 0, `RIGHT` = 1, `DOWN` = 2 and `LEFT` = 3). If the flag is set, there is a wall in the corresponding direction. The\nid for an L-shaped tile for example would be 2 [2] + 2 [3] = 12 as `DOWN` and `LEFT` have walls. The process for determining\nif the agent can move in a given direction _d_ from a position _p_ is ( _¬idp_ ) & (2 _[d]_ ), where _idp_ is the id of the tile at _p_ .\n\n\n**3.2.2** **RBQL Agent**\n\n\nThe RBQL agent is represented by a `Sprite` [5] object – a 2D image – so it can be observed while solving a maze. During\nits runtime, the agent keeps track of a few key things:\n\n\n    - A model of the environment ( `explored_map` )\n\n\n    - A list of rewards for each state-action pair ( `rewards` )\n\n\n    - The last reward received ( `reward` )\n\n\n    - A list of steps taken per episode ( `steps_taken` )\n\n\n    - The Q-table ( `qtable` )\n\n\n    - The current state ( `current_state` )\n\n\n    - The previous state ( `old_state` )\n\n\n    - The last taken action ( `action` )\n\n\nThe model of the environment starts out as an empty dictionary. Every time a new state is discovered, an entry for that state is made and initialized as an empty array. When an action is taken from this state, the resulting new state is entered into the previous state’s array at the index of the taken action’s designated number\n( `explored_map[old_state][action] = current_state` ). When hitting a wall, the “new” state is the same as the\nstate from which the action was taken. Similarly, when an action is taken, the resulting reward is saved in the rewards\nlist ( `rewards[old_state][action] = reward` ). Because the agent uses state-action values, not state values, the\ntiles are treated like nodes in a directed graph. Going from tile A to tile B might result in a different reward than when\ngoing from B to A, so when the agent learns the reward of going from A to B, it does not also learn the reward of going\nfrom B to A.\n\n\nBeing a Q-learner makes it simpler to generalize the agent for other tasks, but it causes a lot of exploratory steps and\nexploratory episodes to only explore one position at a time. If an exploration episode chooses an unexplored state-action\npair that results in hitting a wall, the exploration episode immediately ends with little information gained. To alleviate\nthis problem, the agent takes exploratory “look-ahead” steps. After entering a tile, it takes a step in every direction but\nonly saves the result if it hits a wall. This guarantees that exploratory episodes always take new paths and not just hit a\nwall and continue on the best known path.\n\n\nThe agent also keeps track of a list of the actions it has taken – except for when hitting a wall – for the case that it\nreaches a dead end, or rather a state with no unexplored neighbors. In this case, the agent would normally follow the\noptimal path until it finds a new unexplored path or reaches the terminal state. However, if the path the agent is on has\nnot been explored before it has not yet been evaluated and there is no optimal path to follow. In this case, the agent\nbacktracks by taking the opposite action of the most recent in the list, then removes it from the list, until an unexplored\ntile or an evaluated path to follow is found.\n\n\nFinally, when the terminal state is reached, the Q-table is updated with the rewards saved in `rewards` according to the\nRBQL update rule.\n```\n             qtable[state][action] =\n\n```\n\n`rewards[state][action] + discount_rate` _·_\n\n```\n             qtable[explored_map[state][action]].max()\n\n```\n\nTo do this, a copy of `explored_map` is inverted to be able to traverse it in reverse. This is then done with a breadth-first\nsearch algorithm, starting at the terminal state, and the Q-value is calculated for each state. Breadth-first search is chosen\n\n\n5 `[https://docs.godotengine.org/en/3.5/classes/class_sprite.html](https://docs.godotengine.org/en/3.5/classes/class_sprite.html)`\n\n\n5\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nover a depth-first search algorithm so that each state must only be visited once as the value is directly proportional to\nthe distance from the terminal state. With breadth-first search, each state gets the highest possible value on its first visit\nbecause it is visited from its highest possible valued neighbor.\n\n\nWhen all known states have been evaluated, a new episode begins. After the first episode, episodes are chosen to be\neither exploratory or exploitative, similar to how an _ϵ_ -greedy policy may choose exploratory actions. In an exploitative\nepisode, the agent simply follows the best path it knows, choosing at random if two states are equally good, but still\nalways exploring unknown states directly adjacent to the path above all else. In an exploratory episode, a random state\nwith an unexplored neighbor is chosen. The agent navigates to this state with the help of the A* search algorithm [7]\nand follow the unexplored path from there until it finds a known state again. This exploratory excursion may only find\none new state or it may find a vastly superior path to what was known before. _ϵ_ is decreased after every episode as\nfollows:\n_ϵ_ = `min_epsilon + (max_epsilon - min_epsilon)`\n\n\n_· e_ [(] _[−]_ `[decay_rate]` _[ ·]_ `[ current_episode]` [)]\n\n\nwhere `min_epsilon`, `max_epsilon` and `decay_rate` can be any value within a range of [0 _,_ 1] and `current_episode`\nis the number of the current episode starting with 0. Once every state is explored, the agent is guaranteed to have found\nthe optimal path, or paths, through the maze. In its entirety, the algorithm can be expressed like this:\n\n\n**Algorithm 1** Backwards Q-Learning Algorithm\n\nSet exploration_episode to false\n**while** true **do**\n\n**if** exploration_episode **then**\n\nFind unexplored path\nTravel to unexplored path\n**end if**\n**while** episode is not over **do**\n\n**if** current position has an unexplored neighbor **then**\n\nVisit unexplored neighbor\nUpdate model\nSave reward\n\n**if** no wall hit **then**\n\nSave action in action queue\n**end if**\n**else if** there is an optimal path to follow **then**\n\nVisit best neighbor\n**end if**\n**while** current pos. has no unexplored neighbor **do**\n\nBacktrack\n\n**end while**\n\n**end while**\nCreate state queue with breadth-first search\n**for** state in queue **do**\n\nApply RBQL formula\n**end for**\nSet exploration_episode to random() _<_ = _ϵ_\nApply decay to _ϵ_\n**end while**\n\n\n**3.2.3** **Q-learning agent**\n\n\nA standard Q-learning agent has been implemented in Godot as well to compare the performance of the RBQL agent to.\nThis agent is comparatively simple:\n\n\n**4** **Tests and Results**\n\n\nTo compare the performance of the two agents, three sets of tests have been done for different maze sizes: 5 _×_ 5, 10 _×_ 10\nand 15 _×_ 15. All variables have been set to common values. The decay rate is set somewhat high to account for the\nrelatively low episode amount:\n\n\n6\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**Algorithm 2** Q-Learning Algorithm\n\n\n**while** true **do**\n\n**if** random() _<_ = _ϵ_ **then**\n\nChoose random action\n\n**else**\n\nChoose greedy action\n**end if**\n\nTake action\n\nReceive new state and reward\nUpdate Q-table for old state and action\n**if** terminal state reached **then**\n\nStart new episode\n**end if**\nApply decay to _ϵ_\n**end while**\n\n\n    - _γ_ = 0 _._ 9\n\n\n    - _α_ = 0 _._ 1 (RBQL has _α_ = 1 as explained in equation (2))\n\n\n    - `min_epsilon` = 0 _._ 01\n\n\n    - `max_epsilon` = 1\n\n\n    - `decay_rate` = _−_ 0 _._ 01\n\n\nFor every maze size, each agent is given the same set of 50 randomly generated mazes. Each agent is given 25 episodes\nper maze to train. These values are chosen to offer a reasonably large sample size without requiring an enormous\namount of time to compute. Agents are compared by the number of steps taken per episode, with less steps taken being\na more desirable outcome. The step counter is increased every time `step()` is called, including the look-ahead steps of\nthe RBQL. For a sense of perspective, the best possible solution to any square maze of size _s_ [2] is 2 _s −_ 2. Assuming a\nmaze with no walls, the shortest distance between two points _A_ and _B_ can be expressed as their Manhattan distance\n_|AX −_ _BX_ _|_ + _|AY −_ _BY |_ [8]. In the corners of a square, it holds that _AX_ = _AY_ and _BX_ = _BY_, so the distance can\nbe simplified as 2 _· |A −_ _B|_ . Setting _A_ = 0 and _B_ = _s −_ 1, this further simplifies to 2 _s −_ 2. This means that while\nthe amount of states (and thereby state-action pairs) increases quadratically, the best possible solution only increases\nlinearly. This in turn means that the amount of states that are not on the optimal path that the agent has to evaluate will\noften increase drastically with the size of the maze.\n\n\nLooking at the results, a few things can be observed. First of all, the average number of steps the RBQL agent takes\nis consistently lower than the Q-learning agent in all three maze sizes. It also has much less variation in step counts,\nwhich can be seen when looking at the areas of lighter hue. The light red areas are much more sporadic and spike\nfurther away from the average. The green areas stick much closer together. If the highest two step counts per episode\nwere not removed, RBQL would also have a few small spikes. These spikes would represent exploratory episodes\nwhere a new path is explored, resulting in a higher step count. In cases where the line is flat for a long period of time, it\ncan be assumed that the optimal solution is found. This can be seen in all three figures, where both the average and\nthe min/max range become a straight line close to the minimum. Important to note is that every maze has a different\noptimal solution, hence why the average sits above the blue line which denotes the lowest possible step count in any\nmaze of this size. It can also be observed that none of the lines ever go below this boundary, as is to be expected.\n\n\nSecond, even when removing the highest two step counts per episode, many of the Q-learning agent’s step counts are so\nlarge that scaling the graphs to fit them makes the RBQL agent’s data and the lower boundary difficult to see in the\ngraphs for the larger mazes. The highest step count values that have not been cut are 858 steps in figure 3, 7,585 in\nfigure 4 and 21,147 in figure 5, while the highest in total are 3,716 steps in figure 3, 20,553 in figure 4 and 26,315 in\nfigure 5.\n\n\nThird, it is interesting to see how the differences in average step counts evolve with the grid size. Table 1 shows this\ndifference in the first and last episode. The difference between the average step counts in the last episode especially is\nstriking, as it is close to doubling from each size to the next. Further, looking at the improvement of each agent as seen\nin table 2, one can see that the factor by which RBQL improves massively increases the bigger the maze becomes while\nthe Q-learner only slightly improves its performance in comparison. Additionally, most of the improvement of RBQL is\ndone in the first two episodes, while the Q-learner has a more gradual learning curve.\n\n\n7\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**1** _**,**_ **000**\n\n\n**900**\n\n\n**800**\n\n\n**700**\n\n\n**600**\n\n\n**500**\n\n\n**400**\n\n\n**300**\n\n\n**200**\n\n\n**100**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 3: Number of steps taken to find the goal in a randomly generated grid world maze of size 5 _×_ 5. The blue line is\nthe minimum step threshold for any maze of this size. The light red area shows the range of Q-learning agent’s highest\nand lowest step count, excluding the highest and lowest two. The red line shows the average performance. Similarly, the\nlight green area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest\ntwo, and the green line shows the average performance.\n\n\n8\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-7-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n**8** _**,**_ **000**\n\n\n**6** _**,**_ **000**\n\n\n**5** _**,**_ **000**\n\n\n**4** _**,**_ **000**\n\n\n**3** _**,**_ **000**\n\n\n**2** _**,**_ **000**\n\n\n**1** _**,**_ **000**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 4: Number of steps taken to find the goal in a randomly generated grid world maze of size 10 _×_ 10. The light\nred area shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two.\nThe red shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n9\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-8-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 5: Number of steps taken to find the goal in a randomly generated grid world maze of size 15 _×_ 15. The light red\narea shows the range of Q-learning agent’s highest and lowest step count, excluding the highest and lowest two. The\nred line shows the average performance. Similarly, the light green area shows the range of the RBQL agent’s highest\nand lowest step count, excluding the highest and lowest two, and the green line shows the average performance.\n\n\n10\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-9-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nTable 1: Difference in average step counts of the Q-learner and RBQL. The difference expresses how many times more\nsteps the Q-learner took compared to RBQL.\n\n\nGrid size Q-learner steps RBQL steps Difference\n**Episode 0**\n5 _×_ 5 278.06 191.84 1.45\n\n10 _×_ 10 3,308.46 843.52 3.92\n15 _×_ 15 7,180.98 1,965 3.65\n**Episode 24**\n5 _×_ 5 49.14 9.62 5.11\n\n10 _×_ 10 281.44 23.68 11.89\n\n15 _×_ 15 778.68 35.96 21.65\n\n\nLastly, the RBQL agent seems to find an optimal policy at around episode 4 for the 5 _×_ 5, episode 6 for the 10 _×_ 10 and\nepisode 10 for the 15 _×_ 15 grid. As the previous figures show, the Q-learning agent does not come close to similarly\nlow step counts and therefore does not reach an optimal policy at all with the same amount of training.\n\n\nTable 2: Difference in average step counts of the Q-learner and RBQL. Improvement shows the factor by which the\namount of steps is reduced from episode 0 to 24.\n\n\nGrid size Steps in episode 0 Steps in episode 24 Improvement\n**Q-learning agent**\n5 _×_ 5 278.06 49.14 5.66\n\n10 _×_ 10 3,308.46 281.44 11.76\n15 _×_ 15 7,180.98 778.68 9.22\n**RBQL agent**\n5 _×_ 5 191.84 9.62 19.94\n\n10 _×_ 10 843.52 23.68 35.62\n\n15 _×_ 15 1,965 35.96 90.76\n\n\nTo further show RBQL’s efficiency, it has also been tested under the same parameters in a grid of size 50 _×_ 50. The\nresults can be seen in figure 6. This test is done to demonstrate that even such a large maze can be explored by RBQL.\nAs with the previous examples, by far the largest policy improvement still happens in the first episode. With mazes of\nsuch a large size, a lot more spikes in step counts are seen in later episodes because there are more states to explore. The\ndifference in average step counts goes from 20,811.08 in episode 0 to 344.9 in episode 24, an improvement by a factor\nof 60.34. This is worse than the improvement in the 15 _×_ 15 mazes, but still almost double that of the 10 _×_ 10 mazes.\n\n\n**5** **Discussion**\n\n\nThis chapter explores the practicality of using this algorithm to solve other Markov decision processes. It discusses\nwhich parts of the implementation are and are not specific to the problem of fastest path through a maze, which\nimprovements can be made to make it more applicable for other problems and showcases further points for research in\nthis field. The constraints given in this paper are that the agent will attempt to solve deterministic, episodic tasks with a\nsingle terminal state as its only source of positive rewards. This chapter also discusses which of these constraints can be\ndismissed.\n\n\nThere are a few parts of the implementation as presented in chapter 3.2 that are only applicable to this specific problem.\nThis is not necessarily a bad thing, as the purpose of the RBQL agent is to utilize knowledge of its environment. As a\nresult of this, the only parts that cannot be directly adapted for other problems are the way the agent builds its model.\nIn the grid world maze, it can assume that every state has the same actions it can take and has a neighboring state in\neach direction (though it may sometimes be itself). Further, every action always has an opposite action, going up can\nalways be undone by going down for example. These assumptions allow it to easily build a model of the grid world\nand influence its policy in how it further explores it. They allow the agent to take steps in each direction to check for\nwalls and they allow the agent to backtrack when it is stuck in a dead end. These assumptions cannot be guaranteed\nfor other Markov decision processes or even for grid worlds with more complex behavior like a wind tunnel that if\nwalked through also pushes the agent one tile in the direction the wind is traveling. The way in which the agent builds a\n\n\n11\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n_**·**_ **10** **[4]**\n\n\n**3**\n\n\n**2** _**.**_ **8**\n\n\n**2** _**.**_ **6**\n\n\n**2** _**.**_ **4**\n\n\n**2** _**.**_ **2**\n\n\n**2**\n\n\n**1** _**.**_ **8**\n\n\n**1** _**.**_ **6**\n\n\n**1** _**.**_ **4**\n\n\n**1** _**.**_ **2**\n\n\n**1**\n\n\n**0** _**.**_ **8**\n\n\n**0** _**.**_ **6**\n\n\n**0** _**.**_ **4**\n\n\n**0** _**.**_ **2**\n\n\n**0**\n**0** **2** **4** **6** **8** **10** **12** **14** **16** **18** **20** **22** **24**\n\n\n**Episode**\n\n\nFigure 6: Number of steps taken to find the goal in a randomly generated grid world maze of size 50 _×_ 50. The light\ngreen area shows the range of the RBQL agent’s highest and lowest step count, excluding the highest and lowest two,\nand the green line shows the average performance.\n\n\n12\n\n\n\n![](/Users/steven/Paper-Generator/output/images/user_2404.15822v1.pdf-11-0.png)\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\nmodel has to either be designed for each environment individually or it has to be abstracted so that it is more broadly\napplicable. Finding such an approach to model building is one area of improvement for RBQL. Importantly though,\nnone of these assumptions are required for the agent to function. Backtracking, opposite steps and the same actions for\nevery state simply make the implementation easier and more efficient. As long as no path of a directed graph would\ncause the agent to be stuck with no way to reach a terminal state, it can be explored and evaluated.\n\n\nAnother improvement to the way the implementation builds its model is to simplify it as far as possible. As the amount\nof states directly influences how long a problem takes to solve, RBQL will become more efficient the more it can\nremove unnecessary states. Currently, every position has its own state. If the agent could detect “hallways” – tiles with\nparallel walls – they could be removed without problem in favor of directly connecting the two tiles at either side of the\nhallway – only the negative rewards for the length of the hallway would have to be implemented into the model. Further,\nif there is a non-forking path that leads into a dead end, the entire path could be treated as a wall and ignored entirely.\nThis would leave only the starting state, terminal state, turns and forking paths to evaluate. Both of these additions leave\nthe key part of the algorithm, traversing the model backwards and applying the RBQL update formula, untouched.\n\n\nRBQL can be easily adapted to include multiple terminal states with the same or different rewards and this is already\nsupported by the implementation. There are two possible ways to do this. First is to create an imaginary state that\nall terminal states lead into from which the backtracking always starts. Second is to remember all terminal states and\nbacktrack from each of them. The first option is much more efficient as each state still only gets evaluated once while\nthe second version avoids having to tamper with the model.\n\n\nFinally, RBQL could be adapted to work in non-deterministic environments. To reiterate, deterministic means that a\nstate-action pair always yields the same state-reward pair. If the agent could, while building its model, also estimate the\ntransition probabilities of a state-action pair to a new state, RBQL could still be used to evaluate the states. The RBQL\nupdate rule can be generalized to\n\n\n\n_Q_ ( _St, At_ ) _←_ �\n\n_s∈St_ +1\n\n\n\n( _Rs_ + _γ_ max _Q_ ( _s, a_ )) _· p_ (3)\n_a_\n� �\n\n\n\nwhere _St_ +1 is the set of possible states when taking _At_ from _St_, _p_ is the probability of reaching _s_ when taking _At_ from\n_St_ and _Rs_ is the reward of reaching _s_ . In a deterministic environment, _St_ +1 only consists of one state with _p_ = 1,\nnegating these additions. Whether RBQL would be as effective in non-deterministic environments as in deterministic\nenvironments is something to be explored in further studies.\n\n\nThe only constraint on the algorithm that cannot easily be circumvented is its episodic nature. Because the agent relies\non a terminal state from which to propagate the rewards backwards from, a continuous task implementation seems\nimpossible to implement.\n\n\n**6** **Conclusion**\n\n\nThis paper has introduced recursive backwards Q-learning, a model-based reinforcement learning algorithm that\nevaluates all known state-action pairs of the model at the end of each episode with the Q-learning update rule. It has\nalso shown how recursive backwards Q-learning relates to, adapts and improves on them. This paper has presented\nan implementation of recursive backwards Q-learning in the Godot game engine to test its performance. Through\nmultiple tests, it has been shown to be superior in finding the shortest path through a randomly generated grid world\nmaze. It has been argued that this algorithm could be adapted to solve other deterministic, episodic tasks more quickly\nthan Q-learning. Further, it has given avenues for further research in adapting recursive backwards Q-learning for\nnon-deterministic problems.\n\n\n**References**\n\n\n[1] Richard Bellman, “A markovian decision process,” _Journal of Mathematics and Mechanics_, vol. 6, no. 5, pp. 679–\n684, 1957. [Online]. Available: `[http://www.jstor.org/stable/24900506](http://www.jstor.org/stable/24900506)` .\n\n[2] Christopher John Cornish Hellaby Watkins, “Learning from delayed rewards,” 1989.\n\n[3] Richard S Sutton and Andrew G Barto, _Reinforcement learning: An introduction_ . MIT press, 2018.\n\n[4] Richard S. Sutton, “Learning to predict by the methods of temporal differences,” _Machine Learning_, vol. 3, no. 1,\npp. 9–44, 1988. DOI: `[10.1007/bf00115009](https://doi.org/10.1007/bf00115009)` .\n\n[5] Richard Bellman, “Dynamic programming,” _Princeton, USA: Princeton University Press_, vol. 1, no. 2, p. 3, 1957.\n\n[6] Peter Gabrovšek, “Analysis of maze generating algorithms,” _IPSI Transactions on Internet Research_, vol. 15,\nno. 1, pp. 23–30, 2019.\n\n\n13\n\n\n\n\nRecursive Backwards Q-Learning in Deterministic Environments\n\n\n[7] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael, “A formal basis for the heuristic determination of minimum\ncost paths,” _IEEE Transactions on Systems Science and Cybernetics_, vol. 4, no. 2, pp. 100–107, 1968. DOI:\n`[10.1109/TSSC.1968.300136](https://doi.org/10.1109/TSSC.1968.300136)` .\n\n[8] Eugene F Krause, “Taxicab geometry,” _The Mathematics Teacher_, vol. 66, no. 8, pp. 695–706, 1973.\n\n\n14\n\n\n",
    "ranking": null,
    "citation_key": "Diekhoff2024RecursiveBQ",
    "is_open_access": false,
    "user_provided": true,
    "pdf_path": "output/literature/user_2404.15822v1/user_2404.15822v1.pdf"
  },
  {
    "id": "acda55ebdf39c6634e89a9730ff7d963471f2b0a",
    "title": "Expected Eligibility Traces",
    "published": "2020-07-03",
    "authors": [
      "H. V. Hasselt",
      "Sephora Madjiheurem",
      "Matteo Hessel",
      "David Silver",
      "André Barreto",
      "Diana Borsa"
    ],
    "summary": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state.\nIn this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that some- times substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
    "pdf_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17200/17007",
    "doi": "10.1609/aaai.v35i11.17200",
    "fields_of_study": [
      "Computer Science",
      "Mathematics"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "citation_count": 41,
    "bibtex": "@Article{Hasselt2020ExpectedET,\n author = {H. V. Hasselt and Sephora Madjiheurem and Matteo Hessel and David Silver and André Barreto and Diana Borsa},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Expected Eligibility Traces},\n volume = {abs/2007.01839},\n year = {2020}\n}\n",
    "markdown_text": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)\n\n# **Expected Eligibility Traces**\n\n\n**Hado van Hasselt** [1] **, Sephora Madjiheurem** [2] **, Matteo Hessel** [1]\n\n**David Silver** [1] **, Andr´e Barreto** [1] **, Diana Borsa** [1]\n\n1 DeepMind\n2 University College London, UK\n\n\n\n**Abstract**\n\n\nThe question of how to determine which states and actions\nare responsible for a certain outcome is known as the _credit_\n_assignment problem_ and remains a central research question\nin reinforcement learning and artificial intelligence. _Eligibil-_\n_ity traces_ enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not\nto counterfactual sequences that could also have led to the\ncurrent state. In this work, we introduce _expected eligibility_\n_traces_ . Expected traces allow, with a single update, to update\nstates and actions that could have preceded the current state,\neven if they did not do so on this occasion. We discuss when\nexpected traces provide benefits over classic (instantaneous)\ntraces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide\na way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which\nensures that the resulting algorithm is a strict generalisation\nof TD( _λ_ ). Finally, we discuss possible extensions and connections to related ideas, such as successor features.\n\n\n**Motivation and Summary**\n\n\nAppropriate credit assignment has long been a major research\ntopic in artificial intelligence (Minsky 1963). To make effective decisions and understand the world, we need to accurately associate events, like rewards or penalties, to relevant\nearlier decisions or situations. This is important both for learning accurate predictions, and for making good decisions.\n_Temporal credit assignment_ can be achieved with repeated\ntemporal-difference (TD) updates (Sutton 1988). One-step\nTD updates propagate information slowly: when a surprising value is observed, the state immediately preceding it is\nupdated, but no earlier states or decisions are updated. _Multi-_\n_step_ updates (Sutton 1988; Sutton and Barto 2018) propagate\ninformation faster over longer temporal spans, speeding up\ncredit assignment and learning. Multi-step updates can be\nimplemented online using _eligibility traces_ (Sutton 1988),\nwithout incurring significant additional computational expense, even if the time spans are long; these algorithms have\ncomputation that is independent of the temporal span of the\npredictions (van Hasselt and Sutton 2015).\n\n\nCopyright c _⃝_ 2021, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\nMDP True value TD(0) TD(λ) ET(λ)\n\n\nFigure 1: A comparison of TD(0), TD( _λ_ ), and the new\nexpected-trace algorithm ET( _λ_ ) (with _λ_ = 0 _._ 9). The MDP\nis illustrated on the left. Each episode, the agent moves randomly down and right from the top left to the bottom right,\nwhere any action terminates the episode. Reward on termination are +1 with probability 0.2, and zero otherwise—all\nother rewards are zero. We plot the value estimates after the\nfirst positive reward, which occurred in episode 5. We see\na) TD(0) only updated the last state, b) TD( _λ_ ) updated the\ntrajectory in this episode, and c) ET( _λ_ ) additionally updated\ntrajectories from earlier (unrewarding) episodes.\n\n\nTraces provide temporal credit assignment, but do not assign credit _counterfactually_ to states or actions that _could_\nhave led to the current state, but did not do so this time.\nCredit will eventually trickle backwards over the course of\nmultiple visits, but this can take many iterations. As an example, suppose we collect a key to open a door, which leads\nto an unexpected reward. Using standard one-step TD learning, we would update the state in which the door opened.\nUsing eligibility traces, we would also update the preceding\ntrajectory, including the acquisition of the key. But we would\nnot update other sequences that _could_ have led to the reward,\nsuch as collecting a spare key or finding a different entrance.\nThe problem of credit assignment to counterfactual states\nmay be addressed by learning a model, and using the model\nto propagate credit (cf. Sutton 1990; Moore and Atkeson\n1993; Chelu, Precup, and van Hasselt 2020); however, it\nhas often proven challenging to construct and use models\neffectively in complex environments (cf. van Hasselt, Hessel,\nand Aslanides 2019).\nWe introduce a new approach to counterfactual credit assignment, based on the concept of _expected eligibility traces_ .\nWe present a family of algorithms, which we call ET( _λ_ ), that\nuse expected traces to update their predictions. We analyse\nthe nature of these expected traces, and illustrate their benefits empirically in several settings—see Figure 1 for a first\n\n\n\n9997\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-1.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-3.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-0-4.png)\n\n\nillustration. We introduce a bootstrapping mechanism that\nprovides a spectrum of algorithms between standard eligibility traces and expected eligibility traces, and also discuss\nways to apply these ideas with deep neural networks. Finally,\nwe discuss possible extensions and connections to related\nideas such as successor features.\n\n\n**Background**\nSequential decision problems can be modelled as Markov\ndecision processes [1] (MDP) ( _S, A, p_ ) (Puterman 1994), with\nstate space _S_, action space _A_, and a joint transition and\nreward distribution _p_ ( _r, s_ _[′]_ _|s, a_ ). An agent selects actions according to its policy _π_, such that _At ∼_ _π_ ( _·|St_ ) where _π_ ( _a|s_ )\ndenotes the probability of selecting _a_ in _s_, and observes random rewards and states generated according to the MDP, resulting in trajectories _τt_ : _T_ = _{St, At, Rt_ +1 _, St_ +1 _, . . ., ST }_ .\nA central goal is to predict _returns_ of future discounted rewards (Sutton and Barto 2018)\n\n\n_Gt ≡_ _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _Rt_ +2 + _γt_ +1 _γt_ +2 _Rt_ +3 + _. . ._\n\n\n\n=\n\n\n\n_T_\n� _γt_ [(] +1 _[i][−]_ [1)] _[R][t]_ [+] _[i][,]_\n\n\n_i_ =1\n\n\n\nwhere _T_ is for instance the time the current episode terminates or _T_ = _∞_, and where _γt ∈_ [0 _,_ 1] is a (possibly constant) discount factor and _γt_ [(] _[n]_ [)] = [�] _[n]_ _k_ =0 _[−]_ [1] _[γ][t]_ [+] _[k]_ [, and] _[ γ]_ _t_ [(0)] = 1.\nThe value _vπ_ ( _s_ ) = E [ _Gt|St_ = _s, π_ ] of state _s_ is the expected return for a policy _π_ . Rather than writing the return as\na random variable _Gt_, it will be convenient to instead write it\nas an explicit function _G_ ( _τ_ ) of the random trajectory _τ_ . Note\nthat _G_ ( _τt_ : _T_ ) = _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ).\nWe approximate the value with a function _v_ **w** ( _s_ ) _≈_ _vπ_ ( _s_ ).\nThis can for instance be a table—with a single separate entry\n_w_ [ _s_ ] for each state—a linear function of some input features,\nor a non-linear function such as a neural network with parameters **w** . The goal is to iteratively update **w** with\n\n\n**w** _t_ +1 = **w** _t_ + ∆ **w** _t_\n\n\nsuch that _v_ **w** approaches the true _vπ_ . Perhaps the simplest\nalgorithm to do so is the Monte Carlo (MC) algorithm\n\n\n∆ **w** _t ≡_ _α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._\n\n\nMonte Carlo is effective, but has high variance, which can\nlead to slow learning. TD learning (Sutton 1988; Sutton and\nBarto 2018) instead replaces the return with the current estimate of its expectation _v_ ( _St_ +1) _≈_ _G_ ( _τt_ +1: _T_ ), yielding\n\n\n∆ **w** _t ≡_ _αδt∇_ **w** _v_ **w** ( _St_ ) _,_ (1)\nwhere _δt ≡_ _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ ) _,_\n\n\nwhere _δt_ is called the temporal-difference (TD) error. We\ncan interpolate between these extremes, for instance with\n_λ_ -returns which smoothly mix values and sampled returns:\n\n\n_G_ _[λ]_ ( _τt_ : _T_ ) = _Rt_ +1+ _γt_ +1�(1 _−λ_ ) _v_ **w** ( _St_ +1)+ _λG_ _[λ]_ ( _τt_ +1: _T_ )� _._\n\n\n‘Forward view’ algorithms, like the MC algorithm, use returns\nthat depend on future trajectories and need to wait until the\n\n\n1The ideas in this paper extend naturally to POMDPs (cf. **?** ).\n\n\n\nend of an episode to construct their updates, which can take a\nlong time. Conversely, ‘backward view’ algorithms rely only\non past experiences and can update their predictions online,\nduring an episode. Such algorithms build an _eligibility trace_\n(Sutton 1988; Sutton and Barto 2018). An example is TD( _λ_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ _t,_ with _**e**_ _t_ = _γtλ_ _**e**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ ) _,_\n\n\nwhere _**e**_ _t_ is an accumulating eligibility trace. This trace can\nbe viewed as a function _**e**_ _t ≡_ _**e**_ ( _τ_ 0: _t_ ) of the trajectory of past\ntransitions. The TD update in (1) is known as TD(0), because\nit corresponds to using _λ_ = 0. TD( _λ_ = 1) corresponds to an\nonline implementation of the MC algorithm. Other variants\nexist, using other kinds of traces, and equivalences have been\nshown between these algorithms and their forward views that\nuse _λ_ -returns: these backward-view algorithms converge to\nthe same solution as the corresponding forward view, and can\nin some cases yield equivalent weight updates (Sutton 1988;\nvan Seijen and Sutton 2014; van Hasselt and Sutton 2015).\n\n\n**Expected Traces**\n\n\nThe main idea of this paper is to use the concept of an _ex-_\n_pected eligibility trace_, defined as\n\n\n_**z**_ ( _s_ ) _≡_ E [ _**e**_ _t | St_ = _s_ ] _,_\n\n\nwhere the expectation is over the agent’s policy and the MDP\ndynamics. We introduce a concrete family of algorithms,\nwhich we call ET( _λ_ ) and ET( _λ_, _η_ ), that learn expected traces\nand use them in value updates. We analyse these algorithms\ntheoretically, describe specific instances, and discuss computational and algorithmic properties.\n\n\n**ET(** _λ_ **)**\n\n\nWe propose to learn approximations _**zθ**_ ( _s_ ) _≈_ _**z**_ ( _s_ ), with parameters _**θ**_ _∈_ R _[d]_ (e.g., the weights of a neural network). One\nway to learn _**zθ**_ is by updating it toward the instantaneous\ntrace _**e**_ _t_, by minimizing an empirical loss _L_ ( _**e**_ _t,_ _**zθ**_ ( _St_ )). For\ninstance, _L_ could be a component-wise squared loss, optimized with stochastic gradient descent:\n\n\n_**θ**_ _t_ +1 = _**θ**_ _t_ + ∆ _**θ**_ _t,_ where (2)\n\n\n1\n\n∆ _**θ**_ _t_ = _−β_ _[∂]_\n\n_∂_ _**θ**_ 2 [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))] _[⊤]_ [(] _**[e]**_ _[t][ −]_ _**[z][θ]**_ [(] _[S][t]_ [))]\n\n= _β_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] ( _**e**_ _t −_ _**zθ**_ ( _St_ )) _,_ (3)\n\n_∂_ _**θ**_\n\n\nwhere _[∂z]_ _**[θ]**_ _∂_ [(] _**θ**_ _[S][t]_ [)] is a _|_ _**θ**_ _| × |_ _**e**_ _|_ Jacobian [2] and _β_ is a step size.\n\nThe idea is then to use _**zθ**_ ( _s_ ) _≈_ E [ _**e**_ _t | St_ = _s_ ] in place\nof _**e**_ _t_ in the value update, which becomes\n\n\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (4)\n\n\nWe call this ET( _λ_ ). Below, we prove that this update can\nbe unbiased and can have lower variance than TD( _λ_ ). Algorithm 1 shows pseudo-code for a concrete instance of ET( _λ_ ).\n\n\n2The Jacobian-vector product can efficiently be computed (e.g.,\nvia auto-differentiation) with computational requirements that are\ncomparable to the computation of the loss.\n\n\n\n9998\n\n\n\n\n**Algorithm 1** ET( _λ_ )\n\n\n1: initialise **w**, _**θ**_\n2: **for** _M_ episodes **do**\n3: initialise _**e**_ = **0**\n\n4: observe initial state _S_\n5: **repeat** for each step in episode _m_\n6: generate _R_ and _S_ _[′]_\n\n7: _δ ←_ _R_ + _γv_ **w** ( _S_ _[′]_ ) _−_ _v_ **w** ( _S_ )\n8: _**e**_ _←_ _γλ_ _**e**_ + _∇_ **w** _v_ **w** ( _S_ )\n\n9: _**θ**_ _←_ _**θ**_ + _β_ _[∂]_ _**[z]**_ _∂_ _**[θ]**_ _**θ**_ [(] _[S]_ [)] ( _**e**_ _−_ _**zθ**_ ( _S_ ))\n\n10: **w** _←_ **w** + _αδ_ _**zθ**_ ( _S_ )\n11: **until** _S_ is terminal\n\n12: **end for**\n\n13: **Return w**\n\n\n**Interpretation and ET(** _λ, η_ **)**\n\nWe can interpret TD(0) as taking the MC update and replacing the return from the subsequent state, which is a function\nof the future trajectory, with a state-based estimate of its expectation: _v_ **w** ( _St_ +1) _≈_ E [ _G_ ( _τt_ +1: _T_ ) _|St_ +1 ]. This becomes\nmost clear when juxtaposing the updates:\n\n\n_α_ ( _Rt_ +1 + _γt_ +1 _G_ ( _τt_ +1: _T_ ) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _,_ (MC)\n_α_ ( _Rt_ +1 + _γt_ +1 _v_ **w** ( _St_ +1) _−_ _v_ **w** ( _St_ )) _∇_ **w** _v_ **w** ( _St_ ) _._ (TD)\n\n\nTD( _λ_ ) also uses a function of a trajectory: the trace _**e**_ _t_ . We\npropose replacing this as well with a function of state: the\nexpected trace _**zθ**_ ( _St_ ) _≈_ E [ _**e**_ ( _τ_ 0: _t_ ) _|St_ ]. Again juxtaposing:\n\n\n∆ **w** _t ≡_ _αδt_ _**e**_ ( _τ_ 0: _t_ ) _,_ (TD( _λ_ ))\n∆ **w** _t ≡_ _αδt_ _**zθ**_ ( _St_ ) _._ (ET( _λ_ ))\n\n\nWe can interpolate smoothly between MC and TD(0) via\n_λ_ . This is often useful to trade off variance of the return with\npotential bias of the value estimate. For instance, we might\nnot have access to the true state _s_, and might instead have to\nrely on features **x** ( _s_ ). Then we cannot always represent or\nlearn the true values _v_ ( _s_ )—for instance different states may\nbe aliased (Whitehead and Ballard 1991).\nSimilarly, when moving from TD( _λ_ ) to ET( _λ_ ) we replaced\na trajectory-based trace with a state-based estimate. This\nmight induce bias and, again, we can smoothly interpolate by\nusing a recursively defined mixture trace _**y**_ _t_, as defined as [3]\n\n\n_**y**_ _t_ = (1 _−_ _η_ ) _**zθ**_ ( _St_ ) + _η_ � _γtλ_ _**y**_ _t−_ 1 + _∇_ **w** _v_ **w** ( _St_ )� _._ (5)\n\n\nThis recursive usage of the estimates _**zθ**_ ( _s_ ) at previous states\nis analogous to bootstrapping on future state values when\nusing a _λ_ -return, with the important difference that the arrow\nof time is opposite. This means we do not first have to convert\nthis into a backward view: the quantity can already be computed from past experience directly. We call the algorithm\nthat uses this mixture trace ET( _λ_, _η_ ):\n\n\n∆ **w** _t ≡_ _αδt_ _**y**_ ( _St_ ) _._ (ET( _λ_, _η_ ))\n\n\n3While _**y**_ _t_ depends on both _η_ and _λ_ we leave this dependence\nimplicit, as is conventional for traces.\n\n\n\nNote that if _η_ = 1 then _**y**_ _t_ = _**e**_ _t_ equals the instantaneous\ntrace: ET( _λ_, 1) is equivalent to TD( _λ_ ). If _η_ = 0 then _**y**_ _t_ = _**z**_ _t_\nequals the expected trace; the algorithm introduced earlier\nas ET( _λ_ ) is equivalent to ET( _λ_, 0). By setting _η ∈_ (0 _,_ 1), we\ncan smoothly interpolate between these extremes.\n\n\n**Theoretical Analysis**\n\nWe now analyse the new ET algorithms theoretically. First\nwe show that if we use _**z**_ ( _s_ ) directly and _s_ is Markov then the\nupdate has the same expectation as TD( _λ_ ) (though possibly\nwith lower variance), and therefore also inherits the same\nfixed point and convergence properties.\n\n\n**Lemma 1.** _If s is Markov, then_\n\n\nE [ _δt_ _**e**_ _t | St_ = _s_ ] = E [ _δt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] _._\n\n\n_Proof._ In Appendix .\n\n\n**Proposition 1.** _Let_ _**e**_ _t be any trace vector, updated in any_\n_way. Let_ _**z**_ ( _s_ ) = E [ _**e**_ _t | St_ = _s_ ] _. Consider the ET(λ) algo-_\n_rithm_ ∆ **w** _t_ = _αtδt_ _**z**_ ( _St_ ) _. For all Markov states s the expec-_\n_tation of this update is equal to the expected update under_\n_instantaneous trace_ _**e**_ _t, and its variance is lower or equal:_\n\n\nE [ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] = E [ _αtδt_ _**e**_ _t|St_ = _s_ ] _and_\nV[ _αtδt_ _**z**_ ( _St_ ) _|St_ = _s_ ] _≤_ V[ _αtδt_ _**e**_ _t|St_ = _s_ ] _,_\n\n\n_where the second inequality holds component-wise for the_\n_update vector, and is strict when_ V[ _**e**_ _t|St_ ] _>_ 0 _._\n\n\n_Proof._ We have\n\n\nE [ _αtδt_ _**e**_ _t | St_ = _s_ ]\n= E [ _αtδt | St_ = _s_ ] E [ _**e**_ _t | St_ = _s_ ] (Lemma 1)\n= E [ _αtδt | St_ = _s_ ] _**z**_ ( _s_ )\n= E [ _αtδt_ _**z**_ ( _St_ ) _| St_ = _s_ ] _._ (6)\n\n\nDenote the _i_ -th component of _**z**_ ( _St_ ) by _zt,i_ and the _i_ -th\ncomponent of _**e**_ _t_ by _et,i_ . Then, we also have\n\n\nE � ( _αtδtzt,i_ ) [2] _|St_ = _s_ � = E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � _zt,i_ [2]\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E [ _et,i|St_ = _s_ ] [2]\n\n= E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ ��E � _e_ [2] _t,i_ _[|][S][t]_ [=] _[ s]_ � _−_ V[ _et,i|St_ = _s_ ]�\n\n_≤_ E � _αt_ [2] _[δ]_ _t_ [2] _[|][ S][t]_ [=] _[ s]_ � E � _e_ [2] _t,i_ _[|][ S][t]_ [=] _[ s]_ �\n\n= E � ( _αtδtet,i_ ) [2] _| St_ = _s_ � _,_\n\n\nwhere the last step used the fact that _s_ is Markov, and the inequality is strict when V[ _et,i|St_ ] _>_ 0. Since the expectations\nare equal, as shown in (6), the conclusion follows.\n\n\n**Interpretation** Proposition 1 is a strong result: it holds for\nany trace update, including accumulating traces (Sutton 1984,\n1988), replacing traces (Singh and Sutton 1996), dutch traces\n(van Seijen and Sutton 2014; van Hasselt, Mahmood, and\nSutton 2014; van Hasselt and Sutton 2015), and future traces\nthat may be discovered. It implies convergence of ET( _λ_ )\nunder the same conditions as TD( _λ_ ) (Dayan 1992; Peng 1993;\n\n\n\n9999\n\n\n\n\nTsitsiklis 1994) with lower variance when V[ _**e**_ _t|St_ ] _>_ 0,\nwhich is the common case.\nNext, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a\nlearned approximation _**z**_ _t_ ( _s_ ) _≈_ _**z**_ ( _s_ ) that relies solely on\nobserved experience.\n\n**Proposition 2.** _Let_ _**e**_ _t an instantaneous trace vector. Then_\n1 _nt_ ( _s_ )\n_let_ _**z**_ _t_ ( _s_ ) _be the empirical mean_ _**z**_ _t_ ( _s_ ) = _nt_ ( _s_ ) � _i_ _**e**_ _t_ _[s]_ _i_ _[,]_\n_where t_ _[s]_ _i_ _[denotes past times when we have been in state]_\n_s, that is St_ _[s]_ _i_ [=] _[ s][, and][ n][t]_ [(] _[s]_ [)] _[ is the number of visits to][ s]_\n_in the first t steps. Consider the expected trace algorithm_\n**w** _t_ +1 = **w** _t_ + _αtδt_ _**z**_ _t_ ( _St_ ) _. If St is Markov, the expectation of_\n_this update is equal to the expected update with instantaneous_\n_traces_ _**e**_ _t, while attaining a potentially lower variance:_\n\n\nE [ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] = E [ _αtδt_ _**e**_ _t | St_ ] _and_\nV[ _αtδt_ _**z**_ _t_ ( _St_ ) _| St_ ] _≤_ V[ _αtδt_ _**e**_ _t | St_ ] _,_\n\n\n_where the second inequality holds component-wise. The in-_\n_equality is strict when_ V[ _**e**_ _t | St_ ] _>_ 0 _._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** Proposition 2 mirrors Proposition 1 but, importantly, covers the case where we estimate the expected\ntraces from data, rather than relying on exact estimates. This\nmeans the benefits extend to this pure learning setting. Again,\nthe result holds for any trace update. The inequality is typically strict when the path leading to state _St_ = _s_ is stochastic\n(due to environment or policy).\nNext we consider what happens if we do not have Markov\nstates and instead have to rely on, possibly non-Markovian,\nfeatures **x** ( _s_ ). We then have to pick a function class and for\nthe purpose of this analysis we consider linear expected traces\n_**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) and values _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), as convergence for non-linear values can not always be assured even\nfor standard TD( _λ_ ) (Tsitsiklis and Van Roy 1997), without\nadditional assumptions (e.g., Ollivier 2018; Brandfonbrener\nand Bruna 2020).\n\n**Proposition 3.** _When using approximations z_ **Θ** ( _s_ ) = **Θx** ( _s_ )\n_and v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ) _then, if_ (1 _−_ _η_ ) **Θ** + _η_ I _is non-singular,_\n_ET(λ, η) has the same fixed point as TD(λη)._\n\n\n_Proof._ In Appendix.\n\n\n**Interpretation** This result implies that linear ET( _λ_, _η_ ) converges under similar conditions as linear TD( _λ_ _[′]_ ) for _λ_ _[′]_ = _λ·η_ .\nIn particular, when **Θ** is non-singular, using the approximation _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) in ET( _λ_, 0) = ET( _λ_ ) implies convergence to the fixed point of TD(0).\nThough ET( _λ_, _η_ ) and TD( _λη_ ) have the same fixed point,\nthe algorithms are not equivalent. In general, their updates\nare not the same. Linear approximations are more general\nthan tabular functions (which are linear functions of a indicator vector for the current state), and we have already seen\nin Figure 1 that ET( _λ_ ) behaves quite differently from both\nTD(0) and TD( _λ_ ), and we have seen its variance can be lower\nin Propositions 1 and 2. Interestingly, **Θ** resembles a preconditioner that speeds up the linear semi-gradient TD update,\n\n\n10000\n\n\n\nepisode 5\n1st reward\n\n\n\nepisode 12\n2nd reward\n\n\n\nepisode 100\n20 rewards\n\n\n\nepisode 1K\n~200 rewards\n\n\n\nepisode 10K\n~2K rewards\n\n\n\nFigure 2: In the same setting as Figure 1, we show later value\nestimates after more rewards have been observed. TD(0)\nlearns slowly but steadily, TD( _λ_ ) learns faster but with higher\nvariance, and ET( _λ_ ) learns both fast and stable.\n\n\nsimilar to how second-order optimisation algorithms (Amari\n1998; Martens 2016) precondition the gradient updates.\n\n\n**Empirical Analysis**\n\nFrom the insights above, we expect that ET( _λ_ ) yields lower\nprediction errors because it has lower variance and aggregates information across episodes better. In this section we\nempirically investigate expected traces in several experiments.\nWhenever we refer to ET( _λ_ ), this is equivalent to ET( _λ_, 0).\n\n\n\n**An Open World**\n\nFirst consider the grid world depicted in Figure 1. The agent\nrandomly moves right or down (excluding moves that would\nhit a wall), starting from the top-left corner. Any action in the\nbottom-right corner terminates the episode with +1 reward\nwith probability 0 _._ 2, and 0 otherwise. All other rewards are 0.\nFigure 1 shows value estimates after the first positive reward, which occurred in the fifth episode. TD(0) updated a\nsingle state, TD( _λ_ ) updated earlier states in that episode, and\nET( _λ_ ) additionally updated states from previous episodes.\nFigure 2 additionally shows value estimates after the\nsecond reward (which occurred in episode 12), and after\nroughly 20, 200, and 2000 rewards (or 100, 1000, and 10 _,_ 000\nepisodes, respectively). ET( _λ_ ) converged faster than TD(0),\nwhich propagated information slowly, and faster than TD( _λ_ ),\nwhich exhibited higher variance. All step sizes decayed as\n_α_ = _β_ = ~~�~~ 1 _/k_, where _k_ is the current episode number.\n\n\n**A Multi-Chain**\n\nWe now consider the multi-chain shown in Figure 3. We\nfirst compare TD( _λ_ ) and ET( _λ_ ) with tabular values on various variants of the multi-chain, corresponding to _m ∈_\n_{_ 1 _,_ 2 _,_ 4 _,_ 8 _, ...,_ 128 _}_ parallel chains of length _n_ = 4. The leftmost plot in Figure 4 shows the average root mean squared\nerror (RMSE) of the value predictions after 1024 episodes.\nWe ran 10 seeds for each combination of step size 1 _/t_ _[d]_ with\n_d ∈{_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 1 _}_ and _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ .\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-4-0.png)\n\neither is +1 with probability 0 _._ 9 or _−_ 1 with probability 0 _._ 1.\n\n\nThe left plot in Figure 4 shows value errors for different\n_m_, minimized over _d_ and _λ_ . The prediction error of TD( _λ_ )\n(blue) grew quickly with the number of parallel chains. ET( _λ_ )\n(orange) scaled better, because it updates values in multiple\nchains (from past episodes) upon receiving a surprising reward (e.g., _−_ 1) on termination. The other three plots in Figure\n4 show value error as a function of _λ_ for a subset of problems\ncorresponding to _m ∈{_ 8 _,_ 32 _,_ 128 _}_ . The dependence on _λ_\ndiffers across algorithms and problem instances, but ET( _λ_ )\nconsistently achieved lower error than TD( _λ_ ), especially with\nhigh _λ_ . Further analysis, including on step-size sensitivity, is\nincluded in the appendix.\nNext, we encode each state with a feature vector **x** ( _s_ )\ncontaining a binary indicator vector of the branch, a binary\nindicator of the progress along the chain, a bias that always\nequals one, and two binary features indicating when we are in\nthe start (white) or bottleneck (orange) state. We extend the\nlengths of the chains to _n_ = 16. Both TD( _λ_ ) and ET( _λ_ ) use\na linear value function _v_ **w** ( _s_ ) = **w** _[⊤]_ **x** ( _s_ ), and ET( _λ_ ) uses a\nlinear expected trace _z_ **Θ** ( _s_ ) = **Θx** ( _s_ ). All updates use the\nsame constant step size _α_ . The left plot in Figure 5 shows the\naverage root mean squared value error after 1024 episodes\n(averaged over 10 seeds). For each point the best constant\nstep size _α ∈{_ 0 _._ 01 _,_ 0 _._ 03 _,_ 0 _._ 1 _}_ (shared across all updates)\nand _λ ∈{_ 0 _,_ 0 _._ 5 _,_ 0 _._ 8 _,_ 0 _._ 9 _,_ 0 _._ 95 _,_ 1 _}_ is selected. ET( _λ_ ) (orange)\nattained lower errors across all values of _m_ (left plot), and\nfor all _λ_ (center two plots, for two specific _m_ ). The right plot\nshows results for smooth interpolations via _η_, for _λ_ = 0 _._ 9\nand _m_ = 16. The full expected trace ( _η_ = 0) performed well\nhere, we expect in other settings the additional flexibility of\n_η_ could be beneficial.\n\n\n**Expected Traces in Deep Reinforcement Learning**\n\n(Deep) neural networks are a common choice of function\nclass in reinforcement learning (e.g., Werbos 1990; Tesauro\n1992, 1994; Bertsekas and Tsitsiklis 1996; Prokhorov and\nWunsch 1997; Riedmiller 2005; van Hasselt 2012; Mnih\net al. 2015; van Hasselt, Guez, and Silver 2016; Wang et al.\n2016; Silver et al. 2016; Duan et al. 2016; Hessel et al. 2018).\nEligibility traces are not very commonly combined with deep\nnetworks (but see Tesauro 1992; Elfwing, Uchibe, and Doya\n2018), perhaps in part because of the popularity of experience\n\n\n10001\n\n\n\nreplay (Lin 1992; Mnih et al. 2015; Horgan et al. 2018).\nPerhaps the simplest way to extend expected traces to deep\nneural networks is to first separate the value function into\na representation **x** ( _s_ ) and a value _v_ ( **w** _,_ _**ξ**_ )( _s_ ) = **w** _[⊤]_ **x** _**ξ**_ ( _s_ ),\nwhere **x** _**ξ**_ is some (non-linear) function of the observations\n_s_ . [4] We can then apply the same expected trace algorithm as\nused in the previous sections by learning a separate linear\nfunction _**z**_ **Θ** ( _s_ ) = **Θx** ( _s_ ) using the representation which is\nlearned by backpropagating the value updates:\n\n\n**w** _t_ +1 = **w** _t_ + _αδ_ _**z**_ **Θ** ( _St_ ) _,_\n\n_**ξ**_ _t_ +1 = _**ξ**_ _t_ + _αδ_ _**e**_ _**[ξ]**_ _t_ _[,]_\n\nwhere _**e**_ _**[ξ]**_ _t_ [=] _[ γ][t][λ]_ _**[e][ξ]**_ _t−_ 1 [+] _[ ∇]_ _**[ξ]**_ _[v]_ [(] **[w]** _[,]_ _**[ξ]**_ [)][(] _[S][t]_ [)] _[,]_\n\n_**e**_ **[w]** _t_ [=] _[ γ][t][λ]_ _**[e]**_ **[w]** _t−_ 1 [+] _[ ∇]_ **[w]** _[v]_ ( **w** _,_ _**ξ**_ ) [(] _[S][t]_ [)] _[,]_\n\n\nand then updating **Θ** to minimise component-wise squared\ndifferences between _**e**_ **[w]** _t_ [and] _**[ z]**_ **[Θ]** _t_ [(] _[S][t]_ [)][, as in (2) and (3).]\nInteresting challenges appear outside the fully linear case.\nFirst, the representation will itself be updated and will have\nits own trace _**e**_ _**[ξ]**_ _t_ [. Second, in the control case we optimise]\nbehaviour: the policy will change. Both these properties of\nthe non-linear control setting imply that the expected traces\nmust track a non-stationary target. We found that being able to\ntrack this rather quickly improved performance: the expected\ntrace parameters **Θ** in the following experiment were updated\nwith a relatively high step size of _β_ = 0 _._ 1.\nWe tested this idea on two canonical Atari games: Pong and\nMs. Pac-Man. The results in Figure 6 show that the expected\ntraces helped speed up learning compared to the baseline\nwhich uses accumulating traces, for various step sizes. Unlike\nmost prior work on this domain, which often relies on replay\n(Mnih et al. 2015; Schaul et al. 2016; Horgan et al. 2018)\nor parallel streams of experience (Mnih et al. 2016), these\nalgorithms updated the values online from a single stream\nof experience. Further details on the experimental setup are\ngiven in the appendix.\nThese experiments demonstrate that the idea of expected\ntraces extends to non-linear function approximation, such as\ndeep neural networks. We consider this to be a rich area of\nfurther investigations. The results presented here are similar\nto earlier results (e.g., Mnih et al. 2015) and are not meant to\ncompete with state-of-the-art performance results, which often depend on replay and much larger amounts of experience\n(e.g., Horgan et al. 2018).\n\n\n**Discussion and Extensions**\n\nWe now discuss various interesting interpretations and relations, and discuss promising extensions.\n\n\n**Predecessor Features**\n\nFor linear value functions the expected trace _z_ ( _s_ ) can be\nexpressed non-recursively as follows:\n\n\n\n�\n\n\n\n_**z**_ ( _s_ ) = E\n\n\n\n_∞_\n� _λ_ [(] _t_ _[n]_ [)] _γt_ [(] _[n]_ [)] **x** _t−n | St_ = _s_\n� _n_ =0\n\n\n\n_,_ (7)\n\n\n\n4Here _s_ denotes observations to the agent, not a full environment\nstate— _s_ is not assumed to be Markovian.\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-1.png)\n\nFigure 4: Prediction errors in the multi-chain. ET( _λ_ ) (orange) consistently outperformed TD( _λ_ ) (blue). Shaded areas depict\nstandard errors across 10 seeds.\n\n\nFigure 5: Comparing value error with linear function approximation a) as function of the number of branches (left), b) as\nfunction of _λ_ (center two plots) and c) as function of _η_ (right). The left three plots show comparisons of TD( _λ_ ) (blue) and ET( _λ_ )\n(orange), showing ET( _λ_ ) attained lower prediction errors. The right plot interpolates between these algorithms via ET( _λ_, _η_ ),\nfrom ET( _λ_ ) = ET( _λ_, 0) to ET( _λ_, 1) = TD( _λ_ ), with _λ_ = 0 _._ 9 (corresponding to a vertical slice indicated in the second plot).\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-2.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-3.png)\n\nwhere _γk_ [(] _[n]_ [)] _≡_ [�] _[k]_ _j_ = _k−n_ _[γ][j]_ [. This is interestingly similar to the]\ndefinition of _successor features_ (Barreto et al. 2017):\n\n\n\n�\n\n\n\n_ψ_ ( _s_ ) = E\n\n\n\n_∞_\n� _γt_ [(] +1 _[n][−]_ [1)] **x** _t_ + _n | St_ = _s_\n� _n_ =1\n\n\n\n_._ (8)\n\n\n\nThe summation in (8) is over future features, while in (7)\nwe have a sum over features already observed by the agent.\nWe can thus think of linear expected traces as _predecessor_\n_features_ . A similar connection was made in the tabular setting by Pitis (2018), relating source traces, which aim to\nestimate the source matrix ( _I −_ _γP_ ) _[−]_ [1], to successor representations (Dayan 1993). In a sense, the above generalises\nthis insight. In addition to being interesting in its own right,\nthis connection allows for an intriguing interpretation of _**z**_ ( _s_ )\nas a multidimensional value function. Like with successor\nfeatures, the features **x** _t_ play the role of rewards, discounted\nwith _γ · λ_ rather than _γ_, and with time flowing backwards.\nAlthough the predecessor interpretation only holds in the\nlinear case, it is also of interest as a means to obtain a practical\nimplementation of expected traces with non-linear function\napproximation, for instance applied only to the linear ‘head’\nof a deep neural network. We used this ‘predecessor feature\ntrick’ in our Atari experiments described earlier.\n\n\n**Relation to Model-Based Reinforcement Learning**\n\n\nModel-based reinforcement learning provides an alternative\napproach to efficient credit assignment. The general idea is\nto construct a model that estimates state-transition dynamics,\nand to update the value function based upon hypothetical\n\n\n10002\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-5-4.png)\n\ntransitions drawn from the model (Sutton 1990), for example\nby prioritised sweeping (Moore and Atkeson 1993; van Seijen\nand Sutton 2013). In practice, model-based approaches have\nproven challenging in environments (such as Atari games)\nwith rich perceptual observations, compared to model-free\napproaches that more directly update the agent’s policy and\npredictions (van Hasselt, Hessel, and Aslanides 2019).\nIn some sense, expected traces also construct a model of\nthe environment—but one that differs in several key regards\nfrom standard state-to-state models used in model-based reinforcement learning. First, expected traces estimate _past_\nquantities rather than _future_ quantities. Backward planning\n(e.g., Chelu, Precup, and van Hasselt 2020) is possible with\nexplicit transition models, but is less common in practice.\nSecond, expected traces estimate the accumulation of _gradi-_\n_ents_ over a multi-step trajectory, rather than trying to learn\nthe full transition dynamics, thereby focusing only on those\naspects that matter for the eventual weight update. Third, expected traces allow credit assignment across these potential\npast trajectories with a single update, without the iterative\ncomputation that is typically required when using a dynamics\nmodel. These differences may be important to side-step some\nof the challenges faced in model-based learning.\n\n\n**Batch Learning and Replay**\n\n\nWe have mainly considered the online learning setting in this\npaper. It is often convenient to learn from batches of data, or\nreplay transitions repeatedly, to enhance data efficiency. A\nnatural extension is replay the experiences sequentially (e.g.\nKapturowski et al. 2018), but perhaps alternatives exist. We\n\n\n\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-0.png)\n\n![](output/images/acda55ebdf39c6634e89a9730ff7d963471f2b0a.pdf-6-1.png)\n\nFigure 6: Performance of Q( _λ_ ) ( _η_ = 1, blue) and QET( _λ_ ) ( _η_ = 0, orange) on Pong and Ms.Pac-Man for various learning rates.\nShaded regions show standard error across 10 random seeds. All results are for _λ_ = 0 _._ 95. Further implementation details and\nhyper-parameters are in the appendix.\n\n\n\nnow discuss one potential extension.\nWe defined a mixed trace _**y**_ _t_ that mixes the instantaneous\nand expected traces. Optionally the expected trace _**z**_ _t_ can\nbe updated towards the mixed trace _**y**_ _t_ as well, instead of\ntowards the instantaneous trace _**e**_ _t_ . Analogously to TD( _λ_ ) we\npropose to then use at least one real step of data:\n\n\n∆ _**θ**_ _t ≡_ _β_ ( _**∇**_ _t_ + _γtλt_ _**y**_ _t−_ 1 _−_ _**zθ**_ ( _St_ )) _[⊤]_ _[∂]_ _**[z][θ]**_ [(] _[S][t]_ [)] _,_ (9)\n\n_∂_ _**θ**_\n\n\nwith _**∇**_ _t ≡∇_ **w** _v_ **w** ( _St_ ). This is akin to a forward-view _λ_ return update, with _∇_ **w** _v_ **w** ( _St_ ) in the role of (vector) reward,\nand _**zθ**_ of value, and discounted by _λtγt_, but reversed in time.\nIn other words, this can be considered a sampled Bellman\nequation (Bellman 1957) but backward in time.\nWhen we then choose _η_ = 0, then _**y**_ _t−_ 1 = _z_ _**θ**_ ( _St−_ 1), and\nthen the target in (9) only depends on a single transition.\nInterestingly, that means we can then learn expected traces\nfrom _individual_ transitions, sampled out of temporal order,\nfor instance in batch settings or when using replay.\n\n\n**Application to Other Traces**\n\n\nWe can apply the idea of expected trace to more traces than\nconsidered here. We can for instance consider the characteristic eligibility trace used in REINFORCE (Williams 1992)\nand related policy-gradient algorithms (Sutton et al. 2000).\nAnother appealing application is to the follow-on trace\nor _emphasis_, used in emphatic temporal difference learning\n(Sutton, Mahmood, and White 2016) and related algorithms\n(e.g., Imani, Graves, and White 2018). Emphatic TD was\nproposed to correct an important issue with off-policy learning, which can be unstable and lead to diverging learning\ndynamics. Emphatic TD weights updates according to 1) the\ninherent interest in having accurate predictions in that state\nand, 2) the importance of predictions in that state for updating\n\n\n10003\n\n\n\nother predictions. Emphatic TD uses scalar ‘follow-on’ traces\nto determine the ‘emphasis’ for each update. However, this\nfollow-on trace can have very high, even infinite, variance.\nInstead, we might estimate and use its expectation instead of\nthe instantaneous emphasis. A related idea was explored by\nZhang, Boehmer, and Whiteson (2019) to obtain off-policy\nactor critic algorithms.\n\n\n**Conclusion**\n\n\nWe have proposed a mechanism for efficient credit assignment, using the expectation of an eligibility trace. We have\ndemonstrated this can sometimes speed up credit assignment\ngreatly, and have analyzed concrete algorithms theoretically\nand empirically to increase understanding of the concept.\nExpected traces have several interpretations. First, we can\ninterpret the algorithm as counterfactually updating multiple possible trajectories leading up to the current state. Second, they can be understood as trading off bias and variance,\nwhich can be done smoothly via a unifying _η_ parameter, between standard eligibility traces (low bias, high variance) and\nestimated traces (possibly higher bias, but lower variance).\nFurthermore, with tabular or linear function approximation\nwe can interpret the resulting expected traces as predecessor\nstates or features—object analogous to successor states or features, but time-reversed. Finally, we can interpret the linear\nalgorithm as preconditioning the standard TD update, thereby\npotentially speeding up learning. These interpretations suggest that a variety of complementary ways to potentially\nextend these concepts and algorithms.\nWe have shown expected traces can already be used to\nenhance learning in non-linear settings (i.e., deep reinforcement learning), and in the control setting where we update\nthe policy. Further work is needed to determine the full extent\nof the possibilities of these new algorithms.\n\n\n\n\n**References**\n\n\nAmari, S. I. 1998. Natural gradient works efficiently in\nlearning. _Neural computation_ 10(2): 251–276. ISSN 08997667.\n\n\nBarreto, A.; Dabney, W.; Munos, R.; Hunt, J. J.; Schaul, T.;\nvan Hasselt, H. P.; and Silver, D. 2017. Successor features\nfor transfer in reinforcement learning. In _Advances in neural_\n_information processing systems_, 4055–4065.\n\n\nBellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M.\n2013. The Arcade Learning Environment: An Evaluation\nPlatform for General Agents. _J. Artif. Intell. Res. (JAIR)_ 47:\n253–279.\n\n\nBellman, R. 1957. _Dynamic Programming_ . Princeton University Press.\n\n\nBertsekas, D. P.; and Tsitsiklis, J. N. 1996. _Neuro-dynamic_\n_Programming_ . Athena Scientific, Belmont, MA.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018a. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBradbury, J.; Frostig, R.; Hawkins, P.; Johnson, M. J.; Leary,\nC.; Maclaurin, D.; and Wanderman-Milne, S. 2018b. JAX:\ncomposable transformations of Python+NumPy programs.\nURL http://github.com/google/jax.\n\n\nBrandfonbrener, D.; and Bruna, J. 2020. Geometric Insights\ninto the Convergence of Non-linear TD Learning. In _Interna-_\n_tional Conference on Learning Representations_ .\n\n\nChelu, V.; Precup, D.; and van Hasselt, H. P. 2020. Forethought and Hindsight in Credit Assignment. In Larochelle,\nH.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin, H.,\neds., _Advances in Neural Information Processing Systems_,\nvolume 33, 2270–2281.\n\n\nDayan, P. 1992. The convergence of TD( _λ_ ) for general\nlambda. _Machine Learning_ 8: 341–362.\n\n\nDayan, P. 1993. Improving generalization for temporal difference learning: The successor representation. _Neural Com-_\n_putation_ 5(4): 613–624.\n\n\nDuan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel,\nP. 2016. Benchmarking deep reinforcement learning for\ncontinuous control. In _International Conference on Machine_\n_Learning_, 1329–1338.\n\n\nElfwing, S.; Uchibe, E.; and Doya, K. 2018. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_ 107:\n3–11.\n\n\nHennigan, T.; Cai, T.; Norman, T.; and Babuschkin, I. 2020.\nHaiku: Sonnet for JAX. URL http://github.com/deepmind/\ndm-haiku.\n\n\nHessel, M.; Modayil, J.; van Hasselt, H. P.; Schaul, T.; Ostrovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and\nSilver, D. 2018. Rainbow: Combining Improvements in Deep\nReinforcement Learning. _AAAI_ .\n\n\n10004\n\n\n\nHorgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel,\nM.; van Hasselt, H. P.; and Silver, D. 2018. Distributed\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ .\n\n\nImani, E.; Graves, E.; and White, M. 2018. An Off-policy\nPolicy Gradient Theorem Using Emphatic Weightings. In\nBengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; CesaBianchi, N.; and Garnett, R., eds., _Advances in Neural Infor-_\n_mation Processing Systems 31_, 96–106. Curran Associates,\nInc. URL http://papers.nips.cc/paper/7295-an-off-policypolicy-gradient-theorem-using-emphatic-weightings.pdf.\n\n\nKaelbling, L. P.; Littman, M. L.; and Cassandra, A. R. 1995.\nPlanning and Acting in Partially Observable Stochastic Domains. Unpublished report.\n\n\nKapturowski, S.; Ostrovski, G.; Quan, J.; Munos, R.; and\nDabney, W. 2018. Recurrent experience replay in distributed\nreinforcement learning. In _International conference on learn-_\n_ing representations_ .\n\n\nKingma, D. P.; and Adam, J. B. 2015. A method for stochastic optimization. In _International Conference on Learning_\n_Representation_ .\n\n\nLin, L. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. _Machine learning_\n8(3): 293–321.\n\n\nMartens, J. 2016. _Second-order optimization for neural net-_\n_works_ . University of Toronto (Canada).\n\n\nMinsky, M. 1963. Steps Toward Artificial Intelligence.\nIn Feigenbaum, E.; and Feldman, J., eds., _Computers and_\n_Thought_, 406–450. McGraw-Hill, New York.\n\n\nMnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.;\nHarley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous Methods for Deep Reinforcement Learning. In\n_International Conference on Machine Learning_ .\n\n\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,\nA. K.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg,\nS.; and Hassabis, D. 2015. Human-level control through deep\nreinforcement learning. _Nature_ 518(7540): 529–533.\n\n\nMoore, A. W.; and Atkeson, C. G. 1993. Prioritized Sweeping: Reinforcement Learning with less Data and less Time.\n_Machine Learning_ 13: 103–130.\n\n\nOllivier, Y. 2018. Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies. _CoRR_\nabs/1805.00869.\n\n\nPeng, J. 1993. _Efficient dynamic programming-based learn-_\n_ing for control_ . Ph.D. thesis, Northeastern University.\n\n\nPeng, J.; and Williams, R. J. 1996. Incremental Multi-step\nQ-learning. _Machine Learning_ 22: 283–290.\n\n\nPitis, S. 2018. Source Traces for Temporal Difference Learning. In McIlraith, S. A.; and Weinberger, K. Q., eds., _Pro-_\n_ceedings of the Thirty-Second AAAI Conference on Artificial_\n_Intelligence_, 3952–3959. AAAI Press.\n\n\n\n\nPohlen, T.; Piot, B.; Hester, T.; Azar, M. G.; Horgan, D.;\nBudden, D.; Barth-Maron, G.; van Hasselt, H. P.; Quan, J.;\nVecerˇ ´ık, M.; Hessel, M.; Munos, R.; and Pietquin, O. 2018.\nObserve and look further: Achieving consistent performance\non Atari. _arXiv preprint arXiv:1805.11593_ .\n\n\nProkhorov, D. V.; and Wunsch, D. C. 1997. Adaptive critic\ndesigns. _IEEE Transactions on Neural Networks_ 8(5): 997–\n1007.\n\n\nPuterman, M. L. 1994. _Markov Decision Processes: Discrete_\n_Stochastic Dynamic Programming_ . John Wiley & Sons, Inc.\nNew York, NY, USA.\n\n\nRiedmiller, M. 2005. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning\nMethod. In Gama, J.; Camacho, R.; Brazdil, P.; Jorge, A.; and\nTorgo, L., eds., _Proceedings of the 16th European Conference_\n_on Machine Learning (ECML’05)_, 317–328. Springer.\n\n\nSchaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016.\nPrioritized Experience Replay. In _International Conference_\n_on Learning Representations_ . Puerto Rico.\n\n\nSilver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;\nVan Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;\nPanneershelvam, V.; Lanctot, M.; et al. 2016. Mastering\nthe game of Go with deep neural networks and tree search.\n_Nature_ 529(7587): 484–489.\n\n\nSingh, S. P.; and Sutton, R. S. 1996. Reinforcement Learning\nwith replacing eligibility traces. _Machine Learning_ 22: 123–\n158.\n\n\nSutton, R. S. 1984. _Temporal Credit Assignment in Reinforce-_\n_ment Learning_ . Ph.D. thesis, University of Massachusetts,\nDept. of Comp. and Inf. Sci.\n\n\nSutton, R. S. 1988. Learning to predict by the methods of\ntemporal differences. _Machine learning_ 3(1): 9–44.\n\n\nSutton, R. S. 1990. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In _Proceedings of the seventh international conference_\n_on machine learning_, 216–224.\n\n\nSutton, R. S.; and Barto, A. G. 2018. _Reinforcement Learning:_\n_An Introduction_ . The MIT press, Cambridge MA.\n\n\nSutton, R. S.; Mahmood, A. R.; and White, M. 2016. An\nEmphatic Approach to the Problem of Off-policy TemporalDifference Learning. _Journal of Machine Learning Research_\n17(73): 1–29.\n\n\nSutton, R. S.; McAllester, D.; Singh, S.; and Mansour, Y.\n2000. Policy gradient methods for reinforcement learning\nwith function approximation. _Advances in Neural Informa-_\n_tion Processing Systems 13 (NIPS-00)_ 12: 1057–1063.\n\n\nTesauro, G. 1992. Practical Issues in Temporal Difference\nLearning. In Lippman, D. S.; Moody, J. E.; and Touretzky,\nD. S., eds., _Advances in Neural Information Processing Sys-_\n_tems 4_, 259–266. San Mateo, CA: Morgan Kaufmann.\n\n\nTesauro, G. J. 1994. TD-Gammon, a self-teaching backgammon program, achieves master-level play. _Neural computa-_\n_tion_ 6(2): 215–219.\n\n\n10005\n\n\n\nTsitsiklis, J. N. 1994. Asynchronous stochastic approximation and Q-learning. _Machine Learning_ 16: 185–202.\n\nTsitsiklis, J. N.; and Van Roy, B. 1997. An analysis of\ntemporal-difference learning with function approximation.\n_IEEE Transactions on Automatic Control_ 42(5): 674–690.\n\nvan Hasselt, H. P. 2012. Reinforcement Learning in Continuous State and Action Spaces. In Wiering, M. A.; and\nvan Otterlo, M., eds., _Reinforcement Learning: State of the_\n_Art_, volume 12 of _Adaptation, Learning, and Optimization_,\n207–251. Springer.\n\n\nvan Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver,\nD. 2016. Learning values across many orders of magnitude. In _Advances in Neural Information Processing Systems_\n_29: Annual Conference on Neural Information Processing_\n_Systems 2016, December 5-10, 2016, Barcelona, Spain_, 4287–\n4295.\n\n\nvan Hasselt, H. P.; Guez, A.; and Silver, D. 2016. Deep reinforcement learning with double Q-Learning. In _Proceedings_\n_of the Thirtieth AAAI Conference on Artificial Intelligence_,\n2094–2100.\n\n\nvan Hasselt, H. P.; Hessel, M.; and Aslanides, J. 2019. When\nto use parametric models in reinforcement learning? In _Ad-_\n_vances in Neural Information Processing Systems_, volume 32,\n14322–14333.\n\nvan Hasselt, H. P.; Mahmood, A. R.; and Sutton, R. S. 2014.\nOff-policy TD( _λ_ ) with a true online equivalence. In _Pro-_\n_ceedings of the 30th Conference on Uncertainty in Artificial_\n_Intelligence_, 330–339.\n\nvan Hasselt, H. P.; Quan, J.; Hessel, M.; Xu, Z.; Borsa, D.;\nand Barreto, A. 2019. General non-linear Bellman equations.\n_arXiv preprint arXiv:1907.03687_ .\n\n\nvan Hasselt, H. P.; and Sutton, R. S. 2015. Learning to predict\nindependent of span. _CoRR_ abs/1508.04582.\n\nvan Seijen, H.; and Sutton, R. S. 2013. Planning by Prioritized Sweeping with Small Backups. In _International_\n_Conference on Machine Learning_, 361–369.\n\nvan Seijen, H.; and Sutton, R. S. 2014. True online TD( _λ_ ).\nIn _International Conference on Machine Learning_, 692–700.\n\n\nWang, Z.; de Freitas, N.; Schaul, T.; Hessel, M.; van Hasselt,\nH. P.; and Lanctot, M. 2016. Dueling Network Architectures for Deep Reinforcement Learning. In _International_\n_Conference on Machine Learning_ . New York, NY, USA.\n\nWerbos, P. J. 1990. A menu of designs for reinforcement\nlearning over time. _Neural networks for control_ 67–95.\n\nWhitehead, S. D.; and Ballard, D. H. 1991. Learning to\nperceive and act by trial and error. _Machine Learning_ 7(1):\n45–83.\n\nWilliams, R. J. 1992. Simple statistical gradient-following\nalgorithms for connectionist reinforcement learning. _Machine_\n_Learning_ 8: 229–256.\n\n\nZhang, S.; Boehmer, W.; and Whiteson, S. 2019. Generalized\noff-policy actor-critic. In _Advances in Neural Information_\n_Processing Systems_, 2001–2011.\n\n\n",
    "ranking": {
      "relevance_score": 0.7493067885948483,
      "citation_score": 0.6302697050551669,
      "recency_score": 0.3906854405837399,
      "final_score": 0.6896372370858013
    },
    "citation_key": "Hasselt2020ExpectedET",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "66d76444255be0ac378a0a93ee0379fc721a386f",
    "title": "On Q-learning Convergence for Non-Markov Decision Processes",
    "published": "2018-07-01",
    "authors": [
      "Sultan Javed Majeed",
      "Marcus Hutter"
    ],
    "summary": "Temporal-difference (TD) learning is an attractive, computationally efficient framework for model- free reinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent to learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has only been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state MDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the environment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning when applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We prove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in particular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a necessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.",
    "pdf_url": "https://doi.org/10.24963/ijcai.2018/353",
    "doi": "10.24963/ijcai.2018/353",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "International Joint Conference on Artificial Intelligence",
    "citation_count": 40,
    "bibtex": "@Article{Majeed2018OnQC,\n author = {Sultan Javed Majeed and Marcus Hutter},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {2546-2552},\n title = {On Q-learning Convergence for Non-Markov Decision Processes},\n year = {2018}\n}\n",
    "markdown_text": "[image]\n\n\nNavigation\n\n\n  - Home\n\n  - Conferences\n\n\nFuture Conferences\n\n  Past Conferences\n\n  \n\n  - Proceedings\n\n\n  - IJCAI 2025 Proceedings\n\n  - All Proceedings\n\n\n  - Awards\n\n  - Trustees/officers\n\n\nCurrent trustees\n\n  Trustees Elect\n\n  IJCAI Secretariat\n\n  \n  - IJCAI Sponsorship and Publicity Officers\nIJCAI Team\n\n  \n  - Local Arrangements Chairs\n\n  - Former Trustees serving on the Executive Committee\n\n  - Other Former Officers\n\n\n  - AI Journal\n\n  - About\n\n\nAbout IJCAI\n\n  Contact Information\n\n  \n# **On Q-learning Convergence for Non-Markov** **Decision Processes** **On Q-learning Convergence for Non-Markov** **Decision Processes**\n\n## **Sultan Javed Majeed, Marcus Hutter**\n\n\n[image]\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence\n[Main track. Pages 2546-2552. https://doi.org/10.24963/ijcai.2018/353](https://doi.org/10.24963/ijcai.2018/353)\n[PDF BibTeX](https://www.ijcai.org/proceedings/2018/0353.pdf)\n\n\nTemporal-difference (TD) learning is an attractive, computationally efficient framework for model- free\nreinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent\nto learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has\nonly been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state\nMDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the\nenvironment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning\nwhen applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We\nprove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in\nparticular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a\nnecessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.\nKeywords:\nMachine Learning: Online Learning\nMachine Learning: Reinforcement Learning\nPlanning and Scheduling: Markov Decisions Processes\n\n\nCopyright © 2025,\n\n\n",
    "ranking": {
      "relevance_score": 0.7639998734717994,
      "citation_score": 0.5680913780397937,
      "recency_score": 0.27592885552856466,
      "final_score": 0.6760110725910747
    },
    "citation_key": "Majeed2018OnQC",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "e457f7b24e8af833b15c802d31b91bd048b158e8",
    "title": "Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning",
    "published": "2023-01-26",
    "authors": [
      "Brett Daley",
      "Martha White",
      "Chris Amato",
      "Marlos C. Machado"
    ],
    "summary": "Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, but counteracting off-policy bias without exacerbating variance is challenging. Classically, off-policy bias is corrected in a per-decision manner: past temporal-difference errors are re-weighted by the instantaneous Importance Sampling (IS) ratio after each action via eligibility traces. Many off-policy algorithms rely on this mechanism, along with differing protocols for cutting the IS ratios to combat the variance of the IS estimator. Unfortunately, once a trace has been fully cut, the effect cannot be reversed. This has led to the development of credit-assignment strategies that account for multiple past experiences at a time. These trajectory-aware methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose a multistep operator that can express both per-decision and trajectory-aware methods. We prove convergence conditions for our operator in the tabular setting, establishing the first guarantees for several existing methods as well as many new ones. Finally, we introduce Recency-Bounded Importance Sampling (RBIS), which leverages trajectory awareness to perform robustly across $\\lambda$-values in an off-policy control task.",
    "pdf_url": "http://arxiv.org/pdf/2301.11321",
    "doi": "10.48550/arXiv.2301.11321",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "International Conference on Machine Learning",
    "citation_count": 3,
    "bibtex": "@Article{Daley2023TrajectoryAwareET,\n author = {Brett Daley and Martha White and Chris Amato and Marlos C. Machado},\n booktitle = {International Conference on Machine Learning},\n pages = {6818-6835},\n title = {Trajectory-Aware Eligibility Traces for Off-Policy Reinforcement Learning},\n year = {2023}\n}\n",
    "markdown_text": "## **Trajectory-Aware Eligibility Traces for** **Off-Policy Reinforcement Learning**\n\n**Brett Daley** [1 2] **Martha White** [1 2 3] **Christopher Amato** [4] **Marlos C. Machado** [1 2 3]\n\n\n\n**Abstract**\n\n\nOff-policy learning from multistep returns is crucial for sample-efficient reinforcement learning,\nbut counteracting off-policy bias without exacerbating variance is challenging. Classically, offpolicy bias is corrected in a _per-decision_ manner:\npast temporal-difference errors are re-weighted by\nthe instantaneous Importance Sampling (IS) ratio\nafter each action via eligibility traces. Many offpolicy algorithms rely on this mechanism, along\nwith differing protocols for _cutting_ the IS ratios\nto combat the variance of the IS estimator. Un\nfortunately, once a trace has been fully cut, the\neffect cannot be reversed. This has led to the\n\ndevelopment of credit-assignment strategies that\naccount for multiple past experiences at a time.\nThese _trajectory-aware_ methods have not been extensively analyzed, and their theoretical justification remains uncertain. In this paper, we propose\na multistep operator that can express both perdecision and trajectory-aware methods. We prove\nconvergence conditions for our operator in the\ntabular setting, establishing the first guarantees\nfor several existing methods as well as many new\nones. Finally, we introduce Recency-Bounded\nImportance Sampling (RBIS), which leverages\ntrajectory awareness to perform robustly across\n_λ_ -values in several off-policy control tasks.\n\n\n**1. Introduction**\n\n\nReinforcement learning concerns an agent interacting with\nits environment through trial and error to maximize its expected cumulative reward. One of the great challenges of re\n\n1Department of Computing Science, University of Alberta,\nEdmonton, AB, Canada [2] Alberta Machine Intelligence Institute\n3Canada CIFAR AI Chair 4Khoury College of Computer Sciences,\nNortheastern University, Boston, MA, USA. Correspondence to:\nBrett Daley _<_ brett.daley@ualberta.ca _>_ .\n\n\n_Proceedings of the 40_ _[th]_ _International Conference on Machine_\n_Learning_, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n\n\n\ninforcement learning is the temporal credit assignment problem (Sutton, 1984): upon receiving a reward, which past actions should be held responsible and, hence, be reinforced?\nBasic temporal-difference (TD) methods assign credit to the\nimmediately taken action (e.g., Watkins, 1989; Rummery &\nNiranjan, 1994), bootstrapping from previous experience to\nlearn long-term dependencies. This process requires a large\nnumber of repetitions to generate effective behaviors from\nrewards, motivating research into _multistep_ return estimation\nin which credit is distributed among multiple past actions\naccording to some eligibility rule (e.g., Sutton, 1988).\n\n\nOne challenge of multistep estimators is that they generally\nhave higher variance than 1-step estimators (Kearns &\nSingh, 2000). This is exacerbated in the off-policy setting,\nwhere environment interaction is conducted according to a\nbehavior policy that differs from the target policy for which\nreturns are being estimated. The discrepancy between the\ntwo policies manifests mathematically as bias in the return\nestimation, which can be detrimental to learning if left\nunaddressed (Precup et al., 2000). Despite these challenges,\noff-policy learning is important for exploration and sample\nefficiency. The canonical bias-correction technique is Importance Sampling (IS; Kahn & Harris, 1951), wherein the\nbias due to the differing policies is eliminated by the product\nof their probability ratios (Precup et al., 2000). Although\nIS theoretically resolves the off-policy bias, it can suffer\nfrom extreme variance that makes it largely impractical.\n\n\nDirectly managing the variance of the IS estimator has been\na fruitful avenue for developing efficient off-policy algorithms. Past work has focused on modifying the individual\nIS ratios to reduce the variance of the full update: e.g., Tree\nBackup (Precup et al., 2000), Q _[π]_ ( _λ_ ) (Harutyunyan et al.,\n2016), Retrace (Munos et al., 2016), ABQ (Mahmood et al.,\n2017), and C-trace (Rowland et al., 2020). All of these\nmethods can be implemented online with _per-decision_ rules\n(Precup et al., 2000) that determine how much to reduce,\nor _cut_, the IS ratio according to the current state-action\npair. The re-weighted TD error is then broadcast to previous experiences using eligibility traces (Barto et al., 1983;\nSutton, 1984). The decisions made by these algorithms are\nMarkov in the sense that each iterative off-policy correction\ndepends on only the current state-action pair. One issue\n\n\n\n1\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\nwith this is that it can lead to suboptimal decisions, since\nfully cutting a trace cannot be reversed later. In contrast, a\n_trajectory-aware_ method can examine an entire sequence\nof past state-action pairs to make globally better decisions\nregarding credit assignment; for example, when a specific\ntransition yields a high IS ratio, a trajectory-aware method\ncan choose to not cut the trace if the product of all previous\nIS ratios remains small.\n\n\nIndeed, some existing off-policy methods already conduct\noffline bias correction in a trajectory-aware manner. Perhaps\nthe simplest example is Truncated IS, where the IS ratio\nproducts are pre-calculated offline and then clipped to\nsome finite value (see Section 4). More recently, Munos\net al. (2016) suggested a recursive variant of Retrace that\nautomatically relaxes the clipping bound when its historical\ntrace magnitude becomes small; the authors conjectured that\nthis could lead to faster learning. No theoretical analysis\nhas been conducted on trajectory-aware algorithms such\nas these; their convergence properties are unknown, and the\nspace of possible algorithms has not yet been fully explored.\n\n\nTo better understand these algorithms, and to support new\ndiscoveries of efficient algorithms, we introduce a unifying\ntheoretical perspective on per-decision and trajectory-aware\noff-policy corrections. We propose a multistep operator that\naccounts for arbitrary dependencies on past experiences,\nsignificantly generalizing the per-decision _R_ operator introduced by Munos et al. (2016). We prove that our operator\nconverges for policy evaluation and control. In the latter\ncase, we remove the assumptions of increasingly greedy\npolicies and pessimistic initialization used by Munos et al.\n(2016), which has implications for per-decision methods.\nFinally, we derive a new method from our theory, RecencyBounded Importance Sampling (RBIS), which performs\nfavorably to other trajectory-aware methods across a wide\nrange of _λ_ -values in an off-policy control task.\n\n\n**2. Preliminaries**\n\n\nWe consider Markov Decision Processes (MDPs) of the\nform ( _S, A, P, R, γ_ ). _S_ and _A_ are finite sets of states and\nactions, respectively. Letting ∆ _X_ denote the set of distributions over a set _X_, then _P_ : _S × A →_ ∆ _S_ is the transition function, _R_ : _S × A →_ R is the reward function, and\n_γ ∈_ [0 _,_ 1) is the discount factor. A policy _π_ : _S →_ ∆ _A_\ndetermines an agent’s probability of selecting a given action\nin each state. A value function _Q_ : _S × A →_ R represents\nthe agent’s estimate of the expected return achievable from\neach state-action pair. For a policy _π_, we define the operator\n\n\n\nerators such as _Pπ_ can hence be interpreted as _n × n_ square\nmatrices that multiply these vectors, with repeated application corresponding to exponentiation: _Pπ_ _[t]_ _[Q]_ [ =] _[ P][π]_ [(] _[P]_ _π_ _[ t][−]_ [1] _Q_ ).\n\n\nIn the _policy evaluation_ setting, we seek to estimate the\nexpected discounted returns for policy _π_, given by _Q_ _[π]_ :=\n� _∞t_ =0 _[γ][t][P]_ _π_ _[ t]_ _[R]_ [. The value function] _[ Q][π]_ [ is the unique fixed]\npoint of the Bellman operator _TπQ_ := _R_ + _γPπQ_, i.e., it\nuniquely solves the Bellman equation _TπQ_ _[π]_ = _Q_ _[π]_ (Bellman, 1966). In the _control_ setting, we seek to estimate the\nexpected returns _Q_ _[∗]_ under the optimal policy _π_ _[∗]_ . _Q_ _[∗]_ is\nthe unique fixed point of the Bellman optimality operator\n( _TQ_ )( _s, a_ ) := max _π_ ( _TπQ_ )( _s, a_ ), i.e., it uniquely solves\n\n=\nthe Bellman optimality equation _TQ_ _[∗]_ _Q_ _[∗]_ . We are particularly interested in the _off-policy_ learning case, where trajectories of the form ( _S_ 0 _, A_ 0) _,_ ( _S_ 1 _, A_ 1) _,_ ( _S_ 2 _, A_ 2) _, . . ._ are\ngenerated by interacting with the MDP using a behavior\npolicy _µ_, where _µ ̸_ = _π_ . We define the TD error for policy _π_\nat time _t_ as\n\n\n_δt_ _[π]_ [:=] _[ R][t]_ [+] _[ γ]_ � _π_ ( _a_ _[′]_ _|St_ +1) _Q_ ( _St_ +1 _, a_ _[′]_ ) _−_ _Q_ ( _St, At_ ) _,_\n\n_a_ _[′]_ _∈A_\n\n\nwhere _Rt_ := _R_ ( _St, At_ ). Let _ρk_ := _[π]_ _µ_ ( [(] _A_ _[A]_ _k_ _[k]_ _|_ _[|]_ _S_ _[S]_ _k_ _[k]_ ) [)] [for brevity.]\n\nMunos et al. (2016) introduced the off-policy operator\n\n\n( _RQ_ )( _s, a_ ) := _Q_ ( _s, a_ ) +\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\nE _µ_\n\n\n\n_∞_ _t_\n� _γ_ _[t]_ �\n\n_t_ =0 � _k_ =1\n\n\n\n_ck_\n\n_k_ =1\n\n\n\n_δt_ _[π]_\n\n�\n\n\n\n_,_ (1)\n\n\n\n( _PπQ_ )( _s, a_ ) := �\n\n_s_ _[′]_ _∈S_\n\n\n\n� _P_ ( _s_ _[′]_ _|s, a_ ) _π_ ( _a_ _[′]_ _|s_ _[′]_ ) _Q_ ( _s_ _[′]_ _, a_ _[′]_ ) _._\n\n_a_ _[′]_ _∈A_\n\n\n\nAs a shorthand, we represent value functions and the reward\nfunction as vectors in R _[n]_, where _n_ = _|S × A|_ . Linear op\n\n\nwhere _ck_ := _c_ ( _Sk, Ak_ ) _∈_ [0 _, ρk_ ]. We refer to the\nproduct [�] _[t]_ _k_ =1 _[c][k]_ [ as the] _[ trace]_ [ for][ (] _[s, a]_ [)][ at time] _[ t]_ [.] If\nany _ck < ρk_, we say that the trace has been (partially)\ncut. If any _ck_ = 0, then we have fully cut it. If\nthe trace is fully cut at _t_ = 1, i.e., _c_ 1 = 0, then\n( _RQ_ )( _s, a_ ) = _Q_ ( _s, a_ ) + E[ _δ_ 0 _[π]_ _[|]_ [ (] _[S]_ [0] _[, A]_ [0][) = (] _[s, a]_ [)] =]\n_R_ ( _s, a_ )+ _γ_ E[ [�] _a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S]_ [1][)] _[Q]_ [(] _[S]_ [1] _[, a][′]_ [)] _[|]_ [(] _[S]_ [0] _[, A]_ [0][) = (] _[s, a]_ [)]][,]\n\nwhich is the standard 1-step bootstrap target like in TD(0)\n(Sutton, 1988). Notice that each _ck_ is Markov, as it depends\nonly on ( _Sk, Ak_ ) and is otherwise independent of the preceding trajectory. In other words, the update for _R_ can be\ncalculated _per decision_ (Precup et al., 2000), permitting an\nefficient online implementation with eligibility traces.\n\n\n**3. Trajectory-Aware Eligibility Traces**\n\n\nWhile per-decision traces are convenient from a computational perspective, they require making choices about how\nmuch to cut the trace without considering the effects of\nprevious choices. This can lead to suboptimal decisions;\nfor example, if the trace is cut by setting _ck_ = 0 at some\ntimestep, then the effect cannot be reversed later. Regardless\nof whatever new experiences are encountered by the agent,\nexperiences before time _k_ will be ineligible for credit assignment, resulting in an opportunity cost. In fact, this exact\n\n\n\n2\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\ndecision trace cutting can lead to excessively small eligibilities, especially when _ϵ_ is close to 0.\n\n\n\n_Figure 1._ The Tightrope Problem. Starting from state _s_ 1, the agent\nmust take a specific sequence of _n_ actions to receive +1 reward.\n\n\nphenomenon is why Watkins’ Q( _λ_ ) (Watkins, 1989) often\nlearns more slowly than Peng’s Q( _λ_ ) (Peng & Williams,\n1996), even though the former avoids off-policy bias (Sutton\n& Barto, 1998; Daley & Amato, 2019; Kozuno et al., 2021).\nThe same effect (but to a lesser extent) impacts Tree Backup\nand Retrace, where _ck ≤_ 1 always in Eq. (1), implying that\nthe traces for past experiences can never increase.\n\n\nWe illustrate this phenomenon in a small, deterministic MDP\nthat we call the Tightrope Problem (see Figure 1). The environment consists of _n_ sequential, non-terminal states with\ntwo actions _a_ 1 _, a_ 2 available. The agent starts in state _s_ 1 and\nadvances from _si_ to _si_ +1 whenever it takes action _a_ 1. If\n_i_ = _n_, then the episode terminates and the agent receives\n+1 reward. Taking action _a_ 2 in any state immediately terminates the episode with no reward. Clearly, the optimal\npolicy is to execute _a_ 1 regardless of the state.\n\n\nNow consider the following off-policy learning scenario.\nSuppose the agent’s behavior policy _µ_ is uniform random,\nbut the target policy _π_ is _ϵ_ -greedy with respect to a value\nfunction _Q_ . For each state _s_, it follows that _π_ ( _a|s_ ) = 1 _−_ _ϵ_\nif _a_ = argmax _a′ Q_ ( _s, a_ _[′]_ ) and _π_ ( _a|s_ ) = _ϵ_ otherwise. We\nassume _ϵ_ is small in the sense that _ϵ <_ [1]\n\n2 [, and that] _[ γ]_ [ = 1][.]\nSuppose now that the agent successfully receives the +1\nreward during an episode, implying that it took action _a_ 1\non every timestep. We can compute the eligibility of the\ninitial state-action pair ( _s_ 1 _, a_ 1) as an expression in the number _k_ of incorrect actions in the greedy policy (i.e., where\nargmax _a′ Q_ ( _s, a_ _[′]_ ) _̸_ = _a_ 1). Letting _λ ∈_ [0 _,_ 1] be a decay\nparameter, the standard IS estimator (which does not cut\ntraces when _λ_ = 1) provides an eligibility of\n\n\n\nThis issue stems from the fact that Retrace is not _aware_ of\n\nits past eligibilities, and continues to decay them even when\nthey already form an underestimate compared to IS. This\nissue is not unique to Retrace, and affects other per-decision\nmethods like Tree Backup. Instead, a _trajectory-aware_\nmethod that can actively adapt its trace-cutting behavior\nbased on the magnitude of past eligibilities would be better.\n\n\nOne way to obtain a trajectory-aware method is to compute\nthe exact IS product in Eq. (2), and then make adjustments\nto it to achieve certain properties (e.g., convergence and variance reduction). For example, Truncated IS (see Section 4)\nsimply imposes a fixed bound on the IS estimator:\n\n\n_λ_ _[n]_ min�1 _,_ [2(1 _−_ _ϵ_ )] _[n][−][k]_ (2 _ϵ_ ) _[k]_ [�] _._ (3)\n\n\nIgnoring _λ_, Truncated IS reduces the eligibility only when it\nexceeds a pre-specified threshold, effectively avoiding trace\ncuts when the true IS estimate is small. In Section 6, we\npropose an algorithm, RBIS, which achieves a similar effect\nusing a recursive, time-decaying threshold.\n\n\nAs this example demonstrates, it can be advantageous to\nconsider the agent’s past experiences to produce better decisions regarding credit assignment. One of our principal\ncontributions is the proposal and analysis of an off-policy\noperator _M_ that encompasses this possibility. Let _Ft_ :=\n( _S_ 0 _, A_ 0) _,_ ( _S_ 1 _, A_ 1) _, . . .,_ ( _St, At_ ). We define _M_ such that\n\n\n( _MQ_ )( _s, a_ ) := _Q_ ( _s, a_ ) +\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\nE _µ_\n\n\n\n� _γ_ _[t]_ _βtδt_ _[π]_\n\n\n_t_ =0\n\n\n\n_,_ (4)\n\n\n\n_k_\n= _λ_ _[n]_ [2(1 _−_ _ϵ_ )] _[n][−][k]_ (2 _ϵ_ ) _[k]_ _._ (2)\n�\n\n\n\nwhere _βt_ := _β_ ( _Ft_ ) is a trace that generally depends on\nthe history _Ft_ . We define _β_ 0 := 1 to ensure that the first\nTD error, _δ_ 0 _[π]_ [, is applied. In Section][ 5][, we characterize the]\nvalues of _βt_ for _t ≥_ 1 that lead to convergence.\n\n\nThe major analytical challenge of _M_ —and its main\nnovelty—is the complex dependence on the sequence _Ft_ .\nThis makes the operator difficult to analyze mathematically,\nas the terms in the series 1+ _γβ_ 1+ _γ_ [2] _β_ 2+ _· · ·_ generally share\nno common factors that would allow a recursive formula for\n\neligibility traces. Some off-policy methods, however, cannot\nbe described by factored traces, and therefore removing this\nassumption is necessary to understand existing algorithms\n(see Section 4), while also paving the way for new creditassignment methods. In the special case where _βt_ does\nfactor into Markov coefficients, i.e., _βt_ = [�] _[t]_ _k_ =1 _[c][k]_ [, then]\nEq. (4) reduces to Eq. (1), taking us back to the per-decision\nsetting studied by Munos et al. (2016). _M, therefore, unifies_\n_per-decision and trajectory-aware methods._\n\n\n\n_λ_ [1] _[ −]_ _[ϵ]_\n� 1 _/_ 2\n\n\n\n_n−k_\n_ϵ_\n_λ_\n� � 1 _/_ 2\n\n\n\nThis value can be greater than 1 when _k ≪_ _n_, which suggests that the agent’s behavior should be heavily reinforced\nwhen the greedy policy agrees closely with the optimal policy; however, a per-decision method like Retrace, which cuts\ntraces without considering the full trajectory (see Section 4),\nultimately assigns a much lower eligibility:\n\n\n\n_n−k_\n_ϵ_\n�� � _λ_ min�1 _,_ 1 _/_ 2\n\n\n\n� _λ_ min�1 _,_ [1] 1 _[ −]_ _/_ 2 _[ϵ]_\n\n\n\n1 _/_ 2\n\n\n\n_k_\n= _λ_ _[n]_ (2 _ϵ_ ) _[k]_ _._\n��\n\n\n\nThe eligibility now decays monotonically for every suboptimal action in the greedy policy, illuminating how per\n\n\n3\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\n**4. Unifying Off-Policy Algorithms**\n\n\nThe operator _M_ is a strict generalization of the previous\noperator considered for trace-based methods, allowing us to\nexpress existing algorithms in this form. We provide a nonexhaustive list of examples below with the corresponding\n_βt_ used in _M_ . For brevity, let Π _t_ := [�] _[t]_ _k_ =1 _[ρ][k]_ [.]\n\n\n**Importance Sampling:** _βt_ = _λ_ _[t]_ Π _t_ (Kahn & Harris, 1951).\nThe standard approach for correcting off-policy bias. Although it is the only unbiased estimator in this list (if _λ_ = 1),\nit suffers from high variance, making it difficult to utilize.\n\n\n**Q** _[π]_ **(** _λ_ **):** _βt_ = _λ_ _[t]_ (Harutyunyan et al., 2016). A straightforward algorithm that decays the TD errors by a fixed constant.\nThe algorithm does not require explicitly knowing _µ_, which\nis desirable, but can diverge if _π_ and _µ_ differ too much\n(Harutyunyan et al., 2016, Theorem 1).\n\n\n**Tree Backup:** _βt_ = [�] _[t]_ _k_ =1 _[λπ]_ [(] _[A][k][|][S][k]_ [)][ (][Precup et al.][,][ 2000][).]\nA method that automatically cuts traces according to the\nproduct of probabilities under _π_, which forms a conservative\nlower bound on the IS estimate. Tree Backup converges for\nany behavior policy _µ_, but it is not efficient since traces are\ncut excessively—especially in the on-policy case.\n\n\n**Retrace:** _βt_ = [�] _[t]_ _k_ =1 _[λ]_ [ min(1] _[, ρ][k]_ [)][ (][Munos et al.][,][ 2016][).]\nA convergent algorithm for arbitrary policies _π_ and _µ_ that\nremains efficient in the on-policy case because it does not cut\ntraces (if _λ_ = 1); however, the fact that _βt_ never increases\ncan cause the trace products to decay too quickly in practice\n(Mahmood et al., 2017; Rowland et al., 2020).\n\n\nAll of the above can be analyzed using a per-decision operator. The next two, on the other hand, have weightings based\non the entire trajectory. We use the theory for our general\n_M_ operator to prove properties about these methods.\n\n\n**Recursive Retrace:** _βt_ = _λ_ min(1 _, βt−_ 1 _ρt_ ) (Munos et al.,\n2016). A modification to Retrace conjectured to lead to\nfaster learning. It clips large products of ratios, rather than\nindividual ratios. Its convergence for control is an open\nquestion, which we solve in Section 5.\n\n\n**Truncated Importance Sampling:** _βt_ = _λ_ _[t]_ min(1 _,_ Π _t_ )\n(Ionides, 2008). A simple but effective method to combat\nthe variance of IS. Variations of this algorithm have been applied in the reinforcement learning literature (e.g., Uchibe &\nDoya, 2004; Wawrzynski & Pacut´, 2007; Wawrzynski´, 2009;\nWang et al., 2017), but, to our knowledge, its convergence\nin an MDP setting has not been studied. In Section 5.3, we\nshow that it can diverge in at least one off-policy problem.\n\n\n**5. Convergence Analysis**\n\n\nIn this section, we study the convergence properties of the\n_M_ operator for policy evaluation and control. It will be\nconvenient to re-express Eq. (4) in vector notation for our\n\n\n\n_Bt_ is a linear operator and hence can be represented as a\nmatrix in R _[n][×][n]_, the elements of which are nonnegative.\nEach element of _Bt_, row-indexed by ( _s, a_ ) and columnindexed by ( _s_ _[′]_ _, a_ _[′]_ ), has the form\n\n\n_Bt_ (( _s, a_ ) _,_ ( _s_ _[′]_ _, a_ _[′]_ ))=Pr _µ_ [((] _[S][t][, A][t]_ [)=(] _[s][′][, a][′]_ [)] _[|]_ [(] _[S]_ [0] _[, A]_ [0][)=(] _[s, a]_ [))]\n\n\n_′_ _′_\n_×_ E _µ_ � _βt_ �� ( _S_ 0 _, A_ 0)=( _s, a_ ) _,_ ( _St, At_ )=( _s_ _, a_ )� _._ (8)\n\n\nWe justify this form in Appendix A. Note that _B_ 0 = _I_, the\nidentity matrix, because of our earlier definition of _β_ 0 := 1.\nIn the following sections, all inequalities involving vectors\nor matrices should be interpreted element wise. We let\n_∥X∥_ := _∥X∥∞_ for a matrix (or vector) _X_, which corresponds to the maximum absolute row sum of _X_ . We also\ndefine **1** _∈_ R _[n]_ to be the vector of ones, such that _X_ **1** gives\nthe row sums of _X_ .\n\n\n**5.1. Convergence for Policy Evaluation**\n\n\nWe start in the off-policy policy evaluation setting. Specifically, our goal is to prove that the repeated application of\nthe _M_ operator to an arbitrarily initialized vector _Q ∈_ R _[n]_\n\nconverges to _Q_ _[π]_ .\n\n\n**Condition 5.1.** _βt ≤_ _βt−_ 1 _ρt, ∀Ft, ∀_ _t ≥_ 1 _._\n\n\n**Theorem 5.2.** _If Condition 5.1 holds, then M is a con-_\n_traction mapping with Q_ _[π]_ _as its unique fixed point. Conse-_\n_quently,_ lim _i→∞_ _M_ _[i]_ _Q_ = _Q_ _[π]_ _, ∀_ _Q ∈_ R _[n]_ _._\n\n\n_Proof._ In Lemma B.1 (Appendix B.1), we show that _Q_ _[π]_ is\na fixed point of _M_ and that\n\n\n_MQ −_ _Q_ _[π]_ = _Z_ ( _Q −_ _Q_ _[π]_ ) _,_ (9)\n\n\nwhere _Z_ := [�] _[∞]_ _t_ =1 _[γ][t]_ [(] _[B][t][−]_ [1] _[P][π][ −]_ _[B][t]_ [)][. In Lemma][ B.2][ (Ap-]\npendix B.2), we also show that _Z ≥_ 0 and _Z_ **1** _≤_ _γ_ using the\nassumption that _βt ≤_ _βt−_ 1 _ρt_, _∀Ft_, _∀_ _t ≥_ 1 (Condition 5.1).\nConsequently, _Z_ ( _Q −_ _Q_ _[π]_ ) is a vector whose components\n\n\n\nanalysis. To do this, let us first bring the expectation inside\nthe sum, by linearity of expectation:\n\n\n( _MQ_ )( _s, a_ ) = _Q_ ( _s, a_ ) +\n\n\n_∞_\n� _γ_ _[t]_ E _µ_ � _βtδt_ _[π]_ �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _._ (5)\n\n\n_t_ =0\n\n\nTo write Eq. (5) in vector form, we define an operator _Bt_\nsuch that, for an arbitrary vector _X_ in R _[n]_,\n\n\n( _BtX_ )( _s, a_ ):= E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _,_ (6)\n\n\nallowing us to express the _M_ operator as\n\n\n\n_MQ_ = _Q_ +\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _Q_ ) _._ (7)\n\n\n_t_ =0\n\n\n\n4\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\neach comprise a nonnegative-weighted combination of the\ncomponents of _Q −_ _Q_ _[π]_, where the weights add up to at\nmost _γ_ . This means _∥MQ −_ _Q_ _[π]_ _∥≤_ _γ∥Q −_ _Q_ _[π]_ _∥_, and _M_\nis a contraction mapping. Its fixed point, _Q_ _[π]_, must therefore\nbe unique by the Banach fixed-point theorem, implying that\nlim _i→∞_ _M_ _[i]_ _Q_ = _Q_ _[π]_ for every _Q ∈_ R _[n]_ when _γ <_ 1.\n\n\nGiven that the ratio _βt_\n_βt−_ 1 [is bounded by] _[ ρ][t]_ [ (Condition][ 5.1][),]\nthe _M_ operator converges to _Q_ _[π]_ . Intuitively, we can think\nof this ratio as the _effective_ per-decision factor at time _t_ ;\nconvergence is guaranteed whenever this factor is no greater\nthan _ρt_, analogous to the convergence result for the _R_ operator (Munos et al., 2016, Theorem 1). Our theorem implies\nthe existence of a space of convergent trajectory-aware\nalgorithms, because each trace _βt_ can be chosen arbitrarily\nso long as it always satisfies the bound on this ratio.\n\n\n**5.2. Convergence for Control**\n\n\nWe now consider the more challenging setting of control.\nGiven sequences of target policies ( _πi_ ) _i≥_ 0 and behavior\npolicies ( _µi_ ) _i≥_ 0, we aim to show that the sequence of value\nfunctions ( _Qi_ ) _i≥_ 0 given by _Qi_ +1 := _MiQi_ converges to\n_Q_ _[∗]_ . Here, _Mi_ is the _M_ operator defined for _πi_ and _µi_ .\n\n\nCompared to the convergence proof of the _R_ operator\n(Munos et al., 2016, Theorem 2), the main novelty of our\nproof is the fact that the traces under _M_ are not Markov.\nConsequently, we require new techniques to establish\nbounds on _Q −_ _Q_ _[∗]_, since Eq. (4) is not representable as\nan infinite geometric series and so the summation does not\nhave a closed-form expression. We additionally relax two\nassumptions in the previous work, on initialization of the\nvalue function and on increasing greediness of the policy.\nWe require only that the target policies become _greedy in_\n_the limit_ . We say that a sequence of policies is greedy in the\nlimit if _TπiQi →_ _TQi_ as _i →∞_ . We discuss the significance of these relaxations to the assumptions in Section 5.4.\n\n\nFirst, let _Ci_ := [�] _[∞]_ _t_ =0 _[γ][t][B][t]_ [ for the policies] _[ π][i]_ [ and] _[ µ][i]_ [, and]\nwrite the _M_ operator at iteration _i_ as\n\n\n_MiQ_ = _Q_ + _Ci_ ( _TπiQ −_ _Q_ ) _._ (10)\n\n\nWe now present our convergence theorem for control.\n\n\n**Theorem 5.3.** _Consider a sequence of target policies_\n( _πi_ ) _i≥_ 0 _and a sequence of arbitrary behavior policies_\n( _µi_ ) _i≥_ 0 _. Let Q_ 0 _be an arbitrary vector in_ R _[n]_ _and define_\n_the sequence Qi_ +1 := _MiQi, where Mi is the operator_\n_defined by Eq._ (10) _. Assume that_ ( _πi_ ) _i≥_ 0 _is greedy in the_\n_limit, and let ϵi ≥_ 0 _be the smallest constant such that_\n_TπiQi ≥_ _TQi −_ _ϵi∥Qi∥_ **1** _. If Condition 5.1 holds for all i,_\n_then_\n\n\n_ϵi_\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥_ + (11)\n1 _−_ _γ_ _[∥][Q][i][∥][,]_\n\n\n_and, consequently,_ lim\n_i→∞_ _[Q][i]_ [ =] _[ Q][∗][.]_\n\n\n\n_Proof (sketch; full proof in Appendix B.3)._ We define matrices _Zi_ and _Zi_ _[∗]_ [, which correspond to] _[ Z]_ [ in Eq. (][9][) for]\ntarget policies _πi_ and _π_ _[∗]_, respectively, and behavior policy\n_µi_ . We then derive the inequalities\n\n\n_Zi_ _[∗]_ [(] _[Q][i]_ _[−][Q][∗]_ [)] _[−][ϵ][i][∥][Q][i][∥][C][i]_ **[1]** _[ ≤M][i][Q][i]_ _[−][Q][∗]_ _[≤]_ _[Z][i]_ [(] _[Q][i]_ _[−][Q][∗]_ [)] _[,]_\n\n\nwhich together imply Eq. (11). Thus, _Mi_ is nearly a contraction mapping with _Q_ _[∗]_ as its unique fixed point, excepting\nthe influence of the _O_ ( _∥Qi∥_ ) term. However, the greedyin-the-limit target policies guarantee that _ϵi →_ 0. Showing that _∥Qi∥_ remains finite completes the proof because\n_∥Qi −_ _Q_ _[∗]_ _∥→_ 0 must follow.\n\n\nThe convergence criteria for _βt_ (Condition 5.1) is the same\nfor both policy evaluation and control. In fact, the only\nadditional assumption we need for control is the greedy-inthe-limit target policies. Crucially, the proof allows arbitrary\nbehavior policies and an arbitrary value function initialization _Q_ 0, which we further discuss in Section 5.4.\n\n\n**5.3. Examples of Convergence and Divergence**\n\n\nThe generality of the _M_ operator means that it provides\nconvergence guarantees for a number of credit-assignment\nmethods that we did not discuss in Section 4. These include\n\nvariable or past-dependent _λ_ -values (e.g., Watkins, 1989;\nSingh & Sutton, 1996; Yu et al., 2018). All of these can be\nrepresented in a common form and shown to satisfy Condition 5.1; convergence for policy evaluation and control for\nthe instantiated trajectory-aware operator follows as a corollary, since Condition 5.1 is sufficient to apply Theorems 5.2\nand 5.3.\n\n\n**Proposition 5.4.** _Any traces expressible in the form_\n_βt_ = [�] _[t]_ _k_ =1 _[λ]_ [(] _[F][k]_ [)] _[ρ][k][,][ λ]_ [(] _[F][k]_ [)] _[ ∈]_ [[0] _[,]_ [ 1]] _[, satisfy Condition][ 5.1][.]_\n\n\n_Proof. βt_ = _βt−_ 1 _λ_ ( _Ft_ ) _ρt ≤_ _βt−_ 1 _ρt_ .\n\n\nIn Section 4, we also discussed two existing trajectory-aware\nmethods whose convergence is unknown. We show that\nRecursive Retrace satisfies our required condition.\n\n\n**Proposition 5.5.** _Recursive Retrace satisfies Condition 5.1._\n\n\n_Proof._ For Recursive Retrace, _βt_ = _ctβt−_ 1, where _ct_ =\n_λ_ min� _βt_ 1 _−_ 1 _[, ρ][t]_ � (Munos et al., 2016, Eq. 9). This means\n\n\n\n1\n_βt_ = _λ_ min _, ρt_\n� _βt−_ 1\n\n\n\n_βt−_ 1\n�\n\n\n\n= _λ_ min (1 _, βt−_ 1 _ρt_ )\n\n\n_≤_ _βt−_ 1 _ρt,_ (12)\n\n\nwhich is the bound required by Condition 5.1.\n\n\nUnfortunately, the traces for Truncated IS do not always\nsatisfy the required bound.\n\n\n**Proposition 5.6.** _Truncated IS may violate Condition 5.1._\n\n\n\n5\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\n_Proof._ We show this by providing a counterexample. Recall\nthat Truncated IS has _βt_ = _λ_ _[t]_ min(1 _,_ Π _t_ ). Assume a trajectory _Ft_ such that Π _t−_ 1 = 2 and [1] 2 _[< ρ][t][ < λ]_ [. (It is straight-]\n\nforward to define an MDP, behavior policy, and target policy\nto create such a trajectory.) Because _ρt >_ 1 _/_ Π _t−_ 1, then\nΠ _t_ = Π _t−_ 1 _ρt >_ 1. Thus, _βt_ = _λ_ _[t]_ and _βt−_ 1 = _λ_ _[t][−]_ [1], and\nCondition 5.1 is violated because _βt_\n_βt−_ 1 [=] _[ λ][ ̸≤]_ _[ρ][t]_ [.]\n\n\nBecause Theorems 5.2 and 5.3 cannot be applied, the precise\nconditions under which Truncated IS converges remains an\nopen problem. We do know Condition 5.1 is sufficient for\nconvergence, but it is unlikely to be strictly necessary. This\nis because our proofs of Theorems 5.2 and 5.3 use this\nassumption to guarantee that the matrix _Z_ in Eq. (9) has\nnonnegative elements, making it straightforward to show\nthat its row sums are sufficiently bounded to guarantee that\n_M_ is a contraction mapping. However, _M_ could remain a\ncontraction mapping even when _Z_ has negative elements,\nso long as _∥Z∥_ _<_ 1. It could theoretically be the case for\nTruncated IS that _Z_ occasionally contains negative elements\nbut _∥Z∥_ is still bounded enough to permit convergence.\n\n\nNevertheless, we are able to find at least one off-policy\nproblem for which this is not true, implying that certain\ninitializations of the value function could ultimately cause\nTruncated IS to diverge.\n\n\n**Counterexample 5.7** (Off-Policy Truncated IS) **.** _Consider_\n_Truncated IS with λ_ = 1 _, so βt_ = min(1 _,_ Π _t_ ) _. Assume_\n_the MDP has one state and two actions: S_ = _{s} and_\n_A_ = _{a_ 1 _, a_ 2 _}, the behavior policy µ is uniform random,_\n_and π selects a_ 1 _with probability p ∈_ (0 _,_ 1) _and selects a_ 2\n_otherwise. When p_ = 0 _._ 6 _and γ_ = 0 _._ 94 _, then ∥Z∥_ _>_ 1 _._\n\n\nMany choices of _p_ and _γ_ make _∥Z∥_ _>_ 1, but we discuss\nspecific ones for the counterexample in Appendix C.1.\n\n\nCompared to the per-decision case, where Munos et al.\n(2016) showed that arbitrary trace cuts always produce a\nconvergent algorithm, this result is surprising. Why would\nthe analogous result—in which we ensure that _βt ≤_ Π _t_ for\nall timesteps—not hold here? After all, clipping _βt_ such\nthat it never exceeds the IS estimate Π _t_ would be expected\nto simply incur bias in the return estimation. For some insight, assume the following expectations are conditioned on\n( _S_ 0 _, A_ 0) = ( _s, a_ ), and observe that Eq. (4) is equivalent to\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n_∞_\n�\n� _t_ =1\n\n\n\n_._\n\n�\n\n\n\n�\n\n\n\nE _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_\n\n\n_t_ =0\n\n\n\n+ E _µ_\n\n\n\n� _γ_ _[t]_ ( _βt−_ 1 _ρt −_ _βt_ ) _Q_ ( _St, At_ )\n\n\n_t_ =1\n\n\n\n_._\n\n\n\nWe show the derivation in Appendix A. The first term is\na (partially) bias-corrected estimate of the discounted return. The second term is a weighted combination of valuefunction bootstraps, whose weights are nonnegative when\nCondition 5.1 is met. If the condition is violated on any\ntimestep, then we may actually be subtracting bootstraps\nfrom the return estimate, which does not seem sensible.\n\n\n\nWe believe this is related to the root cause of divergence\nin Counterexample 5.7; however, it remains open whether\nCondition 5.1 is necessary or merely sufficient.\n\n\nAs our next counterexample example will demonstrate, this\neffect can even cause divergence in _on-policy_ settings.\n\n\n**Counterexample 5.8** (On-Policy Binary Traces) **.** _Assume_\n_the MDP has one state and two actions: S_ = _{s} and_\n_A_ = _{a_ 1 _, a_ 2 _}. Define a trajectory-aware method such that_\n_βt_ = 1 _if At_ = _a_ 1 _and βt_ = 0 _if At_ = _a_ 2 _(without loss of_\n_generality). Assume π and µ are uniform random. When_\n_γ ≥_ [2] 3 _[, then][ ∥][Z][∥≥]_ [1] _[.]_\n\n\nWe provide details in Appendix C.2. Even though _βt ≤_\nΠ _t_ = 1 always, we are able to produce a non-contraction.\nThe method either fully cuts a trace or does not cut it at all,\nproducing backups that consist of a sparse sum of on-policy\nTD errors. It is therefore surprising that divergence occurs.\nFor the same reason we described above, the non-Markov\nnature of the trace appears to sometimes cause adverse bootstrapping effects; in this instance, the ability to examine each\ntrajectory allows the method to strategically de-emphasize\ncertain state-action pairs, ultimately producing a detrimental effect on learning. Notice that Condition 5.1 is indeed\nviolated in this case because there is always some chance\nthat _βt_ = 1 after _βt−_ 1 = 0. If we add the restriction that\n_βt−_ 1 = 0 = _⇒_ _βt_ = 0, i.e., we permanently cut the traces,\nthen convergence is reestablished by Theorem 5.2.\n\n\n**5.4. Discussion**\n\n\nIn this section, we summarize our main theoretical\ncontributions and their significance. We focused on\ncharacterizing the contraction properties of the _M_ operator,\nboth for policy evaluation and control, in the tabular setting.\nThese results parallel those for the _R_ operator underlying\nRetrace, where _M_ is a strict generalization of _R_ . These\nresults indicate that using fixed-point updates, like dynamic\nprogramming and temporal difference learning updates,\nmay have divergence issues. It does not, however, imply\nother algorithms, such as gradient-based algorithms, cannot\nfind these fixed points. We show the fixed points still exist\nand are unbiased, but that algorithms based on iterating\nwith the _M_ operator might diverge.\n\n\n**Removal of the Markov assumption.** Removing the\nMarkov (per-decision) assumption of the _R_ operator\n(Munos et al., 2016) to enable trajectory-aware eligibility\ntraces was our primary goal. When the trace factors are\nMarkov, the operator _Bt_ is independent of _t_, allowing the\nsum [�] _[∞]_ _t_ =0 _[γ][t][B][t]_ [ to be reduced to][ �] _[∞]_ _t_ =0 [(] _[γP][cµ]_ [)] _[t]_ [ for a linear]\noperator _Pcµ_ . The resulting geometric series can then be\nevaluated analytically, as was done by Munos et al. (2016).\nIn our proofs, we avoided the Markov assumption by directly analyzing the infinite summation, which generally\n\n\n\n6\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\ndoes not have a closed-form expression. Our work is the\nfirst to do this, establishing the first convergence guarantees\nfor general trajectory-aware methods.\n\n\n**Arbitrary initialization of the value function.** We permit\nany initialization of _Q_ 0 in the control setting. In contrast,\nMunos et al. (2016) made the assumption that _Tπ_ 0 _Q_ 0 _−_\n_Q_ 0 _≥_ 0 in order to produce a lower bound on _RiQi −_ _Q_ _[∗]_,\naccomplished in practice by a pessimistic initialization of\nthe value function: _Q_ 0( _s, a_ ) = _−∥R∥_ _/_ (1 _−_ _γ_ ), _∀_ ( _s, a_ ) _∈_\n_S × A_ . Since _R_ is a special case of our operator _M_ where\neach trace _βt_ factors into Markov coefficients, we deduce as\na corollary that Retrace and all other algorithms described by\n_R_ do not require pessimistic initialization for convergence.\n\n\n**Greedy-in-the-limit policies.** Our requirement of greedyin-the-limit target policies in Theorem 5.3 is less restrictive\nthan the increasingly greedy policies proposed by Munos\net al. (2016). We need only lim _i→∞_ _TπiQi_ = _TQi_, and\nwe do not force the sequence of target policies to satisfy\n_Pπi_ +1 _Qi_ +1 _≥_ _PπiQi_ +1. This implies that the agent may\ntarget non-greedy policies for any finite period of time, as\nlong as the policies do eventually become arbitrarily close\nto the greedy policy. As a corollary, increasingly greedy\npolicies are not necessary for the optimal convergence of\nRetrace and other per-decision methods.\n\n\n**6. Recency-Bounded Importance Sampling**\n\n\nTheorem 5.2 guarantees convergence to _Q_ _[π]_ whenever Condition 5.1 holds, but we do not expect that all choices of\ncoefficients that satisfy this condition will perform well\nin practice. At one extreme, if _βt ≤_ [�] _[t]_ _k_ =1 _[λ]_ [ min(1] _[, ρ][k]_ [)]\nfor every _Ft_, then we have a method that cuts coefficients\nmore aggressively than Retrace does; it seems unlikely that\nsuch a method would learn faster than Retrace, or other\nper-decision methods. At the other extreme, when _βt_ = Π _t_\nfor every _Ft_, we recover the standard IS estimator, which\nsuffers from high variance and is often ineffectual. We therefore know that it is possible to have a method that preserves\ntraces _too much_, to the point of being detrimental. Thus,\nit is important to maintain some minimum efficiency by\navoiding unnecessary cuts, yet equally important to control\nthe overall variance of the traces.\n\n\nIntuitively, we want something that falls between Retrace\nand IS in terms of trace cutting, in order to quickly backpropagate credit while still managing the variance. We further\nhypothesize that effective trajectory-aware methods will\nfirst compute _βt−_ 1 _ρt_ —i.e., the maximum trace permitted\nby Condition 5.1—and then apply some transformation that\nlimits its magnitude to reduce variance. This ensures that\ntraces are cut only as needed.\n\n\nWe propose one method, Recency-Bounded Importance\nSampling (RBIS), which achieves this by cutting the traces\n\n\n\nonly when they exceed an exponentially decaying threshold.\nSpecifically, we define\n\n\n_βt_ = min( _λ_ _[t]_ _, βt−_ 1 _ρt_ ) _._ (RBIS)\n\n\nIt is easy to see that RBIS always converges, by construction.\n\n\n**Proposition 6.1.** _RBIS satisfies Condition 5.1._\n\n\n_Proof. βt_ = min( _λ_ _[t]_ _, βt−_ 1 _ρt_ ) _≤_ _βt−_ 1 _ρt_ .\n\n\nFor further insight, we unroll the recursion to obtain\n\n\nmin( _λ_ _[t]_ _, βt−_ 1 _ρt_ )\n\n= min( _λ_ _[t]_ _,_ min( _λ_ _[t][−]_ [1] _, βt−_ 2 _ρt−_ 1) _ρt_ )\n\n\n_· · ·_\n\n\n= min( _λ_ _[t]_ _, λ_ _[t][−]_ [1] _ρt, λ_ _[t][−]_ [2] _ρt−_ 1 _ρt, . . .,_ Π _t_ ) _._ (13)\n\n\nRBIS effectively takes the minimum of all past, discounted\n_n_ -step IS estimates. This reveals another property of RBIS:\nits traces are never less than those of Retrace, because\n\n\n\nSince the inequality is true for all _j_, it is not possible for\nRetrace’s traces to exceed any of the arguments to the min\nfunction in Eq. (13). We have achieved exactly what we\nwanted earlier: a method that falls somewhere between\n\nRetrace and IS in regard to trace cutting. This does not automatically mean that RBIS will outperform Retrace, though,\nsince preserving the magnitude of the trace _βt_ too much\ncan lead to high variance. However, we do expect RBIS to\nperform well in decision-making problems in which a few\ncritical actions largely determine the long-term outcome\nof an episode. In such scenarios, the agent’s bottleneck to\nlearning is its ability to assign meaningful credit to these\ncritical actions over a potentially long time horizon.\n\n\nIn order to test this empirically, we construct an environment called the Bifurcated Gridworld (see Figure 2). This\n5 _×_ 5 deterministic gridworld has walls arranged such that\ntwo unequal-length paths from the start (S) to the goal (G)\nare available. The agent may move up, down, left, or right;\ntaking any of these actions in the goal yields a reward of\n+1 and terminates the episode. The problem is discounted\n( _γ_ = 0 _._ 9) to encourage the agent to learn the shorter path.\nImportantly, the action taken at the bifurcation (B) solely\ndetermines which path the agent follows, and quickly assigning credit to this state is paramount to learning the task.\n\n\nWe compare RBIS against Retrace, Truncated IS, and\nRecursive Retrace when learning this task from off-policy\n\n\n\n_t_\n� _λ_ min(1 _, ρk_ ) _≤_\n\n\n_k_ =1\n\n\n\n_t_\n� min( _λ, ρk_ )\n\n\n_k_ =1\n\n\n\n_t_\n_≤_ _λ_ _[t][−][j]_ � _ρk, ∀_ _j ∈{_ 0 _,_ 1 _, . . ., t}._\n\n_k_ = _t−j_ +1\n\n\n\n7\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\ndata. Both behavior and target policies were _ϵ_ -greedy with\nrespect to the value function _Q_ . The target policy used\n_ϵ_ = 0 _._ 1. The behavior policy used a piecewise schedule:\n_ϵ_ = 1 for the first 5 episodes and then _ϵ_ = 0 _._ 2 afterwards.\nThe agents learned from online TD updates with eligibility\ntraces (see Appendix D for pseudocode). The policies\nwere updated only at the end of each episode, and then\nthe discounted return obtained by a near-greedy policy\n( _ϵ_ = 0 _._ 05) was evaluated. The area under the curve (AUC)\nof each resulting learning curve was calculated, with the\nhighest AUC achieved over a grid search of stepsizes being\nplotted for each _λ_ -value in Figure 2. We averaged the\nresults over 1,000 independent trials and indicate the 95%\nconfidence interval by the shaded regions. In Appendix E,\nwe repeated the experiment for three more gridworld\ntopologies; the obtained results are qualitatively similar to\nFigure 2. Our experiment code is available online. [1]\n\n\nWe make several observations regarding the results in Figure 2. First, the peak performance obtained by RBIS is\nsignificantly higher than that of the other three methods.\nThis is notable because both Truncated IS and Recursive\n\nRetrace are also trajectory aware, indicating that different\nimplementations of trajectory awareness are beneficial to\nvarying degrees. In particular, the preservation of long-term\neligibilities is not sufficient on its own to guarantee strong\nperformance in general, as it appears that _when_ and _how_\n_much_ the traces are cut are important considerations as well.\nThe role of _λ_ as a decay hyperparameter is evidently critical for all methods to achieve their maximum performance,\nsince _λ_ = 1 never leads to the fastest learning. In fact,\n_λ →_ 1 is especially catastrophic for Truncated IS, which we\nbelieve is related to the divergence issue identified in Section 5.3. Finally, Retrace degrades less for larger _λ_, likely\nbecause it cuts traces more. It would be interesting to develop a trajectory-aware method that obtains the robustness\nof RBIS but also accounts for larger _λ_ -values.\n\n\n**7. Conclusion**\n\n\nIn this work, we extended theory for per-decision eligibility\ntraces to trajectory-aware traces. This extension allows\nus to consider a broader family of algorithms, with more\nflexibility in obtaining off-policy corrections. Specifically,\nwe introduced the _M_ operator as a generalization of the _R_\noperator, and a sufficient condition to ensure convergence\nunder _M_ . Using our general result, we established the\nfirst convergence guarantee for an existing trajectory-aware\nmethod, Recursive Retrace, in the control setting. We also\nshowed that Truncated IS may violate our condition and\nprovided a counterexample showing that it can diverge.\n\n\n[1https://github.com/brett-daley/](https://github.com/brett-daley/trajectory-aware-etraces)\n[trajectory-aware-etraces](https://github.com/brett-daley/trajectory-aware-etraces)\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n|||||**G**|\n||||||\n|**S**||**B**|||\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-7-0.png)\n\n_Figure 2._ The Bifurcated Gridworld environment. The choice\nmade at B greatly impacts the discounted return ultimately earned.\nWe plot the AUC obtained by four off-policy methods across the\n_λ_ -spectrum. The dashed horizontal lines mark the highest AUC\nachieved by each method.\n\n\nWe also proposed a new trajectory-aware method, RBIS,\nthat demonstrates one instance of how trajectory awareness\ncan be utilized for faster learning in off-policy control tasks.\nRBIS is able to outperform the other trajectory-aware methods that we tested in the Bifurcated Gridworld, suggesting\nthat it possesses at least one unique property that is beneficial for long-term, off-policy credit assignment. It would\nbe interesting to search for additional beneficial properties\nin future work, in order to better characterize off-policy\nmethods that reliably lead to efficient and stable learning in\nchallenging reinforcement learning environments.\n\n\nThis work focused on convergence _in expectation_ ; a natural next step is to extend this result to the stochastic algorithms used in practice. Previous results for TD learning\nrely primarily on the properties of the expected update, with\nadditional conditions on the noise in the update and appropriately annealed stepsizes (see Bertsekas & Tsitsiklis, 1996,\nSection 4.3). Similar analysis should be applicable, given\nthat we know the expected update with _M_ is a contraction\nmapping when Condition 5.1 is met.\n\n\nAn important next step is extending these methods and results to function approximation. Incorporating these traces\ninto deep reinforcement learning methods that rely on experience replay (Lin, 1992) should be straightforward. Multistep returns can be computed offline in the replay memory,\nand then randomly sampled in minibatches to train the neural network. Using a TD learning update, though, can suffer from convergence issues under function approximation\nand off-policy learning; this has been previously resolved\nby developing gradient-based updates (Sutton et al., 2009;\nTouati et al., 2018). An important next step is to develop a\ngradient-based trajectory-aware algorithm. The contraction\nproperties of the operator still impact the quality of the solution, as has been shown to be the case for other off-policy\napproaches (Patterson et al., 2022). The insights in this\nwork, therefore, may provide insights on how to get quality\nsolutions with gradient-based approaches.\n\n\n\n8\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\n**References**\n\n\nBarto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike\nadaptive elements that can solve difficult learning control\nproblems. _IEEE Transactions on Systems, Man, and Cyber-_\n_netics_, pp. 834–846, 1983.\n\n\nBellman, R. Dynamic programming. _Science_, 153(3731):\n34–37, 1966.\n\n\nBertsekas, D. P. and Tsitsiklis, J. N. _Neuro-Dynamic Program-_\n_ming_ . Athena Scientific, 1996.\n\n\nDaley, B. and Amato, C. Reconciling _λ_ -returns with experience replay. In _Advances in Neural Information Processing_\n_Systems_, pp. 1133–1142, 2019.\n\n\nHarutyunyan, A., Bellemare, M. G., Stepleton, T., and Munos,\nR. Q( _λ_ ) with off-policy corrections. In _International Confer-_\n_ence on Algorithmic Learning Theory_, pp. 305–320, 2016.\n\n\nIonides, E. L. Truncated importance sampling. _Journal of Com-_\n_putational and Graphical Statistics_, 17(2):295–311, 2008.\n\n\nKahn, H. and Harris, T. E. Estimation of particle transmission\nby random sampling. _National Bureau of Standards: Applied_\n_Mathematics Series_, 12:27–30, 1951.\n\n\nKearns, M. J. and Singh, S. P. Bias-variance error bounds for\ntemporal difference updates. In _Conference on Learning_\n_Theory_, pp. 142–147, 2000.\n\n\nKozuno, T., Tang, Y., Rowland, M., Munos, R., Kapturowski,\nS., Dabney, W., Valko, M., and Abel, D. Revisiting Peng’s\nQ( _λ_ ) for modern reinforcement learning. _arXiv:2103.00107_,\n\n2021.\n\n\nLin, L.-J. Self-improving reactive agents based on reinforcement learning, planning and reaching. _Machine Learning_, 8:\n293–321, 1992.\n\n\nMahmood, A. R., Yu, H., and Sutton, R. S. Multi-step\noff-policy learning without importance sampling ratios.\n_arXiv:1702.03006_, 2017.\n\n\nMunos, R., Stepleton, T., Harutyunyan, A., and Bellemare,\nM. G. Safe and efficient off-policy reinforcement learning. In\n_Advances in Neural Information Processing Systems_, 2016.\n\n\nPatterson, A., White, A., and White, M. A generalized projected Bellman error for off-policy value estimation in reinforcement learning. _Journal of Machine Learning Research_,\n\n2022.\n\n\nPeng, J. and Williams, R. J. Incremental multi-step Q-Learning.\n_Machine Learning_, 22:226–232, 1996.\n\n\nPrecup, D., Sutton, R. S., and Singh, S. Eligibility traces for\noff-policy policy evaluation. In _International Conference on_\n_Machine Learning_, pp. 759–766, 2000.\n\n\n\nRowland, M., Dabney, W., and Munos, R. Adaptive tradeoffs in off-policy learning. In _International Conference on_\n_Artificial Intelligence and Statistics_, pp. 34–44, 2020.\n\n\nRummery, G. A. and Niranjan, M. On-line Q-Learning using connectionist systems. Technical report, Cambridge\nUniversity, 1994.\n\n\nSingh, S. P. and Sutton, R. S. Reinforcement learning with\nreplacing eligibility traces. _Machine Learning_, 22:123–158,\n\n1996.\n\n\nSutton, R. S. _Temporal Credit Assignment in Reinforcement_\n_Learning_ . PhD thesis, University of Massachusetts Amherst,\n\n1984.\n\n\nSutton, R. S. Learning to predict by the methods of temporal\ndifferences. _Machine Learning_, 3(1):9–44, 1988.\n\n\nSutton, R. S. and Barto, A. G. _Reinforcement Learning: An_\n_Introduction_ . MIT Press, 1st edition, 1998.\n\n\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver,\nD., Szepesvari, C., and Wiewiora, E. Fast gradient-descent´\nmethods for temporal-difference learning with linear function approximation. In _International Conference on Machine_\n_Learning_, pp. 993–1000, 2009.\n\n\nTouati, A., Bacon, P.-L., Precup, D., and Vincent, P. Convergent\nTree Backup and Retrace with function approximation. In\n_International Conference on Machine Learning_, pp. 4955–\n4964, 2018.\n\n\nUchibe, E. and Doya, K. Competitive-cooperative-concurrent\nreinforcement learning with importance sampling. In _Inter-_\n_national Conference on Simulation of Adaptive Behavior:_\n_From Animals and Animats_, pp. 287–296, 2004.\n\n\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R.,\nKavukcuoglu, K., and de Freitas, N. Sample efficient actorcritic with experience replay. In _International Conference_\n_on Learning Representations_, 2017.\n\n\nWatkins, C. J. C. H. _Learning from Delayed Rewards_ . PhD\nthesis, King’s College, Cambridge, 1989.\n\n\nWawrzynski, P. Real-time reinforcement learning by sequential´\nactor-critics and experience replay. _Neural Networks_, 22\n(10):1484–1497, 2009.\n\n\nWawrzynski, P. and Pacut, A. Truncated importance sampling´\nfor reinforcement learning with experience replay. _Interna-_\n_tional Multiconference on Computer Science and Informa-_\n_tion Technology_, pp. 305–315, 2007.\n\n\nYu, H., Mahmood, A. R., and Sutton, R. S. On generalized Bellman equations and temporal-difference learning. _Journal of_\n_Machine Learning Research_, 19(1):1864–1912, 2018.\n\n\n\n9\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n**A.** _M_ **Operator Details**\n\n\nIn Section 5, we defined a linear operator _Bt_, where\n\n\n( _BtX_ )( _s, a_ ) = E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _,_ (6)\n\n\nsuch that the expected-value version of our _M_ operator,\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n( _MQ_ )( _s, a_ ) = _Q_ ( _s, a_ ) + E _µ_\n\n\n\n� _γ_ _[t]_ _βtδt_ _[π]_\n\n\n_t_ =0\n\n\n\n(4)\n\n\n\n= _Q_ ( _s, a_ ) +\n\n\nis element-wise equivalent to the vector version,\n\n\n\n_∞_\n� _γ_ _[t]_ E _µ_ � _βtδt_ _[π]_ �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _,_ (5)\n\n\n_t_ =0\n\n\n\n_MQ_ = _Q_ +\n\n\nWe claimed that each element of _Bt_ must have the form\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _Q_ ) _._ (7)\n\n\n_t_ =0\n\n\n\n_Bt_ (( _s, a_ ) _,_ ( _s_ _[′]_ _, a_ _[′]_ )) = Pr _µ_ [((] _[S][t][, A][t]_ [) = (] _[s][′][, a][′]_ [)] _[ |]_ [ (] _[S]_ [0] _[, A]_ [0][) = (] _[s, a]_ [))] _[ ×]_ [ E] _[µ]_ � _βt_ �� ( _S_ 0 _, A_ 0) = ( _s, a_ ) _,_ ( _St, At_ ) = ( _s′, a′_ )� _,_ (8)\n\n\nwith ( _s, a_ ) as the row index and ( _s_ _[′]_ _, a_ _[′]_ ) as the column index. This is because multiplying this matrix _Bt_ with a vector _X_\nresults in the same operation as the weighted expected value in Eq. (5):\n\n\n\n�\n\n\n\n�\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n� _Bt_ (( _s, a_ ) _,_ ( _s_ _[′]_ _, a_ _[′]_ )) _X_ ( _s_ _[′]_ _, a_ _[′]_ ) = E _µ_\n\n_s_ _[′]_ _,a_ _[′]_\n\n\n\nE _µ_ � _βt_ �� ( _S_ 0 _, A_ 0) = ( _s, a_ ) _,_ ( _St, At_ )� _· X_ ( _St, At_ )\n\n\n\n= E _µ_ E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ ) _,_ ( _St, At_ )� [�] ( _S_ 0 _, A_ 0) = ( _s, a_ )\n\n� ��� �\n\n\n\n= E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ )� _._ (14)\n\n\nSo, when _X_ is the expected TD error _TπQ −_ _Q_, Eq. (7) becomes Eq. (5) exactly.\n\n\n_M_ is a contraction mapping whenever _βt ≤_ _βt−_ 1 _ρt_ for all _t_ (Condition 5.1), which Theorem 5.2 establishes. As we discussed\nin Section 5.3, violating this condition can sometimes cause _M_ to no longer contract, even with on-policy updates. We can see\none plausible reason for this by refactoring the definition of _M_ . Let _qt_ := _Q_ ( _St, At_ ) and _vt_ := [�] _a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [)] _[Q]_ [(] _[S][t][, a][′]_ [)][,]\n\nso _δt_ _[π]_ [=] _[ R][t]_ [+] _[ γv][t]_ [+1] _[−]_ _[q][t]_ [. Further, assume the following expectations are conditioned on][ (] _[S]_ [0] _[, A]_ [0][) = (] _[s, a]_ [)][. Eq. (][4][) is]\nequivalent to\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n�\n\n\n\n( _MQ_ )( _s, a_ ) = _q_ 0 + E _µ_\n\n\n\n� _γ_ _[t]_ _βt_ ( _Rt_ + _γvt_ +1 _−_ _qt_ )\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _βtqt_\n\n\n_t_ =0\n\n\n\n_∞_\n�\n\n\n\n�\n\n\n\n_∞_\n� _γ_ _[t]_ _βt−_ 1 _vt −_\n\n\n_t_ =1\n\n\n\n= _q_ 0 + E _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_ +\n\n\n_t_ =0\n\n\n\n_∞_\n�\n� _t_ =0\n\n\n_∞_\n�\n� _t_ =0\n\n\n_∞_\n�\n� _t_ =0\n\n\n\n�\n\n�\n\n\n\n_∞_\n� _γ_ _[t]_ _βtqt_\n\n\n_t_ =1\n\n\n\n_∞_\n�\n\n\n\n�\n\n\n\n_∞_\n� _γ_ _[t]_ _βt−_ 1 _vt −_\n\n\n_t_ =1\n\n\n\n_∞_\n�\n\n\n\n= E _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_ +\n\n\n_t_ =0\n\n\n\n�\n\n�\n\n\n\n_∞_\n�\n� _t_ =1\n\n\n_∞_\n�\n� _t_ =1\n\n\n\n� _γ_ _[t]_ ( _βt−_ 1 _vt −_ _βtqt_ )\n\n\n_t_ =1\n\n\n\n= E _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_\n\n\n_t_ =0\n\n\n\n+ E _µ_\n\n\n\n= E _µ_\n\n\n\n� _γ_ _[t]_ _βtRt_\n\n\n_t_ =0\n\n\n\n+ E _µ_\n\n\n\n� _γ_ _[t]_ ( _βt−_ 1 _ρt −_ _βt_ ) _qt_\n\n\n_t_ =1\n\n\n\n_,_ (15)\n\n\n\nand we discussed in Section 5.3 that these two terms represent a biased return estimate and an infinite sum of weighted\nvalue-function bootstraps, respectively. In particular, this can be problematic if _βt > βt−_ 1 _ρt_ because the corresponding\nbootstrap’s weight becomes negative, causing it to get subtracted from the return estimate.\n\n\n10\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n**B. Additional Proofs**\n\n\n**B.1. Proof of Lemma B.1**\n\n\n**Lemma B.1.** _Q_ _[π]_ _is a fixed point of M; the difference between MQ and Q_ _[π]_ _is given by_\n\n\n_MQ −_ _Q_ _[π]_ = _Z_ ( _Q −_ _Q_ _[π]_ ) _,_ (16)\n\n\n_where Z_ := [�] _[∞]_ _t_ =1 _[γ][t]_ [(] _[B][t][−]_ [1] _[P][π][ −]_ _[B][t]_ [)] _[.]_\n\n\n_Proof._ It is evident from Eq. (7) that _Q_ _[π]_ is a fixed point of _M_ because _TπQ_ _[π]_ _−_ _Q_ _[π]_ = 0, and so _MQ_ _[π]_ = _Q_ _[π]_ . Therefore,\n\n\n_MQ −_ _Q_ _[π]_ = _MQ −MQ_ _[π]_\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ_ _[π]_ _−_ _Q_ _[π]_ )\n\n\n_t_ =0\n\n\n\n= _Q_ +\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _Q_ ) _−_ _Q_ _[π]_ _−_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _Q −_ _Q_ _[π]_ )\n\n\n_t_ =0\n\n\n\n= _Q −_ _Q_ _[π]_ +\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _TπQ_ _[π]_ ) _−_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _Q −_ _Q_ _[π]_ )\n\n\n_t_ =1\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _Q −_ _Q_ _[π]_ )\n\n\n_t_ =1\n\n\n\n=\n\n\n=\n\n\n=\n\n\n=\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _TπQ −_ _TπQ_ _[π]_ ) _−_\n\n\n_t_ =0\n\n\n_∞_\n� _γ_ _[t]_ [+1] _BtPπ_ ( _Q −_ _Q_ _[π]_ ) _−_\n\n\n_t_ =0\n\n\n\n_∞_\n�\n� _t_ =1\n\n\n\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπ −_ _Bt_ )\n\n\n_t_ =1\n\n\n\n( _Q −_ _Q_ _[π]_ )\n\n\n\n_∞_\n� _γ_ _[t]_ [+1] _BtPπ −_\n� _t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_\n\n\n_t_ =1\n\n\n\n�\n\n\n\n( _Q −_ _Q_ _[π]_ )\n\n\n\n�\n\n\n\n= _Z_ ( _Q −_ _Q_ _[π]_ ) _,_\n\n\nwhich is the desired result.\n\n\n**B.2. Proof of Lemma B.2**\n\n\n**Lemma B.2.** _If Condition 5.1 holds, then Z has nonnegative elements and its row sums obey Z_ **1** _≤_ _γ._\n\n\n_Proof._ Define the linear operator _Dt_ := _Bt−_ 1 _Pπ −_ _Bt_ and notice that _Z_ = [�] _[∞]_ _t_ =1 _[γ][t][D][t]_ [. We will show that] _[ D][t]_ [ comprises]\nonly nonnegative elements, and therefore so does _Z_ . For any _X ∈_ R _[n]_, observe that\n\n\n\n( _DtX_ )( _s, a_ ) = E _µ_\n\n�\n\n\n\n_βt−_ 1 �\n\n_St∈S_\n\n\n\n� _P_ ( _St|Ft−_ 1) _π_ ( _At|St_ ) _X_ ( _St, At_ )\n\n_At∈A_\n\n\n\n_−_ E _µ_ � _βtX_ ( _St, At_ ) �� ( _S_ 0 _, A_ 0) = ( _s, a_ )�\n\n\n\n= E _µ_\n\n�\n\n\n\n_βt−_ 1 �\n\n_St∈S_\n\n\n\n� _P_ ( _St|Ft−_ 1) _π_ ( _At|St_ ) _X_ ( _St, At_ )\n\n_At∈A_\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n�� _St∈S_\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n_−_ E _µ_\n\n\n\n_St∈S_\n\n\n\n� _P_ ( _St|Ft−_ 1) _µ_ ( _At|St_ ) _βtX_ ( _St, At_ )\n\n_At∈A_\n\n\n\n�� _St∈S_\n\n\n\n_At∈A_\n\n\n\n( _S_ 0 _, A_ 0) = ( _s, a_ )\n����� �\n\n\n\n= E _µ_\n\n\n\n_P_ ( _St|Ft−_ 1) �\n_St∈S_ _At_\n\n\n\n� _π_ ( _At|St_ ) _βt−_ 1 _−_ _µ_ ( _At|St_ ) _βt_ � _X_ ( _St, At_ )\n\n\n\n_._ (17)\n\n\n\nSince we assumed that _βt ≤_ _βt−_ 1 _ρt_ in Condition 5.1, we have _π_ ( _At|St_ ) _βt−_ 1 _−_ _µ_ ( _At|St_ ) _βt ≥_ 0, which implies that _Dt ≥_ 0.\nFurthermore, this holds for all _t ≥_ 1, so _Z ≥_ 0 follows immediately.\n\n\n11\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\nTo complete the proof, we show that the row sums of _Z_ are bounded by _γ_ . Recall that _Pπ_ **1** = **1** . Hence,\n\n\n\n_Z_ **1** =\n\n\n=\n\n\n=\n\n\n\n_∞_\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπ −_ _Bt_ ) **1**\n\n\n_t_ =1\n\n\n_∞_\n� _γ_ _[t]_ ( _Bt−_ 1 **1** _−_ _Bt_ **1** )\n\n\n_t_ =1\n\n\n\n_∞_\n� _γ_ _[t]_ [+1] _Bt_ **1** _−_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ **1**\n\n\n_t_ =1\n\n\n\n_∞_\n\n= _γ_ **1** + � _γ_ _[t]_ [+1] _Bt_ **1** _−_\n\n\n_t_ =1\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ **1**\n\n\n_t_ =1\n\n\n\n= _γ_ **1** _−_ (1 _−_ _γ_ )\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ **1**\n\n\n_t_ =1\n\n\n\n_≤_ _γ_ **1** _,_ (18)\n\n\nbecause _Bt ≥_ 0, _∀_ _t ≥_ 1.\n\n\n**B.3. Proof of Theorem 5.3**\n\n\n**Theorem 5.3.** _Consider a sequence of target policies_ ( _πi_ ) _i≥_ 0 _and a sequence of arbitrary behavior policies_ ( _µi_ ) _i≥_ 0 _. Let_\n_Q_ 0 _be an arbitrary vector in_ R _[n]_ _and define the sequence Qi_ +1 := _MiQi, where Mi is the operator defined by Eq._ (10) _._\n_Assume that_ ( _πi_ ) _i≥_ 0 _is greedy in the limit, and let ϵi ≥_ 0 _be the smallest constant such that TπiQi ≥_ _TQi −_ _ϵi∥Qi∥_ **1** _. If_\n_Condition 5.1 holds for all i, then_\n\n\n_ϵi_\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥_ + (11)\n1 _−_ _γ_ _[∥][Q][i][∥][,]_\n\n\n_and, consequently,_ lim\n_i→∞_ _[Q][i]_ [ =] _[ Q][∗][.]_\n\n\n_Proof._ We first derive the following upper bound:\n\n\n_TπiQi −_ _TQ_ _[∗]_ = _γPπiQi −_ _γ_ max _PπQ_ _[∗]_ _≤_ _γPπi_ ( _Qi −_ _Q_ _[∗]_ ) _._ (19)\n_π_\n\n\nFrom Eq. (10) and because _Ci_ has nonnegative entries, we can deduce that\n\n\n_MiQi −_ _Q_ _[∗]_ = ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _Ci_ ( _TπiQi −_ _Q_ _[∗]_ ) (20)\n\n= ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _Ci_ ( _TπiQi −_ _TQ_ _[∗]_ )\n\n_≤_ ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _γCiPπi_ ( _Qi −_ _Q_ _[∗]_ )\n\n= _Zi_ ( _Qi −_ _Q_ _[∗]_ ) _,_ (21)\n\n\nwhere _Zi_ := _I −_ _Ci_ ( _I −_ _γPπi_ ). Notice that _Zi_ is analogous to the matrix _Z_ in Eq. (9) because, for policies _πi_ and _µi_,\n\n\n\n_I −_ _Ci_ ( _I −_ _γPπi_ ) = _I_ +\n\n\n= _I_ +\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_ ( _γPπi −_ _I_ )\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ [+1] _BtPπi −_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_\n\n\n_t_ =0\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt_\n\n\n_t_ =1\n\n\n\n=\n\n\n=\n\n\n\n_∞_\n� _γ_ _[t]_ _Bt−_ 1 _Pπi −_\n\n\n_t_ =1\n\n\n\n_∞_\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπi −_ _Bt_ ) _._ (22)\n\n\n_t_ =1\n\n\n12\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\nNext, we derive the following lower bound:\n\n\n_TQi −_ _TQ_ _[∗]_ _≥_ _Tπ∗_ _Qi −_ _TQ_ _[∗]_ = _γPπ∗_ ( _Qi −_ _Q_ _[∗]_ ) _._ (23)\n\n\nAdditionally, for each policy _πi_, there exists some _ϵi ≥_ 0 such that _TπiQi ≥_ _TQi −_ _ϵi∥Qi∥_ **1** (recall that we defined _ϵi_ to be\nas small as possible). Starting again from Eq. (20), and noting that the elements of _Ci_ are nonnegative, we obtain\n\n\n_MiQi −_ _Q_ _[∗]_ _≥_ ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _Ci_ ( _TQi −_ _Q_ _[∗]_ ) _−_ _ϵi∥Qi∥Ci_ **1**\n\n\n= ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _Ci_ ( _TQi −_ _TQ_ _[∗]_ ) _−_ _ϵi∥Qi∥Ci_ **1**\n\n\n_≥_ ( _I −_ _Ci_ )( _Qi −_ _Q_ _[∗]_ ) + _γCiPπ∗_ ( _Qi −_ _Q_ _[∗]_ ) _−_ _ϵi∥Qi∥Ci_ **1**\n\n= _Zi_ _[∗]_ [(] _[Q][i]_ _[−]_ _[Q][∗]_ [)] _[ −]_ _[ϵ][i][∥][Q][i][∥][C][i]_ **[1]** _[,]_ (24)\n\n\nwhere we have defined _Zi_ _[∗]_ [:=] _[ I][ −]_ _[C][i]_ [(] _[I][ −]_ _[γP][π][∗]_ [)][. By Lemma][ B.2][, since we assumed Condition][ 5.1][ holds, both] _[ Z][i]_ [ and] _[ Z]_ _i_ _[∗]_\nhave nonnegative elements and their row sums are bounded by _γ_ . Therefore, when _MiQi −_ _Q_ _[∗]_ _≥_ 0, Eq. (21) implies\n\n\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥,_ (25)\n\n\nbecause element-wise inequality for nonnegative matrices implies the inequality holds also for their norms. When\n_MiQi −_ _Q_ _[∗]_ _≤_ 0, we must use Eq. (24) and multiply both sides by _−_ 1 to get nonnegative matrices, giving\n\n\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥_ + _ϵi∥Qi∥∥Ci∥_\n\n\n_ϵi_\n_≤_ _γ∥Qi −_ _Q_ _[∗]_ _∥_ + (26)\n1 _−_ _γ_ _[∥][Q][i][∥][,]_\n\n\nbecause _∥Ci∥≤_ [�] _[∞]_ _t_ =0 _[γ][t][∥][P][π]_ _i_ _[∥][t]_ [ = (1] _[ −]_ _[γ]_ [)] _[−]_ [1][. Since Eq. (][26][) is looser than Eq. (][25][), its bound holds in the worst case. It]\nremains to show that this bound implies convergence to _Q_ _[∗]_ . Observe that\n\n\n_ϵi_ _ϵi_\n_γ∥Qi −_ _Q_ _[∗]_ _∥_ +\n1 _−_ _γ_ _[∥][Q][i][∥≤]_ _[γ][∥][Q][i][ −]_ _[Q][∗][∥]_ [+] 1 _−_ _γ_ [(] _[∥][Q][i][ −]_ _[Q][∗][∥]_ [+] _[ ∥][Q][∗][∥]_ [)]\n\n\n\n_ϵi_\n= _γ_ +\n� 1 _−_ _γ_\n\n\n\n_ϵi_\n_∥Qi −_ _Q_ _[∗]_ _∥_ + (27)\n� 1 _−_ _γ_ _[∥][Q][∗][∥][.]_\n\n\n\nOur assumption of greedy-in-the-limit policies tells us that _ϵi →_ 0 as _i →∞_ ; thus, there must exist some iteration _i_ _[∗]_ such\nthat _ϵi ≤_ [1] 2 [(1] _[ −]_ _[γ]_ [)][2][,] _[ ∀]_ _[i][ ≥]_ _[i][∗]_ [. Therefore, for] _[ i][ ≥]_ _[i][∗]_ [,]\n\n\n\n\n_[γ]_ _ϵi_\n_∥MiQi −_ _Q_ _[∗]_ _∥≤_ [1 +] _∥Qi −_ _Q_ _[∗]_ _∥_ + (28)\n\n2 1 _−_ _γ_ _[∥][Q][∗][∥][.]_\n\n\n\nIf _γ <_ 1, then 2 [1] [(1 +] _[ γ]_ [)] _[ <]_ [ 1][, and since] _[ ∥][Q][∗][∥]_ [is finite, we conclude that] _[ ∥][Q][i][ −]_ _[Q][∗][∥→]_ [0][ as] _[ i][ →∞]_ [.]\n\n\n**C. Examples of Divergence**\n\n\n**C.1. Counterexample 5.7: Off-Policy Truncated IS**\n\n\nOur definitions of _π_ and _µ_ give us\n\n\n\n_p_ 1 _−_ _p_\n_Pπ_ = � _p_ 1 _−_ _p_\n\n\n\n� _,_ _Pµ_ = 2 [1]\n\n\n\n1 1\n\n_._ (29)\n\n1 1\n� �\n\n\n\nRecall that we assumed _λ_ = 1. We define the following constant, using the definition of _βt_ for Truncated IS:\n\n\n_βt_ [(1)] := E� _βt_ �� ( _St, At_ ) = ( _s, a_ 1)�\n\n\n\n� _Ft_ Pr _µ_ ( _Ft |_ ( _St, At_ ) = ( _s, a_ 1)) _·_ min �1 _,_ [Pr] Pr _[π]_ _µ_ ( [(] _F_ _[F]_ _t_ _[t]_ ) [)]\n\n\n\n=\n�\n\n\n\nPr _µ_ ( _Ft_ )\n\n\n\n�\n\n\n\n\n_[p]_\n_F_ � _t−_ 1 Pr _µ_ ( _Ft−_ 1) min �1 _,_ Pr [Pr] _µ_ _[π]_ ( [(] _F_ _[F]_ _t_ _[t]_ _−_ _[−]_ 1 [1] ) [)] _·_ _[ ·]_ [1] 2\n\n\n\n=\n�\n\n\n\nPr _µ_ ( _Ft−_ 1) _·_ [1] 2\n\n\n\n2\n\n\n\n(30)\n�\n\n\n\n13\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n= � min (Pr _µ_ ( _Ft−_ 1) _,_ 2 _p ·_ Pr _π_ ( _Ft−_ 1))\n\n_Ft−_ 1\n\n\n\n1\n\n= _F_ � _t−_ 1 min � 2 _[t][−]_ [1] _[,]_ [ 2] _[p][ ·]_ [ Pr] _[π]_ [(] _[F][t][−]_ [1][)] � _._ (31)\n\n\n\nEq. (30) is justified because the conditional probability of a trajectory ending in action _a_ 1 is just the probability of _Ft−_ 1\nunder _µ_, due to the 1-state (memoryless) MDP. We can simplify _βt_ [(1)] further by using the binomial theorem to calculate\n_t−_ 1\nPr _π_ ( _Ft−_ 1) = _p_ _[k]_ (1 _−_ _p_ ) _[t][−]_ [1] _[−][k]_, where _k ∈_ [0 _, t −_ 1] is the number of times _a_ 1 is taken in _Ft−_ 1. There are � _k_ � trajectories\nwith this same probability. Therefore,\n\n\n\n1\n\n_βt_ [(1)] = _F_ � _t−_ 1 min � 2 _[t][−]_ [1] _[,]_ [ 2] _[p][ ·]_ [ Pr] _[π]_ [(] _[F][t][−]_ [1][)] � =\n\n\n\n_t−_ 1\n�\n\n\n_k_ =0\n\n\n\n_t −_ 1 1\n� _k_ � min � 2 _[t][−]_ [1] _[,]_ [ 2] _[p][ ·][ p][k]_ [(1] _[ −]_ _[p]_ [)] _[t][−]_ [1] _[−][k]_ � _._ (32)\n\n\n\nLikewise, we can compute _βt_ [(2)] by swapping _p_ and 1 _−_ _p_ above. Let _⊙_ denote element-wise multiplication. Using the fact\nthat _Pµ_ _[t]_ [=] _[ P][µ]_ [,] _[ ∀]_ _[t][ ≥]_ [1][, it follows that]\n\n\n\n_._ (33)\n\n�\n\n\n\n_βt_ [(1)] _βt_ [(2)]\n� _βt_ [(1)] _βt_ [(2)]\n\n\n\n_Bt_ = _Pµ_ _[t]_ _[⊙]_\n\n\n\n_βt_ [(1)] _βt_ [(2)]\n� _βt_ [(1)] _βt_ [(2)] �\n\n\n\n= [1]\n\n2\n\n\n\nUsing a computer program to calculate _Z_, assuming that _p_ = 0 _._ 6 and _γ_ = 0 _._ 94, we obtain\n\n\n\n_Z_ =\n\n\n\n_∞_\n\n0 _._ 704 _−_ 0 _._ 436\n\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπ −_ _Bt_ ) _≈_ 0 _._ 704 _−_ 0 _._ 436 _._ (34)\n\n_t_ =1 � �\n\n\n\nTherefore, _∥Z∥≈_ 1 _._ 14, which is not a contraction, and the norm continues to increase for _p >_ 0 _._ 6 or _γ >_ 0 _._ 94.\n\n\n**C.2. Counterexample 5.8: On-Policy Binary Traces**\n\n\nThe policy _π_ is uniform random, so we have\n\n\n\n_._ (35)\n�\n\n\n\n_Pπ_ = [1] 2\n\n\n\n1 1\n\n1 1\n�\n\n\n\nLet _⊙_ denote element-wise multiplication. Because _βt_ = 1 only when the trajectory _Ft_ terminates in ( _s, a_ 1) and _βt_ = 0\notherwise, and since _Pπ_ _[t]_ [=] _[ P][π]_ [,] _[ ∀]_ _[t][ ≥]_ [1][, we also have]\n\n\n\n1 0\n_Bt_ = _Pπ_ _[t]_ _[⊙]_ 1 0\n�\n\n\n\n= [1]\n� 2\n\n\n\n_._ (36)\n�\n\n\n\n2\n\n\n\n1 0\n\n1 0\n�\n\n\n\nUsing a computer program to calculate _Z_, assuming that _γ_ = 3 [2] [, we obtain]\n\n\n\n3\n\n\n\n_._ (37)\n�\n\n\n\n_Z_ =\n\n\n\n_∞_\n�\n\n\n\n� _γ_ _[t]_ ( _Bt−_ 1 _Pπ −_ _Bt_ ) = 3 [1]\n\n_t_ =1\n\n\n\n_−_ 1 2\n\n_−_ 1 2\n�\n\n\n\nTherefore, _∥Z∥_ = 1, which is not a contraction, and the norm continues to increase for _γ >_ [2] 3 [.]\n\n\n**D. Implementation of Trajectory-Aware Eligibility Traces**\n\n\nThe implementation of trajectory-aware methods is closely related to that of backward-view TD( _λ_ ) in the tabular setting\n(see, e.g., Sutton & Barto, 1998, Chapter 7.3). On each timestep, an environment interaction is conducted according to the\nbehavior policy _µ_ . Then, the eligibilities for previously visited state-action pairs are modified, the eligibility for the current\nstate-action pair is incremented, and the current TD error is applied to all state-action pairs in proportion to their eligibilities.\nThe only difference in the trajectory-aware case is that the eligibilities are not modified by simply multiplying a constant\ndecay factor _γλ_ .\n\n\n14\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\nArbitrary, trajectory-dependent traces _β_ ( _Ft_ ), as studied in our theoretical results, can be complicated to implement. This\nstems from the fact that the timestep _t_ in the _M_ operator is defined _relative_ to when the updated state-action pair was\ntaken. In other words, each state-action pair ( _Sk, Ak_ ) “disagrees” on the start of the current trajectory, generating its update\nfrom the unique sub-trajectory ( _Sk, Ak_ ) _, . . .,_ ( _St, At_ ). Implementing coefficients of this form would be possible using the\ngeneral update\n_Q_ ( _Sk, Ak_ ) _←_ _Q_ ( _Sk, Ak_ ) + _αγ_ _[t][−][k]_ _β_ (( _Sk, Ak_ ) _, . . .,_ ( _St, At_ )) _δt_ _[π][,]_ (38)\n\n\nwhere _α ∈_ (0 _,_ 1] is the stepsize, but this would require repeatedly slicing the list of visited state-action pairs\n( _S_ 0 _, A_ 0) _, . . .,_ ( _St, At_ ). While this is certainly feasible, it does not easily accommodate vectorization or parallelization.\n\n\nFortunately, this level of generality is rarely needed in practice, and specific optimizations can be made depending on the\nfunctional form of _β_ . For example, Truncated IS defines _β_ to be a pure function of the IS estimate Π _t_, which is useful\nbecause per-decision eligibility traces can be used to efficiently generate the IS estimates for every state-action pair visited\nduring the episode. We demonstrate how this can be done in pseudocode (see Algorithm 1).\n\n\nRecursive methods like Recursive Retrace and RBIS, where _βt_ explicitly depends on _βt−_ 1, require only two minor\nchanges compared to Algorithm 1 for their implementations. These changes, which we highlight in blue for RBIS in\nAlgorithm 2, correspond to the fact that the dynamic array _Y_ is now used to store the previous trace _βt−_ 1 rather than the\nprevious IS estimate Π _t−_ 1 at each timestep. The computational requirements for the methods remain nearly identical. The\nimplementation for Recursive Retrace easily follows by changing line 10 of Algorithm 2 to\n\n\n_Y_ ( _k_ ) _←_ _λ_ min(1 _, Y_ ( _k_ ) _· ρt_ ) _._ (39)\n\n\n**E. Additional Experiment Details and Results**\n\n\nWe conducted a grid search to find the best stepsize _α_ for every _λ_ -value for the four off-policy methods we evaluated\nin the Bifurcated Gridworld (Section 6). Using a training set of 1,000 trials, we searched over _λ ∈{_ 0 _,_ 0 _._ 1 _, . . .,_ 1 _}_ and\n_α ∈{_ 0 _._ 1 _,_ 0 _._ 3 _,_ 0 _._ 5 _,_ 0 _._ 7 _,_ 0 _._ 9 _}_, for a total of 55 hyperparameter combinations. At the start of each trial, the initial value\nfunction _Q_ was sampled from a zero-mean Gaussian distribution with standard deviation _σ_ = 0 _._ 01. We trained each agent\nfor 3,000 timesteps, allowing extra time to complete the final episode. We then generated learning curves by plotting the\n100-episode moving average of these returns as a function of the number of timesteps and calculated their AUCs. In Table 1,\nwe report the stepsize _α_ that led to the highest average AUC for each _λ_ -value. Then, using a separate test set of 1,000 trials\nto avoid bias in the search results, these _α_ -values were used to generate the learning curves in Figure 3. The AUCs for these\nlearning curves were finally used in the creation of the _λ_ -sweep plot (Figure 2).\n\n\nIn Figure 4, we repeated this procedure for three additional, more complex gridworld topologies. Like the Bifurcated\nGridworld, these environments feature one or more bifurcations that make fast credit assignment imperative, as well as\nadditional challenges such as multiple goal cells. As before, the agent starts in S and receives +1 reward for taking any\naction in a goal, terminating the episode. The results are qualitatively similar to Figure 2; RBIS outperforms the other three\nmethods by a significant margin over the left-hand portion of the _λ_ -spectrum, and performs similarly to Retrace as _λ →_ 1.\n\n\n_Table 1._ The best stepsizes found by our grid search in the Bifurcated Gridworld.\n\n\n_λ_ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n\n\nRetrace 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.7 0.7 0.5\n\nTruncated IS 0.9 0.9 0.9 0.9 0.9 0.9 0.7 0.5 0.5 0.5 0.3\n\nRecursive Retrace 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.7 0.5 0.5\n\nRBIS 0.9 0.9 0.9 0.9 0.9 0.7 0.7 0.7 0.7 0.7 0.5\n\n\n15\n\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-6.png)\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-4.png)\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-5.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-7.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-8.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-10.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-11.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-17.png)\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-9.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-12.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-15-18.png)\n\n\n\n_Figure 3._ Learning curves for the _λ_ -values we tested in the Bifurcated Gridworld environment. The dashed black line indicates the\noptimal discounted return for this problem.\n\n16\n\n\n\n\n**Trajectory-Aware Eligibility Traces**\n\n|Col1|Col2|G|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||**S**||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||**G**||||\n||||||||\n||||||||\n|**S**|||||||\n\n\n|G|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|**G**||||||\n|**G**||||||\n|||||||\n|||**S**||||\n|||||||\n\n\n\n_Figure 4. λ_ -sweeps conducted on three additional gridworld topologies. The experiment procedure was identical to that used in the\ncreation of Figure 2.\n\n\n17\n\n\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-16-3.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-16-7.png)\n\n![](output/images/e457f7b24e8af833b15c802d31b91bd048b158e8.pdf-16-11.png)\n\n\n**Trajectory-Aware Eligibility Traces**\n\n\n**Algorithm 1** Truncated Importance Sampling\n\n1: **Input:** value function _Q_, stepsize _α ∈_ (0 _,_ 1]\n\n\n2: **for** each episode **do**\n\n\n3: Reset environment and observe state _S_ 0\n\n4: Reset dynamic array _Y_\n\n5: **repeat** _{_ for _t_ = 0 _,_ 1 _,_ 2 _, . . . }_\n\n6: Take action _At ∼_ _µ_ ( _·|St_ ), receive reward _Rt_, and observe next state _St_ +1\n7: _ρt_ = _[π]_ _µ_ ( [(] _A_ _[A]_ _t_ _[t]_ _|_ _[|]_ _S_ _[S]_ _t_ _[t]_ ) [)]\n\n\n\n_a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [+1][)] _[Q]_ [(] _[S][t]_ [+1] _[, a][′]_ [)] else\n\n\n\n8: _δt_ =\n\n\n\n\n\n\n\n\n_Rt −_ _Q_ ( _St, At_ ) if _St_ +1 is terminal\n\n _Rt −_ _Q_ ( _St, At_ ) + _γ_ [�] _a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [+1][)] _[Q]_ [(] _[S][t]_ [+1] _[, a][′]_ [)] else\n\n\n\n_Rt −_ _Q_ ( _St, At_ ) + _γ_ [�]\n\n\n\n9: **for** _k_ = 0 _, . . ., t −_ 1 **do**\n\n10: _Y_ ( _k_ ) _←_ _Y_ ( _k_ ) _· ρt_\n\n11: **end for**\n\n\n12: _Y_ ( _t_ ) _←_ 1\n\n\n13: **for** _k_ = 0 _, . . ., t_ **do**\n\n14: _z ←_ ( _γλ_ ) _[t][−][k]_ min(1 _, Y_ ( _k_ ))\n\n15: _Q_ ( _Sk, Ak_ ) _←_ _Q_ ( _Sk, Ak_ ) + _αzδt_\n\n16: **end for**\n\n\n17: **until** _St_ +1 is terminal\n\n\n18: **end for**\n\n\n**Algorithm 2** Recency-Bounded Importance Sampling (RBIS)\n\n1: **Input:** value function _Q_, stepsize _α ∈_ (0 _,_ 1]\n\n\n2: **for** each episode **do**\n\n\n3: Reset environment and observe state _S_ 0\n\n4: Reset dynamic array _Y_\n\n5: **repeat** _{_ for _t_ = 0 _,_ 1 _,_ 2 _, . . . }_\n\n6: Take action _At ∼_ _µ_ ( _·|St_ ), receive reward _Rt_, and observe next state _St_ +1\n7: _ρt_ = _[π]_ _µ_ ( [(] _A_ _[A]_ _t_ _[t]_ _|_ _[|]_ _S_ _[S]_ _t_ _[t]_ ) [)]\n\n\n\n_a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [+1][)] _[Q]_ [(] _[S][t]_ [+1] _[, a][′]_ [)] else\n\n\n\n8: _δt_ =\n\n\n\n\n\n\n\n\n_Rt −_ _Q_ ( _St, At_ ) if _St_ +1 is terminal\n\n _Rt −_ _Q_ ( _St, At_ ) + _γ_ [�] _a_ _[′]_ _∈A_ _[π]_ [(] _[a][′][|][S][t]_ [+1][)] _[Q]_ [(] _[S][t]_ [+1] _[, a][′]_ [)] else\n\n\n\n_Rt −_ _Q_ ( _St, At_ ) + _γ_ [�]\n\n\n\n9: **for** _k_ = 0 _, . . ., t −_ 1 **do**\n\n10: _Y_ ( _k_ ) _←_ min( _λ_ _[t][−][k]_ _, Y_ ( _k_ ) _· ρt_ )\n\n\n11: **end for**\n\n\n12: _Y_ ( _t_ ) _←_ 1\n\n\n13: **for** _k_ = 0 _, . . ., t_ **do**\n\n14: _z ←_ _γ_ _[t][−][k]_ _Y_ ( _k_ )\n\n15: _Q_ ( _Sk, Ak_ ) _←_ _Q_ ( _Sk, Ak_ ) + _αzδt_\n\n16: **end for**\n\n\n17: **until** _St_ +1 is terminal\n\n\n18: **end for**\n\n\n\n18\n\n\n",
    "ranking": {
      "relevance_score": 0.7753779146649299,
      "citation_score": 0.3439655172413793,
      "recency_score": 0.6093829095338229,
      "final_score": 0.672495934667109
    },
    "citation_key": "Daley2023TrajectoryAwareET",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "c44471e846846bde281779405a3b5c132fd60b00",
    "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods",
    "published": "2024-03-30",
    "authors": [
      "Yuji Cao",
      "Huan Zhao",
      "Yuheng Cheng",
      "Ting Shu",
      "Yue Chen",
      "Guolong Liu",
      "Gaoqi Liang",
      "Junhua Zhao",
      "Jinyue Yan",
      "Yun Li"
    ],
    "summary": "With extensive pretrained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects, such as multitask learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared with conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs’ functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated and provide insights into future directions. Finally, the comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications, such as robotics, autonomous driving, and energy systems.",
    "pdf_url": "http://arxiv.org/pdf/2404.00282",
    "doi": "10.1109/TNNLS.2024.3497992",
    "fields_of_study": [
      "Computer Science",
      "Medicine"
    ],
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "citation_count": 140,
    "bibtex": "@Article{Cao2024SurveyOL,\n author = {Yuji Cao and Huan Zhao and Yuheng Cheng and Ting Shu and Yue Chen and Guolong Liu and Gaoqi Liang and Junhua Zhao and Jinyue Yan and Yun Li},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {9737-9757},\n title = {Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods},\n volume = {36},\n year = {2024}\n}\n",
    "markdown_text": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 1\n\n## Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods\n\n\nYuji Cao, Huan Zhao, _Member, IEEE,_ Yuheng Cheng, _Student Member, IEEE,_ Ting Shu, Yue Chen,\n_Member, IEEE,_ Guolong Liu, _Member, IEEE,_ Gaoqi Liang, _Member, IEEE,_ Junhua Zhao, _Senior Member, IEEE,_\nJinyue Yan, Yun Li, _Fellow, IEEE_\n\n\n\n_**Abstract**_ **—With extensive pre-trained knowledge and high-level**\n**general capabilities, large language models (LLMs) emerge as a**\n**promising avenue to augment reinforcement learning (RL) in as-**\n**pects such as multi-task learning, sample efficiency, and high-level**\n**task planning. In this survey, we provide a comprehensive review**\n**of the existing literature in** _**LLM-enhanced RL**_ **and summarize**\n**its characteristics compared to conventional RL methods, aiming**\n**to clarify the research scope and directions for future studies.**\n**Utilizing the classical agent-environment interaction paradigm,**\n**we propose a structured taxonomy to systematically categorize**\n**LLMs’ functionalities in RL, including four roles: information**\n**processor, reward designer, decision-maker, and generator. For**\n**each role, we summarize the methodologies, analyze the specific**\n**RL challenges that are mitigated, and provide insights into future**\n**directions. Lastly, comparative analysis of each role, potential**\n**applications, prospective opportunities and challenges of the**\n_**LLM-enhanced RL**_ **are discussed. By proposing this taxonomy, we**\n**aim to provide a framework for researchers to effectively leverage**\n**LLMs in the RL field, potentially accelerating RL applications in**\n**complex applications such as robotics, autonomous driving, and**\n**energy systems.**\n\n\n_**Index Terms**_ **—Reinforcement learning (RL), large language**\n**models (LLM), vision-language models (VLM), multimodal RL,**\n**LLM-enhanced RL.**\n\n\nI. INTRODUCTION\n\n\n_Corresponding author: Huan Zhao, Junhua Zhao._\nYuji Cao and Yue Chen are with the Department of Mechanical and Automation Engineering, The Chinese University of Hong\nKong, Hong Kong SAR, 999077, China (email: yjcao@mae.cuhk.edu.hk,\nyuechen@mae.cuhk.edu.hk)\nHuan Zhao and Jinyue Yan are with the Department of Building Environment and Energy Engineering, The Hong Kong Polytechnic University, Hong Kong, 100872, China (email: huan-paul.zhao@polyu.edu.hk, jjerry.yan@polyu.edu.hk)\nYuheng Cheng, Guolong Liu, and Junhua Zhao are with the School of\nScience and Engineering, The Chinese University of Hong Kong, Shenzhen,\n518172, China, and also with the Center for Crowd Intelligence, Shenzhen Institute of Artificial Intelligence and Robotics for Society (AIRS),\nShenzhen, 518129, China (email: yuhengcheng@link.cuhk.edu.cn, liuguolong@cuhk.edu.cn, zhaojunhua@cuhk.edu.cn)\nTing Shu is with Guangdong-Hongkong-Macao Greater Bay Area Weather\nResearch Center for Monitoring Warning and Forecasting (Shenzhen Institute of Meteorological Innovation), Shenzhen, 518125, China. (email:\nshuting@gbamwf.com)\nGaoqi Liang is with the School of Mechanical Engineering and Automation,\nHarbin Institute of Technology, Shenzhen, 518055, China (e-mail: lianggaoqi@hit.edu.cn)\nYun Li is with the Shenzhen Institute for Advanced Study, University of\nElectronic Science and Technology of China, Shenzhen, 518110, China, and\nalso with i4AI Ltd., WC1N 3AX London, U.K (email: Yun.Li@ieee.org)\n\n\n# R EINFORCEMENT learning (RL) is a powerful learningparadigm that focuses on control and decision-making,\n\nwhere an agent learns to optimize a specified target through\ntrial and error interactions with the environment. Traditional\n\nRL methods, however, often struggled with high-dimensional\nstate spaces and complex environments [1]. The integration\nof deep learning techniques with RL, known as deep RL, has\nled to significant breakthroughs. In 2015, Deep Q-Networks\n(DQN) [2] marked a turning point, demonstrating human-level\nperformance on Atari games using raw pixel inputs. Subsequent innovations such as proximal policy optimization [3]\nand soft actor-critic [4] have further expanded the capabilities\nof deep RL. In different realms, deep RL algorithms have\nachieved promising performance, such as real-time strategy\ngames [5], [6], board games [7], [8], energy management [9]\nand imperfect information games [10], [11]. Concurrent advancements in natural language processing (NLP) and computer vision (CV) [12], [13], have fostered new RL paradigms,\nsuch as language-conditional RL [14], which uses natural\nlanguage to instruct agents, and vision-based RL [15], where\nagents learn from high-dimensional visual inputs.\nThe integration of language and vision capabilities into deep\nRL has introduced new challenges, as the agent has to learn\nthe high-dimensional features and a control policy jointly. To\nreduce the burden of visual feature learning, reference [16] decoupled representation learning from RL. To handle languageinvolved tasks, a survey [17] called for potential uses of NLP\ntechniques in RL. Nevertheless, the capabilities of language\nmodels were limited at that time and the following four challenges still have not been addressed: 1) _Sample inefficiency_ :\nLanguage and vision tasks involve large, complex state-action\nspaces, making it challenging for RL agents to learn effective\npolicies. Moreover, agents must understand tasks and connect\nthem to corresponding states, necessitating even more extensive interactions [18], [19], [20]. 2) _Reward function design_ : In\nlanguage and vision tasks, designing effective reward functions\nis particularly challenging. These functions must capture subtle\nlinguistic nuances and complex visual features, significantly\nincreasing the complexity of an already difficult process.\nMoreover, aligning rewards with high-level task objectives in\nthese domains often requires domain expertise and extensive\ntrial-and-error [21], [22], [1], [23]. 3) _Generalization_ : RL\nagents often overfit to training data, especially in vision-based\nenvironments, leading to poor performance when deployed in\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 2\n\n\n\nstates with interventions (e.g., added noise). Agents must learn\ninvariant features robust to such interventions, enabling generalization across varied linguistic contexts and visual scenes.\nHowever, the complexity of these domains makes extracting\nsuch features and adapting to new environments particularly\nchallenging [24], [25]. 4) _Natural language understanding_ :\ndeep RL faces difficulties in natural language processing and\nunderstanding scenarios, where the nuances and complexities\nof human language present unique challenges that are not\nadequately addressed by current RL methodologies [26].\nThe field of NLP has undergone a revolutionary transformation since the introduction of the Transformer architecture\n\nin 2017 [12]. This breakthrough paved the way for the development of Large Language Models (LLMs), with landmark\nmodels such as BERT [27], GPT [28], and more recent iterations such as GPT-3 [29] and PaLM [30] marking significant\nmilestones. The emergence of these LLMs has demonstrated\npowerful capabilities across various real-world applications,\nincluding medicine [31], chemical research [32], energy system [33], [34], [35] and embodied control in robotics [36].\nThese models have not only advanced the field of NLP but\nalso shown remarkable potential in tackling complex, multidisciplinary challenges. Compared to small language models,\nLLMs have emergent capabilities that are not present in small\nlanguage models [37], such as in-context learning [38], reasoning ability [39] etc. Additionally, leveraging the vast amounts\nof training data, pre-trained LLMs are equipped with a broad\nspectrum of world knowledge [40]. Benefiting from these\ncapabilities, the applications of language models have been\nshifted from language modeling to task-solving, ranging from\nbasic text classification and sentiment analysis to complex\nhigh-level task planning [41] and decision-making [42], [43].\nWith emergent capabilities, the potential of LLMs to address\nthe inherent challenges of RL has recently gained popularity [44], [45]. The capabilities of LLMs, particularly in\nnatural language understanding, reasoning, and task planning,\nprovide a unique approach to solving the above-mentioned\nRL issues. For sample inefficiency, reference [46] proposed\na framework where LLMs can be employed to improve the\nsample efficiency of RL agents by providing rich, contextually\ninformed predictions or suggestions, thereby reducing the need\nfor extensive environment interactions. For reward function\n\ndesign, LLMs can aid in constructing more nuanced and\neffective reward functions, enhancing the learning process by\noffering a deeper understanding of complex scenarios [47]. For\ngeneralization, reference [48] proposed a framework that leverages language-based feedback for improving the generalization\nof RL policy in unseen environments. For natural language\nunderstanding, Pang _et al._ [49] used LLMs to translate\ncomplex natural language-based instructions to simple taskspecified languages for RL agents. These works show that\nLLM is a promising and powerful role that can contribute to\nthe longstanding RL challenges.\nDespite the advancements in the domain of integrating\nLLMs into the RL paradigm, there is currently a notable\nabsence of comprehensive review in this rapidly evolving area.\nAdditionally, though various methods are proposed to integrate\nLLMs into the RL paradigm, there is no unified framework for\n\n\n\nsuch integration. Our survey paper seeks to fill these gaps by\nproviding an extensive review of the related literature, defining\nthe scope of the novel paradigm called _LLM-enhanced RL_, and\nfurther proposing a taxonomy to categorize the functionalities\nof LLMs in the proposed paradigm.\n\n\n_A. Contributions_\n\n\nThis survey makes the following contributions:\n\n\n_• LLM-enhanced RL paradigm_ : This paper presents the first\ncomprehensive review in the emerging field of integrating\nLLM into the RL paradigm. To clarify the research scope\nand direction for future works, we define the term _LLM-_\n_enhanced RL_ to encapsulate this class of methodologies,\nsummarize the characteristics and provide a corresponding framework that clearly illustrates 1) how to integrate\nLLMs in classical agent-environment interaction and 2)\nthe multifaceted enhancements that LLMs offer to the\n\nconventional RL paradigm.\n\n_• Unified taxonomy_ : Further classifying the functionalities\nof LLMs in the LLM-enhanced RL paradigm, we propose\na structured taxonomy to systematically categorize LLMs\nwithin the classical agent-environment paradigm, where\nLLMs are classified as information processors, reward\ndesigners, decision-makers, and generators. By such a\ncategorization, a clear view of how LLMs integrate into\nthe classical RL paradigm is offered.\n\n_• Algorithmic review_ : For each role of LLM, we review\nemerging works in this direction and discuss different\nalgorithmic characteristics from the perspective of capabilities. Based on this foundation, future applications,\nopportunities, and challenges of LLM-enhanced RL are\nanalyzed to provide a potential roadmap for advancing\nthis interdisciplinary field.\n\n\n_B. Text Organization_\n\n\nThe remaining sections are organized as follows. Section II provides foundational knowledge of both RL and\nLLM. Section III presents the concept of LLM-enhanced\nRL and provides its overall framework. Following this, Sections IV, V, VI, and VII offer an in-depth analysis of LLMs\nwithin the RL context, exploring their roles as information\nprocessor, reward designer, decision-maker, and generator,\nrespectively. Last, Section VIII discusses the application,\nopportunities and challenges of LLM-enhanced RL. Finally,\nSection IX concludes the survey.\n\n\nII. BACKGROUND\n\n\nIn this section, we provide a concise overview of the classical RL paradigm and related challenges. Next, we explore the\nprevailing trend in RL—specifically, the fusion of multimodal\ndata sources, including language and visual information. Following this, we offer an introductory background on LLMs and\noutline the key capabilities that can enhance the RL learning\nparadigm.\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 3\n\n\n\n_A. Background of Reinforcement Learning_\n\n\n_1) Classical Reinforcement Learning:_ In a classical RL\nparadigm shown in Fig. 1, the agent interacts with an environment through a trial-and-error process to maximize the\nspecified rewards in the trajectory. In each step, the agent\ntakes an action _a_ based on the observed state _s_ from the\n\nenvironment. By optimizing the policy _π_ (action controller),\nthe agent maximizes the cumulative rewards. Such an optimization problem is usually formalized through the concept\nof Markov Decision Process (MDP), defined by the quintuple\n_⟨S, A, T, R, γ⟩_ . Here, _S_ denotes a set comprising all possible\nstates, _A_ denotes a set of all possible actions, _T_ represents the\nstate transition probability function _T_ : _S × A × S →_ [0 _,_ 1],\n_R_ is a reward function _R_ : _S × A × S →_ R, and _γ_ (with\n0 _≤_ _γ ≤_ 1) is the discount factor. The objective in RL is to\noptimize the policy _π_ ( _a|s_ ) such that the cumulative returns\n� _∞k_ =0 _[γ][k][r][k]_ [+1][ is maximized.]\n\n\nFig. 1. Classical reinforcement learning paradigm.\n\n\n_2) Challenges of Reinforcement Learning:_ While RL\nalgorithms have made remarkable performance in recent\nyears [2], [7], [50], there are still several longstanding challenges that limit the real-world applicability of RL:\n\n\n_• Generalization in Unseen Environment_ : Generalization in\nunseen environments remains a significant challenge in\nthe field of RL [51]. The core issue lies in the ability of\nRL algorithms to transfer learned knowledge or behaviors\nto new, previously unseen environments. RL models are\noften trained in simulated or specific settings, excelling\nin those scenarios but struggling to maintain performance\nwhen faced with novel or dynamic conditions. This\nlimitation hinders the practical application of RL in realworld situations, where environments are rarely static or\nperfectly predictable. Achieving generalization requires\nmodels to not only learn specific task solutions but also\nto understand underlying principles that can be adapted\nto a range of situations.\n\n_• Reward Function Design_ : Reward function is the principal contributing factor to the performance of an RL agent.\nDespite their fundamental importance, reward functions\nare known to be notoriously difficult to design, especially\nin contexts involving sparse reward environments and\ncomplex scenarios [1]. In sparse reward settings, where\nfeedback is limited, reward shaping becomes essential\nto guide agents toward meaningful behaviors; however,\nthis introduces the risk of inadvertently biasing the agent\ntowards sub-optimal policies or overfitting to specific\nscenarios [52], [53]. Conversely, for complex tasks, highperformance reward functions usually require massive\n\n\n\nmanual trial-and-error since most designed rewards are\nsub-optimal [23] or lead to unintended behavior [54].\n\n_• Compounding Error in Model-based Planning_ : Modelbased RL is prone to the issue of compounding errors\nduring planning. As the prediction horizon extends, errors\nin the model’s predictions accumulate, leading to significant deviations from optimal trajectories [55], [56]. This\nproblem is particularly acute in complex environments\nwith high-dimensional state spaces. With their advanced\npredictive capabilities and understanding of sequential\ndependencies, LLMs could help mitigate these errors,\nleading to more accurate and reliable planning in modelbased RL.\n\n\n_• Multi-task Learning:_ Multi-task RL faces several key\nchallenges that limit its effectiveness. One major issue is\nmanaging varying task difficulties, where simpler tasks\ncan overshadow learning of more complex ones, leading\nto negative transfer [57]. Task interference is another\ncritical problem, as shared parameters or data between\ntasks can result in suboptimal performance on individual\ntasks [58]. Determining optimal parameter-sharing strategies is complex, as it must balance learning efficiency\nwith task-specific requirements [59]. Sample efficiency\nremains a significant hurdle, with traditional data-sharing\napproaches not fully leveraging learned behaviors across\ntasks [60]. Finally, effectively transferring knowledge\nbetween tasks without negative interference is an ongoing\nchallenge that impacts the agent’s ability to accelerate\nlearning across multiple objectives [61].\n\n\n_3) Multimodal Reinforcement Learning:_ With the advances\nin both CV and NLP, pattern recognition in vision and natural\nlanguage has become increasingly powerful, and multimodal\ndata has been involved in the RL paradigm recently. Visual\ndata is commonly involved in the observation space of RL\nwhen agents receive image-based information from the environment, e.g. in applications such as robots [62], video game\ncontrol [2] etc. Compared to visual data, natural languages\nare usually included when RL agents are given specific tasks\nwhen interacting with the environments. The use of natural\nlanguages in RL can be divided into the following two\ncategories [17]:\n\n\n_• Language-conditional RL_ : In language-conditional RL,\nthe problem itself requires the agent to interact with\nthe environment through language. Specifically, there are\ntwo ways to integrate natural language in RL: 1) _task_\n_description_ : the task or instruction is described in natural\nlanguages, _e.g._ instruction following, where the agents\nlearn to interpret the instructions first and then execute\nactions; 2) _action space or observation space_ : natural\nlanguage is part of the state and action space, _e.g._ . text\ngames, dialogue systems, and question answering (Q&A).\nThis class of RL leverages natural language as a direct\ncomponent of the RL process, guiding the agent’s actions\nand decisions within the language environment.\n\n_• Language-assisted RL_ : In language-assisted RL, natural\nlanguage is used to facilitate learning but not as a\npart of problem formulation. Two usages of language\n\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-2-0.png)\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 4\n\n\n\nassisted RL are: 1) _communicating domain knowledge_ :\nthe text containing task-related information can be helpful\nfor agents. Therefore, wikis and manuals related to the\nenvironment can potentially assist the agents in such\ncases; 2) _structuring policies_ : structuring policies is to\ncommunicate information about the state or dynamics\nof the environment based on language instead of representations of the environment or models. In such cases,\nlanguage can be leveraged to shape representations towards a generalizable abstraction, such as using “avoid\nhitting the wall” instead of representations of a policy.\nThis approach represents a more indirect use of natural\nlanguage, serving as a guide or enhancer to the primary\nRL tasks.\n\nThe integration of multimodal data challenges the RL\nparadigm since the agent has to simultaneously learn how to\nprocess complex multimodal data and optimize the control\npolicy in the environment [16]. Issues such as natural language understanding [63] and visual-based reward function\ndesign [64] require to be addressed.\n\n\n_B. Background of Large Language Models_\n\n\nLLMs typically refer to the Transformer-based language\nmodels [65] containing billions of parameters and being\ntrained on massive text data (i.e., several terabytes (TB)\nscale) [66], such as GPT-3 [29] and LLaMA [67]. The extensive number of parameters and internet-scale training data\nenable LLMs to master a diverse array of tasks, resulting\nin enhanced capabilities in language generation, knowledge\nrepresentation, and logical reasoning, as well as improved\ngeneralization to novel tasks.\nThe development and effectiveness of LLMs are largely\ndriven by _Scaling Laws_, i.e., as these models grow in size\n– both in terms of their parameter count and the data they\nare trained on – they tend to exhibit _emergent abilities_ that\nare not present in small models [68], [69], [70], such as\nin-context learning, reasoning, and generalization. Here, we\nbriefly introduce such capabilities of LLMs in detail:\n\n\n_• In-context Learning_ : In-context learning capability eliminates the need for explicit model retraining or gradient\nupdate [29], as it can generate better responses or perform\ntasks by inputs cueing examples or related knowledge.\nSpecifically, task-related texts are included in the prompts\nas context information, helping the LLMs to understand\nthe situations and execute instructions.\n\n\n_• Instruction Following_ : Leveraging diverse task-specific\ndatasets formatted with natural language descriptions\n(also called _instruction tuning_ ), LLMs are shown to\nperform well on unseen tasks that are also described\nin the form of natural language [71], [72], [73]. Therefore, this capability equips LLMs with the ability to\ncomprehend instructions for new tasks and effectively\ngeneralize across tasks not previously encountered, even\nin the absence of explicit examples.\n\n_• Step-by-step Reasoning_ : For smaller models, tackling\nmulti-step tasks, such as solving math word problems,\noften proves to be challenging. However, large language\n\n\n\nmodels can address the complex task effectively with sophisticated prompting strategies such as Chain of Thought\n(CoT) [74], Tree of Thought (ToT) [75], and Graph\nof Thought (GoT) [76]. These strategies structure the\nproblem-solving process into sequential or hierarchical\nsteps, facilitating a more articulated and understandable\nreasoning pathway. Additionally, prompts designed for\nplanning enable LLMs to output sequences that reflect\na progression of thoughts or actions, proving invaluable\nfor tasks demanding logical sequence or decision-making\n\noutputs.\n\n\nIII. LARGE LANGUAGE MODEL-ENHANCED\n\nREINFORCEMENT LEARNING\n\n\n_A. Definition_\n\n\nRL agents are often tasked with making robust and deliberate decisions using multimodal information in real-real\napplications, whether in the MDP setting or within the context\nof specific task descriptions. Examples include robots designed\nto follow natural language instructions while navigating physical environments or visual games with tasks described in\nnatural language [77], [78], [79]. However, it is challenging for\nconventional RL methods as the agent is required to simultaneously interpret complex multimodal data and optimize control\npolicies amidst ever-changing environments [80]. Compounding these challenges are issues like sample inefficiency, the\ndifficulty of crafting reward functions that accurately reflect\nmultimodal inputs, and the need for robust generalization\nacross varied tasks and settings.\nThe rapid advancements in LLMs present a viable solution\nto these challenges, thanks to their potent natural language\nunderstanding and reasoning abilities, coupled with recent\nprogress in incorporating visual data processing [81]. This dual\ncapability enables LLMs to interpret and act upon complex\nmultimodal information effectively, serving as a robust helper\nfor enhancing the RL paradigm for real-world applications.\nNevertheless, despite the powerful functionalities of LLMs,\nthe current studies are varied and lack a standard concept\ncorrectly specifying the systematic methodology, which impedes the advancement of research in this area. Therefore, we\nintroduce the concept called _LLM-enhanced RL_ as follows:\n_**LLM-enhanced RL**_ _refers to the methods that utilize the_\n_multimodal information processing, generating, reasoning,_\n_and other high-level cognitive capabilities of pre-trained,_\n_knowledge-inherent LLM models to assist the RL paradigm._\nLLM-enhanced RL differs from traditional model-based RL\n\nby leveraging knowledge-rich LLM models. This approach\nprovides two key advantages: First, LLM equips the agent\nwith substantial pre-trained capabilities at the beginning of\nthe learning process, such as reasoning and high-level planning, etc. Second, it offers superior generalization capabilities.\nPre-trained on diverse data, LLMs can effectively transfer\nknowledge across domains, enabling better adaptation to unseen environments than conventional data-driven models. Last,\nLLM-enhanced RL addresses a key limitation of pre-trained\nmodels: their inability to interact with environments to expand\ntheir knowledge and ground themselves in specific domains.\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 5\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-4-0.png)\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-4-1.png)\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-4-4.png)\n\n\n**Specification** **Agent** **Model Simulator** **Policy Interpreter**\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-4-2.png)\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-4-3.png)\n\n\n_**Reward Designer**_ _**Decision-maker**_\n\n\nFig. 2. Framework of LLM-enhanced RL in classical Agent-Environment interactions, where LLM plays different roles in enhancing RL.\n\n\n\nThrough environmental interactions, this approach generates\ntask-specific data, grounds the LLM in particular domains\nby in-context learning, and eventually helps them adapt to\ndynamic changes with continuous learning.\n\n\n_B. Framework_\n\nThe framework of LLM-enhanced RL is illustrated in the\n\ncenter of Fig. 2, which is founded on the classical agentenvironment interaction paradigm. Along with the trial-anderror learning process, LLM processes the state information,\nredesigns the reward, assists in action selection, and interprets\nthe policy after the action selection.\nSpecifically, on the one hand, when the agent receives the\nstate and reward information from the environment, LLM\nis able to process or modify the information to either filter\nunnecessary natural language-based information or design appropriate rewards to accelerate the learning process, based on\nthe natural language understanding and reasoning capabilities.\nOn the other hand, when the agent is about to choose an\naction based on the observation, LLM can assist the action\nselection process by either simulating a world model or serving\nas the policy network to generate reasonable actions based\non the modeling capability and common-sense knowledge.\nAdditionally, after the action selection process, integrating\nstate, reward, and action information, LLM can interpret the\nunderlying possible reasons behind the policy selection, which\nhelps human supervisors understand the scenarios for further\nsystem optimization.\nBased on the functions of LLM in the framework, we extract\n\ncharacteristics of LLM-enhanced RL and further divide four\n\ndifferent LLM roles in LLM-enhanced RL, including information processor, reward designer, generator, and decision-maker,\nwhich will be elaborated in the next subsections.\n\n\n_C. Characteristics_\n\nThe LLM-enhanced RL paradigm enhances the vanilla RL\nparadigm with the following characteristics:\n\n_•_ **Multimodal Information Understanding** : LLMs enhance RL agents’ comprehension of scenarios involving\n\n\n\nmultimodal information, enabling them to learn from\ntasks or environments described in natural language and\nvision data more effectively.\n\n_•_ **Multi-task Learning and Generalization** : Benefiting\nfrom the multi-disciplinary pre-trained knowledge and\npowerful sequence modeling capability, LLMs empower\nRL agents by providing a high-capacity model capable\nof accommodating task variances and transferring knowledge across multiple tasks, assisting in handling multiple\ntasks.\n\n\n_•_ **Improved Sample Efficiency** : Given the inherent exploratory nature, the RL paradigm demands significant\nsamples to learn. Pre-trained LLM can enhance data\ngeneration by simulation or leverage the prior knowledge\nto improve the sample efficiency of RL.\n\n_•_ **Long-Horizon Handling** : RL becomes more challenging\nas the length of trajectory increases, due to the credit assignment problem. LLMs can decompose complex tasks\ndown into sub-tasks to assist RL agents in planning\nover longer temporal horizons, aiding in the decisionmaking process for complex, multi-step tasks such as the\nMinecraft game.\n\n_•_ **Reward Signal Generation** : Based on the context understanding and domain knowledge, LLMs contribute to\nthe reward shaping and reward function designing, which\nhelp guide the RL towards effective policy learning in\nsparse-reward environments.\n\n\n_D. Taxonomy_\n\n\nIn this subsection, we illustrate the different roles of LLMs\nwithin the above framework, by detailing their functions and\ncorresponding issues of RL they address:\n\n\n_•_ **Information Processor** : When observation or task de\nscription involves language or visual features, it is challenging for the agent to comprehend the complex information and optimize the control policy simultaneously.\nTo release the agent from the burden of understanding\nthe multimodal data, LLM can serve as an information\nprocessor for environment information or task instruction\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 6\n\n\n\ninformation by 1): extracting meaningful feature representations for speeding up network learning; 2) translating natural language-based environment information\nor task instruction information into formal specific task\nlanguages to reduce learning complexity.\n_Application example_ : In instruction-following RL for\nrobots, tasks can have unbounded natural language forms\ndue to users’ diverse speaking habits, which can impede\nRL learning performance. LLMs transform these varied\nnatural language instructions into a unique task language,\nenabling more robust RL performance [49].\n\n_•_ **Reward Designer** : In complex task environments where\nthe reward is sparse or a high-performance reward function is hard to define, using the prior world knowledge,\nreasoning abilities, and code generation ability, LLM can\nserve as two roles: 1) an implicit reward model to provide\nreward values based on the environment information, either by training or prompting; 2) an explicit reward model\nthat generates executable codes of reward functions that\ntransparently specifies the logical calculation process of\nreward scalars based on the environment specifications\nand language-based instructions or goals.\n_Application example_ : For complex robotic control problems, such as dexterous manipulation, reward design\nrequires both expertise and trial and error. LLM designs\nreward functions based on knowledge and iteratively\nimproves it based on the performance [82].\n\n_•_ **Decision-maker** : RL faces challenges such as sample\ninefficiency and exploration inefficiency. To address these\nchallenges, LLMs can be leveraged as decision-makers\nin RL, offering promising solutions through two main\napproaches: 1) action-making: LLMs treat offline RL\nas a sequence modeling problem, using rewards for the\nconditional generation of actions. Pretrained on diverse,\ninternet-scale data, LLMs possess advanced semantic\nunderstanding capabilities, which can be exploited to\naccelerate offline RL learning. 2) action-guiding: LLMs\nact as expert instructors to produce a reduced set of\naction candidates or expert actions. The action candidates\nconstrain the original action space, thereby enhancing\nexploration efficiency. Expert actions encapsulate prior\nknowledge from LLMs. When incorporated to regularize\nthe policy learning, this expert knowledge is distilled into\nan RL agent, resulting in better sample efficiency.\n_Application example_ : In embodied robot, given humaninstruction and language-based description, LLM generates potential actions for the robot to choose from [83].\n\n_•_ **Generator** : Model-based RL hinges on precise world\nmodels to learn accurate environment dynamics and simulate high-fidelity trajectories. Additionally, interpretability\nremains another important issue in RL. Using the multimodal information understanding capability and prior\ncommon-sense reasoning ability, LLMs can be 1) a\ngenerator to generate accurate trajectories in model-based\nRL; 2) generate policy explanations with the prompts of\nrelated information in explainable RL. _Application exam-_\n_ple_ : In Minecraft item crafting, LLMs generate Abstract\nWorld Models—hypothesized sequences of subgoals for\n\n\n\na given task. These LLM-generated world models guide\nRL agents’ exploration and learning and the RL agent\nverifies and corrects the world model through gameplay,\ncombining LLM knowledge with grounded experience to\nachieve an order of magnitude improvement in sample\nefficiency over traditional methods [84].\n\n\nIV. LLM AS INFORMATION PROCESSOR\n\n\nThe normal way for deep RL with language or visual\ninformation is to jointly process the information and learn\na control policy, end-to-end. However, this demands the RL\nagent to learn to comprehend the information and manage\nthe task simultaneously. Additionally, learning the language\nor visual features by simply relying on the reward function is\nchallenging and may narrow the learned features to a narrow\nutility, hampering the generalization ability of the agent [16].\nWith the advances in unsupervised techniques and largescale pre-trained models for CV and NLP, the decoupled\nstructure, where the encoders are separately trained, has gained\npopularity [16], [85], [86]. Utilizing the powerful representation ability and prior knowledge, the pre-trained LLM or\nvision-language model (VLM) model can serve as an information processor for RL. They can extract observation representations for downstream networks or translate natural language\ninto formal specifications, enabling the execution of multiple\ntasks. This multi-task capability improves sample efficiency\nand zero-shot performance, allowing agents to generalize\neffectively across diverse and sparse-reward environments.\n\n\n_A. Feature Representation Extractor_\n\n\nAdopting the large pre-trained models in CV and NLP, the\nlearned feature representation can be a scaffold embedding\nfor downstream network learning and increase the sample\nefficiency. As illustrated in Fig. 3 (i), the usages can be further\ndivided into two categories according to whether the model is\ntrained simultaneously. One way is to directly use the frozen\npre-trained model to extract embeddings from the observation\n_Ot_ and another way is to further fine-tune the pre-trained\nmodel using contrastive learning with a contrastive loss _L_ _[c]_ _t_\nto achieve better adaptation in new environments.\n_1) Frozen Pre-trained Model:_ Using the frozen large-scale\npre-trained model is the straightforward way. In reference [87],\nthe author proposed History Compression via Language Models (HELM) to utilize a frozen pre-trained Language Transformer to extract history representation and compression and\nthus addresses the problem of partially observed MDP by\napproximating the underlying MDP with the past representation. Specifically, the framework first uses FrozenHopfield, a\nfrozen associative memory to map observations [ _ot−_ 2 _, ot−_ 1 _, ot_ ]\nto a compressed representation _ht_ and then concatenate it\nwith a learned encoding of the current observation via a\nconvolutional neural network. Such a method solves the problem of how to effectively utilize the compressed history for\npolicy optimization. After that, Semantic HELM [88] proposed\na human-readable memory mechanism that summarizes past\nvisual observations in human language and uses multimodal\nmodels to associate visual inputs with language tokens. The\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 7\n\n\n_2) Fine-tuning Pre-trained Model:_ When trained RL agents\nare deployed in real-world applications, their performance\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-6-0.png)\n\nActor-Critic often deteriorates under significant appearance variations (out\nNetwork of-distribution data) due to overfitting to training scenarios. For\n\nInvariantFeature example, robots with vision-based navigation tasks may fail\n\nwhen the environment color changes. Invariant feature repre\nLLM / VLM sentations serve as a form of state abstraction that remains con\nFine-tune Encoder sistent across out-of-distribution appearance variations such\n\naccording to the task as added noise, brightness changes, or slight rotations. When\n\nencountering appearance changes or out-of-distribution data,\nthough the observation changed, the representation (feature\nembedding) that fed into the policy/value network is nearly\n\n**Instruction Information Translator** unchanged, leading to robust RL performance and increased\n\ngeneralization in unseen environments.\nContrastive learning is a common way to learn the invariant\n\ndirectly exposed to NL feature representation. It learns representations from high\ndimensional data by contrasting positive examples against\nnegatives. Given a query _q_, the goal is to match query _q_\n\n[0 1 0] more closely to a positive key _k_ + than to any negative keys\n\nK _\\ {k_ + _}_ in a set K. This process is modeled using similarity\n\ntask language translation measures, such as the dot product ( _q_ _[T]_ _k_ ) or the bilinear\n\nproduct ( _q_ _[T]_ _Wk_ ), where _W_ is a weight matrix. To effectively\n\n**Environment Information Translator** learn these representations, a contrastive loss function such as\n\nInfoNCE [91] is used:\n\n\n\nFig. 3. LLM as an information processor. (i) Feature Representation Extractor:\nfrozen/fine-tuned LLM extracts meaningful representations for downstream\nRL networks. In the fine-tuning process, given observation ( _Ot_ ), invariant\nfeature abstraction ( _S_ [˜] _t_ ) is learned with the contrastive loss ( _L_ _[c]_ _t_ [). Then,]\nthe invariant is fed into the actor-critic network. After fine-tuning, given\ndifferent observations ( _Ot_ ) and ( _Ot_ _[′]_ [) with appearance variation, the extracted]\nrepresentation is invariant, leading to robust RL performance. (ii) Language\nTranslator: LLM interprets diverse natural language inputs, converting them\ninto a standardized, task-specific format that the RL agent can efficiently\nprocess and act upon.\n\n\nmemory is a semantic database _S_ constructed by encoding\nprompt-augmented tokens from the vocabularies of Contrastive\nLanguage-Image Pre-training (CLIP) [89] and the pre-trained\nlanguage models. Given an observation _ot_, the agent retrieves\ntop- _k_ embeddings from _S_ as actor-critic input to assist the\npolicy optimization. Such a memory mechanism provides a\nhuman-readable representation of the past and helps the agent\nto cope with partially observable environments. In the experiment, they used the proximal policy optimization (PPO) [3]\nalgorithm and pretrained Transformer XL [90] model. When\ntesting on the partially observable environments, they found\nthe extracted semantics help the memory-less agent obtain\ncomparable scores with memory-based methods trained on\nlong trajectories. However, one limitation of such frozen pretrained models is that the representations cannot dynamically\nadjust according to the task and environment.\n\n\n\nexp( _q_ _[T]_ _Wk_ +)\n_L_ _[c]_ = log (1)\n\nexp( _q_ _[T]_ _Wk_ +) + ~~[�]~~ _[K]_ _k_ _[−]_ [1] exp( _q_ _[T]_ _Wki_ )\n\n\nwhere exp is the exponential symbol and the whole _L_ _[c]_ can\nbe viewed as the log-loss of a softmax classifier, treating the\nmatching of _q_ to _k_ + as a multi-class classification problem\namong _K_ classes. By maximizing alignment between different\nchanges of the same observation via the above loss, the model\ncan learn the invariant representations.\nWhen combining with RL, given different RL tasks, the required invariant feature representations should be adjusted accordingly. Therefore, researchers have explored different ways\nto improve contrastive learning. In reference [92], to achieve\nthe zero-shot capability of embodied agents, the author devised\na visual prompt-based contrastive learning framework that uses\na pre-trained VLM to learn the visual state representations.\nThe visual prompts are learned on expert demonstrations from\ndomain factors such as camera settings and stride length. By\ncontrastively training the VLM on a pool of visual prompts\nalong with the RL policy learning process, the learned representations are robust to the variations of environments, leading\nan increase of 18-20% success rates on unseen scenarios and\n\nimproved generalization capability. Based on the contrastive\nlearning, another method ReCoRe [93] added an interventioninvariant regularizer in the form of an auxiliary task such as\ndepth prediction and image denoising to explicitly enforce\ninvariance of learned representations to environment changes.\n\n\n_B. Language Translator_\n\n\nThe unbounded and diverse representation of natural languages in both human instruction and environmental information impedes policy learning. LLM can be leveraged as\na language translator to reduce the additional burden of\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 8\n\n\n\ncomprehending natural language for RL agents and increase\nsample efficiency. As illustrated in Fig. 3 (ii), LLM transforms\nthe diverse and informal natural language information into\nformal task-specific information, such as feature representation\nor task-specific languages, thus assisting the learning process\nof the RL agent.\n_1) Instruction Information Translation:_ One application\nof LLM is to translate natural language-based instructions\nfor instruction-following applications. In reference [49], the\nauthor investigated an _inside-out_ scheme for natural languageconditioned RL by training an LLM that translates the natural\nlanguage to a task-related unique language. Such an insideout scheme prevents the policy from being directly exposed to\nnatural language instructions and helps efficient policy learning. Another literature _STARLING_ [94] used LLM to translate\nnatural language-based instructions to game information and\nexample game metadata. The translated information is then\nfed forward to _Inform7_, an interactive fiction game engine, to\ndevelop a large amount of text-based games for RL agents to\nmaster the desired skills.\n\n_2) Environment Information Translation:_ On the other\nhand, LLM can also be used to translate the natural language\nenvironment information into formal domain-specific language\nthat can specify MDP information, which converts natural\nlanguage sentences into grounded usable knowledge for the\nagent. Previous works generally ground natural language into\nindividual task components such as task objectives description [95], rewards [96] and polices [97], [98]. To unify the\ninformation about all the components of a task, [99] introduces\n_RLang_, a grounded formal language capable of expressing\ninformation about every element of an MDP and solutions such\nas policies, plans, reward functions and transition functions.\nBy using LLM to translate natural language to RLang and train\nRL agents upon RLang, the agents are capable of leveraging\ninformation and avoid having to learn _tabula rasa_ .\n\n\n_C. Summarization and Outlook_\n\n\nFor information processing, LLM is used to accelerate RL\nlearning processing by decoupling the information processing\ntask and the controlling task, where LLM extracts feature\nrepresentations or handles the natural language-based information.\n\nWhen multimodal data are involved in the environment,\ne.g., robot manipulation tasks, the information processing task\nbecomes more challenging. For one thing, the misalignment\nor contradiction between different modalities may exist [100].\nModality weighting techniques that adaptively learn the importance of different modalities by attention mechanisms provide\na potential solution for this problem [101]. In addition, CLIPbased multimodal foundation models using large-scale imagetext pairs have shown outstanding zero-shot ability in various\nmultimodal tasks [89]. By learning cross-modal and task\nsemantics, the misaligned modality can be replaced with a\nvirtually generated modality [102]. For another, how to effectively combine the information between different modalities\ninto a unified representation, i.e., multimodal fusion, is also an\nimportant issue. In reference [103], a multimodal contrastive\n\n\n\nlearning with attention mechanisms, which learns the intra\nand inter-modal representations, was proposed. The different\nattention heads align the agreement from one modality to\nanother and vice versa, providing an informative representation\nfor the RL agent. However, most multimodal learning methods\ndo not consider the task information and only focus on the\nmodality alignment, remaining an area to be explored.\nIn the following, we list potential directions for future\nresearch.\n\n\n_• Feature Representation Extractor_ : Although the use of\nLLMs as feature representation extractors has shown\npromise in enhancing RL, several challenges persist in\nfuture research. Short-term goals include developing a\ncomputationally efficient feature extractor and improving\nthe generalization of LLM-derived representations. In the\nlong term, researchers should focus on exploiting task\ncompositionality for better generalization and creating\nadaptive extraction methods for diverse control tasks.\n\n_• Language Translator_ : Current existing works are still\nlimited. Short-term objectives involve exploring LLMs’\nability to handle more tasks and improving translation\nefficiency and accuracy in RL contexts. Long-term goals\ninclude developing multimodal translation capabilities\nand integrating these with RL algorithms, achieving a\nmore general translator, and helping agent learning.\n\n\nV. LLM AS REWARD DESIGNER\n\n\nThe reward signal is the most important information to\ninstruct agent learning in RL [1]. However, despite the fundamental importance, high-performing reward functions are\nknown to be notoriously difficult to design [104]. First,\nspecifying human notions of desired behavior is difficult via\ndesigned reward functions or requires huge expert demonstrations. Moreover, dense rewards that accurately provide\nlearning signals require either manually decomposing the\ngeneral goal into sub-goals [105] or rewarding interesting\nauxiliary objectives [106]. Nevertheless, both of these methods\nsuffer from the need for expert input and meticulous manual\ncrafting [23].\nBenefiting from pre-trained common-sense knowledge, code\ngeneration, and in-context learning ability, LLM has the potential to design or shape reward functions for DRL by leveraging\nnatural language-based instructions and environment information. In this section, we review recent literature in which LLMs\nact as reward models that implicitly provide reward values or\nexplicitly write executable reward function codes detailing the\ncalculation process of reward scalars.\n\n\n_A. Implicit Reward Model_\n\n\nA large pre-trained model can be an implicit reward model\nthat directly provides auxiliary or overall reward value based\non the understanding of task objectives and observations. The\nmethods are illustrated in Fig. 4 (i). One way is by directly\nprompting with language descriptions and another way is by\nscoring the alignment between the feature representation of\nthe visual observations and language-based instructions.\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 9\n\n\nbetween agent and environment.\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-8-0.png)\n\n\n_**Directly Prompting**_ _**Alignment Scoring**_ **Algorithm** **1** Language Reward Modulated Pretraining\n\n(LAMP) in [110]\n\n1: Initialize parameters for Masked World Models (MWM)\n\nPrompting Similarity Language 2: Load pretrained DistilBERT [111] for language prepro\n\n3: Load pretrained R3M visual encoder and score predictor\n\nReward 4: Initialize empty replay buffer\n\nVisual 5: Populate language prompt buffer and synonym buffers\n\nwith predefined samples\n\n7: Randomize scene textures from Ego4D and RLBench\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-8-1.png)\n\n\n`self.current = ...` 10: Generate and process language embeddings using Dis\n\n\nFig. 4. LLM as a reward designer. (i) Implicit Reward Model: LLMs\nprovide rewards through direct prompting or alignment scoring between\nlanguage instructions and visual observations. (ii) Explicit Reward Model:\nLLMs generate executable code for reward functions, with potential for selfrefinement through evaluation loops.\n\n\n_1) Direct Prompting:_ Reference [107] simplified the manual reward design by prompting an LLM as a proxy reward\nfunction with examples of desirable behaviors and the preferences description of the desired behaviors. Reference [108]\nproposed a Read and Reward framework that utilizes LLMs\nto read instruction manuals to boost the learning policies\nof specific tasks. The framework includes a Question &\nAnswer (QA) extraction module for information retrieval\nand summarization and a Reasoning module for evaluation.\nExperimentally, they show RL algorithms, by their design, can\nobtain significant improvement in performance and training\nspeed. In reference [45], Carta _et al._ proposed an automated\nreward shaping method where the agent extracts auxiliary\nobjectives from the general language goal. Using a question\ngeneration and QA system, the framework guides the agent\nin reconstructing partial information about the global goal\nand provides an intrinsic reward signal for the agent. This\nintrinsic reward incentivizes the agent to produce trajectories\nthat help them reconstruct the partial information about the\ngeneral language goal. To acquire a generalizable policy by\ncontinually learning a set of tasks is challenging for RL agents\nsince the agent is required to retain the previous knowledge\nwhile quickly adapting to new tasks. Reference [109] introduced the Lafite-RL (Language agent feedback interactive\nRL) framework that provides interactive rewards mimicking\nhuman feedback based on LLMs’ real-time understanding of\nthe agent’s behavior. By designing two prompts, one to let\nLLM understand the scenario and the other to instruct it about\n\nthe evaluation criterion, their framework can accelerate the RL\nprocess while freeing up human effort during the interaction\n\n\n\n12: Assign LAMP rewards using R3M score predictos\n13: Update buffers and train MWM with augmented rewards\n\n14: **end for**\n\n\n_2) Alignment Scoring:_ For visual RL, some literature utilizes vision-language models as reward models to align multimodal data and calculate the similarity score using metrics\nsuch as cosine similarity. Rocamonde _et al._ employs the\nCLIP model as a zero-shot reward model to specify tasks via\nnatural language [64]. They first compute the probability _pot,l_\nthat the agent achieves a goal given by language description\n_l_ out of a set of potential goals _l_ _[′]_ _∈L_ in the task set\n_L_ using the softmax computation with temperature _τ_ over\nthe cosine similarity between visual state embeddings _fθ_ ( _ot_ )\nand language description embeddings _gθ_ ( _l_ ) across the set of\npotential goals _l_ _[′]_ :\n\n\nexp( _fθ_ ( _ot_ ) _·_ _gϕ_ ( _l_ ) _/τ_ )\n_pot,l_ = ~~�~~ _l_ _[′]_ [ exp(] _[f][θ]_ [(] _[o][t]_ [)] _[ ·][ g][ϕ]_ [(] _[l][′]_ [)] _[/τ]_ [)] _[.]_ (2)\n\n\nThen the reward is obtained by a binary reward function _rt_ =\n_r_ ( _ot_ +1 _, l_ ) = I[ _pot_ +1 _,l ≥_ _β_ ], which thresholding the probability.\nTheir framework only requires a single-sentence text prompt\ndescription of the desired task with minimal prompt engineering. Another work [112] constructed reward signals based on\nthe similarity between natural language-based description and\nembeddings from the pre-trained VLM encoder. By labeling\nthe expert demonstration with the reward signals, the framework effectively mitigates the problem of mis-generalization.\nIn reference [110], the authors proposed the Language Reward\nModulated Pretraining (LAMP) framework as a pertaining\nutility for RL as opposed to a downstream task reward to\nwarm-start sample-efficient learning. The framework leverages\nfrozen, pre-trained VLMs such as R3M [113] to generate\nnoisy, albeit shaped exploration rewards by computing the\nalignment score between instructions and image observations.\nThe algorithm is presented in Algorithm 1. They used Masked\nWorld Model [114], a visual model-based RL algorithm for\nrobot manipulation based on image and instructions. The\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 10\n\n\n\nimages were downloaded from Ego4D [115] and the language\ninstructions were obtained by querying ChatGPT. The reward\nis then calculated from the R3M alignment score. After that,\nthe generated rewards are optimized with standard noveltyseeking exploration rewards for language-conditioned policy.\nReference [116] explored preference-based RL, where the\nagent learns a reward function from preference labels over\nthe behaviors. A VLM is leveraged to generate preference\nlabels given visual observations and a text description of the\ntask goal. The evaluation is based on a series of vision-based\nmanipulation tasks. Results suggested that prompting VLMs\nto produce preference labels for reward learning leads to better\nperformance, in contrast to treating them as reward functions\nto produce raw reward scores.\n\n\n_B. Explicit Reward Model_\n\nAnother way to design reward functions is by generating\nexecutable codes that explicitly specify the details of the calculation process, as illustrated in Fig. 4 (ii). Compared to the\nimplicit reward value provision, this explicit way transparently\nreflects the reasoning and logical process of LLMs and thus\nis readable for humans to further evaluate and optimize.\nTo help robots learn low-level actions, reference [117]\nharnessed the code generation ability of LLMs to define the\nlower-level reward parameters based on high-level instructions.\nUsing such a reward design paradigm, their work bridges\nthe gap between high-level language instructions to low-level\nrobot actions and can reliably tackle 90% of the designed\ntasks compared to the 50% of the baseline. Motivated by the\ncapability of LLM for self-refinement [118], reference [119]\nproposed a framework with a self-refinement mechanism for\nautomated reward function design, including initial design,\nevaluation and self-refinement loop. Their results indicate that\nthe LLM-designed reward functions are able to rival or surpass\nmanually designed reward functions. Similarly, _Eureka_ [82]\ndeveloped a reward optimization algorithm with self-reflection.\nThe algorithm is outlined in Algorithm 2. In each iteration,\nit uses an environment source code and task description to\nsample different reward function candidates from a coding\nLLM. Then the candidates are used to instruct RL training.\nAfter training, the results are used to calculate the scores of the\nreward candidates. Then the best reward function code are se\nlected for reflection, where LLM uses the reasoning capability\nto progressively improve the reward code. In the experiment,\nresults show that their proposed method can achieve humanlevel performance on reward design and solve dexterous\nmanipulation tasks that were previously infeasible by manual\nreward engineering. Another work Text2Reward [120] generated shaped dense reward functions as executable programs\nbased on the environment description. Given the sensitivity of\nRL training and the ambiguity of language, the RL policy may\nfail to achieve the goal. Text2Reward addresses the problem\nby executing the learned policy in the environment, requesting\nhuman feedback and refining the reward accordingly.\n\n\n_C. Summarization and Outlook_\n\nThe use of LLMs as reward designers in RL offers a more\nnatural and efficient way to design complex reward functions.\n\n\n\n**Algorithm 2** Eureka [82]\n**Require:** Task description _l_, environment code _C_, coding\nLLM _Mc_, evaluation function _E_, initial prompt prompt,\noptimization iterations _N_, iteration batch size _K_\n1: **for** _i ←_ 1 **to** _N_ **do**\n\n2: // Sample _K_ reward code candidates\nfrom coding LLM _Mc_\n3: _R_ 1 _, . . ., RK ←_ sample( _l, Mc, C,_ prompt)\n\n4: // Evaluate reward candidates\n5: _s_ _[i]_ 1 [=] _[ E]_ [(] _[R]_ 1 _[i]_ [)][,] _[ s]_ 2 _[i]_ [=] _[ E]_ [(] _[R]_ 2 _[i]_ [)][, ...,] _[ s][i]_ _K_ [=] _[ E]_ [(] _[R]_ _K_ _[i]_ [)]\n6: // Select the best reward code\n7: best = arg max _k_ ( _s_ _[i]_ 1 _[, . . ., s][i]_ _K_ [)]\n8: // Reward reflection\n9: prompt _←_ prompt:Reflection( _R_ best _[i]_ _[, s][i]_ best [)]\n\n10: // Optimize Eureka reward code\n11: **if** _s_ _[i]_ best _[> s]_ [Eureka] **[ then]**\n12: _R_ Eureka _, s_ Eureka _←_ ( _R_ best _[i]_ _[, s][i]_ best [)]\n13: **end if**\n\n14: **end for**\n\n15: **return** _R_ Eureka\n\n\nBy leveraging natural language processing capabilities, LLMs\nsimplify the traditionally challenging task of reward function\ndesign, enhancing both the efficiency and effectiveness of RL\nalgorithms.\nHowever, the inherent biases in LLMs may transfer to the\ndesigned reward functions, potentially resulting in suboptimal\nor harmful behaviors [121]. This presents a potential risk requiring careful consideration. While existing mitigation efforts\nprimarily address biases in demographic, cultural, and political\nbeliefs [122], task-specific biases remain understudied, thus\nlimiting the applicability of LLM-designed reward functions.\nToward this end, we list some potential solutions. First, in\npreference-based RL, when an RL agent optimizes against\nbiased reward models generated by LLMs to predict human\npreferences, overoptimization and overfitting may occur [123],\nimpeding the learning of the true reward function. Reward\nfunction regularization [124] includes the agent preference\ngenerated by the value function as a regularization term to\nmitigate the risk, which helps recover the true underlying\nreward function. Secondly, human-in-the-loop approaches are\nanother direction to prevent harmful behaviors from designed\nreward functions. One viable solution is to design an evaluation module where humans can intervene and correct unde\nsired behaviors and reward functions when the agent’s action\nviolates a predefined set of rules. Finally, principles from\nensemble learning suggest that combining the reward functions\nfrom different LLM models mitigates bias from individual\nLLMs, leading to improved unbiased performance compared\nto a single LLM [125].\nIn the future, we expect advancements in the following\n\nareas:\n\n\n_• Implicit Reward Model_ : An immediate focus may consider improving how well LLM-generated rewards align\nwith human intentions. This involves refining the quality of language instructions to reduce ambiguities and\ninaccuracies, ensuring that the reward functions align\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 11\n\n\n\nprecisely with human notions of desired behavior. In the\nlonger term, generalization and transferability of LLMgenerated rewards across different tasks and environments, especially in complex, high-dimensional visual\nenvironments, is also an important direction to explore.\n\n_• Explicit Reward Model_ : A key limitation in reward code\ngeneration is its dependency on pre-trained commonsense knowledge, which can be restrictive for highly\nspecialized tasks not covered in training data. Therefore,\nshort-term goals may include enhancing prompts with detailed, task-specific information and external knowledge.\nAdditionally, the reliance on manually designed templates\nfor motion descriptions limits the adaptability. Looking\nfurther ahead, researchers might develop automated or\nunified processes for designing templates, moving beyond\nthe current limitations of manual motion description\ntemplates.\n\n\nVI. LLM AS DECISION-MAKER\n\nLLMs trained on a massive amount of data show impressive results in language understanding tasks [29], instructionfollowing [126], vision-language navigation [127] and tasks\nrequiring planning and sequential reasoning [83]. Such success\nmotivates researchers to explore the potential of LLMs for\ndecision-making problems. In this section, we divide the role\nof LLM as 1) the _action-maker_ that generates actions; and 2)\nthe _action-guider_ that instructs the actions.\n\n\nFig. 5. LLM as a decision-maker. (i) Action-Making: given a _T_ -length\ntrajectory _τ_ = ( _R_ [ˆ] 1 _, s_ 1 _, a_ 1 _, . . .,_ _R_ [ˆ] _T, sT, aT_ ) as a sequence of ordered\nreturn-to-go _R_ [ˆ], action _a_, and states _s_, LLM learns to predict future action\n_a_ _[′]_ _t_ [by minimizing the mean squared error loss] _[ L]_ [ =][ �] _t_ _[∥][a][t][ −]_ _[a]_ _t_ _[′]_ _[∥]_ [2] 2 [. (ii)]\nAction-Guiding: LLM generates a reduced set of action candidates for agents\nor generates expert actions to regularize RL learning.\n\n\n_A. Action-Making_\n\nTransformer-based models such as Decision Transformer\n\n(DT) [128] have shown great potential in offline RL domain.\n\n\n\nInstead of using the traditional trial-and-error way, these models treat offline RL as a sequence modeling problem, yielding\npromising results. As LLM itself is a large-scale Transformerbased model, a natural thought is to leverage the pre-trained\npower of LLM within this paradigm.\nWe term this function of LLM as action-making, and\nidentify two typical approaches, as illustrated in Fig. 5 (i).\nIn the first approach (left figure), pre-trained LLM is finetuned and then employed for action generation. In the second\napproach (right figure), goal/instruction along with trajectory\nare fed to pre-trained LLM. Moreover, a task-specific smaller\nmodel is appended after the fine-tuned LLM to facilitate rapid\nadaptation to diverse tasks.\nPre-trained LLMs outperform basic DT in generalization\nand sample efficiency, especially for sparse-reward and longhorizon tasks. LLMs’ latent representations, learned from diverse linguistic data, provide valuable prior knowledge for new\ntasks. This knowledge enables LLMs to solve unseen tasks\nwith less training data, by transferring knowledge from similar\ntasks and predicting high-reward actions even with sparse\nfeedback. For instance, comparing pre-trained LLM with basic\nDT, Li et al. [129] reported a 43.6% improvement in out-ofdistribution (novel) task completion rates while requiring less\ntraining data (e.g., 500 vs 10K). Additionally, Shi et al. [130]\ndemonstrated a 50% performance gain in sparse-reward environments like Kitchen and Reacher2d. For long-horizon tasks,\npre-trained representations encode future information, guiding\ndecision-making over extended sequences. For instance, in\nAntMaze, a long-horizon navigation environment, pre-trained\nrepresentations yield five times higher scores compared to nonpre-trained counterparts [131].\nSeveral studies have further explored the application of\nLLMs in offline RL, demonstrating their versatility and effectiveness across various tasks and benchmarks. Reference [132]\ninvestigated the transferability of general language models on\nspecific RL tasks. Fined-tuned on offline RL tasks (control,\ngames), these general language models outperform Decision\nTransformer and reduce training time by 3-6x on D4RL\nbenchmark [133]. Reference [129] used pre-trained LLM as a\ngeneral scaffold for task-specific model learning, where goals\nwere added along with observations as the input for LLM.\nResults on embodied decision-making tasks demonstrate that\ntheir proposed method outperforms others with less training\ndata, especially when generalizing to novel tasks. In addition,\nthey found representations in pre-trained language models\ncan aid learning and generalization even outside of language.\nTo unify language reasoning with actions in a single policy,\nreference [134] generated textual captions interleaved with\nactions when training the Transformer-based policy. Results\nshow that by using captions describing the next subgoals,\nthe reasoning policy can consistently outperform the captionfree baseline. For scenarios where data collection is costly\nand risky, reference [130] proposed a general framework to\neffectively use pre-trained LLM for offline RL. The pretrained LLM are based on DT. To combine the pre-trained\nknowledge and task-related domain knowledge, they finetuned pre-trained LLM with Low-Rank Adaptation (LoRA)\nmethod. The architecture of Transformer is based on GPT\n\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-10-0.png)\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 12\n\n\n\n2 model with 12 layers and 12 attention heads. To fine-tune\nthe LLM, they obtained the trajectory data from the D4RL\ndataset. Results show their method achieves state-of-the-art\n\nperformance in sparse-reward tasks with limited data samples.\nTo integrate multimodal data, e.g., vision and language, into\nthe offline RL, reference [135] co-fine-tuned vision-language\nmodels on both robotic trajectory data and Internet-scale\nvision-language tasks, e.g., visual question answering. In the\nframework, they incorporate the actions as natural language\ntokens and co-fine-tune models on both vision and language\ndatasets. Results show that such co-fine-tune methods can\nincrease generalization performance and the chain of thought\nreasoning can help the agent perform multi-stage semantic\nreasoning and solve complex tasks.\n\n\n_B. Action-Guiding_\n\n\nThe action-guider role is illustrated in Fig. 5 (ii). As\nan action-guider, LLM guides the action selection by either\ngenerating reasonable action candidates or expert actions.\nBy instructing the action selection, LLM improves sample\nefficiency and exploration efficiency posed by enormous action\nspaces and natural language.\n_1) Action Candidates:_ In environments such as text-based\ngames, action spaces are large and only a tiny fraction of\nactions are accessible. Although RL agents can learn through\nextensive trials, they often face exploration efficiency issues,\nespecially in multi-task settings where agents must manage\nvarious tasks simultaneously. LLMs address this challenge\nby generating a reduced set of action candidates based on\ntask understanding. These candidates are likely to yield high\nrewards and are applicable across multiple tasks, enhancing\nexploration efficiency and reducing the need for ineffective\nexploration. Reference [136] trained a GPT-2 model to generate the candidates. To maximize long-term rewards, another\nneural network is used to calculate the Q-values of these\ncandidates. When tested in 28 man-made text games from\nJericho framework [137], they found the method excludes\nnon-useful actions, speeds up the exploration and consistently\nachieves higher scores by more than 20%. Following this work,\nanother study [83] proposed the SayCan framework, where\ninstruction-following robots are integrated with an embodied\nLLM to understand tasks. When receiving instructions, the\nembodied LLM generates a high-level step-by-step plan. When\nacting, LLM produces action candidates based on task prompts\nand then the candidate with the largest critic value is executed.\nBeing evaluated in an office kitchen with real-world robotic\ntasks from 101 instructions, their method can complete longhorizon, abstract, natural language instructions on a mobile\nmanipulator.\n_2) Expert Actions:_ Traditional RL agents cannot converge\nto desirable equilibrium in human-AI collaboration or learn\nefficiently for complex tasks, due to the lack of expert\ndemonstrations. With the understanding of human behavior\nand general knowledge, LLM solves the issues by producing\nhigh-quality expert actions to regularize RL agents. In humanAI collaboration, _instructRL_ [138] used LLM to generate prior\npolicy based on human instructions and uses this prior to\n\n\n\nregularizing the RL objective. Specifically, _instructRL_ augments the policy update function with an auxiliary term\n_p_ LLM[lang( _at_ ) _|_ lang( _τt_ _[i]_ [)] _[,]_ [ inst]][, the probability of choosing an]\naction based on the trajectory and instructions. Experiments\nshow _instructRL_ converges to policies aligned with human\npreferences. Similarly, to address the sample inefficiency issue\nof RL, Zhou _et al._ [139] included the policy difference between\nthe student model and LLM-based teacher into RL learning\nloss. Their experiments on simulation platforms demonstrate\nthat the method reduced training iterations by a factor of 1\nto 9. In reference [140], LLM was leveraged to generate a\nhigh-level expert motion plan of robotics tasks, guiding RL\npolicies to efficiently solve robotics control tasks. The highlevel language plan breaks long-horizon tasks into stages to\nexecute. Then, a single RL policy was trained across all states\nand stepped through the language plan. Their results show the\nproposed method solves long-horizon tasks from raw visual\ninput spanning different benchmarks at success rates of over\n85%, out-performing classical, language-based, and end-toend approaches.\n\n\n_C. Summarization and Outlook_\n\n\nSample inefficiency and exploration inefficiency remain\nlong-standing challenges for deep RL, particularly in environments with sparse rewards or where data collection is\nexpensive or risky. LLM provides three ways to solve the\nproblems. First, LLM as action-makers treat RL as a conditional sequence modeling problem. The supervised way finetunes pre-trained LLMs to predict future actions. Benefiting\nfrom learned prior knowledge, LLM can perform well even\nin out-of-distribution, sparse-reward, and long-horizon tasks.\nSecond, LLM as action-guiders generates potential action\ncandidates for RL. With task comprehension, these action\ncandidates promote agents to explore potentially high taskvalue states, thus increasing exploration efficiency. Last, LLM\ngenerates expert actions to help RL learn from demonstrations.\nBy incorporating demonstrations from LLM, and RL learns\nspecific prior knowledge and improves sample efficiency.\nSafety issues are another important topic when using LLMs\nas decision-makers in RL, particularly for costly and risky\ntasks [141]. For action-making, DT-based offline RL learns\nthe optimal policy from pre-collected datasets. Recently, integrating safety constraints has been explored by some works\nusing methods such as pessimistic estimations [142] and\nstationary distribution correction [143]. These methods set a\nconstant constraint threshold before training. To dynamically\nadjust the threshold during deployment, a constrained decision transformer that dynamically relabels the reward based\non safety-reward trade-offs was proposed [144]. For actionguiding, LLMs are viewed as instructors to provide expert\nactions. Ensuring the safety of actions generated by LLMs\ndiffers from that in action-making. Leveraging ideas from\nLLM-based agent research, we propose several potential solutions. First, LLMs can be equipped with testing modules that\nexecute code to evaluate action safety and feasibility [145].\nSecond, implementing a carefully designed human-in-theloop framework enables safety intervention through human\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 13\n\n\noversight [146]. Finally, developing memory modules with\nreasoning mechanisms that adaptively learn safety boundaries\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-12-0.png)\nfrom past experiences provides a continual learning-based Enhance\napproach to ensuring long-term safety [147]. Real Sim.\nIn the following, we list the challenges and future directions World Collect World\nof the two roles as below:\n\n_• Action-making_ : Directly employing pre-trained large- Acquire Real Data Render Rollout\nscale LLM to generate actions demands huge computational resources and huge data for fine-tuning it in specific Guide Model\ntasks. In the short term, future work may consider more\ncost-effective methods such as the Low-Rank Adaptation World Model Dynamics Policy\n(LoRA) [148] to exploit the power of LLM in direct Simulator Reconstruction Learning\ndecision-making. Long-term goals involve developing\n\n![](output/images/c44471e846846bde281779405a3b5c132fd60b00.pdf-12-1.png)\ninnovative techniques to efficiently exploit the power _**(ii) Policy Interpreter**_\nof LLMs in direct decision-making, potentially creating\nhybrid models that combine the strengths of LLMs with Prompts Further Prompts\nmore lightweight, task-specific architectures.\n\n\n\n\n_• Action-guiding_ : Since the LLM acts as an instructor\nto provide expert actions, the bias and limitations are\nalso inherited by the agent. In addition, LLM itself\ncannot inherently interrogate or intervene in the environment, which limits LLMs’ capabilities to intentionally\naggregate information. Short-term goal is to address\nthe inherited biases and limitations when LLMs act as\n\ninstructors providing expert actions, focusing on methods to filter or correct biased information. In the long\nterm, it is crucial to develop mechanisms for LLMs to\nactively interrogate and intervene in the environment,\nenabling them to intentionally aggregate information and\nimprove their capabilities through real-world interactions.\nTherefore, how to use the information gained from realworld interactions to improve the LLM itself in terms of\nactuality and reasoning is another important problem.\n\n\nVII. LLM AS GENERATOR\n\nThe generative capability of LLMs can be applied to environmental simulation and behavior explanation. On the one\nhand, growing interests in applying RL to the real world and\nrisky data-collection process suggests a need for imaginary\nrollouts [149], [150]. Possessed with a powerful modeling\ncapability and world knowledge, LLMs can serve as a _world_\n_model simulator_ to learn complex environmental dynamics\nwith high fidelity by iteratively predicting the next state and\nreward, thus increasing the sample efficiency in model-based\nRL [151], [152]. On the other hand, in RL, interpretability\nremains an important security issue in current black-box AI\nsystems as they are increasingly being deployed to help endusers with everyday tasks. Explanations of policy can improve\nthe end-user understanding of the agent’s decision-making and\ninform the reward function design for agent learning. In such\naspects, LLM can act as a policy interpreter based on their\nknowledge and reasoning ability. In this section, we classify\nthe above two roles of LLM as a generator and review recent\nrelated works.\n\n\n_A. World Model Simulator_\n\nServing as a world model simulator, LLM is trained as a\n1) _trajectory rolloutor_, which auto-regressively generates ac\n\n\nFig. 6. LLM as a generator. (i) World Model Simulator: LLM uses realworld data and knowledge to model dynamics, generate simulated worlds, and\nassist policy learning. (ii) Policy Interpreter: LLM generates interpretations of\nagent behavior based on state-action history and prompts, potentially leading\nto explainable RL.\n\n\ncurate trajectories for the agent to learn and plan; 2) _dynamics_\n_representation learner_, which predicts the latent representation\nof the world using representation learning. A flow chart of\nthe world model is illustrated in Fig. 6 (i). From the real\nworld, knowledge and real data can be collected to construct\na world model simulator, which further models the dynamics\nrepresentation of the world, generates trajectories and helps\nthe policy learning of the agent in the real world.\n\n\n_1) Trajectory Rolloutor:_ Similar to the decision transformer [153] in offline RL, pre-trained large-scale models\nwere used in model-based to synthesize trajectories. In 2022,\nMicheli _et al._ proposed IRIS, an agent that employs a discrete\nautoencoder and an autoregressive Transformer to learn the\nworld model for Atari games [154]. With the equivalent of\ntwo hours of gameplay in the Atari 100k benchmark, the\nproposed method outperforms humans in 10 out of 26 games.\nSimilarly, reference [155] applied a transformer to build a\nsample-efficient world model for Atari games. Utilizing such\na Transformer-based world model (TWM), the RL agent can\nsolve the long-term dependency and train a state-of-the-art policy on the Atari 100k benchmark based on generated meaningful experiences from TWM. Visual RL enables the RL agent\nto learn from visual observations effectively. Reference [156]\nproposed TransDreamer, a Transformer-based model-based RL\nagent that leverages a Transformer for dynamics predictions\nin 2D and 3D visual RL tasks. Experiments showed the\nTransDreamer agent can outperform Dreamer with long-range\nmemory access and memory-based reasoning. Based on the\ndevelopment of supervised pre-training methods, Seo _et al._\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 14\n\n\n\nproposed a framework that learns world model dynamics from\naction-free video representations [157]. The framework added\na video-based intrinsic bonus for better-guiding exploration to\neffectively encourage agents to learn diverse behaviors. The\nexperimental results demonstrated their proposed method can\nimprove the performances of vision-based RL on manipulation\nand locomotion tasks by transferring the pre-trained representations from unseen domains.\n\n_2) Dynamics Representation Learner:_ Using representation\nlearning techniques, the latent representation of the future\ncan be learned to assist decision-making. Reference [114]\nintroduced a visual model-based RL framework that decou\nples visual representation learning and dynamic learning by\ntraining an autoencoder with a vision Transformer to reconstruct pixel-given masked observations and learn the dynamics\nfrom the latent space. By such a decoupling approach, their\nproposed method achieved state-of-the-art performance on\nvisual robotic tasks from Meta-world and RLBench. Utilizing\nthe fact that language contains rich information signals and\ncan help agents to predict the future, Lin _et al._ proposed\nDynalang [46], where an agent that learns a multimodal world\nmodel to predict future text and image representations and\nthereby instruct the decision-making process. The algorithm\nis presented in Algorithm 3. In the training part, a LLMbased world model implemented with Recurrent State Space\nModel (RSSM) [158] computes the state representation _zt_\nbased on the collected transitions. After that, the representation\n_zt_ and the action _at_ are fed into the RSSM to predict the\nfuture representations _zt_ +1. In addition, the language [ˆ] _lt_ and\nimages ˆ _xt_ are predicted to reconstruct the original state. Then,\nthe world model is trained to learn the next representation\nand reconstruct observations from the representations. After\nupdating, the world model imagined (sampled) rollouts and the\npolicy is trained to maximize the imagined rewards. Compared\nto other works that use language only for predicting actions,\nmultimodal information enables Dynalang to handle tasks\nthat require grounded language generation, obtaining higher\nscore than methods provided with only task descriptions.\nTo solve the out-of-distribution generalization problem in\nvisual control tasks of RL, reference [159] proposed the\nLanguage Grounded World Model (LanGWM), which focuses\non learning language-grounded visual features to enhance the\nworld model learning. To improve the generalization of the\nlearned visual features, they masked the bounding boxes and\npredicted them with given language descriptions. Utilizing the\nexpressing ability of language in higher-level concepts and\nglobal contexts, the proposed LanGWM method yields stateof-the-art results on out-of-distribution tests.\n\n\n_B. Policy Interpreter_\n\n\nExplainable RL (XRL) is an emerging subfield of both\nexplainable machine learning and RL that has attracted considerable attention recently. XRL aims to elucidate the decisionmaking process of learning agents. According to a survey\nin explainable RL [160], the categories of XRL include the\nfeature importance, learning process and MDP, and policy\nlevel. Currently, the usage of LLMs in XRL has only been\n\n\n\n**Algorithm 3** Training part of Dynalang [46]\n**Require:** Rewards _rt_, episode continue flag _ct_, images _xt_,\nlanguage tokens _lt_, actions _at_, model state ( _ht, zt_ ).\n1: **while** training **do**\n2: Draw batch of transitions _{_ ( _rt, ct, xt, lt, at_ ) _}_ from replay buffer.\n3: Use world model to compute multimodal representations _zt_, future predictions ˆ _zt_ +1, and decode ˆ _xt_, [ˆ] _lt_, ˆ _rt_,\n_c_ ˆ _t_ .\n4: Update world model to minimize _L_ pred + _L_ repr.\n5: Imagine rollouts from all _zt_ using _π_ .\n6: Update actor to minimize _Lπ_ .\n\n7: Update critic to minimize _LV_ .\n8: **end while**\n\n\nlimited to the policy level, _i.e._, as the policy interpreter.\nTherefore, though there is limited literature related to this area,\nwe think this field is important and requires more attention in\nthe future. The following will introduce the role of LLMs as\npolicy interpreters and an outlook regarding other categories\nof XRL will be provided in the last subsection.\nAs the policy interpreter, LLM generates explanations with\nthe prompts of state and action description or trajectory\ninformation. An illustration is depicted in Fig. 6. Using the\ntrajectory history of states and actions as context information,\nLLMs can be prompts to generate readable interpretations of\ncurrent policies or situations for humans.\nDas _et al._ [161] proposed a unified framework called\nState2Explanation (S2E), that learns a joint embedding model\nbetween state-action pairs and concept-based explanation.\nBased on the learned models, the explanation can help inform\nreward shaping during an agent’s training and provide insights\nto end-users at deployment. Another work [162] first distilled\nthe policy into a decision tree, derives the decision path,\nand then prompts an LLM to generate a natural language\nexplanation based on the decision path. Additionally, _Lu et_\n_al._ [163] introduced a framework that decomposes the overall\nreward into multiple sub-rewards based on specific object\nproperties, defines actions as high-level motion primitives executed at precise 3D positions to simplify decision-making, and\nintegrates LLMs to enable interactive and flexible querying of\nexplanations.\n\n\n_C. Summarization and Outlook_\n\n\nAs a generator, LLMs can be integrated into model-based\nRL or explainable RL, _i.e._, serving as world model simulators\nor policy interpreters, respectively. As world model simulators,\nLLMs enhance model-based RL by auto-regressively generating accurate trajectories (trajectory rollout) and by predicting\nlatent world representations (world representation learners),\nsignificantly improving sample efficiency and decision-making\naccuracy. In the realm of explainable RL, LLMs provide\nvaluable insights for both end-user understanding and reward\nshaping by generating explanations based on trajectory information. However, the current usages of LLMs in explainable\nRL are rather limited and have great potential for future work.\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 15\n\n\n\nBelow are discussions on the key limitations and future work\ndirections of the two roles:\n\n\n_• World Model Simulator_ : LLMs encounter challenges in\naligning their abstract knowledge with the specific requirements of different environments, leading to limitations in functional competence and grounding. This\nmisalignment affects their effectiveness in generating\ntrajectories and interacting with the environment. Additionally, the current model-based agents, which rely on\npurely observational world models, present difficulties for\nhuman adaptation. These models are typically modifiable\nonly through observational data, which is not an effective\nmeans for humans to communicate complex intentions or\nadjustments. Furthermore, while LLMs hold promise for\nenhancing multi-task learning by generating trajectories\nand dynamic representations applicable to multiple tasks,\nthere remains a scarcity of research exploring this potential.Looking ahead, in the short term, researchers might\nprioritize improving LLMs’ alignment with specific environment requirements. For the long term, a promising\ndirection would be to integrate language instructions or\nan adapter into the world model. This approach could\nlead to a more flexible and adaptive world model that can\nbetter accommodate human intentions and adjustments, as\nwell as support more effective multi-task learning through\nenhanced trajectory generation and dynamic representations.\n\n\n_• Policy Interpreter_ : As policy interpreters, the quality of\nexplanations depends on the LLM’s understanding of\nfeature representations and implicit logic of policy. How\nto utilize domain knowledge or examples to improve the\nunderstanding of a complex policy is still a major issue.\nIn the short term, researchers could focus on enhancing\nLLMs’ ability to interpret the correlation between observations and policy selection, providing insights into the\ndecision-making process of RL agents. This could involve\ndeveloping techniques to better leverage domain knowledge and examples for improving LLMs’ understanding\nof complex policies. In the long-term, the field could\nexplore advanced applications of LLMs in explainable\nRL, such as analyzing the learning process and MDP to\nreveal influences on agent behavior. Researchers could\ndevelop sophisticated prompting techniques for LLMs to\nanswer nuanced “why” and “why-not” questions with\nMDP context, providing deeper explanations of agent\ndecision-making.\n\n\nVIII. DISCUSSION\n\n\nIn previous sections, we introduced the concept “LLMenhanced RL” and developed a corresponding framework,\nwhich we extended to the integration of multimodal AI\nmodels such as visual-language models. We then discussed\nthe different LLM-enhanced RL approaches. This section\nprovides a comprehensive analysis of the LLM-enhanced RL\napproach. We begin with a comparative analysis of the different LLM roles, highlighting their advantages and limitations.\nFollowing this, we explore potential real-world applications of\n\n\n\nthe LLM-enhanced RL paradigm. Finally, we discuss future\nopportunities and challenges, taking into account both the\nuntapped capabilities and inherent limitations of LLMs, with a\nparticular focus on their application in multimodal information\nenvironments.\n\n\n_A. Comparison of Different LLM-Enhanced RL Approaches_\n\n\nThis section provides a comparative analysis of the four\nLLM-enhanced RL approaches, helping researchers understand the strengths and limitations of each approach.\n\n\n_•_ **Information Processor** : As an information processor,\nLLM excels in handling complex, multimodal inputs,\nparticularly in translating natural language instructions\nor environment information into a format more readily\nusable by RL agents. This role significantly enhances the\nagent’s ability to understand and interact with complex\nenvironments. However, it faces challenges in computational efficiency and may struggle with highly specialized\ndomain knowledge not covered in its pre-training.\n\n_•_ **Reward Designer** : LLMs serving as reward designers\noffer a more intuitive and flexible approach to defining\nreward functions, especially in complex or sparse-reward\nenvironments. This role can significantly improve the\nalignment of RL objectives with human intentions. The\nmain limitation lies in ensuring that generated rewards\naccurately reflect task-specific nuances and long-term\ngoals, particularly in highly specialized domains.\n\n_•_ **Decision-Maker** : As decision-makers, LLMs can either\ndirectly generate actions or guide action selection, leveraging their vast knowledge base to improve sample efficiency and exploration in RL. This role is particularly\neffective in tasks requiring complex reasoning or longterm planning. However, it may face challenges in realtime decision-making scenarios due to computational\noverhead and may inherit biases present in the LLM’s\ntraining data.\n\n_•_ **Generator** : In the generator role, LLMs can simulate\ncomplex environments for model-based RL and provide\ninterpretable explanations of RL policies. This capability\nis invaluable for improving sample efficiency and making\nRL more transparent and understandable. The main challenges include aligning generated simulations with realworld dynamics and ensuring the relevance and accuracy\nof policy explanations.\n\n\n_B. Applications of LLM-Enhanced RL_\n\n\nBased on the characteristics of LLM-enhanced RL, such as\nmultimodal information understanding and multi-task learning\nand generation, we believe that LLM-enhanced RL opens up\na wide array of potential applications. Here we list several\napplications to inspire researchers.\n\n\n_• Robotics_ : RL is widely used in robots to learn how to\nmake decisions and execute actions to achieve goals. Utilizing natural language understanding and general logical\nreasoning abilities, LLM-enhanced RL can 1) improve\nthe efficiency of human-robot interaction, 2) help robots\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 16\n\n\n\nunderstand human needs and behavioral logic, and 3)\nenhance decision-making and planning capabilities.\n\n_• Autonomous Driving_ : Autonomous driving uses RL to\nmake decisions in complex, ever-changing environments\nthat involve understanding both sensor data (visual, lidar,\nradar) and contextual information (traffic laws, human\nbehavior). LLM-enhanced RL could employ LLMs to 1)\nprocess this multimodal information and natural language\ninstructions; or 2) design comprehensive rewards based\non multi-disciplinary metrics such as safety, efficiency,\nand passenger comfort.\n\n_• Energy Management_ : In the energy system, operators or\nusers apply RL to efficiently manage the usage, transportation, conversion and storage of multiple energy with\nhigh uncertainty brought by renewable resources. LLMenhanced RL in such cases can 1) improve the RL agents’\nability to handle multi-objective tasks, such as economy,\nsafety, and low carbon, by reward function designing,\nand 2) increase the sample efficiency for the new energy\n\nsystem.\n\n_• Healthcare Recommendation_ : RL is used to learn rec\nommendations or suggestions in healthcare [164]. LLMs\ncan utilize the domain knowledge to analyze the vast\namount of patient data and medical histories, therefore, 1)\naccelerating the learning process of RL recommendation\nand 2) providing more accurate diagnostic and treatment\nrecommendations in healthcare.\n\n\n_C. Opportunities for LLM-Enhanced RL_\n\n\nAlthough current works in LLM-enhanced RL have already\nshown better performance in several aspects, more unexplored\nareas remain to be explored and may lead to significant improvement. Below, we summarize the potential opportunities\nof LLM-enhanced RL from both the perspectives of RL and\nLLM capabilities, respectively.\n\n\n_• RL_ : Existing work such as [138], [134], [129] mainly focus on the general RL while various specialized branches\nof RL are still under-exploited, e.g. multi-agent RL,\nsafe RL, transfer RL, explainable RL, multi-task RL,\nin-context RL and human-centric RL. In the multi\nagent area, compared to various works about multiLLM-agent collaboration [165], [166], LLM-based multiagent RL remains largely unexplored [167]. A recent\nsurvey discussed some open research problems within this\nfield [168]. LLM-enhanced strategies could be employed\nto facilitate communication and collaboration among RL\nagents. The natural language understanding capabilities\nof LLMs can be used to interpret and generate instructions or strategies among agents, enhancing cooperative\nbehaviors or competition strategies; Safe RL could benefit\nfrom the reasoning and predictive capabilities of LLMs\nto design cost functions that encourage safety criteria\ncompliance, reducing the risks of dangerous exploration;\nIn transfer RL, LLMs can assist in identifying similarities between tasks or environments, leveraging their\nvast knowledge base to facilitate knowledge transfer and\nthus improve learning efficiency and adaptability across\n\n\n\ndifferent tasks; For multi-task RL, LLMs enhance agents\nby processing diverse entity representations and aligning\ninstructions (Information Processor), generating reduced\naction sets and expert actions for various tasks (DecisionMaker), and creating task-specific trajectories and dynamic representations (Generator). Recently, in-context\nRL has emerged as a promising field by leveraging the\nin-context learning capability of LLMs to solve decisionmaking problems [169]. Given a query state and an incontext offline dataset, a trained LLM exhibits both online exploration and offline conservation while avoiding\nextensive trial and error and is sample-efficient. Additionally, human-centric RL is another prominent field in\nhealthcare and robotics, where AI and humans learn and\ncommunicate with each other for collaboration [170]. In\nthis setting, an LLM naturally serves as a perfect mediator\nto facilitate bidirectional communication through natural\nlanguage interactions.\n\n_• LLM_ : While LLMs have been integrated with RL in\nvarious methods, several promising directions remain to\nbe explored to further enhance LLM capabilities for RL.\nIt can be divided into the language model view and the\nlanguage agent view as follows:\n\n\n**– From the model perspective** : LLM could be enhanced by an external knowledge base and continual learning. Retrieval-augmented generation (RAG)\ntechniques help LLM to retrieve the most relevant\ndata in an external database, which could be used\nto attach knowledge in the task domains [171].\nContinual learning techniques are also a hot field\nthat enables models to continuously acquire new\nknowledge while retaining previously learned capabilities [172]. This technique allows both LLMs\nand RL agents to continuously evolve and adapt to\nnew tasks or environments while maintaining their\nfundamental pre-trained abilities, leading to more\nflexible and robust learning systems.\n**– From the agent perspective** : Equipping LLMbased agents with specialized modules—namely,\nplanning modules, memory modules, and action\nmodules—can significantly enhance the capabilities\nof LLMs [173], [174]. In the planning module, multistep reasoning techniques such as CoT-based reasoning [74] and Monte Carlo tree search (MCTS) [175]\ncan be used to enhance LLM’s long-term planning\nability, improving the long-term task decomposition\nability of RL agents; for the memory module, longand short-term human-memories and corresponding\nmemory retrieval mechanisms provide a way to\nstore previous experiences and learn adaptively along\nwith the RL agents [176]; for the action module,\ntool integration presents another promising direction to unlock new possibilities for LLMs. External tools such as mathematical solvers [177] and\ninternet browsers [178] can augment LLMs’ capabilities in RL tasks requiring complex mathematical\ncomputations and real-time information processing,\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 17\n\n\n\nrespectively. Furthermore, collaboration of multiple\nLLM agents [179] is another promising direction.\nWith each LLM playing different roles and acquiring different information, multiple distinctive LLM\nagents solve complex problems by communicating\nand exchanging information. In the context of LLMenhanced RL, different LLMs could potential serves\ndifferent roles and work together to guiding RL. For\nexample, in a group of three LLM agents, one LLM\nfocuses on guiding short-term or immediate problemsolving for RL; another LLM is responsible for longterm planning for RL; the last LLM serves as a\ncoordinator responsible for overseeing the group and\nadjusting horizon consideration accordingly.\n\n\n_D. Challenges of LLM-Enhanced RL_\n\nWhile LLM improves different issues in the RL paradigm,\nthe success of LLM-enhanced RL is inherently tied to the capabilities and limitations of the underlying LLM. The primary\nconcerns revolve around the inherent limitations of LLMs,\nadaptability of LLMs in RL environments, computational\ndemands, and the broader implications of deploying such\nsystems in real-world scenarios.\n\n_• Inherent Limitations of LLMs_ : The effectiveness of LLMenhanced RL systems is fundamentally constrained by\nthe inherent capabilities and limitations of the underlying\nlanguage models. The presence of systematic biases and\npotential hallucinations in pre-trained LLMs can significantly impact the reliability of multimodal input interpretation, potentially compromising the overall performance\nof the RL agent. This necessitates the development of\nrobust evaluation frameworks to systematically characterize and delineate the capability boundaries of LLMs\ngiven specified RL contexts. Furthermore, incorporating\nuncertainty quantification methods aids in identifying unreliable answers, increasing the trustworthiness of LLMs’\nresponses [180].\n\n_• Adaptability of LLMs in RL Environments_ : Despite the\nvast knowledge base of LLMs, they may struggle to\nadapt to specific or novel RL task environments that are\nnot well-represented in their training data. This calls for\nmethods to expand the task-related knowledge for LLM\nand ground LLMs’ inherent knowledge in specific domains. In terms of expanding the task-related knowledge,\nRAG-related techniques attach LLMs with the external\ndomain-knowledge without further training [171]. When\nfine-tuning LLMs, employing data augmentation techniques such as generating synthetic data is another way\nto expand the diversity of training scenarios and improve\ngeneralization to novel environments [181]. Additionally,\ncontinual learning mechanisms help preserve previously\nacquired knowledge while adapting to new information [182]. For domain-specific knowledge grounding,\napproaches include training value functions to evaluate\nthe long-term utility of LLM instructions [83]. Expert\ntrajectories provide another valuable source of information, allowing LLMs to learn optimal decision-making\npatterns through in-context learning.\n\n\n\n\n_• Computational Demands_ : The integration of LLMs into\nthe RL learning process introduces complexities in terms\nof computational overhead and inference times, which\nslow down the learning process of RL. To address this\nchallenge, several potential solutions have been proposed\nacross different levels. At the data level, input compression techniques such as prompt pruning [183] can be employed to directly shorten the model input, significantly\nreducing inference time without substantial loss in performance. At the model level, efficient architecture designs\nlike mixture-of-experts (MoE) [184] enable conditional\ncomputation, activating only relevant parts of the model\nfor each input, thus reducing computational costs. Similarly, structured state space models (SSM) [185] offer\nlinear-time complexity for sequence modeling, providing\na more efficient alternative to traditional attention mechanisms. At the system level, advanced caching strategies [186] and asynchronous processing techniques [187]\ncan be implemented to reuse intermediate computations\nand parallelize the execution, respectively.\n\n_• Ethical, Legal, and Safety Concerns_ : In practical usages,\nthe use of LLMs involves complex ethical, legal, and\nsafety concerns. Data privacy, intellectual property, and\naccountability for AI decisions should be carefully discussed. To address these issues, researchers are developing robust frameworks for responsible AI deployment.\nAt the privacy level, differential privacy techniques [188]\nare being implemented to protect individual data during\ntraining and inference. For transparency, efforts focus\non developing interpretable AI systems, allowing stakeholders to audit AI-driven decisions [189]. To enhance\nsystem robustness, adversarial training methods are being\nexplored to strengthen LLM-RL systems against potential\nattacks [190]. Regarding ethical and legal issues, a recent\nsurvey also analyzed potential solutions to integrate ethical standards and societal values into LLM [191].\n\n\nIX. CONCLUSION\n\n\nLLMs, with their pre-trained knowledge bases and powerful\ncapabilities such as reasoning and in-context learning, present\nas a viable solution to enhance RL in terms of natural lan\nguage understanding, multi-task generalization, task planning,\nand sample efficiency. In this survey, we have defined this\nparadigm as _LLM-enhanced RL_ and summarized its characteristics, together with opportunities and challenges. To formalize\nthe research scope and methodology of LLM-enhanced RL, we\npropose a structured framework to systematically categorize\nthe roles of LLM based on the functionalities within the\n\nclassical agent-environment interaction paradigm. According\nto functionalities, we categorize the roles of LLMs into\ninformation processor, reward designer, decision-maker, and\ngenerator. For each category, we review current literature based\non their methods and applications, outline the methodologies,\ndiscuss the addressed issues, and provide insights into future\ndirections. These are listed below:\n\n\n_• Information Processor_ : LLMs extract observational representations and formal specification languages for RL\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 18\n\n\n\nagents, thus increasing the sample efficiency. Future\ndirections include incorporating goal-based information\nand integrating multimodal environmental information to\nobtain stronger information extraction ability.\n\n_• Reward Designer_ : For intricate or un-quantifiable tasks,\nLLMs can leverage pre-trained knowledge to generate\nhigh-performing rewards that are notoriously difficult\nfor humans to design. Current methods still require\nconsistently modifying the prompts and instructions of\nLLMs, calling for future work on automated self-evolving\nframeworks without human intervention.\n\n\n_• Decision-Maker_ : LLMs generate direct actions or indirect\nadvice for the agent to improve exploration efficiency.\nHuge computational overhead is a major issue in online\ninteraction. Cost-effective methods to reduce the huge\ncomputational overhead of LLM in online RL is an\nimportant direction.\n\n_• Generator_ : With the generative capability and world\nknowledge, LLMs are used as 1) a high-fidelity world\nmodel to reduce real-world learning cost; and 2) a\nlanguage-based policy interpreter to explain the agent\npolicy. Leveraging human instructions to improve the\naccuracy and generalizability of the world model and\npolicy interpreter would be a crucial direction.\n\n\nREFERENCES\n\n\n[1] R. S. Sutton and A. G. Barto, _Reinforcement learning: An introduction_ .\nMIT press, 2018.\n\n[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller _et al._, “Human-level control\nthrough deep reinforcement learning,” _Nature_, vol. 518, no. 7540, pp.\n529–533, 2015.\n\n[3] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” _arXiv preprint arXiv:1707.06347_,\n2017.\n\n[4] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in _International conference on machine learning_ . PMLR, 2018,\npp. 1861–1870.\n\n[5] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik,\nJ. Chung, D. H. Choi, R. Powell _et al._, “Grandmaster level in starcraft ii\nusing multi-agent reinforcement learning,” _Nature_, vol. 575, no. 7782,\npp. 350–354, 2019.\n\n[6] C. Berner, G. Brockman, B. Chan, V. Cheung, P. D´eak, C. Dennison,\nD. Farhi, Q. Fischer _et al._, “Dota 2 with large scale deep reinforcement\nlearning,” _arXiv preprint arXiv:1912.06680_, 2019.\n\n[7] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou _et al._, “Mastering the\ngame of go with deep neural networks and tree search,” _Nature_, vol.\n529, no. 7587, pp. 484–489, 2016.\n\n[8] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre,\nS. Schmitt, A. Guez, E. Lockhart _et al._, “Mastering atari, go,\nchess and shogi by planning with a learned model,” _arXiv preprint_\n_arXiv:1911.08265_, 2020.\n\n[9] H. Zhao, Z. Liu, X. Mai, J. Zhao, J. Qiu, G. Liu, Z. Y. Dong, and\nA. M. Ghias, “Mobile battery energy storage system control with\nknowledge-assisted deep reinforcement learning,” _Energy Conversion_\n_and Economics_, vol. 3, no. 6, pp. 381–391, 2022.\n\n[10] N. B. Schmid, A. Botev, A. Hennig, A. Lerer, Q. Wu, D. Yarats,\nJ. Foerster, T. Rockt¨aschel _et al._, “Rebel: A general game playing ai,”\n_Science_, vol. 373, no. 6556, pp. 664–670, 2021.\n\n[11] N. Brown, A. Lerer, S. Gross, and T. Sandholm, “Superhuman ai for\nmultiplayer poker,” _Science_, vol. 365, no. 6456, pp. 885–890, 2020.\n\n[12] A. Vaswani, “Attention is all you need,” _Advances in Neural Informa-_\n_tion Processing Systems_, 2017.\n\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\nwith deep convolutional neural networks,” _Advances in neural infor-_\n_mation processing systems_, vol. 25, 2012.\n\n\n\n\n[14] Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn, “Language as an\nabstraction for hierarchical deep reinforcement learning,” _Advances in_\n_Neural Information Processing Systems_, vol. 32, 2019.\n\n[15] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S¨underhauf,\nI. Reid, S. Gould _et al._, “Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real environments,” in\n_Proceedings of the IEEE conference on computer vision and pattern_\n_recognition_, 2018, pp. 3674–3683.\n\n[16] A. Stooke, K. Lee, P. Abbeel, and M. Laskin, “Decoupling representation learning from reinforcement learning,” in _International Conference_\n_on Machine Learning_ . PMLR, 2021, pp. 9870–9879.\n\n[17] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas, E. Grefenstette, S. Whiteson, and T. Rockt¨aschel, “A Survey of Reinforcement\nLearning Informed by Natural Language,” Jun. 2019.\n\n[18] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni,\nL. Fei-Fei, S. Savarese _et al._, “What matters in learning from offline\nhuman demonstrations for robot manipulation,” 2021.\n\n[19] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch,\nT. Armstrong, and P. Florence, “Interactive language: Talking to robots\nin real time,” 2022.\n\n[20] Z. Yang, K. Ren, X. Luo, M. Liu, W. Liu, J. Bian, W. Zhang, and\nD. Li, “Towards applicable reinforcement learning: Improving the\ngeneralization and sample efficiency with policy ensemble,” in _Inter-_\n_national Joint Conference on Artificial Intelligence_, 2022. [Online].\n[Available: https://api.semanticscholar.org/CorpusID:248887230](https://api.semanticscholar.org/CorpusID:248887230)\n\n[21] W. B. Knox, A. Allievi, H. Banzhaf, F. Schmitt, and P. Stone, “Reward\n(mis) design for autonomous driving,” _Artificial Intelligence_, vol. 316,\np. 103829, 2023.\n\n[22] F. Dworschak, S. Dietze, M. Wittmann, B. Schleich, and S. Wartzack,\n“Reinforcement learning for engineering design automation,” _Advanced_\n_Engineering Informatics_, vol. 52, p. 101612, 2022.\n\n[23] S. Booth, W. B. Knox, J. Shah, S. Niekum, P. Stone, and A. Allievi, “The perils of trial-and-error reward design: misdesign through\noverfitting and invalid task specifications,” in _Proceedings of the AAAI_\n_Conference on Artificial Intelligence_, vol. 37, no. 5, 2023, pp. 5920–\n5929.\n\n[24] L. L. Di Langosco, J. Koch, L. D. Sharkey, J. Pfau, and D. Krueger,\n“Goal misgeneralization in deep reinforcement learning,” in _Interna-_\n_tional Conference on Machine Learning_ . PMLR, 2022, pp. 12 004–\n12 019.\n\n[25] R. Yang, X. Sun, and K. Narasimhan, “A generalized algorithm for\nmulti-objective reinforcement learning and policy adaptation,” _Ad-_\n_vances in neural information processing systems_, vol. 32, 2019.\n\n[26] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas,\nE. Grefenstette, S. Whiteson, and T. Rockt¨aschel, “A survey of reinforcement learning informed by natural language,” _arXiv preprint_\n_arXiv:1906.03926_, 2019.\n\n[27] J. Devlin, “Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding,” _arXiv preprint arXiv:1810.04805_, 2018.\n\n[28] A. Radford and K. Narasimhan, “Improving language understanding\n[by generative pre-training,” 2018. [Online]. Available: https://api.](https://api.semanticscholar.org/CorpusID:49313245)\n[semanticscholar.org/CorpusID:49313245](https://api.semanticscholar.org/CorpusID:49313245)\n\n[29] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam _et al._, “Language models are few-shot\nlearners,” _Advances in neural information processing systems_, vol. 33,\npp. 1877–1901, 2020.\n\n[30] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung _et al._, “Palm: Scaling language modeling with\npathways,” _Journal of Machine Learning Research_, vol. 24, no. 240,\npp. 1–113, 2023.\n\n[31] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F.\nTan, and D. S. W. Ting, “Large language models in medicine,” _Nature_\n_medicine_, vol. 29, no. 8, pp. 1930–1940, 2023.\n\n[32] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes, “Autonomous\nchemical research with large language models,” _Nature_, vol. 624, no.\n7992, pp. 570–578, 2023.\n\n[33] G. Jiang, Z. Ma, L. Zhang, and J. Chen, “Eplus-llm: A large language\nmodel-based computing platform for automated building energy modeling,” _Applied Energy_, vol. 367, p. 123431, 2024.\n\n[34] H. Tan, Z. Guo, Z. Lin, Y. Chen, D. Huang, W. Yuan, H. Zhang, and\nJ. Yan, “General generative ai-based image augmentation method for\nrobust rooftop pv segmentation,” _Applied Energy_, vol. 368, p. 123554,\n2024.\n\n[35] X. Zhou, H. Zhao, Y. Cheng, Y. Cao, G. Liang, G. Liu, and J. Zhao,\n“Elecbench: a power dispatch evaluation benchmark for large language\nmodels,” _arXiv preprint arXiv:2407.05365_, 2024.\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 19\n\n\n\n\n[36] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\nand A. Zeng, “Code as policies: Language model programs for embodied control,” in _2023 IEEE International Conference on Robotics_\n_and Automation (ICRA)_ . IEEE, 2023, pp. 9493–9500.\n\n[37] T. Webb, K. J. Holyoak, and H. Lu, “Emergent analogical reasoning\nin large language models,” _Nature Human Behaviour_, vol. 7, no. 9, pp.\n1526–1541, 2023.\n\n[38] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu\n_et al._, “Larger language models do in-context learning differently,”\n_arXiv preprint arXiv:2303.03846_, 2023.\n\n[39] J. Huang and K. C.-C. Chang, “Towards reasoning in large language\nmodels: A survey,” _arXiv preprint arXiv:2212.10403_, 2022.\n\n[40] J. Yu, X. Wang, S. Tu, S. Cao, D. Zhang-Li, X. Lv, H. Peng,\nZ. Yao _et al._, “Kola: Carefully benchmarking world knowledge of large\nlanguage models,” 2023.\n\n[41] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay,\nD. Fox, J. Thomason _et al._, “Progprompt: program generation for\nsituated robot task planning using large language models,” _Autonomous_\n_Robots_, pp. 1–14, 2023.\n\n[42] N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. Voss,\nA. Radford, D. Amodei _et al._, “Learning to summarize from human\nfeedback,” 2022.\n\n[43] E. Aky¨urek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou, “What\nlearning algorithm is in-context learning? investigations with linear\nmodels,” 2023.\n\n[44] Y. Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta,\nand J. Andreas, “Guiding Pretraining in Reinforcement Learning with\nLarge Language Models,” Sep. 2023.\n\n[45] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P.-Y. Oudeyer,\n“Grounding Large Language Models in Interactive Environments with\nOnline Reinforcement Learning,” Sep. 2023.\n\n[46] J. Lin, Y. Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein, and\nA. Dragan, “Learning to Model the World with Language,” Jul. 2023.\n\n[47] H. Li, X. Yang, Z. Wang, X. Zhu, J. Zhou, Y. Qiao, X. Wang, H. Li\n_et al._, “Auto mc-reward: Automated dense reward design with large\nlanguage models for minecraft,” 2023.\n\n[48] S. Chakraborty, K. Weerakoon, P. Poddar, M. Elnoor, P. Narayanan,\nC. Busart, P. Tokekar, A. S. Bedi _et al._, “RE-MOVE: An Adaptive\nPolicy Design for Robotic Navigation Tasks in Dynamic Environments\nvia Language-Based Feedback,” Sep. 2023.\n\n[49] J.-C. Pang, X.-Y. Yang, S.-H. Yang, and Y. Yu, “Natural languageconditioned reinforcement learning with inside-out task language\ndevelopment and translation,” 2023. [Online]. Available: [https:](https://arxiv.org/abs/2302.09368)\n[//arxiv.org/abs/2302.09368](https://arxiv.org/abs/2302.09368)\n\n[50] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang,\nD. Quillen, E. Holly _et al._, “Qt-opt: Scalable deep reinforcement\nlearning for vision-based robotic manipulation,” 2018.\n\n[51] K. Wang, B. Kang, J. Shao, and J. Feng, “Improving generalization\nin reinforcement learning with mixture regularization,” _Advances in_\n_Neural Information Processing Systems_, vol. 33, pp. 7968–7978, 2020.\n\n[52] R. Devidze, P. Kamalaruban, and A. Singla, “Exploration-guided\nreward shaping for reinforcement learning under sparse rewards,”\n_Advances in Neural Information Processing Systems_, vol. 35, pp. 5829–\n5842, 2022.\n\n[53] A. Singh, L. Yang, K. Hartikainen, C. Finn, and S. Levine, “End-toend robotic reinforcement learning without reward engineering,” _arXiv_\n_preprint arXiv:1904.07854_, 2019.\n\n[54] D. Hadfield-Menell, S. Milli, P. Abbeel, S. J. Russell, and A. Dragan,\n“Inverse reward design,” _Advances in neural information processing_\n_systems_, vol. 30, 2017.\n\n[55] C. Xiao, Y. Wu, C. Ma, D. Schuurmans, and M. M¨uller, “Learning\nto combat compounding-error in model-based reinforcement learning,”\n_arXiv preprint arXiv:1912.11206_, 2019.\n\n[56] T. M. Moerland, J. Broekens, A. Plaat, C. M. Jonker _et al._, “Modelbased reinforcement learning: A survey,” _Foundations and Trends® in_\n_Machine Learning_, vol. 16, no. 1, pp. 1–118, 2023.\n\n[57] M. Cho, J. Park, S. Lee, and Y. Sung, “Hard tasks first: Multitask reinforcement learning through task scheduling,” in _Forty-first_\n_International Conference on Machine Learning_, 2024. [Online].\n[Available: https://openreview.net/forum?id=haUOhXo70o](https://openreview.net/forum?id=haUOhXo70o)\n\n[58] J. Feng, M. Chen, Z. Pu, T. Qiu, and J. Yi, “Efficient multi-task\nreinforcement learning via task-specific action correction,” 2024.\n\n[[Online]. Available: https://arxiv.org/abs/2404.05950](https://arxiv.org/abs/2404.05950)\n\n[59] L. Sun, H. Zhang, W. Xu, and M. Tomizuka, “Paco: Parametercompositional multi-task reinforcement learning,” _Advances in Neural_\n_Information Processing Systems_, vol. 35, pp. 21 495–21 507, 2022.\n\n\n\n\n[60] G. Zhang, A. Jain, I. Hwang, S.-H. Sun, and J. J. Lim, “Efficient\nmulti-task reinforcement learning via selective behavior sharing,” 2024.\n\n[[Online]. Available: https://openreview.net/forum?id=LYGHdwyXUb](https://openreview.net/forum?id=LYGHdwyXUb)\n\n[61] N. Vithayathil Varghese and Q. H. Mahmoud, “A survey of multi-task\ndeep reinforcement learning,” _Electronics_, vol. 9, no. 9, 2020.\n\n[[Online]. Available: https://www.mdpi.com/2079-9292/9/9/1363](https://www.mdpi.com/2079-9292/9/9/1363)\n\n[62] T. Xiao, H. Chan, P. Sermanet, A. Wahid, A. Brohan, K. Hausman,\nS. Levine, and J. Tompson, “Robotic Skill Acquisition via Instruction\nAugmentation with Vision-Language Models,” Jul. 2023.\n\n[63] H. Yuan, C. Zhang, H. Wang, F. Xie, P. Cai, H. Dong, and Z. Lu,\n“Plan4mc: Skill reinforcement learning and planning for open-world\nminecraft tasks,” _arXiv preprint arXiv:2303.16563_, 2023.\n\n[64] J. Rocamonde, V. Montesinos, E. Nava, E. Perez, and D. Lindner,\n“Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning,” Oct. 2023.\n\n[65] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin _Neural Information Processing Systems_, 2017. [Online]. Available:\n[https://api.semanticscholar.org/CorpusID:13756489](https://api.semanticscholar.org/CorpusID:13756489)\n\n[66] M. Shanahan, “Talking about large language models,” _ArXiv_, vol.\n[abs/2212.03551, 2022. [Online]. Available: https://api.semanticscholar.](https://api.semanticscholar.org/CorpusID:254366666)\n[org/CorpusID:254366666](https://api.semanticscholar.org/CorpusID:254366666)\n\n[67] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal _et al._, “Llama: Open and\nefficient foundation language models,” _ArXiv_, vol. abs/2302.13971,\n[2023. [Online]. Available: https://api.semanticscholar.org/CorpusID:](https://api.semanticscholar.org/CorpusID:257219404)\n[257219404](https://api.semanticscholar.org/CorpusID:257219404)\n\n[68] J. Kaplan, S. McCandlish, T. J. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford _et al._, “Scaling laws for neural language\nmodels,” _ArXiv_, vol. abs/2001.08361, 2020. [Online]. Available:\n[https://api.semanticscholar.org/CorpusID:210861095](https://api.semanticscholar.org/CorpusID:210861095)\n\n[69] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\nE. Rutherford, D. de Las Casas, L. A. Hendricks _et al._, “Training\ncompute-optimal large language models,” _ArXiv_, vol. abs/2203.15556,\n[2022. [Online]. Available: https://api.semanticscholar.org/CorpusID:](https://api.semanticscholar.org/CorpusID:247778764)\n[247778764](https://api.semanticscholar.org/CorpusID:247778764)\n\n[70] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud,\nD. Yogatama, M. Bosma _et al._, “Emergent abilities of large language\nmodels,” _Trans. Mach. Learn. Res._, vol. 2022, 2022. [Online].\n[Available: https://api.semanticscholar.org/CorpusID:249674500](https://api.semanticscholar.org/CorpusID:249674500)\n\n[71] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\nA. Chaffin, A. Stiegler _et al._, “Multitask prompted training enables\nzero-shot task generalization,” _arXiv preprint arXiv:2110.08207_, 2021.\n\n[72] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal _et al._, “Training language models to follow\ninstructions with human feedback,” 2022.\n\n[73] Y. Cheng, H. Zhao, X. Zhou, J. Zhao, Y. Cao, and C. Yang, “Gaia–\na large language model for advanced power dispatch,” _arXiv preprint_\n_arXiv:2408.03847_, 2024.\n\n[74] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi,\nQ. Le _et al._, “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” 2023.\n\n[75] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\nlarge language models,” 2023.\n\n[76] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi,\nJ. Gajda, T. Lehmann, M. Podstawski _et al._, “Graph of thoughts:\nSolving elaborate problems with large language models,” 2023.\n\n[77] Z. Rao, Y. Wu, Z. Yang, W. Zhang, S. Lu, W. Lu, and Z. Zha, “Visual\nnavigation with multiple goals based on deep reinforcement learning,”\n_IEEE Transactions on Neural Networks and Learning Systems_, vol. 32,\nno. 12, pp. 5445–5455, 2021.\n\n[78] C. Huang, R. Zhang, M. Ouyang, P. Wei, J. Lin, J. Su, and L. Lin,\n“Deductive reinforcement learning for visual autonomous urban driving\nnavigation,” _IEEE Transactions on Neural Networks and Learning_\n_Systems_, vol. 32, no. 12, pp. 5379–5391, 2021.\n\n[79] M. Yang, W. Huang, W. Tu, Q. Qu, Y. Shen, and K. Lei, “Multitask\nlearning and reinforcement learning for personalized dialog generation: An empirical study,” _IEEE transactions on neural networks and_\n_learning systems_, vol. 32, no. 1, pp. 49–62, 2020.\n\n[80] Z. He, J. Li, F. Wu, H. Shi, and K.-S. Hwang, “Derl: Coupling\ndecomposition in action space for reinforcement learning task,” _IEEE_\n_Transactions on Emerging Topics in Computational Intelligence_, 2023.\n\n[81] Y. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. Zhang, Z. Shi\n_et al._, “A survey of visual transformers,” _IEEE Transactions on Neural_\n_Networks and Learning Systems_, 2023.\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 20\n\n\n\n\n[82] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman,\nY. Zhu, L. Fan _et al._, “Eureka: Human-Level Reward Design via\nCoding Large Language Models,” Oct. 2023.\n\n[83] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David,\nC. Finn, C. Fu _et al._, “Do As I Can, Not As I Say: Grounding Language\nin Robotic Affordances,” Aug. 2022.\n\n[84] K. Nottingham, P. Ammanabrolu, A. Suhr, Y. Choi, H. Hajishirzi,\nS. Singh, and R. Fox, “Do Embodied Agents Dream of Pixelated Sheep:\nEmbodied Decision Making using Language Guided World Modelling,”\nApr. 2023.\n\n[85] A. Srinivas, M. Laskin, and P. Abbeel, “CURL: Contrastive Unsupervised Representations for Reinforcement Learning,” Sep. 2020.\n\n[86] M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. Courville, and\nP. Bachman, “Data-efficient reinforcement learning with self-predictive\nrepresentations,” _arXiv preprint arXiv:2007.05929_, 2020.\n\n[87] F. Paischer, T. Adler, V. Patil, A. Bitto-Nemling, M. Holzleitner,\nS. Lehner, H. Eghbal-zadeh, and S. Hochreiter, “History Compression\nvia Language Models in Reinforcement Learning,” Feb. 2023.\n\n[88] F. Paischer, T. Adler, M. Hofmarcher, and S. Hochreiter, “Semantic\nhelm: A human-readable memory for reinforcement learning,” _Ad-_\n_vances in Neural Information Processing Systems_, vol. 36, 2024.\n\n[89] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell _et al._, “Learning transferable visual models from\nnatural language supervision,” in _International conference on machine_\n_learning_ . PMLR, 2021, pp. 8748–8763.\n\n[90] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov,\n“Transformer-xl: Attentive language models beyond a fixed-length\n[context,” 2019. [Online]. Available: https://arxiv.org/abs/1901.02860](https://arxiv.org/abs/1901.02860)\n\n[91] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with\ncontrastive predictive coding,” _arXiv preprint arXiv:1807.03748_, 2018.\n\n[92] W. K. Kim, S. Kim, H. Woo _et al._, “Efficient policy adaptation with\ncontrastive prompt ensemble for embodied agents,” _Advances in Neural_\n_Information Processing Systems_, vol. 36, 2024.\n\n[93] R. P. Poudel, H. Pandya, S. Liwicki, and R. Cipolla, “Recore:\nRegularized contrastive representation learning of world model,” in\n_Proceedings of the IEEE/CVF Conference on Computer Vision and_\n_Pattern Recognition_, 2024, pp. 22 904–22 913.\n\n[94] S. Basavatia, K. Murugesan, and S. Ratnakar, “Starling: Self-supervised\ntraining of text-based reinforcement learning agent with large language\nmodels,” _arXiv preprint arXiv:2406.05872_, 2024.\n\n[95] R. Patel, E. Pavlick, and S. Tellex, “Grounding language to nonmarkovian tasks with no supervision of task specifications.” in\n_Robotics: Science and Systems_, vol. 2020, 2020.\n\n[96] T. R. Sumers, M. K. Ho, R. D. Hawkins, K. Narasimhan, and T. L.\nGriffiths, “Learning rewards from linguistic feedback,” in _Proceedings_\n_of the AAAI Conference on Artificial Intelligence_, vol. 35, no. 7, 2021,\npp. 6002–6010.\n\n[97] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\nand A. Zeng, “Code as Policies: Language Model Programs for\nEmbodied Control,” May 2023.\n\n[98] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su,\n“Llm-planner: Few-shot grounded planning for embodied agents with\nlarge language models,” in _Proceedings of the IEEE/CVF International_\n_Conference on Computer Vision_, 2023, pp. 2998–3009.\n\n[99] B. A. Spiegel, Z. Yang, W. Jurayj, B. Bachmann, S. Tellex, and\nG. Konidaris, “Informing reinforcement learning agents by grounding\nlanguage to markov decision processes,” in _Workshop on Training_\n_Agents with Foundation Models at RLC 2024_, 2024. [Online].\n[Available: https://openreview.net/forum?id=uFm9e4Ly26](https://openreview.net/forum?id=uFm9e4Ly26)\n\n[100] B. Gordon, Y. Bitton, Y. Shafir, R. Garg, X. Chen, D. Lischinski,\nD. Cohen-Or, and I. Szpektor, “Mismatch quest: Visual and textual\nfeedback for image-text misalignment,” in _European Conference on_\n_Computer Vision_ . Springer, 2025, pp. 310–328.\n\n[101] Y. Wang, J. He, D. Wang, Q. Wang, B. Wan, and X. Luo, “Multimodal\ntransformer with adaptive modality weighting for multimodal sentiment\nanalysis,” _Neurocomputing_, vol. 572, p. 127181, 2024.\n\n[102] X. Zhao, S. Poria, X. Li, Y. Chen, and B. Tang, “Toward robust\nmultimodal learning using multimodal foundational models,” _arXiv_\n_preprint arXiv:2401.13697_, 2024.\n\n[103] F. Lygerakis, V. Dave, and E. Rueckert, “M2curl: Sample-efficient\nmultimodal reinforcement learning via self-supervised representation\nlearning for robotic manipulation,” _arXiv preprint arXiv:2401.17032_,\n2024.\n\n[104] J. Eschmann, “Reward function design in reinforcement learning,”\n_Reinforcement Learning Algorithms: Analysis and Applications_, pp.\n25–33, 2021.\n\n\n\n\n[105] J. Andreas, D. Klein, and S. Levine, “Modular multitask reinforcement\nlearning with policy sketches,” in _International conference on machine_\n_learning_ . PMLR, 2017, pp. 166–175.\n\n[106] S. Mirchandani, S. Karamcheti, and D. Sadigh, “Ella: Exploration\nthrough learned language abstraction,” _Advances in Neural Information_\n_Processing Systems_, vol. 34, pp. 29 529–29 540, 2021.\n\n[107] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, “Reward Design\nwith Language Models,” in _The Eleventh International Conference on_\n_Learning Representations_, Sep. 2022.\n\n[108] Y. Wu, Y. Fan, P. P. Liang, A. Azaria, Y. Li, and T. M. Mitchell,\n“Read and Reap the Rewards: Learning to Play Atari with the Help of\nInstruction Manuals,” Oct. 2023.\n\n[109] K. Chu, X. Zhao, C. Weber, M. Li, and S. Wermter, “Accelerating\nReinforcement Learning of Robotic Manipulations via Feedback from\nLarge Language Models,” Nov. 2023.\n\n[110] A. Adeniji, A. Xie, C. Sferrazza, Y. Seo, S. James, and P. Abbeel,\n“Language reward modulation for pretraining reinforcement learning,”\n_arXiv preprint arXiv:2308.12270_, 2023.\n\n[111] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,” 2020. [Online].\n[Available: https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)\n\n[112] C. Kim, Y. Seo, H. Liu, L. Lee, J. Shin, H. Lee, and K. Lee, “Guide\nYour Agent with Adaptive Multimodal Rewards,” Oct. 2023.\n\n[113] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, “R3m: A\nuniversal visual representation for robot manipulation,” 2022. [Online].\n[Available: https://arxiv.org/abs/2203.12601](https://arxiv.org/abs/2203.12601)\n\n[114] Y. Seo, D. Hafner, H. Liu, F. Liu, S. James, K. Lee, and P. Abbeel,\n“Masked World Models for Visual Control,” May 2023.\n\n[115] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar,\nJ. Hamburger, H. Jiang _et al._, “Ego4d: Around the world in 3,000 hours\nof egocentric video,” in _Proceedings of the IEEE/CVF Conference on_\n_Computer Vision and Pattern Recognition_, 2022, pp. 18 995–19 012.\n\n[116] Y. Wang, Z. Sun, J. Zhang, Z. Xian, E. Biyik, D. Held,\nand Z. Erickson, “Rl-vlm-f: Reinforcement learning from vision\nlanguage foundation model feedback,” 2024. [Online]. Available:\n[https://arxiv.org/abs/2402.03681](https://arxiv.org/abs/2402.03681)\n\n[117] W. Yu, N. Gileadi, C. Fu, S. Kirmani, K.-H. Lee, M. G. Arenas, H.T. L. Chiang, T. Erez _et al._, “Language to rewards for robotic skill\nsynthesis,” _arXiv preprint arXiv:2306.08647_, 2023.\n\n[118] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe,\nU. Alon, N. Dziri _et al._, “Self-refine: Iterative refinement with selffeedback,” 2023.\n\n[119] J. Song, Z. Zhou, J. Liu, C. Fang, Z. Shu, and L. Ma, “Self-Refined\nLarge Language Model as Automated Reward Function Designer for\nDeep Reinforcement Learning in Robotics,” Oct. 2023.\n\n[120] T. Xie, S. Zhao, C. H. Wu, Y. Liu, Q. Luo, V. Zhong,\nY. Yang, and T. Yu, “Text2reward: Reward shaping with language\nmodels for reinforcement learning,” in _The Twelfth International_\n_Conference on Learning Representations_, 2024. [Online]. Available:\n[https://openreview.net/forum?id=tUM39YTRxH](https://openreview.net/forum?id=tUM39YTRxH)\n\n[121] E. Ferrara, “Should chatgpt be biased? challenges and risks of bias in\nlarge language models,” _arXiv preprint arXiv:2304.03738_, 2023.\n\n[122] I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim,\nF. Dernoncourt, T. Yu, R. Zhang _et al._, “Bias and fairness in large\nlanguage models: A survey,” _Computational Linguistics_, pp. 1–79,\n2024.\n\n[123] L. Gao, J. Schulman, and J. Hilton, “Scaling laws for reward model\noveroptimization,” in _Proceedings of the 40th International Conference_\n_on Machine Learning_, ser. Proceedings of Machine Learning Research,\nA. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and\nJ. Scarlett, Eds., vol. 202. PMLR, 23–29 Jul 2023, pp. 10 835–10 866.\n\n[124] S. Chakraborty, A. Bhaskar, A. Singh, P. Tokekar, D. Manocha,\nand A. S. Bedi, “Rebel: A regularization-based solution for reward\noveroptimization in reinforcement learning from human feedback,”\n_arXiv preprint arXiv:2312.14436_, 2023.\n\n[125] A. Radwan, L. Zaafarani, J. Abudawood, F. AlZahrani, and F. Fourat,\n“Addressing bias through ensemble learning and regularized finetuning,” _arXiv preprint arXiv:2402.00910_, 2024.\n\n[126] Y. Inoue and H. Ohashi, “Prompter: Utilizing large language model\nprompting for a data efficient embodied instruction following,” _arXiv_\n_preprint arXiv:2211.03267_, 2022.\n\n[127] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and\nD. Batra, “Improving vision-and-language navigation with image-text\npairs from the web,” in _Computer Vision–ECCV 2020: 16th European_\n_Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI_\n_16_ . Springer, 2020, pp. 259–274.\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 21\n\n\n\n\n[128] M. Janner, Q. Li, and S. Levine, “Offline reinforcement learning as\none big sequence modeling problem,” _Advances in neural information_\n_processing systems_, vol. 34, pp. 1273–1286, 2021.\n\n[129] S. Li, X. Puig, C. Paxton, Y. Du, C. Wang, L. Fan, T. Chen, D.-A.\nHuang _et al._, “Pre-trained language models for interactive decisionmaking,” _Advances in Neural Information Processing Systems_, vol. 35,\npp. 31 199–31 212, 2022.\n\n[130] R. Shi, Y. Liu, Y. Ze, S. S. Du, and H. Xu, “Unleashing the power of\npre-trained language models for offline reinforcement learning,” _arXiv_\n_preprint arXiv:2310.20587_, 2023.\n\n[131] Z. Zeng, C. Zhang, S. Wang, and C. Sun, “Goal-conditioned predictive\ncoding for offline reinforcement learning,” _Advances in Neural Infor-_\n_mation Processing Systems_, vol. 36, 2024.\n\n[132] M. Reid, Y. Yamada, and S. S. Gu, “Can Wikipedia Help Offline\nReinforcement Learning?” Jul. 2022.\n\n[133] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, “D4rl:\nDatasets for deep data-driven reinforcement learning,” _arXiv preprint_\n_arXiv:2004.07219_, 2020.\n\n[134] L. Mezghani, P. Bojanowski, K. Alahari, and S. Sukhbaatar, “Think\nbefore you act: Unified policy for interleaving language reasoning with\nactions,” _arXiv preprint arXiv:2304.11063_, 2023.\n\n[135] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart\n_et al._, “Rt-2: Vision-language-action models transfer web knowledge\nto robotic control,” in _Conference on Robot Learning_ . PMLR, 2023,\npp. 2165–2183.\n\n[136] S. Yao, R. Rao, M. Hausknecht, and K. Narasimhan, “Keep calm and\nexplore: Language models for action generation in text-based games,”\n_arXiv preprint arXiv:2010.02903_, 2020.\n\n[137] M. Hausknecht, P. Ammanabrolu, M.-A. Cˆot´e, and X. Yuan, “Interactive fiction games: A colossal adventure,” in _Proceedings of the AAAI_\n_Conference on Artificial Intelligence_, vol. 34, no. 05, 2020, pp. 7903–\n7910.\n\n[138] H. Hu and D. Sadigh, “Language Instructed Reinforcement Learning\nfor Human-AI Coordination,” Jun. 2023.\n\n[139] Z. Zhou, B. Hu, P. Zhang, C. Zhao, and B. Liu, “Large language model\nis a good policy teacher for training reinforcement learning agents,”\n_arXiv preprint arXiv:2311.13373_, 2023.\n\n[140] M. Dalal, T. Chiruvolu, D. S. Chaplot, and R. Salakhutdinov,\n“Plan-seq-learn: Language model guided RL for solving long\nhorizon robotics tasks,” in _The Twelfth International Conference_\n_on_ _Learning_ _Representations_, 2024. [Online]. Available: [https:](https://openreview.net/forum?id=hQVCCxQrYN)\n[//openreview.net/forum?id=hQVCCxQrYN](https://openreview.net/forum?id=hQVCCxQrYN)\n\n[141] J. Garcıa and F. Fern´andez, “A comprehensive survey on safe reinforcement learning,” _Journal of Machine Learning Research_, vol. 16,\nno. 1, pp. 1437–1480, 2015.\n\n[142] H. Xu, X. Zhan, and X. Zhu, “Constraints penalized q-learning for safe\noffline reinforcement learning,” in _Proceedings of the AAAI Conference_\n_on Artificial Intelligence_, vol. 36, no. 8, 2022, pp. 8753–8760.\n\n[143] J. Lee, C. Paduraru, D. J. Mankowitz, N. Heess, D. Precup, K.-E.\nKim, and A. Guez, “Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation,” _arXiv preprint_\n_arXiv:2204.08957_, 2022.\n\n[144] Z. Liu, Z. Guo, Y. Yao, Z. Cen, W. Yu, T. Zhang, and D. Zhao, “Constrained decision transformer for offline safe reinforcement learning,”\nin _International Conference on Machine Learning_ . PMLR, 2023, pp.\n21 611–21 630.\n\n[145] S. Naihin, D. Atkinson, M. Green, M. Hamadi, C. Swift, D. Schonholtz,\nA. T. Kalai, and D. Bau, “Testing language model agents safely in the\nwild,” _arXiv preprint arXiv:2311.10538_, 2023.\n\n[146] W. Huang, H. Liu, Z. Huang, and C. Lv, “Safety-aware human-inthe-loop reinforcement learning with shared control for autonomous\ndriving,” _IEEE Transactions on Intelligent Transportation Systems_, pp.\n1–12, 2024.\n\n[147] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\nand S. Yao, “Reflexion: Language Agents with Verbal Reinforcement\nLearning,” Oct. 2023.\n\n[148] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\n2021.\n\n[149] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, “Dream to Control:\nLearning Behaviors by Latent Imagination,” Mar. 2020.\n\n[150] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba, “Mastering Atari with\nDiscrete World Models,” Feb. 2022.\n\n[151] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, “Dream to control:\nLearning behaviors by latent imagination,” 2020.\n\n\n\n\n[152] Y. Matsuo, Y. LeCun, M. Sahani, D. Precup, D. Silver, M. Sugiyama,\nE. Uchibe, and J. Morimoto, “Deep learning, reinforcement learning,\nand world models,” _Neural Networks_, vol. 152, pp. 267–275, 2022.\n\n[153] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin,\nP. Abbeel, A. Srinivas _et al._, “Decision transformer: Reinforcement\nlearning via sequence modeling,” _Advances in neural information_\n_processing systems_, vol. 34, pp. 15 084–15 097, 2021.\n\n[154] V. Micheli, E. Alonso, and F. Fleuret, “Transformers are SampleEfficient World Models,” in _The Eleventh International Conference on_\n_Learning Representations_, Sep. 2022.\n\n[155] J. Robine, M. H¨oftmann, T. Uelwer, and S. Harmeling, “Transformerbased World Models Are Happy With 100k Interactions,” Mar. 2023.\n\n[156] C. Chen, Y.-F. Wu, J. Yoon, and S. Ahn, “TransDreamer: Reinforcement Learning with Transformer World Models,” Feb. 2022.\n\n[157] Y. Seo, K. Lee, S. James, and P. Abbeel, “Reinforcement Learning\nwith Action-Free Pre-Training from Videos,” Jun. 2022.\n\n[158] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and\nJ. Davidson, “Learning latent dynamics for planning from pixels,” in\n_International conference on machine learning_ . PMLR, 2019, pp.\n2555–2565.\n\n[159] R. P. K. Poudel, H. Pandya, C. Zhang, and R. Cipolla, “LanGWM:\nLanguage Grounded World Model,” Nov. 2023.\n\n[160] S. Milani, N. Topin, M. Veloso, and F. Fang, “A survey of explainable\nreinforcement learning,” 2022.\n\n[161] D. Das, S. Chernova, and B. Kim, “State2explanation: Concept-based\nexplanations to benefit agent learning and user understanding,”\nin _Thirty-seventh Conference on Neural Information Processing_\n_Systems_ [, 2023. [Online]. Available: https://openreview.net/forum?id=](https://openreview.net/forum?id=xGz0wAIJrS)\n[xGz0wAIJrS](https://openreview.net/forum?id=xGz0wAIJrS)\n\n[162] J. Lin, Y. Du, O. Watkins, D. Hafner, P. Abbeel, D. Klein, and\nA. Dragan, “Learning to model the world with language,” _arXiv_\n_preprint arXiv:2308.01399_, 2023.\n\n[163] W. Lu, X. Zhao, S. Magg, M. Gromniak, M. Li, and S. Wermterl,\n“A closer look at reward decomposition for high-level robotic explanations,” in _2023 IEEE International Conference on Development and_\n_Learning (ICDL)_ . IEEE, 2023, pp. 429–436.\n\n[164] C. Yu, J. Liu, S. Nemati, and G. Yin, “Reinforcement learning in\nhealthcare: A survey,” _ACM Computing Surveys (CSUR)_, vol. 55, no. 1,\npp. 1–36, 2021.\n\n[165] S. Agashe, Y. Fan, and X. E. Wang, “Evaluating multi-agent\ncoordination abilities in large language models,” _arXiv preprint_\n_arXiv:2310.03903_, 2023.\n\n[166] S. S. Kannan, V. L. Venkatesh, and B.-C. Min, “Smart-llm: Smart\nmulti-agent robot task planning using large language models,” _arXiv_\n_preprint arXiv:2309.10062_, 2023.\n\n[167] O. Slumbers, D. H. Mguni, K. Shao, and J. Wang, “Leveraging large\nlanguage models for optimised coordination in textual multi-agent\nreinforcement learning,” 2023.\n\n[168] C. Sun, S. Huang, and D. Pompili, “Llm-based multi-agent reinforcement learning: Current and future directions,” _arXiv preprint_\n_arXiv:2405.11106_, 2024.\n\n[169] J. Lee, A. Xie, A. Pacchiano, Y. Chandak, C. Finn, O. Nachum, and\nE. Brunskill, “Supervised pretraining can learn in-context reinforcement learning,” _Advances in Neural Information Processing Systems_,\nvol. 36, 2024.\n\n[170] Z. Buc¸inca, S. Swaroop, A. E. Paluch, S. A. Murphy, and K. Z.\nGajos, “Towards optimizing human-centric objectives in ai-assisted\ndecision-making with offline reinforcement learning,” _arXiv preprint_\n_arXiv:2403.05911_, 2024.\n\n[171] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K¨uttler, M. Lewis _et_ _al._, “Retrieval-augmented generation\nfor knowledge-intensive nlp tasks,” 2021. [Online]. Available:\n[https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)\n\n[172] L. Wang, X. Zhang, H. Su, and J. Zhu, “A comprehensive survey of\ncontinual learning: theory, method and application,” _IEEE Transactions_\n_on Pattern Analysis and Machine Intelligence_, 2024.\n\n[173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\nJ. Tang _et al._, “A survey on large language model based autonomous\nagents,” _Frontiers of Computer Science_, vol. 18, no. 6, p. 186345, 2024.\n\n[174] Y. Cheng, C. Zhang, Z. Zhang, X. Meng, S. Hong, W. Li, Z. Wang,\nZ. Wang _et al._, “Exploring large language model based intelligent agents: Definitions, methods, and prospects,” _arXiv preprint_\n_arXiv:2401.03428_, 2024.\n\n[175] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling,\nP. Rohlfshagen, S. Tavener, D. Perez _et al._, “A survey of monte carlo\ntree search methods,” _IEEE Transactions on Computational Intelligence_\n_and AI in games_, vol. 4, no. 1, pp. 1–43, 2012.\n\n\n\n\nIEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. X, NO. X, XXX 2024 22\n\n\n[176] Z. Zhang, X. Bo, C. Ma, R. Li, X. Chen, Q. Dai, J. Zhu, Z. Dong _et al._,\n“A survey on the memory mechanism of large language model based\n[agents,” 2024. [Online]. Available: https://arxiv.org/abs/2404.13501](https://arxiv.org/abs/2404.13501)\n\n[177] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla:\nLarge language model connected with massive apis,” _arXiv preprint_\n_arXiv:2305.15334_, 2023.\n\n[178] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse,\nS. Jain _et al._, “Webgpt: Browser-assisted question-answering with\nhuman feedback,” _arXiv preprint arXiv:2112.09332_, 2021.\n\n[179] Y. Talebirad and A. Nadiri, “Multi-agent collaboration: Harnessing\nthe power of intelligent llm agents,” _arXiv preprint arXiv:2306.03314_,\n2023.\n\n[180] Z. Lin, S. Trivedi, and J. Sun, “Generating with confidence:\nUncertainty quantification for black-box large language models,”\n[2024. [Online]. Available: https://arxiv.org/abs/2305.19187](https://arxiv.org/abs/2305.19187)\n\n[181] R. S. Y. C. Tan, Q. Lin, G. H. Low, R. Lin, T. C. Goh, C. C. E.\nChang, F. F. Lee, W. Y. Chan _et al._, “Inferring cancer disease response\nfrom radiology reports using large language models with data augmentation and prompting,” _Journal of the American Medical Informatics_\n_Association_, vol. 30, no. 10, pp. 1657–1664, 2023.\n\n[182] Q. Gao, C. Zhao, Y. Sun, T. Xi, G. Zhang, B. Ghanem, and J. Zhang, “A\nunified continual learning framework with general parameter-efficient\ntuning,” in _Proceedings of the IEEE/CVF International Conference on_\n_Computer Vision_, 2023, pp. 11 483–11 493.\n\n[183] W. Zhou, Y. E. Jiang, R. Cotterell, and M. Sachan, “Efficient\nprompting via dynamic in-context learning,” 2023. [Online]. Available:\n[https://arxiv.org/abs/2305.11170](https://arxiv.org/abs/2305.11170)\n\n[184] Z.-F. Gao, P. Liu, W. X. Zhao, Z.-Y. Lu, and J.-R. Wen, “Parameterefficient mixture-of-experts architecture for pre-trained language\n[models,” 2022. [Online]. Available: https://arxiv.org/abs/2203.01104](https://arxiv.org/abs/2203.01104)\n\n[185] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling\nwith selective state spaces,” 2024. [Online]. Available: [https:](https://arxiv.org/abs/2312.00752)\n[//arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)\n\n[186] L. D. Corro, A. D. Giorno, S. Agarwal, B. Yu, A. Awadallah,\nand S. Mukherjee, “Skipdecode: Autoregressive skip decoding with\nbatching and caching for efficient llm inference,” 2023. [Online].\n[Available: https://arxiv.org/abs/2307.02628](https://arxiv.org/abs/2307.02628)\n\n[187] Y. Chen, Z. han Ding, Z. Wang, Y. Wang, L. Zhang, and S. Liu,\n“Asynchronous large language model enhanced planner for autonomous\n[driving,” 2024. [Online]. Available: https://arxiv.org/abs/2406.14556](https://arxiv.org/abs/2406.14556)\n\n[188] Z. Charles, A. Ganesh, R. McKenna, H. B. McMahan, N. Mitchell,\nK. Pillutla, and K. Rush, “Fine-tuning large language models\nwith user-level differential privacy,” 2024. [Online]. Available:\n[https://arxiv.org/abs/2407.07737](https://arxiv.org/abs/2407.07737)\n\n[189] E. Cambria, L. Malandri, F. Mercorio, N. Nobani, and A. Seveso,\n“Xai meets llms: A survey of the relation between explainable\n[ai and large language models,” 2024. [Online]. Available: https:](https://arxiv.org/abs/2407.15248)\n[//arxiv.org/abs/2407.15248](https://arxiv.org/abs/2407.15248)\n\n[190] S. Xhonneux, A. Sordoni, S. G¨unnemann, G. Gidel, and L. Schwinn,\n“Efficient adversarial training in llms with continuous attacks,” 2024.\n\n[[Online]. Available: https://arxiv.org/abs/2405.15589](https://arxiv.org/abs/2405.15589)\n\n[191] C. Deng, Y. Duan, X. Jin, H. Chang, Y. Tian, H. Liu, H. P. Zou,\nY. Jin _et al._, “Deconstructing the ethics of large language models from\nlong-standing issues to new-emerging dilemmas,” 2024. [Online].\n[Available: https://arxiv.org/abs/2406.05392](https://arxiv.org/abs/2406.05392)\n\n\n",
    "ranking": {
      "relevance_score": 0.7234767329855272,
      "citation_score": 0.9,
      "recency_score": 0.7469354320643911,
      "final_score": 0.7611272562963081
    },
    "citation_key": "Cao2024SurveyOL",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7",
    "title": "Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for Addressing Value Estimation Errors",
    "published": "2020-01-09",
    "authors": [
      "Jingliang Duan",
      "Yang Guan",
      "S. Li",
      "Yangang Ren",
      "Qi Sun",
      "B. Cheng"
    ],
    "summary": "In reinforcement learning (RL), function approximation errors are known to easily lead to the <inline-formula> <tex-math notation=\"LaTeX\">$Q$ </tex-math></inline-formula>-value overestimations, thus greatly reducing policy performance. This article presents a distributional soft actor–critic (DSAC) algorithm, which is an off-policy RL method for continuous control setting, to improve the policy performance by mitigating <inline-formula> <tex-math notation=\"LaTeX\">$Q$ </tex-math></inline-formula>-value overestimations. We first discover in theory that learning a distribution function of state–action returns can effectively mitigate <inline-formula> <tex-math notation=\"LaTeX\">$Q$ </tex-math></inline-formula>-value overestimations because it is capable of adaptively adjusting the update step size of the <inline-formula> <tex-math notation=\"LaTeX\">$Q$ </tex-math></inline-formula>-value function. Then, a distributional soft policy iteration (DSPI) framework is developed by embedding the return distribution function into maximum entropy RL. Finally, we present a deep off-policy actor–critic variant of DSPI, called DSAC, which directly learns a continuous return distribution by keeping the variance of the state–action returns within a reasonable range to address exploding and vanishing gradient problems. We evaluate DSAC on the suite of MuJoCo continuous control tasks, achieving the state-of-the-art performance.",
    "pdf_url": "https://arxiv.org/pdf/2001.02811",
    "doi": "10.1109/TNNLS.2021.3082568",
    "fields_of_study": [
      "Computer Science",
      "Medicine",
      "Engineering"
    ],
    "venue": "IEEE Transactions on Neural Networks and Learning Systems",
    "citation_count": 241,
    "bibtex": "@Article{Duan2020DistributionalSA,\n author = {Jingliang Duan and Yang Guan and S. Li and Yangang Ren and Qi Sun and B. Cheng},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {6584-6598},\n title = {Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for Addressing Value Estimation Errors},\n volume = {33},\n year = {2020}\n}\n",
    "markdown_text": "1\n\n\n## Distributional Soft Actor-Critic: Off-Policy Reinforcement Learning for Addressing Value Estimation Errors\n\nJingliang Duan, Yang Guan, Shengbo Eben Li*, Yangang Ren, Qi Sun, and Bo Cheng\n\n\n\n_**Abstract**_ **—In reinforcement learning (RL), function approxi-**\n**mation errors are known to easily lead to the Q-value overesti-**\n**mations, thus greatly reducing policy performance. This paper**\n**presents a distributional soft actor-critic (DSAC) algorithm,**\n**which is an off-policy RL method for continuous control setting,**\n**to improve the policy performance by mitigating Q-value overes-**\n**timations. We first discover in theory that learning a distribution**\n**function of state-action returns can effectively mitigate Q-value**\n**overestimations because it is capable of adaptively adjusting the**\n**update stepsize of the Q-value function. Then, a distributional soft**\n**policy iteration (DSPI) framework is developed by embedding the**\n**return distribution function into maximum entropy RL. Finally,**\n**we present a deep off-policy actor-critic variant of DSPI, called**\n**DSAC, which directly learns a continuous return distribution**\n**by keeping the variance of the state-action returns within a**\n**reasonable range to address exploding and vanishing gradient**\n**problems. We evaluate DSAC on the suite of MuJoCo continuous**\n**control tasks, achieving the state-of-the-art performance.**\n\n\n_**Index Terms**_ **—Reinforcement learning, overestimation, distri-**\n**butional soft actor-critic (DSAC).**\n\n\nI. INTRODUCTION\n# D EEP neural networks (NNs) provide rich representationsthat can enable reinforcement learning (RL) algorithms\n\nto master a variety of challenging domains, from games to\nrobotic control [1]–[5]. However, most RL algorithms tend to\nlearn unrealistically high state-action values (i.e., Q-values),\nknown as overestimations, thereby resulting in suboptimal\npolicies.\nThe overestimations of RL were first found in the Qlearning algorithm [6], which is the prototype of most existing\nvalue-based RL algorithms [7]. For this algorithm, van Hasselt\n_et al._ (2016) demonstrated that any kind of estimation errors\ncan induce an upward bias, irrespective of whether these errors\nare caused by system noise, function approximation, or any\nother sources [8]. The overestimation bias is firstly induced\nby the max operator over all noisy Q-estimates of the same\n\n\nThis study is supported by Beijing NSF with JQ18010, and NSF China\nwith 51575293, and U20A20334. Special thanks should be given to TOYOTA\nfor funding this study. Jingliang Duan and Yang Guan contributed equally\nto this work. All correspondences should be sent to S. Li with email:\nlisb04@gmail.com.\nJ. Duan, Y. Guan, S. Li, Y. Ren, Q. Sun, and B. Cheng are with\nState Key Lab of Automotive Safety and Energy, School of Vehicle\nand Mobility, Tsinghua University, Beijing, 100084, China. They are\nalso with Center for Intelligent Connected Vehicles and Transportation,\nTsinghua University. Email: duanjl15@163.com; (guany17,\nryg18)@mails.tsinghua.edu.cn; (lishbo, qisun,\nchengbo)@tsinghua.edu.cn.\n\n\n\nstate, which tends to prefer overestimated to underestimated\nQ-values [9]–[11]. This overestimation bias will be further\npropagated and exaggerated through the temporal difference\nlearning [7], wherein the Q-estimate of a state is updated using\nthe Q-estimate of its subsequent state. Deep RL algorithms,\nsuch as Deep Q-Networks (DQN) [1], employ a deep NN to\nestimate the Q-value. Although the deep NN can provide rich\nrepresentations with the potential for low asymptotic approximation errors, overestimations still exist, even in deterministic\nenvironments [8], [12]. Fujimoto _et al._ (2018) showed that the\noverestimation problem also persists in actor-critic RL [12],\nsuch as Deterministic Policy Gradient (DPG) and Deep DPG\n(DDPG) [13], [14]. In practice, inaccurate estimation exists\nin almost all RL algorithms because, on the one hand, any\nalgorithm will introduce some estimation biases and variances,\nsimply due to the true Q-values are initially unknown [7].\nOn the other hand, function approximation errors are usually\nunavoidable. This is particularly problematic because inaccurate estimation can cause arbitrarily suboptimal actions to be\noverestimated, resulting in a suboptimal policy.\nTo reduce overestimations in standard Q-learning, Double\nQ-learning [15] was developed to decouple the max operation\ninto action selection and evaluation. To update one of these two\nQ-networks, one Q-network is used to determine the greedy\npolicy, while another Q-network is used to determine its value,\nresulting in unbiased estimates. Double DQN [8], a deep\nvariant of Double Q-learning, deals with the overestimation\nproblem of DQN, in which the target Q-network of DQN provides a natural candidate for the second Q-network. However,\nthese two methods can only handle discrete action spaces.\nFujimoto _et al._ (2018) developed actor-critic variants of the\nstandard Double DQN and Double Q-learning for continuous\ncontrol, by making action selections using the policy optimized\nwith respect to the corresponding Q-estimate [12]. However,\nthe actor-critic Double DQN suffers from similar overestimations as DDPG, because the online and target Q-estimates\nare too similar to provide an independent estimation. While\nactor-critic Double Q-learning is more effective, it introduces\nadditional Q and policy networks at the cost of increasing\nthe computation time for each iteration. Finally, Fujimoto _et_\n_al._ (2018) proposed Clipped Double Q-learning by taking\nthe minimum value between the two Q-estimates [12], which\nis used in Twin Delayed Deep Deterministic policy gradient\n(TD3) and Soft Actor-Critic (SAC) [16], [17]. However, this\nmethod may introduce a considerable underestimation bias and\nstill requires an additional Q-network.\n\n\n\n©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other works.\n\n\n\n\nIn this paper, we propose a new RL algorithm, called\ndistributional soft actor-critic (DSAC), to improve policy performance by mitigating Q-value overestimations. The contributions and novelty of this paper are summarized as follows:\n\n\n1) A distributional soft policy iteration (DSPI) framework is\ndeveloped by embedding the return distribution function\nin maximum entropy RL to learn a continuous distribution\nof state-action returns (also called return distribution).\nThe impact of the return distribution learning on the\naccuracy of Q-value estimation was barely discussed in\nexisting distributional RL algorithms, such as [18]–[23].\nIn this paper, we first found that the Q-value overestimations can be mitigated by learning a distribution function\nof state-action returns. This is because that compared with\nmost RL algorithms that directly learn the expectation\nof state-action returns (i.e., Q-value) [1], [3], [8], [12],\n\n[14], [16], the return distribution learning is capable of\nadaptively adjusting the update stepsize of Q-values.\n2) Based on the developed DSPI framework, we propose\nthe DSAC algorithm by replacing the clipped double Qlearning of SAC [16], [17] with the return distribution\nlearning. In comparison with RL algorithms that use\ndouble value networks to mitigate overestimations [8],\n\n[12], [15]–[17], DSAC improves the Q-value estimation\naccuracy by only employing a single return distribution\nnetwork, which also leads to higher time efficiency.\n3) Different from existing distributional RL algorithms that\nlearn a discrete return distribution [18]–[23], the proposed DSAC is capable of learning a continuous return\ndistribution by keeping the variance of the state-action\nreturns within a reasonable range to address exploding\nand vanishing gradient problems. Therefore, DSAC relaxes the need for human-designed discrete ranges and\nintervals. Besides, compared with most distributional\nRL algorithms that can only handle discrete and lowdimensional action spaces [18]–[22], DSAC is applicable\nto continuous control settings by optimizing an independent stochastic policy network.\n4) Experiments on MuJoCo benchmarks demonstrate that\nthe proposed DSAC algorithm outperforms or matches\nall baselines across all benchmark tasks in terms of the\n\nfinal performance.\n\nThe paper is organized as follows. In Section II, we introduce the related works. Section III describes some preliminaries of RL and develops a DSPI framework. In Section IV, we\nanalyze the role of the distributional return function in solving\noverestimations. Section V presents the DSAC algorithm and\nPABAL architecture. In Section VI, we present experimental\nresults that show the efficacy of DSAC. Section VII concludes\nthis paper.\n\n\nII. RELATED WORK\n\n\nOver the last decade, numerous deep RL algorithms have\nappeared [1], [3], [12], [14], [16], [23]–[26]. This paper aims\nto propose a new RL algorithm to mitigate Q-value overestimations by learning a distribution of state-action returns,\nthereby improving policy performance. We also incorporate\n\n\n\n2\n\n\nthe off-policy formulation to improve sample efficiency, and\nthe maximum entropy framework based on the stochastic\npolicy to encourage exploration. Besides, our algorithm mainly\nfocuses on continuous control setting. With reference to algorithms such as DDPG [14], the off-policy learning and\ncontinuous control can be easily enabled by learning separate\nQ and policy networks in an actor-critic architecture. Therefore, we mainly review prior works on the maximum entropy\nframework and distributional RL in this section.\n\nMaximum entropy RL favors stochastic policies by augmenting the optimization objective with the expected policy\nentropy. While many prior RL algorithms consider the policy\nentropy, they only use it as a regularizer [3], [24], [25].\nRecently, several papers have noted the connection between\nQ-learning and policy gradient methods in the setting of the\nmaximum entropy framework [27]–[29]. Early maximum entropy RL algorithms usually only consider the policy entropy\nof current states [27], [30], [31]. Unlike them, soft Q-learning\ndirectly augments the reward with an entropy term, such that\nthe optimal policy aims to reach states where they will have\nhigh policy entropy in the future [32]. Haarnoja _et al._ (2018)\nfurther developed an off-policy actor-critic variant of the Soft\nQ-learning for large continuous domains, called SAC [16],\n\n[17]. In this paper, we build on the work of [16], [17] for\nimplementing the maximum entropy framework.\n\nThe distributional RL, in which one models the distribution\nover returns, whose expectation is the value function, was\nrecently introduced by Bellemare _et al._ [18]. They proposed\na distributional RL algorithm, called C51, which achieved\ngreat performance improvements on many Atari 2600 benchmarks. Since then, many distributional RL algorithms and\ntheir inherent analyses have appeared in literature [19]–[22].\nLike DQN, these works can only handle discrete and lowdimensional action spaces, as they select actions according\nto their Q-networks. Barth-Maron _et al._ (2018) combined the\ndistributional return function within an actor-critic framework\n\nfor policy learning in continuous control setting domains, and\nproposed the Distributed Distributional Deep Deterministic\nPolicy Gradient algorithm (D4PG) [23]. Inspired by these distributional RL researches, Dabney _et al._ (2020) found that the\nbrain represents possible future rewards not as a single mean,\nbut instead as a probability distribution through mouse experiments [33]. Existing distributional RL algorithms usually learn\na discrete return distribution because it is computationally\nfriendly. However, this poses a problem: we need to divide the\nreturn distribution into multiple discrete intervals in advance.\nThis is inconvenient because different tasks usually require\ndifferent division numbers and intervals. In addition, the role\nof distributional return function in solving overestimations was\nbarely discussed before.\n\n\nIII. PRELIMINARIES AND DISTRIBUTIONAL SOFT POLICY\n\nITERATION\n\n\nIn this section, we first describe the notations and introduce\nthe concept of maximum entropy RL. Then the distributional\nsoft policy iteration (DSPI) framework is developed.\n\n\n\n\n_A. Notation_\n\n\nWe consider the standard reinforcement learning (RL) setting wherein an agent interacts with an environment _E_ in\ndiscrete time. This environment can be modeled as a Markov\nDecision Process, defined by the tuple ( _S, A, R, p_ ). The state\nspace _S_ and action space _A_ are assumed to be continuous,\n_R_ ( _rt|st, at_ ) : _S × A →P_ ( _rt_ ) is a stochastic reward function\nmapping a state-action pair ( _st, at_ ) to a distribution over a\nset of bounded rewards, and the unknown state transition\nprobability _p_ ( _st_ +1 _|st, at_ ) : _S × A →P_ ( _st_ +1) maps a given\n( _st, at_ ) to the probability distribution over _st_ +1. For the sake\nof simplicity, the current and next state-action pairs are also\ndenoted as ( _s, a_ ) and ( _s_ _[′]_ _, a_ _[′]_ ), respectively.\nAt each time step _t_, the agent receives a state _st ∈S_ and\nselects an action _at ∈A_ . In return, the agent receives the\nnext state _st_ +1 _∈S_ and a scalar reward _rt ∼_ _R_ ( _st, at_ ). The\nprocess continues until the agent reaches a terminal state after\nwhich the process restarts. The agent’s behavior is defined\nby a stochastic policy _π_ ( _at|st_ ) : _S →P_ ( _at_ ), which maps a\ngiven state to a probability distribution over actions. We will\nuse _ρπ_ ( _s_ ) and _ρπ_ ( _s, a_ ) to denote the state and state-action\ndistribution induced by policy _π_ .\n\n\n_B. Maximum Entropy RL_\n\n\nThe goal in standard RL is to learn a policy which\nmaximizes the expected future accumulated return\nE( _si≥t,ai≥t_ ) _∼ρπ,ri≥t∼R_ ( _·|si,ai_ )[ [�] _[∞]_ _i_ = _t_ _[γ][i][−][t][r][i]_ []][, where] _[ γ][ ∈]_ [[0] _[,]_ [ 1)]\nis the discount factor. In this paper, we consider a more\ngeneral entropy-augmented objective [16], [17], [32], which\naugments the reward with a policy entropy term _H_,\n\n\n\n3\n\n\nimprovement, called soft policy iteration. In the soft policy\nevaluation process, given a policy _π_, the soft Q-value can be\nlearned by repeatedly applying a soft Bellman operator _T_ _[π]_\n\nunder policy _π_ given by\n\n\n_T_ _[π]_ _Q_ _[π]_ ( _s,a_ ) = E _r∼R_ ( _·|s,a_ )[ _r_ ]+\n\n(3)\n_γ_ E _s′∼p,a′∼π_ [ _Q_ _[π]_ ( _s_ _[′]_ _, a_ _[′]_ ) _−_ _α_ log _π_ ( _a_ _[′]_ _|s_ _[′]_ )� _._\n\n\nThe goal of the soft policy improvement process is to find\na new policy _π_ new that is better than the current policy _π_ old,\nsuch that _Jπ_ new _≥_ _Jπ_ old. Hence, we can update the policy\ndirectly by maximizing the entropy-augmented objective in\n(1) in terms of the soft Q-value,\n\n\n\n_π_ new = arg max _Jπ_\n_π_\n\n\n= arg max E\n_π_ _s∼ρπ,a∼π_\n\n\n\n� _Q_ _[π]_ [old] ( _s, a_ ) _−_ _α_ log _π_ ( _a|s_ )� _._ [(4)]\n\n\n\n_Jπ_ = E\n( _si≥t,ai≥t_ ) _∼ρπ,_\n_ri≥t∼R_ ( _·|si,ai_ )\n\n\nwhere\n\n\n\n� � _[∞]_ _γ_ _[i][−][t]_ [ _ri_ + _αH_ ( _π_ ( _·|si_ ))]� _,_ (1)\n\n_i_ = _t_\n\n\n\n_H_ ( _π_ ( _·|s_ )) = _−_ _π_ ( _a|s_ ) log _π_ ( _a|s_ )d _a_\n� _a∈A_\n\n\n\n= E\n_a∼π_ ( _·|s_ )\n\n\n\n� _−_ log _π_ ( _a|s_ )� _._\n\n\n\nThis objective improves the exploration efficiency of the policy\nby maximizing both the expected future return and policy\nentropy. The temperature parameter _α_ determines the relative\nimportance of the entropy term against the reward. Maximum\nentropy RL gradually approaches the conventional RL as\n_α →_ 0.\nWe use _Gt_ = [�] _[∞]_ _i_ = _t_ _[γ][i][−][t]_ [[] _[r][i][ −]_ _[α]_ [ log] _[ π]_ [(] _[a][i][|][s][i]_ [)]][ to denote the]\nentropy-augmented accumulated return from _st_, also called\nsoft return. The soft Q-value of policy _π_ is defined as\n\n\n\n_Q_ _[π]_ ( _st, at_ ) = E E\n_r∼R_ ( _·|st,at_ ) [[] _[r]_ [] +] _[ γ]_ ( _si>t,ai>t_ ) _∼ρπ,_\n_ri>t∼R_ ( _·|si,ai_ )\n\n\n\n\n[ _Gt_ +1] _,_ (2)\n\n\n\nThe convergence and optimality of soft policy iteration have\nbeen verified in [16], [17], [28], [32].\n\n\n_C. Distributional Soft Policy Iteration_\n\n\nNext, we develop the distributional soft policy iteration\n(DSPI) framework by extending the maximum entropy RL\ninto a distributional learning version. Firstly, we define the\nsoft state-action return of policy _π_ from a state-action pair\n( _st, at_ ) as\n\n\n_Z_ _[π]_ ( _st, at_ ) = _rt_ + _γGt_ +1��( _si>t,ai>t_ ) _∼ρπ,ri≥t∼R_ ( _·|si,ai_ ) _,_\n\n\nwhich is usually a random variable due to the randomness in\nthe state transition _p_, reward function _R_ and policy _π_ . From\n(2), it is clear that\n\n\n_Q_ _[π]_ ( _s, a_ ) = E[ _Z_ _[π]_ ( _s, a_ )] _._ (5)\n\n\nInstead of just considering the expected state-action return\n_Q_ _[π]_ ( _s, a_ ), one can choose to directly model the distribution\nof the soft returns _Z_ _[π]_ ( _s, a_ ). We define _Z_ _[π]_ ( _Z_ _[π]_ ( _s, a_ ) _|s, a_ ) :\n_S × A →P_ ( _Z_ _[π]_ ( _s, a_ )) as a mapping from ( _s, a_ ) to a distribution over soft state-action returns, and call it the _soft state-_\n_action return distribution_ or distributional value function. The\n\ndistributional variant of the Bellman operator in the maximum\nentropy framework can be derived as\n\n\n_TD_ _[π][Z]_ _[π]_ [(] _[s, a]_ [)] = _D r_ + _γ_ ( _Z_ _π_ ( _s′, a′_ ) _−_ _α_ log _π_ ( _a′|s′_ )) _,_ (6)\n\n\n_D_\nwhere _r ∼_ _R_ ( _·|s, a_ ) _, s_ _[′]_ _∼_ _p, a_ _[′]_ _∼_ _π_, and _A_ = _B_ denotes\nthat two random variables _A_ and _B_ have equal probability\nlaws. The distributional variant of policy iteration has been\nproved to converge to the optimal return distribution and\npolicy uniformly in [18]. We can further prove that DSPI\nwhich alternates between (6) and (4) also leads to policy\nimprovement with respect to the maximum entropy objective\n(1). Details are provided in Appendix A.\nSuppose _TD_ _[π][Z]_ [(] _[s, a]_ [)] _[ ∼T]_ _D_ _[ π][Z]_ [(] _[·|][s, a]_ [)][, where] _[ T]_ _D_ _[ π][Z]_ [(] _[·|][s, a]_ [)]\ndenotes the distribution of _TD_ _[π][Z]_ [(] _[s, a]_ [)][. To implement (6), we]\ncan directly update the soft return distribution by\n\n\n\nwhich describes the expected soft return for selecting _at_ in\nstate _st_ and thereafter following policy _π_ .\nThe optimal maximum entropy policy is learned by a\nmaximum entropy variant of the policy iteration method,\nwhich alternates between soft policy evaluation and soft policy\n\n\n\n_Z_ new = arg min E\n_Z_ ( _s,a_ ) _∼ρπ_\n\n\n\n� _d_ ( _TD_ _[π][Z]_ [old][(] _[·|][s, a]_ [)] _[,][ Z]_ [(] _[·|][s, a]_ [))] � _,_ (7)\n\n\n\nwhere _d_ is some metric to measure the distance between\n\ntwo distributions. For calculation convenience, many practical\n\n\n\n\ndistributional RL algorithms employ Kullback-Leibler (KL)\ndivergence, denoted as _D_ KL, as the metric [18], [23].\n\n\nIV. OVERESTIMATION BIAS\n\nThis section mainly focuses on the impact of the stateaction return distribution learning on reducing overestimation.\nTherefore, the entropy coefficient _α_ is assumed to be 0 here.\nPrevious studies analyzed the Q-value estimation bias of Qlearning in tabular cases [6], [15]. In section IV-A, we derive\nthe analytical expression of Q-value estimation bias from the\nperspective of function approximation. Then, Section IV-B\nanalyzes the Q-estimate bias of the return distribution learning\nand reveals its mechanism to mitigate overestimations.\n\n\n_A. Overestimation in Q-learning_\n\nIn Q-learning with discrete actions, suppose the Q-value\nis approximated by a Q-function _Qθ_ ( _s, a_ ) with parameters _θ_ .\nDefining the greedy target _y_ = E[ _r_ ] + _γ_ E _s′_ [max _a′ Qθ_ ( _s_ _[′]_ _, a_ _[′]_ )],\nthe Q-estimate _Qθ_ ( _s, a_ ) can be updated by minimizing the\nloss ( _y −_ _Qθ_ ( _s, a_ )) [2] _/_ 2 using gradient descent methods, i.e.,\n\n\n_θ_ new = _θ_ + _β_ ( _y −_ _Qθ_ ( _s, a_ )) _∇θQθ_ ( _s, a_ ) _,_ (8)\n\n\nwhere _β_ is the learning rate. However, in practical applications,\nthe Q-estimate _Qθ_ ( _s, a_ ) usually contains random errors, which\nmay be caused by system noises and function approximation.\nDenoting the current true Q-value as _Q_ [˜], we assume\n\n\n_Qθ_ ( _s, a_ ) = _Q_ [˜] ( _s, a_ ) + _ϵQ,_ (9)\n\n\nwhere the random error _ϵQ_ has zero mean and is independent\nof ( _s, a_ ) and _θ_ . To distinguish the random error of _Qθ_ ( _s, a_ )\nand _Qθ_ ( _s_ _[′]_ _, a_ _[′]_ ), the random error of _Qθ_ ( _s_ _[′]_ _, a_ _[′]_ ) is denoted as\n_ϵ_ _[′]_ _Q_ [. Clearly,] _[ ϵ][′]_ _Q_ [may cause inaccuracy on the right-hand side]\nof (8). Let _θ_ true represent the post-update parameters obtained\nbased on true target ˜ _y_, that is,\n\n\n_θ_ true = _θ_ + _β_ (˜ _y −_ _Qθ_ ( _s, a_ )) _∇θQθ_ ( _s, a_ ) _,_\n\n\nwhere ˜ _y_ = E[ _r_ ] + _γ_ E _s′_ [max _a′_ _Q_ [˜] ( _s_ _[′]_ _, a_ _[′]_ )].\nSupposing _β_ is sufficiently small, the post-update Qfunction can be well-approximated by linearizing around _θ_\nusing Taylor’s expansion:\n\n\n_Qθ_ true( _s, a_ ) _≈_ _Qθ_ ( _s, a_ ) + _β_ (˜ _y −_ _Qθ_ ( _s, a_ )) _∥∇θQθ_ ( _s, a_ ) _∥_ 2 [2] _[,]_\n\n\n_Qθ_ new ( _s, a_ ) _≈_ _Qθ_ ( _s, a_ ) + _β_ ( _y −_ _Qθ_ ( _s, a_ )) _∥∇θQθ_ ( _s, a_ ) _∥_ 2 [2] _[.]_\n\n\nThen, in expectation, the estimate bias of post-update Qestimate _Qθ_ new ( _s, a_ ) is\n\n\n∆( _s, a_ ) = E _ϵ_ _[′]_ _Q_ [[] _[Q][θ]_ [new] [(] _[s, a]_ [)] _[ −]_ _[Q][θ]_ [true][(] _[s, a]_ [)]]\n\n_≈_ _β_ �E _ϵ_ _[′]_ _Q_ [[] _[y]_ []] _[ −]_ _[y]_ [˜] � _∥∇θQθ_ ( _s, a_ ) _∥_ 2 [2]\n= _βγ_ �E _ϵ_ _[′]_ _Q_ �E _s′_ [max _a_ _[′][ Q]_ [(] _[s][′][, a][′]_ [)]] � _−_\n\n\n˜\nE _s′_ [max _Q_ ( _s_ _[′]_ _, a_ _[′]_ )]� _∥∇θQθ_ ( _s, a_ ) _∥_ 2 [2] _[.]_\n_a_ _[′]_\n\n\nDefining\n\n\n˜\n_δ_ = E _ϵ_ _[′]_ _Q_ �E _s′_ [max _a_ _[′][ Q]_ [(] _[s][′][, a][′]_ [)]] � _−_ E _s′_ [max _a_ _[′]_ _Q_ ( _s_ _[′]_ _, a_ _[′]_ )]\n\n\n\n4\n\n\n∆( _s, a_ ) can be rewritten as:\n\n\n∆( _s, a_ ) _≈_ _βγδ∥∇θQθ_ ( _s, a_ ) _∥_ 2 [2] _[.]_\n\n\nAlthough _ϵ_ _[′]_ _Q_ [is independent of][ (] _[s][′][, a][′]_ [)][, it cannot be extracted]\nfrom the max operator of max _a′_ ( _Q_ [˜] ( _s_ _[′]_ _, a_ _[′]_ ) + _ϵ_ _[′]_ _Q_ [)][. This is]\nbecause for each ( _s_ _[′]_ _, a_ _[′]_ ), _ϵ_ _[′]_ _Q_ [is a random variable rather than a]\nfixed value. In fact, it has been verified by previous researches\nthat E _ϵ_ _[′]_ _Q_ [[max] _[a][′]_ [( ˜] _[Q]_ [(] _[s][′][, a][′]_ [) +] _[ ϵ]_ _Q_ _[′]_ [)]] _[ −]_ [max] _[a][′]_ [ ˜] _[Q]_ [(] _[s][′][, a][′]_ [)] _[ ≥]_ [0][ [9],]\n\n[15]. Therefore, it is clear that\n\n\n∆( _s, a_ ) _≥_ 0 _,_\n\n\nwhich indicates that ∆( _s, a_ ) is an upward bias. In fact, any\nkind of estimation errors can induce an upward bias due to\nthe max operator. Although it is reasonable to expect a small\nupward bias caused by single update, these overestimation\nerrors can be further exaggerated through temporal difference\n(TD) learning, which may result in large overestimation bias\nand suboptimal policy updates.\n\n\n_B. Return Distribution for Reducing Overestimation_\n\n\nBefore discussing the distributional version of Q-learning,\nwe first assume that the random returns _Z_ ( _s, a_ ) obey a\nGaussian distribution _Z_ ( _·|s, a_ ). Suppose the mean (i.e., Qvalue) and standard deviation of the Gaussian distribution\nare approximated by two independent functions _Qθ_ ( _s, a_ )\nand _σψ_ ( _s, a_ ), with parameters _θ_ and _ψ_, i.e., _Zθ,ψ_ ( _·|s, a_ ) =\n_N_ ( _Qθ_ ( _s, a_ ) _, σψ_ ( _s, a_ ) [2] ).\nSimilar to standard Q-learning, we first define a random greedy target _yD_ = _r_ + _γZ_ ( _s_ _[′]_ _, a_ _[′∗]_ ), where _a_ _[′∗]_ =\narg max _a′ Qθ_ ( _s_ _[′]_ _, a_ _[′]_ ). Suppose _yD ∼Z_ [target] ( _·|s, a_ ), which is\nalso assumed to be a Gaussian distribution. Note that even\nif _Z_ ( _s, a_ ) and _yD_ are not strictly Gaussian, we can still\nuse the Gaussian to approximate their distributions, which\nwill not affect the subsequent analysis. Since E[ _yD_ ] =\nE[ _r_ ] + _γ_ E _s′_ [max _a′ Qθ_ ( _s_ _[′]_ _, a_ _[′]_ )] is equal to _y_ in (8), it follows\n_Z_ [target] ( _·|s, a_ ) = _N_ ( _y, σ_ [target2] ). Considering the loss function\nin (7) under the KL divergence measurement, _Qθ_ ( _s, a_ ) and\n_σψ_ ( _s, a_ ) are updated by minimizing\n\n\n_D_ KL( _Z_ [target] ( _·|s, a_ ) _, Zθ,ψ_ ( _·|s, a_ ))\n\n\n\n\n_[ψ]_ [(] _[s][,][ a]_ [)]\n\n_σ_ [target] [+] _[ σ]_ [target2][ + ][(] _[y][ −]_ _[Q]_ [2] _[θ]_ [(] _[s][,][ a]_ [))][2]\n\n\n\n= log _[σ][ψ]_ [(] _[s][,][ a]_ [)]\n\n\n\n\n[ + ][(] _[y][ −]_ _[Q][θ]_ [(] _[s]_ _[,][ a]_ [))]\n\n_−_ [1]\n2 _σψ_ ( _s, a_ ) [2] 2\n\n\n\n\n[1]\n\n2 _[,]_ [ (11)]\n\n\n\nthat is,\n\n\n\n_θ_ new = _θ_ + _β_ _[y][ −]_ _[Q][θ]_ [(] _[s][,][ a]_ [)]\n\n\n\n\n_[,]_ _∇θQθ_ ( _s, a_ ) _,_\n\n_σψ_ ( _s, a_ ) [2]\n\n\n\n(12)\n\n[(] _[y][ −]_ _[Q][θ]_ [(] _[s][,][ a]_ [))][2]\n\n_∇ψσψ_ ( _s, a_ ) _._\n_σψ_ ( _s, a_ ) [3]\n\n\n\n_ψ_ new = _ψ_ + _β_ [∆] _[σ]_ [2][ + ][(] _[y][ −]_ _[Q][θ]_ [(] _[s][,][ a]_ [))][2]\n\n\n\n˜\n= E _s′_ [�] E _ϵ_ _[′]_ _Q_ [[max] _a_ _[′][ Q][θ]_ [(] _[s][′][, a][′]_ [)]] _[ −]_ [max] _a_ _[′]_ _Q_ ( _s_ _[′]_ _, a_ _[′]_ )�\n\n\n˜\n= E _s′_ [�] E _ϵ_ _[′]_ _Q_ [[max] _a_ _[′]_ [ ( ˜] _[Q][θ]_ [(] _[s][′][, a][′]_ [) +] _[ ϵ]_ _Q_ _[′]_ []] _[ −]_ [max] _a_ _[′]_ _Q_ ( _s_ _[′]_ _, a_ _[′]_ )� _,_\n\n\n\n(10)\n\n\n\nwhere ∆ _σ_ [2] = _σ_ [target2] _−_ _σψ_ ( _s, a_ ) [2] . Compared with standard\nQ-learning, _σψ_ ( _s, a_ ) plays a role of adaptively adjusting the\nupdate stepsize of _Qθ_ ( _s, a_ ). In particular, the update stepsize\nof _Qθ_ ( _s, a_ ) decreases squarely as _σφ_ ( _s, a_ ) increases. Supposing _Qθ_ ( _s, a_ ) also obeys (9), the post-update parameters\nobtained based on the true target value ˜ _y_ is given by\n\n_θ_ true = _θ_ + _β_ _[y]_ [˜] _[ −]_ _[Q][θ]_ [(] _[s][,][ a]_ [)] _∇θQθ_ ( _s, a_ ) (13)\n\n_σψ_ ( _s, a_ ) [2]\n\n\n\n\nSimilar to the derivation of ∆( _s, a_ ), the overestimation bias\nof _Qθ_ new ( _s, a_ ) in distributional Q-learning is\n\n∆ _D_ ( _s, a_ ) _≈_ _[βγ][δ][∥][∇][θ][Q][θ]_ [(] _[s][,][ a]_ [)] _[∥]_ 2 [2] = [∆][(] _[s][,][ a]_ [)] _[.]_ (14)\n_σψ_ ( _s, a_ ) [2] _σψ_ ( _s, a_ ) [2]\n\n\nObviously, the overestimation errors ∆ _D_ ( _s, a_ ) is inversely proportional to _σψ_ ( _s, a_ ) [2] . In an ideal situation, when _Q_ [˜] ( _s, a_ ) = ˜ _y_,\nthat is, _Q_ [˜] ( _s, a_ ) has converged after a period of learning, we\ncan derive that\n\nE _ϵQ,ϵ_ _[′]_ _Q_ [[] _[σ][ψ]_ [new] [(] _[s, a]_ [)]] _[ ≥]_ _[σ][ψ]_ [(] _[s, a]_ [)+]\n\n_β_ _[σ]_ [target2] _[ −]_ _[σ][ψ]_ [(] _[s, a]_ [)][2][ +] _[ γ]_ [2] _[δ]_ [2][ +][ E] _[ϵ][Q]_ [[] _[ϵ][Q]_ [2][]] _∥∇ψσψ_ ( _s, a_ ) _∥_ 2 [2] _[,]_\n\n_σψ_ ( _s, a_ ) [3]\n\n\nwhere this inequality holds approximately since we drop\nhigher order terms out in Taylor approximation. See Appendix\nB-A for details of derivation.\nBecause _σψ_ new is also the standard deviation for the next\ntime step, this indicates that by repeatedly applying (12), the\nstandard deviation _σψ_ ( _s, a_ ) of the return distribution tends\nto be a larger value in areas with high _σ_ [target] and random\nerrors _ϵQ_ . Moreover, _σ_ [target] is often positively related to the\nrandomness of systems _p_, reward function _R_ and the return\ndistribution _Z_ ( _·|s_ _[′]_ _, a_ _[′]_ ) of subsequent state-action pairs. Since\nthe overestimation bias ∆ _D_ ( _s, a_ ) is inversely proportional to\n_σψ_ ( _s, a_ ) [2] according to (14), distributional Q-learning can be\nused to mitigate overestimations caused by task randomness\nand approximation errors.\n\n\nV. DISTRIBUTIONAL SOFT ACTOR-CRITIC\n\nIn this section, based on the developed DSPI framework, we\nderive the learning rules of the continuous return distribution,\nand propose the DSAC algorithm by replacing the clipped double Q-learning of SAC [16], [17] with the return distribution\nlearning. We will consider a parameterized distributional value\nfunction _Zθ_ ( _·|s, a_ ) and a stochastic policy _πφ_ ( _·|s_ ), where _θ_\nand _φ_ are parameters. In this paper, both the state-action return\ndistribution and policy functions are modeled as Gaussian with\nmean and covariance given by neural networks (NNs). We will\nnext derive update rules for parameters of these NNs.\n\n\n_A. Algorithm_\n\n_1) Distributional Soft Policy Evaluation:_ Considering the\nloss function in (7), the soft state-action return distribution\ncan be trained to minimize the loss function in (7) under the\nKL-divergence measurement\n\n\n\n5\n\n\nWe provide details of derivation in Appendix B-B.\nThe parameters _θ_ can be optimized with the following\ngradients\n\n\n\n_πφ′_\n\n_∇θJZ_ ( _θ_ ) = _−_ E _∇θ_ log _P_ ( _TD_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )) _._\n( _s,a,r,s_ _[′]_ ) _∼B,_ � �\n_a_ _[′]_ _∼πφ′_ _,_\n_Z_ ( _s_ _[′]_ _,a_ _[′]_ ) _∼Zθ′_\n\n\nSince _Zθ_ is assumed to be a Gaussian model, it can be\nexpressed as _Zθ_ ( _·|s, a_ ) = _N_ ( _Qθ_ ( _s, a_ ) _, σθ_ ( _s, a_ ) [2] ), where\n_Qθ_ ( _s, a_ ) and _σθ_ ( _s, a_ ) are the outputs of value network. This\nmakes the Gaussian variant of update gradients\n\n\n_∇θJZ_ ( _θ_ )\n\n\n\n_TDπφ′_ _Z_ ( _s,a_ ) _−Qθ_ ( _s,a_ )�2\n\n2 _σθ_ ( _s,a_ ) ~~[2]~~\n\n~~_√_~~ 2 _πσθ_ ( _s, a_ )\n\n\n\n= _−_ E\n( _s,a,r,s_ _[′]_ ) _∼B,_\n_a_ _[′]_ _∼πφ′_ _,_\n_Z_ ( _s_ _[′]_ _,a_ _[′]_ ) _∼Zθ′_\n\n\n\n_−_ �\n\n_e_\n_∇θ_ log\n� �\n\n\n\n��\n\n\n\n� _TDπφ′_ _Z_ ( _s, a_ ) _−_ _Qθ_ ( _s, a_ )�2\n\n\n\n_s, a_ ) _−_ _Qθ_ ( _s, a_ )� + _[∇][θ][σ][θ]_ [(] _[s][,][ a]_ [)]\n\n2 _σθ_ ( _s, a_ ) [2] _σθ_ ( _s, a_ )\n\n\n\n_σθ_ ( _s, a_ )\n\n\n\n= E\n( _s,a,r,s_ _[′]_ ) _∼B,_\n_a_ _[′]_ _∼πφ′_ _,_\n_Z_ ( _s_ _[′]_ _,a_ _[′]_ ) _∼Zθ′_\n\n\n\n_∇θ_\n�\n\n\n\n_._\n�\n\n\n\nDenoting Ψ _Z_ ( _θ_ ) = log _P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )), to understand the composition of _∇θJZ_ ( _θ_ ) more intuitively, we can\nrewrite it as\n\n\n\n_∇θJZ_ ( _θ_ ) = E _−_ _[∂]_ [Ψ] _[Z]_ [(] _[θ]_ [)]\n( _s,a,r,s_ _[′]_ ) _∼B,_ � _∂Qθ_ ( _s, a_ ) _[∇][θ][Q][θ]_ [(] _[s, a]_ [)]\n_a_ _[′]_ _∼πφ′_ _,_\n_Z_ ( _s_ _[′]_ _,a_ _[′]_ ) _∼Zθ′_\n\n\n\n_−_ _[∂]_ [Ψ] _[Z]_ [(] _[θ]_ [)]\n\n\n\n\n_[Z]_\n\n\n_,_\n_∂σθ_ ( _s, a_ ) _[∇][θ][σ][θ]_ [(] _[s, a]_ [)] �\n\n\n\n(16)\nwhere\n\n\n\n_∂_ Ψ _Z_ ( _θ_ )\n_∂Qθ_ ( _s, a_ ) [=]\n\n\n_∂_ Ψ _Z_ ( _θ_ )\n_∂σθ_ ( _s, a_ ) [=]\n\n\n\n� _TDπφ′_ _Z_ ( _s, a_ ) _−_ _Qθ_ ( _s, a_ )�\n\n\n_,_\n_σθ_ ( _s, a_ ) [2]\n\n� _TDπφ′_ _Z_ ( _s, a_ ) _−_ _Qθ_ ( _s, a_ )�2 _−_ 1\n\n_σθ_ ( _s, a_ ) [3] _σθ_ ( _s, a_ ) _[.]_\n\n\n\n_∂_ Ψ _Z_ ( _θ_ )\nIt can be easily deduced from _∂Qθ_ ( _s,a_ ) [that the update stepsize]\nof _Qθ_ ( _s, a_ ) decreases squarely as _σθ_ ( _s, a_ ) increases, thereby\nmitigating Q-value overestimations. However, the gradients\n_∇θJZ_ ( _θ_ ) are prone to explode as _σθ_ ( _s, a_ ) _→_ 0, or to vanish\nas _σθ_ ( _s, a_ ) _→∞_ . To address this problem, we propose two\noptions to keep _σθ_ ( _s, a_ ) within a reasonable range. The first\npoint is to limit the minimum value of _σθ_ ( _s, a_ ) by\n\n\n_σθ_ ( _s, a_ ) = max( _σθ_ ( _s, a_ ) _, σ_ min) _,_ (17)\n\n\nNoted that if _σ_ min _≥_ 1, we always have ∆ _D_ ( _s, a_ ) _≤_ ∆( _s, a_ ).\nTherefore, in this paper, we let _σ_ min = 1. And the second\n_πφ′_ _∂_ Ψ _Z_ ( _θ_ )\npoint is to clip _TD_ _Z_ ( _s, a_ ) of _∂σθ_ ( _s,a_ ) [to keep it close]\nto the expectation value _Qθ_ ( _s, a_ ) of the current soft return\ndistribution, thus stabilizing the learning process of _σθ_ ( _s, a_ )\nand indirectly controlling its range, i.e.,\n\n\n\n_JZ_ ( _θ_ ) = E\n( _s,a_ ) _∼B_\n\n\n\n� _D_ KL( _TDπφ′_ _Zθ′_ ( _·|s, a_ ) _, Zθ_ ( _·|s, a_ ))� (15)\n\n\n\nwhere _B_ is a replay buffer of previously sampled experience,\n_θ_ _[′]_ and _φ_ _[′]_ are parameters of target return distribution and policy\nfunctions, which are used to stabilize the learning process and\nevaluate the target. For practical applications, _σ_ [target] in (11)\nis unknown. Therefore, we cannot directly update _Zθ_ ( _·|s, a_ )\nusing the objective shown in (11). After analysis, we get the\nfollowing objective function equivalent to (15)\n\n\n\n_∂_ Ψ _Z_ ( _θ_ )\n_∂σθ_ ( _s, a_ ) [=]\n\n\n\n_JZ_ ( _θ_ ) = _−_ E\n( _s,a,r,s_ _[′]_ ) _∼B,a_ _[′]_ _∼πφ′_ _,_\n_Z_ ( _s_ _[′]_ _,a_ _[′]_ ) _∼Zθ′_ ( _·|s_ _[′]_ _,a_ _[′]_ )\n\n\n\nlog _P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )) _._\n� �\n\n\n\n� _TD_ ~~_π_~~ _φ′_ _Z_ ( _s, a_ ) _−_ _Qθ_ ( _s, a_ )�2 _−_ 1\n\n_σθ_ ( _s, a_ ) [3] _σθ_ ( _s, a_ ) _[,]_\n\n\n\n\nwhere\n\n\n_TD_ ~~_π_~~ _φ′_ _Z_ ( _s, a_ ) = clip( _TDπφ′_ _Z_ ( _s, a_ ) _, Qθ_ ( _s, a_ ) _−_ _b, Qθ_ ( _s, a_ ) + _b_ ) _,_\n(18)\nwhere clip[ _x, A, B_ ] denotes that _x_ is clipped into the range\n\n[ _A, B_ ] and _b_ is the clipping boundary.\nThe target networks mentioned above use a slow-moving\nupdate rate, parameterized by _τ_, such as\n\n\n_θ_ _[′]_ _←_ _τθ_ + (1 _−_ _τ_ ) _θ_ _[′]_ _,_ _φ_ _[′]_ _←_ _τφ_ + (1 _−_ _τ_ ) _φ_ _[′]_ _._\n\n\n_2) Distributional Soft Policy Improvement:_ The policy can\nbe learned by directly maximizing a parameterized variant of\nthe objective in (4):\n\n\n_Jπ_ ( _φ_ ) = E [ _Qθ_ ( _s, a_ ) _−_ _α_ log( _πφ_ ( _a|s_ ))]\n_s∼B,a∼πφ_\n\n\n\n= E\n_s∼B,a∼πφ_\n\n\n\nE _._\n� _Z_ ( _s,a_ ) _∼Zθ_ ( _·|s,a_ ) [[] _[Z]_ [(] _[s, a]_ [)]] _[ −]_ _[α]_ [ log(] _[π][φ]_ [(] _[a][|][s]_ [))] �\n\n\n\nIf _a_ is unbounded, given the parameters of the action distribution, such as the mean and variance of the Gaussian\ndistribution, log( _πφ_ ( _a|s_ )) can be easily calculated. On the\nother hand, if _a_ is bounded to a finite interval, its log-likelihood\ncan also be obtained in the manner given in Appendix B-C.\nThere are several options, such as log derivative and reparameterization tricks, for maximizing _Jπ_ ( _φ_ ) [34]. In this paper,\nwe apply the reparameterization trick because it can reduce the\ngradient estimation variance.\nIf the soft Q-value function _Qθ_ ( _s, a_ ) is explicitly parameterized through parameters _θ_, we only need to express the\nrandom action _a_ as a deterministic variable, i.e.,\n\n\n_a_ = _fφ_ ( _ξa_ ; _s_ ) _,_ (19)\n\n\nwhere _ξa ∈_ R [dim(] _[A]_ [)] is an auxiliary variable which is sampled\nform some fixed distribution. In particular, since _πφ_ ( _·|s_ ) is\nassumed to be a Gaussian in this paper, _fφ_ ( _ξa_ ; _s_ ) can be\nformulated as\n\n\n_fφ_ ( _ξa_ ; _s_ ) = _a_ mean + _ξa ⊙_ _a_ std _,_\n\n\nwhere _a_ mean _∈_ R [dim(] _[A]_ [)] and _a_ std _∈_ R [dim(] _[A]_ [)] are the mean\nand standard deviation of _πφ_ ( _·|s_ ), _⊙_ represents the Hadamard\nproduct and _ξai ∼N_ (0 _,_ **I** dim( _A_ )). Then the policy update\ngradients can be approximated with\n\n\n_∇φJπ_ ( _φ_ ) = E _s∼B,ξa_ _−_ _α∇φ_ log( _πφ_ ( _a|s_ ))+\n�\n\n\n( _∇aQθ_ ( _s, a_ ) _−_ _α∇a_ log( _πφ_ ( _a|s_ ))) _∇φfφ_ ( _ξa_ ; _s_ ) _._\n�\n\n\nIf _Qθ_ ( _s, a_ ) cannot be expressed explicitly through _θ_, the policy\nupdate gradients can be obtained in the manner given in\nAppendix B-D.\n_3) Pseudo-code:_ Finally, according to [17], the temperature\n_α_ is updated by minimizing the following objective\n\n\n_J_ ( _α_ ) = E( _s,a_ ) _∼B_ [ _α_ ( _−_ log _πφ_ ( _a|s_ ) _−H_ )] _,_\n\n\nwhere _H_ is the expected entropy. In addition, two-timescale\nupdates, i.e., less frequent policy updates, usually result in\nhigher quality policy updates [12]. Therefore, the policy,\ntemperature and target networks are updated every _m_ iterations\nin this paper. The final algorithm is listed in Algorithm 1. Fig.\n1 shows the diagram of DSAC.\n\n\n\n6\n\n\n**Algorithm 1** DSAC Algorithm\n\n\nInitialize parameters _θ_, _φ_ and _α_\nInitialize target parameters _θ_ _[′]_ _←_ _θ_, _φ_ _[′]_ _←_ _φ_\nInitialize learning rate _βZ_, _βπ_, _βα_ and _τ_\nInitialize iteration index _k_ = 0\n\n**repeat**\n\nSelect action _a ∼_ _πφ_ ( _a|s_ )\nObserve reward _r_ and new state _s_ _[′]_\n\nStore transition tuple ( _s, a, r, s_ _[′]_ ) in buffer _B_\n\n\nSample _N_ transitions ( _s, a, r, s_ _[′]_ ) from _B_\nUpdate soft return distribution _θ ←_ _θ −_ _βZ_ _∇θJZ_ ( _θ_ )\n**if** _k_ mod _m_ **then**\n\nUpdate policy _φ ←_ _φ_ + _βπ∇φJπ_ ( _φ_ )\nAdjust temperature _α ←_ _α −_ _βα∇αJ_ ( _α_ )\nUpdate target networks:\n\n_θ_ _[′]_ _←_ _τθ_ + (1 _−_ _τ_ ) _θ_ _[′]_, _φ_ _[′]_ _←_ _τφ_ + (1 _−_ _τ_ ) _φ_ _[′]_\n\n**end if**\n\n_k_ = _k_ + 1\n**until** Convergence\n\n\nFig. 1. DSAC diagram. The return distribution and policy are approximated by two NNs, called distributional value network and\npolicy network respectively. DSAC first updates the distributional\nvalue network based on the samples collected from the buffer. Then,\nthe output of the value network is used to guide the update of the\npolicy network.\n\n\n_B. Architecture_\n\n\nAlgorithm 1 and Fig. 1 show the operation process of DSAC\nin a serial way. Like most off-policy RL algorithms, we can\nuse parallel or distributed learning techniques to improve the\nlearning efficiency of DSAC. Therefore, we build a new parallel asynchronous buffer-actor-learner architecture (PABAL)\nreferring to the other high-throughput learning architectures,\nsuch as IMPALA and Ape-X [3], [35], [36]. As shown in\nFig. 2, buffers, actors and learners are all distributed across\nmultiple workers, which are used to improve the efficiency of\nstorage and sampling, exploration, and updating, respectively.\nAnd all communication between modules is asynchronous.\nBoth actors and learners asynchronously synchronize the\nparameters from the shared memory. The experience generated\nby each actor is asynchronously and randomly sent to a\ncertain buffer at each time step. Each buffer continuously\nstores data and sends the sampled experience to a random\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-5-0.png)\n\n\nlearner. Relying on the received sampled data, the learners\ncalculate the update gradients using their local functions, and\nthen use these gradients to update the shared value and policy\nfunctions. In this paper, we implement DSAC and other offpolicy baseline algorithms within the PABAL architecture.\n\n\nSampled experience Generated experience\n\n\nFig. 2. The PABAL architecture. Buffers, actors, and learners are\nall distributed across multiple workers. Communication between\ndifferent modules is asynchronous.\n\n\nVI. EXPERIMENTAL VERIFICATION\n\n\n_A. Benchmarks_\n\n\nTo evaluate our algorithm, we measure its performance and\nQ-value estimation bias on a suite of MuJoCo continuous control tasks without modifications to environment [37], interfaced\nthrough OpenAI Gym [38]. Fig. 3 shows the benchmark tasks\nused in this paper. See Appendix C-A for brief descriptions\nof these benchmarks.\n\n\n(a) (b) (c)\n\n\n(d) (e)\n\n\nFig. 3: Tasks. (a) Humanoid-v2: ( _s × a_ ) _∈_ R [376] _×_ R [17] . (b)\nHalfCheetah-v2: ( _s × a_ ) _∈_ R [17] _×_ R [6] . (c) Ant-v2: ( _s × a_ ) _∈_\nR [111] _×_ R [8] . (d) Walker2d-v2: ( _s × a_ ) _∈_ R [17] _×_ R [6] . (e)\nInvertedDoublePendulum-v2: ( _s × a_ ) _∈_ R [11] _×_ R [1] .\n\n\n_B. Baselines_\n\n\nWe compare our algorithm against Deep Deterministic\nPolicy Gradient (DDPG) [14], Trust Region Policy Optimization (TRPO) [24], Proximal Policy Optimization (PPO) [25],\nDistributed Distributional Deep Deterministic Policy Gradients\n(D4PG) [23], Twin Delayed Deep Deterministic policy gradient (TD3) [12], Soft Actor-Critic (SAC) [17]. DDPG, TRPO,\n\n\n\n7\n\n\nPPO, D4PG, TD3 and SAC are mainstream RL algorithms,\nwhich have been extensively verified and applied in a variety\nof challenging domains. Using these algorithms as baselines,\nthe performance of the proposed DSAC algorithm can be\nevaluated objectively.\n\nWe additionally compare our method with our proposed\nTwin Delayed Distributional Deep Deterministic policy gradient algorithm (TD4), which is developed by replacing the\nclipped double Q-learning in TD3 with the distributional\nreturn learning; Double Q-learning variant of SAC (Double-Q\nSAC), in which we replace the clipped double Q-learning of\nSAC with the actor-critic variant of double Q-learning [12],\n\n[15]; and single Q-value variant of SAC (Single-Q SAC), in\nwhich we replace the clipped double Q-learning of SAC with\ntraditional TD learning. See Appendix C-B, C-C and C-D\nfor detailed descriptions of Double-Q SAC, Single-Q SAC\nand TD4 algorithms. Double-Q SAC and Single-Q SAC are\nadapted from SAC. Table I gives a basic description of DSAC\nand each baseline. It is clear that DSAC, SAC, Double-Q\nSAC and Single-Q SAC algorithms respectively use the return\ndistribution learning, clipped double Q-learning, double Qlearning and traditional TD learning for policy evaluation. This\nis the only difference between these algorithms. Therefore, we\ncan assess the impact of the distribution learning by comparing\nDSAC with SAC, Single-Q SAC and Double-Q SAC. Besides,\nwe compare DSAC with TD4, which uses the distribution\nlearning but not maximum entropy, to assess the impact of\npolicy entropy.\n\nAll the off-policy algorithms mentioned above are implemented in the proposed PABAL architecture, including 4\nlearners, 6 actors and 3 buffers. We use a fully connected\nnetwork with 5 hidden layers, consisting of 256 units per\nlayer, with Gaussian Error Linear Units (GELU) each layer\n\n[39], for both actor and critic. For distributional value function\nand stochastic policy, we use a Gaussian distribution with\nmean and covariance given by a NN, where the covariance\nmatrix is diagonal. In this case, each NN maps the input\nstates to the mean and logarithm of standard deviation of the\nGaussian distribution. The Adam method [40] with a cosine\nannealing learning rate is used to update all the parameters.\nAll algorithms adopt almost the same NN architecture and\nhyperparameters. Table IV in Appendix C-E provides more\ndetailed hyperparameters of all algorithms.\n\n\n_C. Results_\n\n\n_1) Performance:_ We train 5 different runs of each algorithm\nwith different random seeds, with evaluations every 20000\niterations. Each evaluation calculates the average return over\n5 episodes without exploration noise, where the maximum\nlength of each episode is 1000 time steps. The learning curves\nare shown in Fig. 4 and results in Table II. Results show\nthat the proposed DSAC algorithm outperforms or matches all\nother baseline algorithms across all benchmark tasks in terms\nof the final performance. For example, compared with famous\nRL algorithms such as SAC, TD3, PPO, and DDPG, DSAC\ngains 20.0%, 63.8%, 39.8%, 97.6% improvements on the\nmost complex Humanoid-v2 task, respectively. This indicates\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-6-0.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-6-1.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-6-2.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-6-3.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-6-4.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-6-5.png)\n\n\n8\n\n\n\n\n\nTABLE I\n\nBASIC DESCRIPTION OF DSAC AND BASELINES.\n\n\nAlgorithm Algorithm Type Policy Type Policy Evaluation Policy Improvement\nDSAC (Ours) off-policy Stochastic Continuous soft return distribution learning Soft policy gradient\nSAC [17] off-policy Stochastic Clipped double Q-learning Soft policy gradient\nDouble-Q SAC off-policy Stochastic Double Q-learning Soft policy gradient\nSingle-Q SAC off-policy Stochastic Traditional TD learning Soft policy gradient\nTD4 off-policy Deterministic Continuous return distribution learning Policy gradient\nTD3 [12] off-policy Deterministic Clipped double Q-learning Policy gradient\nDDPG [14] off-policy Deterministic Traditional TD learning Policy gradient\nD4PG [23] off-policy Deterministic Discrete return distribution learning Policy gradient\nTRPO [24] on-policy Stochastic Traditional TD learning Constrained Policy Optimization\nPPO [25] on-policy Stochastic Traditional TD learning Proximal Policy Optimization\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-7-8.png)\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-7-3.png)\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-7-2.png)\n\n(a) Humanoid-v2 (b) Ant-v2 (c) Walker2d-v2\n\n\n(d) HalfCheetah-v2 (e) InvertedDoublePendulum-v2\n\n\nFig. 4: Training curves on continuous control benchmarks. The solid lines correspond to the mean and the shaded regions\ncorrespond to 95% confidence interval over 5 runs.\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-7-9.png)\n\nthat the final performance of DSAC on these benchmarks\nexceeds the state of the art. Fig. 5 visually shows the control performance of DSAC and SAC on Humanoid-v2. It is\nobvious that DSAC realizes a movement closer to human\n\nrunning. Among DSAC, SAC, Single-Q SAC and DoubleQ SAC, DSAC has achieved the best performance on all\ntasks, which shows that the return distribution learning is an\nimportant measure to improve policy performance. Besides,\nTD4 also outperforms TD3 and DDPG on most tasks, which\nshows that algorithms with deterministic policies also benefit\ngreatly from the return distribution learning. As TD4 exceeds\nthe performance of D4PG, which learns a discrete return\ndistribution, with a wide margin on Humanoid-v2, Ant-v2\nand HalfCheetah-v2, this indicates that learning a continuous\ndistribution causes significant performance improvements in\nmost cases. Compared with TD4, DSAC achieves 33.8%,\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-7-10.png)\n\n22.1%, 10.4%, 8.0% improvements on Humanoid-v2, Antv2, Walker2d-v2, and HalfCheetah-v2, respectively, suggesting\nthat the maximum entropy framework is an effective measure\nto achieve good performance.\n\n\n_2) Q-value Estimation Accuracy:_ To evaluate the impact of\nthe return distribution learning on Q-value estimation accuracy,\nthis section compares the estimation bias of DSAC, SAC,\nDouble-Q SAC and Single-Q SAC on different benchmarks.\nThe Q-value estimation bias is equal to the difference between\nthe Q-value estimate and the true Q-value. To approximate the\ntrue Q-value, we calculate the average actual discounted return\nover states of 10 episodes every 20000 iterations (evaluate up\nto the first 200 states per episode). Fig. 6 graphs the average Qvalue estimation and true Q-value curves during learning. Table III gives the average relative Q-value estimation bias which\nequals the Q-value estimation bias divided by the true Q-value.\n\n\n\n\n9\n\n\nTABLE II\n\nAVERAGE FINAL RETURN. MAXIMUM VALUE FOR EACH TASK IS BOLDED. _±_ CORRESPONDS TO A SINGLE STANDARD DEVIATION OVER\n\n5 RUNS.\n\n\nTask Humanoid-v2 Ant-v2 Walker2d-v2 HalfCheetah-v2 InvDoublePendulum-v2\n\nDSAC (Ours) **10824** _±_ **347** **9547** _±_ **346** **6920** _±_ **405** **17479** _±_ **148** **9359.7** _±_ **0.2**\nSAC 9019 _±_ 292 7856 _±_ 416 5878 _±_ 580 17300 _±_ 39 9359.6 _±_ 0.2\nDouble-Q SAC 9844 _±_ 396 7682 _±_ 428 5881 _±_ 227 16926 _±_ 132 9359.4 _±_ 0.6\nSingle-Q SAC 8525 _±_ 488 6783 _±_ 197 2176 _±_ 1251 16445 _±_ 815 9355.2 _±_ 3.6\nTD4 8090 _±_ 789 7821 _±_ 262 6270 _±_ 435 16187 _±_ 538 9320.2 _±_ 18.3\n\nTD3 6610 _±_ 1062 7828 _±_ 642 4864 _±_ 512 5619 _±_ 5779 9315.5 _±_ 10.4\n\nDDPG 5477 _±_ 2438 6060 _±_ 747 2849 _±_ 690 11214 _±_ 6861 9198.0 _±_ 13.1\n\nD4PG 175 _±_ 53 2367 _±_ 303 6588 _±_ 260 7215 _±_ 89 9300.9 _±_ 16.3\n\nPPO 7743 _±_ 267 5889 _±_ 111 6654 _±_ 492 9517 _±_ 936 9318.7 _±_ 0.7\n\nTRPO 581 _±_ 56 3767 _±_ 573 2870 _±_ 28 3274 _±_ 346 9324.6 _±_ 2.8\n\n\n(a) DSAC (Ours)\n\n\n(b) SAC\n\n\nFig. 5: DSAC vs SAC on Humanoid-v2.\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-8-0.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-8-1.png)\n\nNoted that this part excludes the InvDoublePendulum-v2 task,\nbecause due to its simplicity, a good policy has been learned\nbefore the value function converges.\n\nCompared with Single-Q SAC that updates Q-value using\nthe traditional TD learning method, the overestimation bias\nof DSAC is reduced by 10.53%, 5.76%, 926.09%, 1.89%\non Humanoid-v2, Ant-v2, Walker2d-v2, and HalfCheetah-v2,\nrespectively. Our results demonstrate the theoretical analysis\nin Section IV-B, i.e., the return distribution learning can be\nused to reduce overestimations without introducing any additional value or policy network. As a comparison, SAC (uses\nclipped double Q-learning) and Double-Q SAC (uses double\nQ-learning) suffer from underestimations during the learning\nprocedure. While the effect of each value learning method\nvaries from task to task, the Q-value estimation accuracy\nof DSAC is higher than SAC and Double-Q SAC in most\ncases. This explains why DSAC exceeds Single-Q SAC, SAC,\nand Double-Q SAC on most benchmarks by a wide margin.\nTherefore, our results demonstrate that the return distribution\nlearning can greatly improve policy performance by mitigating\noverestimations.\n\n\n_3) Time Efficiency:_ Fig. 7 compares the time efficiency of\ndifferent off-policy algorithms. Results show that the average\nwall-clock time consumption per 1000 iterations of DSAC\nis comparable to DDPG, and much lower than SAC, TD3,\nand Double-Q SAC. This is because that unlike double Qlearning and clipped double Q-learning, the return distribution\nlearning does not need to introduce any additional value\nnetwork or policy network (excluding target networks) to\n\n\n\nreduce overestimations.\n\n\n_D. Ablation Studies_\n\nAs shown in Table IV, compared with SAC, DSAC introduces two hyperparameters: 1) the minimum standard deviation _σ_ min in (17), and 2) the clipping boundary _b_ in (18).\nThese two hyperparameters are employed to prevent exploding\nand vanishing gradient problems when learning the continuous\ndistributional value function _Zθ_ ( _·|s, a_ ).\nWe first take the Ant-v2 task as an example to analyze the\ninfluence of _σ_ min on the final performance. From (16), the\ngradients _∇θJZ_ ( _θ_ ) are prone to explode as _σθ_ ( _s, a_ ) _→_ 0.\nTherefore, _σθ_ ( _s, a_ ) should be bounded above by a specific\npositive value. Besides, according to the analysis in Section\nIV-B, if _σ_ min _≥_ 1, we always have ∆ _D_ ( _s, a_ ) _≤_ ∆( _s, a_ ). But\na too large _σ_ min may reduce the estimation accuracy of the\nreturn distribution. Therefore, this paper sets _σ_ min = 1. Fig.\n8a graphs the average final return of DSAC under different\n_σ_ min values on Ant-v2. Our results show that when _σ_ min = 1,\nDSAC achieves the best final performance on Ant-v2, which\nis consistent with the above analysis.\nWe additionally perform the ablation study to compare the\nperformance of DSAC with different clipping boundaries _b_ .\nOur results are presented in Fig. 8b. In this paper, the clipping\nboundary _b_ is employed to stabilize the learning process of\n_σθ_ ( _s, a_ ) and keep it in a reasonable range. Results indicate\nthat compared with the performance of removing the clipping\nboundary trick from DSAC (i.e., _b_ = + _∞_ ), the inclusion\nof _b_ (for different _b_ values) generally improves performance.\n\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-9-0.png)\n\n\n\nAnt-v2 benchmark. Each boxplot is drawn based on values of 50\nevaluations. All evaluations were performed on a single computer\nwith a 2.4 GHz 20 core Intel Xeon CPU.\n\n\nTherefore, DSAC appears to benefit greatly from the clipping\nboundary trick. However, the final performance is a little bit\nsensitive to the value of _b_ . This is because that a too small _b_\n\nwill reduce the learning accuracy of the return distribution,\nwhile a too large _b_ cannot effectively limit the range of\n_σθ_ ( _s, a_ ). In practical applications, it is usually necessary to\nselect an appropriate _b_ value according to the range of the\nstate-action return _Z_ ( _s, a_ ), which limits the flexibility of the\nDSAC algorithm. We will focus on this issue in the future.\n\n\nVII. CONCLUSIONS\n\n\nIn this paper, we propose an off-policy RL algorithm for\ncontinuous control setting, called distributional soft actorcritic (DSAC), to mitigate Q-value overestimations, thereby\nimproving policy performance. We first discover in theory that\nthe update stepsize of the Q-value function in distributional RL\n\n\n\ndecreases squarely as the standard deviation of state-action\nreturns increases, thus mitigating Q-value overestimations.\nThen, a distributional soft policy iteration (DSPI) framework is\ndeveloped by embedding the return distribution function into\nmaximum entropy RL, which alternates between distributional\nsoft policy evaluation and soft policy improvement. Next, a\ndeep off-policy actor-critic variant of DSPI, i.e., DSAC, is\nproposed to directly learn a continuous return distribution\nby keeping the variance of the state-action returns within\nreasonable range to address exploding and vanishing gradient\nproblems. We evaluate DSAC and 9 baselines (such as SAC,\nTD3, PPO, DDPG) on the suite of MuJoCo tasks. Results\nshow that DSAC outperforms or matches all other baseline\nalgorithms across all benchmarks.\n\n\nAPPENDIX A\n\nPROOF OF CONVERGENCE OF DISTRIBUTIONAL SOFT\n\nPOLICY ITERATION\n\n\nIn this appendix, we present proofs to show that Distributional Soft Policy Iteration (DSPI), which alternates between\n\n\n\n\n(6) and (4), would lead to policy improvement with respect\nto the maximum entropy objective. The proofs borrow heavily\nfrom the policy evaluation and policy improvement theorems\nof Q-learning, distributional RL and soft Q-learning [7], [16],\n\n[18].\n\n\n**Lemma 1.** _(Distributional Soft Policy Evaluation). Consider_\n_the distributional soft bellman backup operator TD_ _[π]_ _[in]_ [ (6)] _[ and]_\n_a soft state-action distribution function Z_ [0] ( _Z_ [0] ( _s, a_ ) _|s, a_ ) :\n_S×A →P_ ( _Z_ [0] ( _s, a_ )) _, which maps a state-action pair_ ( _s, a_ ) _to_\n_a distribution over random soft state-action returns Z_ [0] ( _s, a_ ) _,_\n_and define Z_ _[i]_ [+1] ( _s, a_ ) = _TD_ _[π][Z]_ _[i]_ [(] _[s, a]_ [)] _[, where][ Z]_ _[i]_ [+1][(] _[s, a]_ [)] _[ ∼]_\n_Z_ _[i]_ [+1] ( _·|s, a_ ) _. Then the sequence Z_ _[i]_ _will converge to Z_ _[π]_ _as_\n_i →∞._\n\n\n_Proof._ Let _Z_ denote the space of soft return function _Z_ .\nDefine the entropy augmented reward as _rπ_ ( _s, a_ ) = _r_ ( _s, a_ ) _−_\n_γα_ log _π_ ( _a_ _[′]_ _|s_ _[′]_ ) and rewrite the distributional soft Bellman\noperator as\n\n\n_D_ _′_ _′_\n_TD_ _[π][Z]_ [(] _[s, a]_ [)] = _rπ_ ( _s, a_ ) + _γZ_ ( _s_ _, a_ ) _,_\n\n\nwhere _r ∼_ _R_ ( _·|s, a_ ) _, s_ _[′]_ _∼_ _p, a_ _[′]_ _∼_ _π_ . Then we can directly\napply the standard convergence results for policy evaluation of\ndistributional RL [18], that is, _TD_ _[π]_ [:] _[ Z][ →]_ _[Z]_ [ is a] _[ γ]_ [-contraction]\nin terms of some measure. Therefore, _TD_ _[π]_ [has a unique fixed]\npoint, which is _Z_ _[π]_, and the sequence _Z_ _[i]_ will converge to it\nas _i →∞_, i.e., _Z_ _[i]_ will converge to _Z_ _[π]_ as _i →∞_ .\n\n\n**Lemma 2.** _(Soft Policy Improvement) Let πnew be the optimal_\n_solution of the maximization problem defined in_ (4) _. Then_\n_Q_ _[π][new]_ ( _s, a_ ) _≥_ _Q_ _[π][old]_ ( _s, a_ ) _for ∀_ ( _s, a_ ) _∈S × A._\n\n\n_Proof._ From (4), one has\n\n\n_π_ new( _·|s_ ) = arg max E _∀s ∈S,_\n_π_ _a∼π_ [[] _[Q][π]_ [old][(] _[s, a]_ [)] _[−][α]_ [ log] _[ π]_ [(] _[a][|][s]_ [)]] _[,]_\n(20)\nthen it is obvious that\n\n\nE\n_a∼π_ new [[] _[Q][π]_ [old][(] _[s, a]_ [)] _[ −]_ _[α]_ [ log] _[ π]_ [new][(] _[a][|][s]_ [)]] _[ ≥]_\n\n(21)\nE _∀s ∈S._\n_a∼π_ old [[] _[Q][π]_ [old][(] _[s, a]_ [)] _[ −]_ _[α]_ [ log] _[ π]_ [old][(] _[a][|][s]_ [)]] _[,]_\n\n\nNext, from (3), it follows that\n\n\n_Q_ _[π]_ [old] ( _s, a_ )\n\n= E E [ _Q_ _[π]_ [old] ( _s_ _[′]_ _, a_ _[′]_ ) _−_ _α_ log _π_ old( _a_ _[′]_ _|s_ _[′]_ )]\n_r∼R_ ( _·|s,a_ ) [[] _[r]_ [] +] _[ γ]_ _s_ _[′]_ _∼p,a_ _[′]_ _∼π_ old\n\n_≤_ E E [ _Q_ _[π]_ [old] ( _s_ _[′]_ _, a_ _[′]_ ) _−_ _α_ log _π_ new( _a_ _[′]_ _|s_ _[′]_ )]\n_r∼R_ ( _·|s,a_ ) [[] _[r]_ [] +] _[ γ]_ _s_ _[′]_ _∼p,a_ _[′]_ _∼π_ new\n\n\n...\n\n\n_≤_ _Q_ _[π]_ [new] ( _s, a_ ) _,_ _∀_ ( _s, a_ ) _∈S × A,_\n\n\nwhere we have repeatedly expanded _Q_ _[π]_ [old] on the right-hand\nside by applying (3).\n\n\n**Theorem 1.** _(Distributional Soft Policy Iteration). The distri-_\n_butional soft policy iteration, which alternates between distri-_\n_butional soft policy evaluation and soft policy improvement,_\n_can converge to a policy π_ _[∗]_ _such that Q_ _[π][∗]_ ( _s, a_ ) _≥_ _Q_ _[π]_ ( _s, a_ )\n_for ∀π and ∀_ ( _s, a_ ) _∈S × A, assuming that |A| < ∞_ _and_\n_reward is bounded._\n\n\n\n11\n\n\n_Proof._ Let _πk_ denote the policy at iteration _k_ . For _∀πk_, we can\nalways find its associated _Z_ _[π][k]_ through distributional soft policy evaluation process follows from Lemma 1. Therefore, we\ncan obtain _Q_ _[π][k]_ according to (5). By Lemma 2, the sequence\n_Q_ _[π][k]_ ( _s, a_ ) is monotonically increasing for _∀_ ( _s, a_ ) _∈S × A_ .\nSince _Q_ _[π]_ is bounded everywhere for _∀π_ (both the reward and\npolicy entropy are bounded), the policy sequence _πk_ converges\nto some _π_ _[†]_ as _k →∞_ . At convergence, it must follow that\n\n\nE\n_a∼π_ _[†]_ [[] _[Q][π][†]_ [(] _[s, a]_ [)] _[ −]_ _[α]_ [ log] _[ π][†]_ [(] _[a][|][s]_ [)]] _[ ≥]_\n\n(22)\nE _∀π, ∀s ∈S._\n_a∼π_ [[] _[Q][π][†]_ [(] _[s, a]_ [)] _[ −]_ _[α]_ [ log] _[ π]_ [(] _[a][|][s]_ [)]] _[,]_\n\n\nUsing the same iterative argument as in Lemma 2, we have\n\n_Q_ _[π][†]_ ( _s, a_ ) _≥_ _Q_ _[π]_ ( _s, a_ ) _,_ _∀π, ∀_ ( _s, a_ ) _∈S × A._\n\n\nHence _π_ _[†]_ is optimal, i.e., _π_ _[†]_ = _π_ _[∗]_ .\n\n\nAPPENDIX B\n\nDERIVATIONS\n\n_A. Derivation of the Standard Deviation in Distributional Q-_\n_learning_\n\nSince the random error _ϵQ_ in (9) is assumed to be independent of ( _s, a_ ), _δ_ in (10) can be further expressed as\n\n\n˜\n_δ_ = E _ϵ_ _[′]_ _Q_ �E _s′_ [max _a_ _[′][ Q]_ [(] _[s][′][, a][′]_ [)]] � _−_ E _s′_ [max _a_ _[′]_ _Q_ ( _s_ _[′]_ _, a_ _[′]_ )]\n\n\n˜\n= E _ϵ_ _[′]_ _Q_ �E _s_ _[′]_ [max _a_ _[′][ Q][θ]_ [(] _[s][′][, a][′]_ [)] _[ −]_ [max] _a_ _[′]_ _Q_ ( _s_ _[′]_ _, a_ _[′]_ )]� _._\n\n\nDefining _η_ = E _s′_ [�] max _a′ Qθ_ ( _s_ _[′]_ _, a_ _[′]_ ) _−_ max _a′_ _Q_ [˜] ( _s_ _[′]_ _, a_ _[′]_ )�, it\nfollows that\n\n_δ_ = E _ϵ_ _[′]_\n_Q_ [[] _[η]_ []] _[.]_\n\n\nFrom (12), we linearize the post-update standard deviation\naround _ψ_ using Taylor’s expansion\n\n\n_σψ_ new ( _s, a_ ) _≈_\n\n_σψ_ ( _s, a_ ) + _β_ [∆] _[σ]_ [2][ + ][(] _[y][ −]_ _[Q][θ]_ [(] _[s][,][ a]_ [))][2] _∥∇ψσψ_ ( _s, a_ ) _∥_ 2 [2] _[.]_\n\n_σψ_ ( _s, a_ ) [3]\n\n\nThen, in expectation, the post-update standard deviation is\n\n\nE _ϵQ,ϵ_ _[′]_ _Q_ [[] _[σ][ψ]_ [new] [(] _[s, a]_ [)]] _[ ≈]_ _[σ][ψ]_ [(] _[s, a]_ [)+]\n\n∆ _σ_ [2] + E _ϵQ,ϵ_ _[′]_ _Q_ [[(] _[y][ −]_ _[Q][θ]_ [(] _[s, a]_ [))][2][]]\n_β_ _∥∇ψσψ_ ( _s, a_ ) _∥_ 2 [2] _[.]_\n\n_σψ_ ( _s, a_ ) [3]\n\nSince E _ϵQ_ [ _ϵQ_ ] = 0, the E _ϵQ,ϵ_ _[′]_ _Q_ [[(] _[y][ −]_ _[Q][θ]_ [(] _[s, a]_ [))][2][]][ term can be]\nexpanded as\n\nE _ϵQ,ϵ_ _[′]_ _Q_ [[(] _[y][ −]_ _[Q][θ]_ [(] _[s, a]_ [))][2][]]\n\n= E _ϵQ,ϵ_ _[′]_ _Q_ �(E[ _r_ ] + _γ_ E _s′_ [max _a_ _[′][ Q][θ]_ [(] _[s][′][, a][′]_ [)]] _[ −]_ _[Q][θ]_ [(] _[s, a]_ [))][2][�]\n\n\n˜ ˜\n= E _ϵQ,ϵ_ _[′]_ _Q_ �(E[ _r_ ] + _γ_ E _s′_ [max _a_ _[′]_ _Q_ ( _s_ _[′]_ _, a_ _[′]_ )] + _γη −_ _Q_ ( _s, a_ ) _−_ _ϵQ_ ) [2][�]\n\n= E _ϵQ,ϵ_ _[′]_ _Q_ �(˜ _y −_ _Q_ [˜] ( _s, a_ ) + _γη −_ _ϵQ_ ) [2][�]\n\n= (˜ _y −_ _Q_ [˜] ( _s, a_ )) [2] + E _ϵQ,ϵ_ _[′]_ _Q_ �( _γη −_ _ϵQ_ ) [2][�] +\n\nE _ϵQ,ϵ_ _[′]_ _Q_ �2(˜ _y −_ _Q_ [˜] ( _s, a_ ))( _γη −_ _ϵQ_ )�\n\n\n2\n= (˜ _y −_ _Q_ [˜] ( _s, a_ )) [2] + _γ_ [2] E _ϵ_ _[′]_ _Q_ [[] _[η]_ [2][] +][ E] _[ϵ][Q]_ [[] _[ϵ][Q]_ ]+\n\n2 _γ_ (˜ _y −_ _Q_ [˜] ( _s, a_ ))E _ϵ_ _[′]_ _Q_ [[] _[η]_ []] _[ −]_ [2(] _[γ]_ [E] _[ϵ][′]_ _Q_ [[] _[η]_ [] + ˜] _[y][ −]_ _[Q]_ [˜][(] _[s, a]_ [))][E] _[ϵ][Q]_ [[] _[ϵ][Q]_ []]\n\n\n2 ˜\n= (˜ _y −_ _Q_ [˜] ( _s, a_ )) [2] + _γ_ [2] E _ϵ_ _[′]_ _Q_ [[] _[η]_ [2][] +][ E] _[ϵ][Q]_ [[] _[ϵ][Q]_ ] + 2 _γδ_ (˜ _y −_ _Q_ ( _s, a_ )) _._\n\n\n\n\n12\n\n\n\nIn an ideal situation, when _Q_ [˜] ( _s, a_ ) = ˜ _y_, that is, _Q_ [˜] ( _s, a_ ) has\nconverged after a period of learning, we further have\n\n\nE _ϵQ,ϵ_ _[′]_ _Q_ [[(] _[y][ −]_ _[Q][θ]_ [(] _[s, a]_ [))][2][] =] _[ γ]_ [2][E] _[ϵ]_ _Q_ _[′]_ [[] _[η]_ [2][] +][ E] _[ϵ][Q]_ [[] _[ϵ][Q]_ 2] _._\n\n\nFurthermore, since E _ϵ_ _[′]_\n_Q_ [[] _[η]_ [2][]] _[ ≥]_ [E] _[ϵ][′]_ _Q_ [[] _[η]_ []][2][, we have]\n\n\nE _ϵQ,ϵ_ _[′]_ _Q_ [[] _[σ][ψ]_ [new] [(] _[s, a]_ [)]]\n\n∆ _σ_ [2] + _γ_ [2] E _ϵ_ _[′]_ _Q_ [[] _[η]_ [2][] +][ E] _[ϵ][Q]_ [[] _[ϵ][Q]_ [2][]]\n_≈_ _σψ_ ( _s, a_ ) + _β_ _∥∇ψσψ_ ( _s, a_ ) _∥_ 2 [2]\n\n_σψ_ ( _s, a_ ) [3]\n\n∆ _σ_ [2] + _γ_ [2] E _ϵ_ _[′]_ _Q_ [[] _[η]_ []][2][ +][ E] _[ϵ][Q]_ [[] _[ϵ][Q]_ [2][]]\n_≥_ _σψ_ ( _s, a_ ) + _β_ _∥∇ψσψ_ ( _s, a_ ) _∥_ 2 [2]\n\n_σψ_ ( _s, a_ ) [3]\n\n= _σψ_ ( _s, a_ ) + _β_ [∆] _[σ]_ [2][ +] _[ γ]_ [2] _[δ]_ [2][ +][ E] _[ϵ][Q]_ [[] _[ϵ][Q]_ [2][]] _∥∇ψσψ_ ( _s, a_ ) _∥_ 2 [2] _[.]_\n\n_σψ_ ( _s, a_ ) [3]\n\n\n_B. Derivation of the Objective Function for Soft Return Dis-_\n_tribution Update_\n\n\nFrom (7), the loss function for soft state-action return\ndistribution under the KL-divergence measurement is\n\n\n_JZ_ ( _θ_ )\n\n= E( _s,a_ ) _∼B_ � _D_ KL( _TDπφ′_ _Zθ′_ ( _·|s, a_ ) _, Zθ_ ( _·|s, a_ ))�\n\n\n\nwhere _⊙_ represents the Hadamard product and tanh is applied\nelement-wise. From [16], the probability density of _a_ is given\nby\n\n\n\nd _a_\n_π_ ( _a|s_ ) = _µ_ ( _u|s_ ) det\n��� � d _u_\n\n\n\nd _u_\n\n\n\n_−_ 1\n\n_._\n����\n\n\n\nThe log-likelihood of _π_ ( _a|s_ ) can be expressed as\n\n\nlog _π_ ( _a|s_ ) = log _µ_ ( _u|s_ ) _−_\n\n\n\ndim( _A_ )\n�\n\n\n_i_ =1\n\n\n\n\n_[−]_ _[a]_ [min] _[i]_\nlog(1 _−_ tanh [2] ( _ui_ )) + log _[a]_ [max] _[i]_\n� 2\n\n\n\n_._\n�\n\n\n\n= E( _s,a_ ) _∼B_ � �\n\n_TDπφ′_ _Z_ ( _s,a_ )\n\n\n= _−_ E( _s,a_ ) _∼B_ � �\n\n\n\n_P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|TDπφ′_ _Zθ′_ ( _·|s, a_ ))\n\n\n\n_Dπφ′_ _Z_ ( _s, a_ ) _|TDπφ′_ _Zθ′_ ( _·|s, a_ ))\nlog _[P]_ [(] _P_ _[T]_ ( _TD_ ~~_π_~~ _φ′_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )) �\n\n_P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|TDπφ′_ _Zθ′_ ( _·|s, a_ ))\n\n\n\n_D. Policy Update Gradients Based on the Soft-Action Return_\n\n\nIf _Qθ_ ( _s, a_ ) cannot be expressed explicitly through _θ_, besides\n(19), we also need to reparameterize the random return _Z_ ( _s, a_ )\n\nas\n_Z_ ( _s, a_ ) = _gθ_ ( _ξZ_ ; _s, a_ ) _._\n\n\nIn this case, we have\n\n\n_∇φJπ_ ( _φ_ ) = E _s∼B,ξZ_ _,ξa_ _−_ _α∇φ_ log( _πφ_ ( _a|s_ ))+\n�\n\n\n( _∇agθ_ ( _ξZ_ ; _s, a_ ) _−_ _α∇a_ log( _πφ_ ( _a|s_ ))) _∇φfφ_ ( _ξa_ ; _s_ ) _._\n�\n\n\nBesides, the distribution _Zθ_ offers a richer set of predictions\nfor learning than its expected value _Qθ_ . Therefore, we can\nalso choose to maximize the _i_ th percentile of _Zθ_\n\n\n_Jπ,i_ ( _φ_ ) = E _s∼B,a∼πφ_ [ _Pi_ ( _Zθ_ ( _s, a_ )) _−_ _α_ log( _πφ_ ( _a|s_ ))] _,_\n\n\nwhere _Pi_ denotes the _i_ th percentile. For example, _i_ should be\na smaller value for risk-aware policies learning. The gradients\nof this objective can also be easily approximated using the\nreparamterization trick.\n\n\nAPPENDIX C\n\nEXPERIMENTAL DETAILS\n\n\n_A. Brief Descriptions of Benchmarks_\n\n\nThe Humanoid-v2 task aims to make a three-dimensional\n\nbipedal robot walk forward as fast as possible, without falling\nover. Its state is described by 376-dimensional information,\nincluding the position and velocity of joints, the inertia and\nvelocity at the center of mass, and actuator forces. The action\nof this task is composed of the torque applied over 17 joints.\nThe reward function is designed to punish the actions that cost\na lot of energy or cause mission failure. Similarly, Walker2dv2 is a two-dimensional bipedal robot which possesses 17dimensional states and 6-dimensional actions. The Ant-v2 task\n\naims to make a four-legged creature walk forward as fast as\npossible with a 111-dimensional state vector to describe the\nposition and velocity of each joint. Its action consists of the\ntorque of 8 joints, and the reward is also designed to punish\nthe actions that cost a lot of energy or cause mission failure.\nAnalogously, HalfCheetah-v2 is a two-legged cheetah with\n17-dimensional states and 6-dimensional actions. The goal of\nInvertedDoublePendulum-v2, which is described by an 11dimensional state vector, is to make two linked poles stand\nup on a cart as long as possible by applying a force on the\ncart. See https://github.com/openai/gym/tree/master/gym/envs\nfor all details.\n\n\n\n_TDπφ′_ _Z_ ( _s,a_ )\n\n\nlog _P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )) + _c_\n�\n\n= _−_ E( _s,a_ ) _∼B_ �E _TDπφ′_ _Z_ ( _s,a_ ) _∼TDπφ′_ _Zθ′_ ( _·|s,a_ )\n\nlog _P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )) + _c_\n�\n\n= _−_ E( _s,a_ ) _∼B_ � ( _r,s_ _[′]_ ) _∼B_ E _,a_ _[′]_ _∼πφ′_ _,_\n_Z_ ( _s_ _[′]_ _,a_ _[′]_ ) _∼Zθ′_ ( _·|s_ _[′]_ _,a_ _[′]_ )\n\n\nlog _P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )) + _c_\n�\n\n\n\n= _−_ E\n( _s,a,r,s_ _[′]_ ) _∼B,a_ _[′]_ _∼πφ′_ _,_\n_Z_ ( _s_ _[′]_ _,a_ _[′]_ ) _∼Zθ′_ ( _·|s_ _[′]_ _,a_ _[′]_ )\n\n\n\nlog _P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )) + _c,_\n� �\n\n\n\nwhere _c_ is an item independent of _θ_ .\n\n\n_C. Probability Density of the Bounded Actions_\n\n\nFor algorithms with stochastic policy, we use an unbounded\nGaussian as the action distribution _µ_ . However, in practice, the\naction usually needs to be bounded to a finite interval denoted\nas [ _a_ min _, a_ max], where _a_ min _∈_ R [dim(] _[A]_ [)] and _a_ max _∈_ R [dim(] _[A]_ [)] . Let\n_u ∈_ R [dim(] _[A]_ [)] denote a random variable sampled from _µ_ . To\naccount for the action constraint, we project _u_ into a desired\naction by\n\n\n\n\n_[a]_ [min] _⊙_ tanh( _u_ ) + _[a]_ [max][ +] _[ a]_ [min]\n\n2 2\n\n\n\n_a_ = _[a]_ [max] _[ −]_ _[a]_ [min]\n\n\n\n2 _,_\n\n\n\n\n_B. Double-Q SAC Algorithm_\n\n\nSuppose the soft Q-value and policy are approximated by\nparameterized functions _Qθ_ ( _s, a_ ) and _πφ_ ( _a|s_ ) respectively.\nA pair of soft Q-value functions ( _Qθ_ 1 _, Qθ_ 2) and policies\n( _πφ_ 1 _, πφ_ 2) are required in Double-Q SAC, where _πφ_ 1 is\nupdated with respect to _Qθ_ 1 and _πφ_ 2 with respect to _Qθ_ 2.\nGiven separate target soft Q-value functions ( _Qθ_ 1 _[′]_ _[, Q][θ]_ 2 _[′]_ [)][ and]\npolicies ( _πφ_ _[′]_ 1 _[, π][φ][′]_ 2 [)][, the update targets of] _[ Q][θ]_ [1][ and] _[ Q][θ]_ [2][ are]\ncalculated as:\n\n\n_y_ 1 = _r_ + _γ_ ( _Qθ_ 2 _[′]_ [(] _[s][′][, a][′]_ [)] _[ −]_ _[α]_ [ log(] _[π][φ][′]_ 1 [(] _[a][′][|][s][′]_ [)))] _[, a][′][ ∼]_ _[π][φ][′]_ 1 _[,]_\n\n_y_ 2 = _r_ + _γ_ ( _Qθ_ 1 _[′]_ [(] _[s][′][, a][′]_ [)] _[ −]_ _[α]_ [ log(] _[π][φ][′]_ 2 [(] _[a][′][|][s][′]_ [)))] _[, a][′][ ∼]_ _[π][φ][′]_ 2 _[.]_\n\n\nThe soft Q-value can be trained by directly minimizing\n\n\n\n13\n\n\nThe policy can be learned by directly maximizing a parameterized variant of the objective function in (4)\n\n\n_Jπ_ ( _φ_ ) = E _s∼B_ �E _a∼πφ_ [ _Qθ_ ( _s, a_ ) _−_ _α_ log( _πφ_ ( _a|s_ ))]� _._\n\n\nThe pseudo-code of Single-Q SAC is shown in Algorithm 3.\n\n\n**Algorithm 3** Single-Q SAC Algorithm\n\n\nInitialize parameters _θ_, _φ_ and _α_\nInitialize target parameters _θ_ _[′]_ _←_ _θ_, _φ_ _[′]_ _←_ _φ_\nInitialize learning rate _βQ_, _βπ_, _βα_ and _τ_\nInitialize iteration index _k_ = 0\n\n**repeat**\n\nSelect action _a ∼_ _πφ_ ( _a|s_ )\nObserve reward _r_ and new state _s_ _[′]_\n\nStore transition tuple ( _s, a, r, s_ _[′]_ ) in buffer _B_\nSample _N_ transitions ( _s, a, r, s_ _[′]_ ) from _B_\nUpdate soft Q-function _θ ←_ _θ −_ _βQ∇θJQ_ ( _θ_ )\n**if** _k_ mod _m_ **then**\n\nUpdate policy _φ ←_ _φ_ + _βπ∇φJπ_ ( _φ_ )\nAdjust temperature _α ←_ _α −_ _βα∇αJ_ ( _α_ )\nUpdate target networks:\n\n_θ_ _[′]_ _←_ _τθ_ + (1 _−_ _τ_ ) _θ_ _[′]_, _φ_ _[′]_ _←_ _τφ_ + (1 _−_ _τ_ ) _φ_ _[′]_\n\n**end if**\n\n_k_ = _k_ + 1\n**until** Convergence\n\n\n_D. TD4 Algorithm_\n\n\nConsider a parameterized state-action return distribution\nfunction _Zθ_ ( _·|s, a_ ) and a deterministic policy _πφ_ ( _s_ ), where\n_θ_ and _φ_ are parameters. The target networks _Zθ′_ ( _·|s, a_ ) and\n_πφ′_ ( _s_ ) are used to stabilize learning. The return distribution\ncan be trained to minimize\n\n\n\n_JQ_ ( _θi_ ) = E\n( _s,a,r,s_ _[′]_ ) _∼B,a_ _[′]_ _∼πφ′i_\n\n\n\n�( _yi−Qθi_ ( _s, a_ )) [2][�] _,_ for _i ∈{_ 1 _,_ 2 _}._\n\n\n\nThe policy can be learned by directly maximizing a parameterized variant of the objective function in (4)\n\n\n_Jπ_ ( _φi_ ) = E _s∼B_ �E _a∼πφi_ [ _Qθi_ ( _s, a_ ) _−_ _α_ log( _πφi_ ( _a|s_ ))]� _._\n\n\nThe pseudo-code of Double-Q SAC is shown in Algorithm 2.\n\n\n**Algorithm 2** Double-Q SAC Algorithm\n\n\nInitialize parameters _θ_ 1, _θ_ 2, _φ_ 1, _φ_ 2, and _α_\nInitialize target parameters _θ_ 1 _[′]_ _[←]_ _[θ]_ [1][,] _[ θ]_ 2 _[′]_ _[←]_ _[θ]_ [2][,] _[ φ][′]_ 1 _[←]_ _[φ]_ [1][,]\n_φ_ _[′]_ 2 _[←]_ _[φ]_ [2]\nInitialize learning rate _βQ_, _βπ_, _βα_ and _τ_\nInitialize iteration index _k_ = 0\n\n**repeat**\n\nSelect action _a ∼_ _πφ_ 1( _a|s_ )\nObserve reward _r_ and new state _s_ _[′]_\n\nStore transition tuple ( _s, a, r, s_ _[′]_ ) in buffer _B_\nSample _N_ transitions ( _s, a, r, s_ _[′]_ ) from _B_\nUpdate soft Q _θi ←_ _θi −_ _βQ∇θiJQ_ ( _θi_ ) for _i ∈{_ 1 _,_ 2 _}_\n**if** _k_ mod _m_ **then**\n\nUpdate policy _φi ←_ _φi_ + _βπ∇φiJπ_ ( _φi_ ) for _i ∈{_ 1 _,_ 2 _}_\nAdjust temperature _α ←_ _α −_ _βα∇αJ_ ( _α_ )\nUpdate target networks:\n\n_θi_ _[′]_ _[←]_ _[τθ][i]_ [ + (1] _[ −]_ _[τ]_ [)] _[θ]_ _i_ _[′]_ [for] _[ i][ ∈{]_ [1] _[,]_ [ 2] _[}]_\n_φ_ _[′]_ _i_ _[←]_ _[τφ][i]_ [ + (1] _[ −]_ _[τ]_ [)] _[φ][′]_ _i_ [for] _[ i][ ∈{]_ [1] _[,]_ [ 2] _[}]_\n**end if**\n\n_k_ = _k_ + 1\n**until** Convergence\n\n\n_C. Single-Q SAC Algorithm_\n\n\nSuppose the soft Q-value and policy are approximated by\nparameterized functions _Qθ_ ( _s, a_ ) and _πφ_ ( _a|s_ ) respectively.\nGiven separate target soft Q-value function _Qθ′_ and policy\n_πφ′_, the update target of _Qθ_ is calculated as:\n\n\n_y_ = _r_ + _γ_ ( _Qθ_ _[′]_ ( _s_ _[′]_ _, a_ _[′]_ ) _−_ _α_ log( _πφ_ _[′]_ ( _a_ _[′]_ _|s_ _[′]_ ))) _, a_ _[′]_ _∼_ _πφ_ _[′]_ _._\n\n\nThe soft Q-value can be trained by directly minimizing\n\n\n\nwhere\n_D_ _′_ _′_\n_TD_ _[π][Z]_ [(] _[s, a]_ [)] = _r_ ( _s, a_ ) + _γZ_ ( _s_ _, a_ )\n\n\nand\n_a_ _[′]_ = _πφ_ _[′]_ ( _s_ _[′]_ ) + _ϵ, ϵ ∼_ clip( _N_ (0 _, σ_ [2] ) _, −c, c_ ) _._\n\n\nThe calculation of _∇θJZ_ ( _θ_ ) is similar to DSAC. The policy\ncan be learned by directly maximizing the expected return\n\n\n_Jπ_ ( _φ_ ) = E _s∼B_ � _Qθ_ ( _s, πφ_ ( _s_ ))� _._\n\n\nThe pseudo-code is shown in Algorithm 4.\n\n\n_E. Hyperparameters_\n\n\nTable IV lists the hyperparameters of all algorithms.\n\n\nACKNOWLEDGMENT\n\n\nWe would like to acknowledge Dongjie Yu for his valuable\nsuggestions. The authors are grateful to the Editor-in-Chief,\nthe Associate Editor, and anonymous reviewers for their valuable comments.\n\n\n\n_JZ_ ( _θ_ ) = _−_ E\n( _s,a,r,s_ _[′]_ ) _∼B,a_ _[′]_ _∼πφ′_ _,_\n_Z_ ( _s_ _[′]_ _,a_ _[′]_ ) _∼Zθ′_ ( _·|s_ _[′]_ _,a_ _[′]_ )\n\n\n\nlog _P_ ( _TDπφ′_ _Z_ ( _s, a_ ) _|Zθ_ ( _·|s, a_ )) _,_\n� �\n\n\n\n_JQ_ ( _θ_ ) = E\n( _s,a,r,s_ _[′]_ ) _∼B,a_ _[′]_ _∼πφ′_\n\n\n\n�( _y −_ _Qθ_ ( _s, a_ )) [2][�] _._\n\n\n\n\n**Algorithm 4** TD4 Algorithm\n\n\nInitialize parameters _θ_, _φ_ and _α_\nInitialize target parameters _θ_ _[′]_ _←_ _θ_, _φ_ _[′]_ _←_ _φ_\nInitialize learning rate _βZ_, _βπ_, _βα_ and _τ_\nInitialize iteration index _k_ = 0\n\n**repeat**\n\nSelect action with exploration noise _a_ = _πφ_ ( _s_ ) + _ϵ_, _ϵ ∼_\n_N_ (0 _,_ ˆ _σ_ [2] )\nObserve reward _r_ and new state _s_ _[′]_\n\nStore transition tuple ( _s, a, r, s_ _[′]_ ) in buffer _B_\nSample _N_ transitions ( _s, a, r, s_ _[′]_ ) from _B_\nCalculate action for target policy smoothing _a_ _[′]_ =\n_πφ′_ ( _s_ _[′]_ ) + _ϵ_, _ϵ ∼_ clip( _N_ (0 _, σ_ [2] ) _, −c, c_ )\nUpdate return distribution _θ ←_ _θ −_ _βZ_ _∇θJZ_ ( _θ_ )\n**if** _k_ mod _m_ **then**\n\nUpdate policy _φ ←_ _φ_ + _βπ∇φJφ_ ( _φ_ )\nUpdate target networks:\n\n_θ_ _[′]_ _←_ _τθ_ + (1 _−_ _τ_ ) _θ_ _[′]_, _φ_ _[′]_ _←_ _τφ_ + (1 _−_ _τ_ ) _φ_ _[′]_\n\n**end if**\n\n_k_ = _k_ + 1\n**until** Convergence\n\n\nTABLE IV\n\nDETAILED HYPERPARAMETERS.\n\n\nHyperparameters Value\n_Shared_\nOptimizer Adam ( _β_ 1 = 0 _._ 9 _, β_ 2 = 0 _._ 999)\nNumber of hidden layers 5\nNumber of hidden units per layer 256\nNonlinearity of hidden layer GELU\nReplay buffer size 5 _×_ 10 [5]\nBatch size 256\nActor learning rate cos anneal 5e _−_ 5 _→_ 1e _−_ 6\nCritic learning rate cos anneal 8e _−_ 5 _→_ 1e _−_ 6\nDiscount factor ( _γ_ ) 0.99\nUpdate interval ( _m_ ) 2\nTarget smoothing coefficient ( _τ_ ) 0.001\nReward scale 0.2\nNumber of actor processes 6\nNumber of learner processes 4\nNumber of buffer processes 3\n_Stochastic policy_\nLearning rate of _α_ cos anneal 5e _−_ 5 _→_ 1e _−_ 6\nExpected entropy ( _H_ ) _H_ = _−_ dim( _A_ )\n_Deterministic policy_\nExploration noise _ϵ ∼N_ (0 _,_ 0 _._ 1 [2] )\n_Distributional value function_\nBounds of variance _σ_ min = 1\nClipping boundary _b_ = 10\n_TD4,TD3_\nPolicy smoothing noise _ϵ ∼_ clip( _N_ (0 _,_ 0 _._ 2 [2] ) _, −_ 0 _._ 5 _,_ 0 _._ 5)\n\n\nREFERENCES\n\n\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\n_et al._, “Human-level control through deep reinforcement learning,”\n_Nature_, vol. 518, no. 7540, p. 529, 2015.\n\n[2] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot, _et al._, “Mastering the game of go with deep neural networks\nand tree search,” _Nature_, vol. 529, no. 7587, p. 484, 2016.\n\n[3] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,” in _Proceedings of the 33rd International Con-_\n\n\n\n14\n\n\n_ference on Machine Learning, (ICML 2016)_, (New York City, NY, USA),\npp. 1928–1937, 2016.\n\n[4] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, _et al._, “Mastering\nthe game of go without human knowledge,” _Nature_, vol. 550, no. 7676,\np. 354, 2017.\n\n[5] J. Duan, S. E. Li, Y. Guan, Q. Sun, and B. Cheng, “Hierarchical\nreinforcement learning for self-driving decision-making without reliance\non labelled driving data,” _IET Intelligent Transport Systems_, vol. 14,\nno. 5, pp. 297–305, 2020.\n\n[6] C. J. C. H. Watkins, _Learning from delayed rewards_ . PhD thesis, King’s\nCollege, Cambridge, 1989.\n\n[7] R. S. Sutton and A. G. Barto, _Reinforcement learning: An introduction_ .\nMIT press, 2018.\n\n[8] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning,” in _Proceedings of the 30th Conference on_\n_Artificial Intelligence (AAAI 2016)_, (Phoenix, Arizona,USA), pp. 2094–\n2100, 2016.\n\n[9] S. Thrun and A. Schwartz, “Issues in using function approximation\nfor reinforcement learning,” in _Proceedings of the 1993 Connectionist_\n_Models Summer School_, (Hillsdale NJ. Lawrence Erlbaum), 1993.\n\n[10] D. Lee, B. Defourny, and W. B. Powell, “Bias-corrected q-learning to\ncontrol max-operator bias in q-learning,” in _2013 IEEE Symposium on_\n_Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)_,\npp. 93–99, IEEE, 2013.\n\n[11] D. Lee and W. B. Powell, “Bias-corrected q-learning with multistate\nextension,” _IEEE Transactions on Automatic Control_, vol. 64, no. 10,\npp. 4011–4023, 2019.\n\n[12] S. Fujimoto, H. van Hoof, and D. Meger, “Addressing function approximation error in actor-critic methods,” in _Proceedings of the 35th_\n_International Conference on Machine Learning (ICML 2018)_, (Stockholmsm¨assan, Stockholm Sweden), pp. 1587–1596, PMLR, 2018.\n\n[13] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,\n“Deterministic policy gradient algorithms,” in _Proceedings of the 31st_\n_International Conference on Machine Learning (ICML 2014)_, (Bejing,\nChina), pp. 387–395, PMLR, 2014.\n\n[14] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” in _4th International Conference on Learning Representations_\n_(ICLR 2016)_, (San Juan, Puerto Rico), 2016.\n\n[15] H. van Hasselt, “Double q-learning,” in _23rd Advances in Neural_\n_Information Processing Systems (NeurIPS 2010)_, (Vancouver, British\nColumbia, Canada), pp. 2613–2621, 2010.\n\n[16] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” in _Proceedings of the 35th International Conference on Ma-_\n_chine Learning (ICML 2018)_, (Stockholmsm¨assan, Stockholm Sweden),\npp. 1861–1870, PMLR, 2018.\n\n[17] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan,\nV. Kumar, H. Zhu, A. Gupta, P. Abbeel, _et al._, “Soft actor-critic\nalgorithms and applications,” _arXiv preprint arXiv:1812.05905_, 2018.\n\n[18] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective on reinforcement learning,” in _Proceedings of the 34th International_\n_Conference on Machine Learning, (ICML 2017)_, (Sydney, NSW, Australia), pp. 449–458, 2017.\n\n[19] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos, “Distributional reinforcement learning with quantile regression,” in _Proceedings_\n_of the 32nd Conference on Artificial Intelligence, (AAAI 2018)_, (New\nOrleans, Louisiana, USA), pp. 2892–2901, 2018.\n\n[20] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, “Implicit quantile\nnetworks for distributional reinforcement learning,” in _Proceedings of_\n_the 35th International Conference on Machine Learning (ICML 2018)_,\n(Stockholmsm¨assan, Stockholm Sweden), pp. 1096–1105, PMLR, 2018.\n\n[21] M. Rowland, M. Bellemare, W. Dabney, R. Munos, and Y. W. Teh, “An\nanalysis of categorical distributional reinforcement learning,” in _Inter-_\n_national Conference on Artificial Intelligence and Statistics, (AISTATS_\n_2018)_, (Playa Blanca, Lanzarote, Canary Islands, Spain), pp. 29–37,\nPMLR, 2018.\n\n[22] C. Lyle, M. G. Bellemare, and P. S. Castro, “A comparative analysis of\nexpected and distributional reinforcement learning,” in _Proceedings of_\n_the 33rd Conference on Artificial Intelligence (AAAI 2019)_, (Honolulu,\nHawaii,USA), pp. 4504–4511, 2019.\n\n[23] G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan,\nD. TB, A. Muldal, N. Heess, and T. P. Lillicrap, “Distributed distributional deterministic policy gradients,” in _6th International Conference_\n_on Learning Representations, (ICLR 2018)_, (Vancouver, BC, Canada),\n2018.\n\n\n\n\n[24] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz,\n“Trust region policy optimization,” in _Proceedings of the 32nd Interna-_\n_tional Conference on Machine Learning, (ICML 2015)_, (Lille, France),\npp. 1889–1897, 2015.\n\n[25] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” _arXiv preprint arXiv:1707.06347_,\n2017.\n\n[26] N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez,\nZ. Wang, S. Eslami, M. Riedmiller, _et al._, “Emergence of locomotion\nbehaviours in rich environments,” _arXiv preprint arXiv:1707.02286_,\n2017.\n\n[27] B. O’Donoghue, R. Munos, K. Kavukcuoglu, and V. Mnih, “Combining\npolicy gradient and q-learning,” in _4th International Conference on_\n_Learning Representations (ICLR 2016)_, (San Juan, Puerto Rico), 2016.\n\n[28] J. Schulman, X. Chen, and P. Abbeel, “Equivalence between policy\ngradients and soft q-learning,” _arXiv preprint arXiv:1704.06440_, 2017.\n\n[29] O. Nachum, M. Norouzi, K. Xu, and D. Schuurmans, “Bridging the\ngap between value and policy based reinforcement learning,” in _30th_\n_Advances in Neural Information Processing Systems (NeurIPS 2017)_,\n(Long Beach, CA, USA), pp. 2775–2785, 2017.\n\n[30] B. Sallans and G. E. Hinton, “Reinforcement learning with factored\nstates and actions,” _Journal of Machine Learning Research_, vol. 5, no. 8,\npp. 1063–1088, 2004.\n\n[31] R. Fox, A. Pakman, and N. Tishby, “Taming the noise in reinforcement\nlearning via soft updates,” in _Proceedings of the 32nd Conference on_\n_Uncertainty in Artificial Intelligence (UAI 2016)_, (Arlington, Virginia,\nUnited States), pp. 202–211, AUAI Press, 2016.\n\n[32] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement learning\nwith deep energy-based policies,” in _Proceedings of the 34th Interna-_\n_tional Conference on Machine Learning, (ICML 2017)_, (Sydney, NSW,\nAustralia), pp. 1352–1361, 2017.\n\n[33] W. Dabney, Z. Kurth-Nelson, N. Uchida, C. K. Starkweather, D. Hassabis, R. Munos, and M. Botvinick, “A distributional code for value in\ndopamine-based reinforcement learning,” _Nature_, pp. 1–5, 2020.\n\n[34] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” _arXiv_\n_preprint arXiv:1312.6114_, 2013.\n\n[35] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward,\nY. Doron, V. Firoiu, T. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu,\n“IMPALA: Scalable distributed deep-RL with importance weighted\nactor-learner architectures,” in _Proceedings of the 35th International_\n_Conference on Machine Learning (ICML 2018)_, (Stockholmsm¨assan,\nStockholm Sweden), pp. 1407–1416, PMLR, 2018.\n\n[36] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel,\nH. Van Hasselt, and D. Silver, “Distributed prioritized experience\nreplay,” _arXiv preprint arXiv:1803.00933_, 2018.\n\n[37] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for\nmodel-based control,” in _2012 IEEE/RSJ International Conference on_\n_Intelligent Robots and Systems, (IROS 2012)_, (Vilamoura, Algarve,\nPortugal), pp. 5026–5033, IEEE, 2012.\n\n[38] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, “Openai gym,” _arXiv preprint_\n_arXiv:1606.01540_, 2016.\n\n[39] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” _arXiv_\n_preprint arXiv:1606.08415_, 2016.\n\n[40] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin _3rd International Conference on Learning Representations, (ICLR_\n_2015)_, (San Diego, CA, USA), 2015.\n\n\n**Jingliang Duan** received the B.S. degree from the\nCollege of Automotive Engineering, Jilin University,\nChangchun, China, in 2015. He studied as a visiting\nstudent researcher in Department of Mechanical Engineering, University of California, Berkeley, USA,\nin 2019. He received his Ph.D. degree in the School\nof Vehicle and Mobility, Tsinghua University, Beijing, China, in 2021. His research interests include\ndecision and control of autonomous vehicle, reinforcement learning and adaptive dynamic programming, and driver behaviour analysis.\n\n\n\n15\n\n\n**Yang Guan** received the B.S. degree from school\nof mechanical engineering, Beijing institute of technology, Beijing, China, in 2017. He is pursuing his\nPh.D. degree in the School of Vehicle and Mobility,\nTsinghua University, Beijing, China. His research\ninterests include decision-making of autonomous\nvehicle, and reinforcement learning.\n\n\n**Shengbo Eben Li** (SM’16) received the M.S. and\nPh.D. degrees from Tsinghua University in 2006 and\n2009. He worked at Stanford University, University\nof Michigan, and University of California, Berkeley.\nHe is currently a tenured professor at Tsinghua University. His active research interests include intelligent vehicles and driver assistance, reinforcement\nlearning and distributed control, optimal control and\nestimation, etc.\nHe is the author of over 100 journal/conference\npapers, and the co-inventor of over 20 Chinese\npatents. He was the recipient of Best Paper Award in 2014 IEEE ITS\nSymposium, Best Paper Award in 14th ITS Asia Pacific Forum, National\nAward for Technological Invention in China (2013), Excellent Young Scholar\nof NSF China (2016), Young Professorship of Changjiang Scholar Program\n(2016). He is now the IEEE senior member and serves as associated editor\nof IEEE ITSM and IEEE Trans. ITS, etc.\n\n\n**Yangang Ren** received the B.S. degree from the\nDepartment of Automotive Engineering, Tsinghua\nUniversity, Beijing, China, in 2018. He is currently\npursuing his Ph.D. degree in the School of Vehicle\nand Mobility, Tsinghua University, Beijing, China.\nHis research interests include decision and control\nof autonomous driving, reinforcement learning, and\nadversarial learning.\n\n\n**Qi Sun** received his Ph.D. degree in Automotive\nEngineering from Ecole Centrale de Lille, France,\nin 2017. He did scientific research and completed\nhis Ph.D. dissertation in CRIStAL Research Center\n\nat Ecole Centrale de Lille, France, between 2013\nand 2016. He is currently a Postdoctor at the State\nKey Laboratory of Automotive Safety and Energy\nand at the School of Vehicle and Mobility, Tsinghua\nUniversity, Beijing, China. His active research interests include intelligent vehicles, automatic driving\ntechnology, distributed control and optimal control.\n\n\n**Bo Cheng** received the B.S. and M.S. degrees in\nautomotive engineering from Tsinghua University,\nBeijing, China, in 1985 and 1988, respectively, and\nthe Ph.D. degree in mechanical engineering from\nthe University of Tokyo, Tokyo, Japan, in 1998.\nHe is currently a Professor with School of Vehicle and Mobility, Tsinghua University, and the\nDean of Tsinghua University–Suzhou Automotive\nResearch Institute. He is the author of more than 100\npeer-reviewed journal/conference papers and the coinventor of 40 patents. His active research interests\ninclude autonomous vehicles, driver-assistance systems, active safety, and\nvehicular ergonomics, among others. Dr. Cheng is also the Chairman of the\nAcademic Board of SAE-Beijing, a member of the Council of the Chinese\nErgonomics Society, and a Committee Member of National 863 Plan, among\nothers.\n\n\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-14-0.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-14-1.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-14-2.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-14-3.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-14-4.png)\n\n![](output/images/23a061f719c6c6f1f8c0b5157b5c5145ca2ef8b7.pdf-14-5.png)\n",
    "ranking": {
      "relevance_score": 0.7427345129054699,
      "citation_score": 0.9,
      "recency_score": 0.3593879551606573,
      "final_score": 0.7358529545498946
    },
    "citation_key": "Duan2020DistributionalSA",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "d4e0d8645fe6972c1974f01300f7a0ffa8d85fff",
    "title": "Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities",
    "published": "2024-01-30",
    "authors": [
      "Charles Retzlaff",
      "Srijita Das",
      "Christabel Wayllace",
      "Payam Mousavi",
      "Mohammad Afshari",
      "Tianpei Yang",
      "Anna Saranti",
      "Alessa Angerschmid",
      "Matthew E. Taylor",
      "Andreas Holzinger"
    ],
    "summary": "Artificial intelligence (AI) and especially reinforcement learning (RL) have the potential to enable agents to learn and perform tasks autonomously with superhuman performance. However, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even when an agent eventually performs its task autonomously. \nIn cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous.\nThe application of Reinforcement Learning from Human Feedback (RLHF) in systems such as ChatGPT demonstrates the effectiveness of optimizing for user experience and integrating their feedback into the training loop. In HITL RL, human input is integrated during the agent’s learning process, allowing iterative updates and fine-tuning based on human feedback, thus enhancing the agent’s performance. Since the human is an essential part of this process, we argue that human-centric approaches are the key to successful RL, a fact that has not been adequately considered in the existing literature. This paper aims to inform readers about current explainability methods in HITL RL. It also shows how the application of explainable AI (xAI) and specific improvements to existing explainability approaches can enable a better human-agent interaction in HITL RL for all types of users, whether for lay people, domain experts, or machine learning specialists.\nAccounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating HITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and (4) Agent Deployment. We highlight human involvement, explanation requirements, new challenges, and goals for each phase.\nWe furthermore identify low-risk, high-return opportunities for explainability research in HITL RL and present long-term research goals to advance the field. Finally, we propose a vision of human-robot collaboration that allows both parties to reach their full potential and cooperate effectively.",
    "pdf_url": "https://jair.org/index.php/jair/article/download/15348/27006",
    "doi": "10.1613/jair.1.15348",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "Journal of Artificial Intelligence Research",
    "citation_count": 89,
    "bibtex": "@Article{Retzlaff2024HumanintheLoopRL,\n author = {Charles Retzlaff and Srijita Das and Christabel Wayllace and Payam Mousavi and Mohammad Afshari and Tianpei Yang and Anna Saranti and Alessa Angerschmid and Matthew E. Taylor and Andreas Holzinger},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {359-415},\n title = {Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities},\n volume = {79},\n year = {2024}\n}\n",
    "markdown_text": "Journal of Artificial Intelligence Research 79 (2024) 359-415 Submitted 08/2023; published 01/2024\n\n### **Human-in-the-Loop Reinforcement Learning: A Survey and** **Position on Requirements, Challenges, and Opportunities**\n\n\n**Carl Orge Retzlaff** carl.retzlaff@human-centered.ai\n_Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria_\n\n\n**Srijita Das** srijita1@ualberta.ca\n_Department of Computing Science, University of Alberta, Canada_\n\n\n**Christabel Wayllace** cwayllac@nmsu.edu\n_Department of Computer Science, New Mexico State University, USA_\n\n\n**Payam Mousavi** payam.mousavi@amii.ca\n_Alberta Machine Intelligence Institute (Amii), Canada_\n\n\n**Mohammad Afshari** mafshari@ualberta.ca\n\n_Department of Computing Science, University of Alberta, Canada_\n\n\n**Tianpei Yang** tianpei.yang@ualberta.ca\n_Department of Computing Science, University of Alberta, Canada_\n\n\n**Anna Saranti** anna.saranti@human-centered.ai\n\n_Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria_\n\n\n**Alessa Angerschmid** alessa.angerschmid@human-centered.ai\n_Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria_\n\n\n**Matthew E. Taylor** matthew.e.taylor@ualberta.ca\n_Department of Computing Science, University of Alberta, Canada &_\n_Alberta Machine Intelligence Institute (Amii), Canada &_\n_AI Redefined, Canada_\n\n\n**Andreas Holzinger** andreas.holzinger@human-centered.ai\n_Human-Centered AI Lab, University of Natural Resources and Life Sciences Vienna, Austria &_\n_xAI Lab, Alberta Machine Intelligence Institute, University of Alberta, Canada_\n\n\n©2024 The Authors. Published by AI Access Foundation under Creative Commons Attribution License CC BY 4.0.\n\n\n\n\nRetzlaff et al.\n\n\n**Abstract**\n\n\nArtificial intelligence (AI) and especially reinforcement learning (RL) have the potential\nto enable agents to learn and perform tasks autonomously with superhuman performance.\nHowever, we consider RL as fundamentally a Human-in-the-Loop (HITL) paradigm, even\nwhen an agent eventually performs its task autonomously.\nIn cases where the reward function is challenging or impossible to define, HITL approaches are considered particularly advantageous.\nThe application of Reinforcement Learning from Human Feedback (RLHF) in systems\nsuch as ChatGPT demonstrates the effectiveness of optimizing for user experience and\nintegrating their feedback into the training loop. In HITL RL, human input is integrated\nduring the agent’s learning process, allowing iterative updates and fine-tuning based on\nhuman feedback, thus enhancing the agent’s performance. Since the human is an essential\npart of this process, we argue that human-centric approaches are the key to successful RL,\na fact that has not been adequately considered in the existing literature. This paper aims\nto inform readers about current explainability methods in HITL RL. It also shows how the\napplication of explainable AI (xAI) and specific improvements to existing explainability\napproaches can enable a better human-agent interaction in HITL RL for all types of users,\nwhether for lay people, domain experts, or machine learning specialists.\nAccounting for the workflow in HITL RL and based on software and machine learning methodologies, this article identifies four phases for human involvement for creating\nHITL RL systems: (1) Agent Development, (2) Agent Learning, (3) Agent Evaluation, and\n(4) Agent Deployment. We highlight human involvement, explanation requirements, new\nchallenges, and goals for each phase.\nWe furthermore identify low-risk, high-return opportunities for explainability research\nin HITL RL and present long-term research goals to advance the field. Finally, we propose\na vision of human-robot collaboration that allows both parties to reach their full potential\nand cooperate effectively.\n\n\n**1. Introduction**\n\n\nReinforcement learning (Sutton & Barto, 2018) (RL) is a general framework in which an\nagent can autonomously learn to take actions to best maximize the discounted sum of future\nrewards, allowing agents to learn to outperform humans, sometimes generating novel and\nunanticipated strategies. RL agents have had many impressive successes in board games,\nvideo games, robotics, natural language processing, and other applications (Li, 2017). RL\nis also increasingly finding its way into the industry, with it being applied in some of the\nlargest companies in the world, such as in recommender systems in Netflix and Spotify\n(Akanksha et al., 2021), for video optimization at Meta (Mao et al., 2020), or in robotic\nautomation at Covariant (Liu et al., 2022).\nThe development and impact of models like ChatGPT and the emergence of reinforcement learning from human feedback (RLHF) exemplify the remarkable success achieved\nby combining reinforcement learning with Human-in-the-Loop (HITL) approaches. These\nmodels have demonstrated the ability to generate high-quality human-like responses and\nhave greatly benefited from human feedback during the training process. Using RLHF,\nthese models are able to learn from human guidance and iterate through multiple rounds of\nimprovement, resulting in impressive performance gains and enhanced capabilities in natural language understanding and generation tasks (Aiyappa et al., 2023). This combination\nof reinforcement learning and Human-in-the-Loop interaction has proven to be a power\n\n360\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nful paradigm for training AI models that can surpass human-level performance in specific\ndomains while incorporating human values and expertise into the learning process (Choi\net al., 2023).\nAlthough the strategies and approaches learned by AI and specifically RL systems are\neffective in solving many specified problems, they often prove to be substantially different\nfrom the causal approaches a human would take (Lake et al., 2017) and can lack robustness and generalization (Holzinger & M¨uller, 2021). As model complexity increases, there\nis an increased risk of model bias due to the inability to sanitize large amounts of training data, resulting in undesired model behaviour. Data drift further amplifies this issue,\ndecreasing the reproducibility of possibly important decisions (Baniecki et al., 2022). Especially in high-stakes situations where failures can cause direct harm to humans, the safety\nand responsibility of AI systems are essential, not only due to regulatory but also ethical\nconcerns (Baniecki et al., 2022; Holzinger et al., 2020).\nExplainability approaches play a major role in overcoming these problems by ensuring\nthe safety and responsibility of such systems and, consequently, generating acceptance for\nits application in fields like medicine, finance, and defense (Baniecki et al., 2022; Heuillet\net al., 2021). In addition to this, explainability can also help with the practical deployment\nprocess by allowing programmers to discover and fix bugs in the development process of\ncomplex models, which can speed up implementation (Heuillet et al., 2021).\nLearning from human feedback is one of the key techniques leading to the success of\nChatGPT as one of the first Large Language Models (LLM) to gain mass adoption. However, this process also resulted in data contamination, casting doubts about its robustness\nin different domains (Aiyappa et al., 2023). We argue that explainability forms the basis\nfor further improvement of this interactive process, as the underlying algorithms and their\ndecisions must be understood by a variety of different audiences with different goals to allow\nhumans to understand and trust the behavior of the agent (Heuillet et al., 2021).\nEven in cases with a low degree of human-agent interaction, such as abstract software\ndevelopment or industrial applications, HITL can be beneficial. In these applications, humans can, for example, improve agent robustness by monitoring the agent’s performance\nand fixing potential errors or biases in its decision-making process and by providing the\nnecessary input to adapt the agent’s behavior to accommodate changes in the environment\nor new requirements (Hussein et al., 2017). A well-known example of incorporating human\nfeedback is ChatGPT, one of the most impactful RL applications of 2022. ChatGPT learns\na reward model from human feedback and then optimizes the policy against this reward\nmodel, showing how incorporating human feedback can outperform traditional approaches\nbased on supervised learning alone (Stiennon et al., 2020).\nWe claim that the current framing of RL overlooks the significant human input and\nbiases encoded in the RL problems and argue that:\n\n\n1. Reinforcement learning is fundamentally a Human-in-the-Loop paradigm.\n\n\n2. Explainability is critical for the success of real-world RL applications.\n\n\nFirstly, we argue that RL is fundamentally a HITL paradigm and identify four phases\nwhere human involvement is critical to the goal of deploying and using RL agents: Agent\nDevelopment, Agent Learning, Agent Evaluation and Agent Deployment (see Subsection\n\n\n361\n\n\n\n\nRetzlaff et al.\n\n\n1.1). We emphasize that these phases are sequential but cyclical, meaning that individual\nphases can be repeated throughout the overall deployment process.\nSecondly, this article serves as a position paper. We argue that ignoring humans and\ntreating RL as a fundamentally autonomous learning paradigm is short-sighted — we highlight where and how explainability can play a critical role in those four phases of humanagent collaboration.\nThirdly, this article is a survey of explainability in RL. We teach readers about current\nexplainability methods in HITL RL, describe how they can enable a better human-agent\ninteraction in HITL RL applications, and present long-term research goals to advance the\nfield.\nWhile we emphasize that we consider the HITL paradigm critical for ML as a whole, in\nthis paper, we focus on human-agent interaction in an RL setting. We argue that the HITL\nparadigm is particularly applicable to RL because it allows for integrating human input and\noversight into the learning process, which helps to ensure controlled agent behavior (Lee\net al., 2021).\nWe want to clarify that HITL applications in RL also cover topics such as bias, fairness,\nand personalization. As these issues are complex and require a more in-depth and specialized\nanalysis, we refer to the related work of others, such as the European Parliament (2020),\nMehrabi et al. (2021), and Arrieta et al. (2019). Our work focuses on explainable RL and\ncontributes to the field of HITL by shedding light on why explainable RL is important and\nshowing how we can ensure that these systems are transparent, trustworthy, and provide a\nhuman-understandable decision-making processes.\n\n\n**1.1 Four Phases of HITL RL Deployment**\n\n\nThe following steps contain the identified, distinct phases for RL deployment and how\nhumans are involved in each. Figure 1 gives an overview of the sequence of phases for agent\ndeployment. We argue that explainability is a critical and underdeveloped technology in\neach of these four phases.\n_1. Initial Agent Development:_ An ML specialist lays the technical groundwork of the\nplanned RL model. The team defines the problem to be solved, defines the agent’s environment, and makes decisions about hyperparameters. Explainability helps to show how those\ndecisions influence the agent’s learning process.\n_2. Agent Learning:_ The model is trained in interactively, with the human expert providing feedback and guidance to the agent. The expert can also bias the learning process\nor disallow certain actions to help the model learn faster. Explainability is used during\ntraining to show the current policy and the impact of the human expert’s guidance on the\nmodel.\n\n_3. Agent Evaluation:_ In the evaluation phase, the model is tested to see if it is ready for\ndeployment. Specifically, domain experts must decide if the model is ready for deployment,\nif more training is needed, or if the problem definition needs to be changed in this phase.\nHere, explainability can help with an in-depth inspection of learned policies and emerging\nbehavior.\n\n_4._ _Agent Deployment:_ The agent is deployed in a working environment and needs\nexplainability to be safe, understandable, and reliable. It also has to provide fluent inter\n\n362\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nactions to be trustworthy and effective at its intended task. In this phase, the developers\ndetermine if the agent should continue learning, have a fixed policy, or retrain if there is\na change in the environment. The customer and the end-user decide where and how the\nagent should be used in the real world. Explainability can help users understand the final\npolicy, improve trust, evaluate safety, and understand the stability of the policy.\n\n\nFigure 1: Overview of the sequence of the four phases for deployment. The ideal path\nfrom the initial formulation is shown with the bold arrows and goes over initial agent\ndevelopment, agent learning, and agent evaluation to agent deployment. The dotted arrows\nshow the possible paths for revising different aspects of the agent and are centered around\nthe agent evaluation phase for assessing the model flaws.\n\n\nWe base these phases on software and ML models and adapt them to the nature of a\nHITL RL workflow. The general life cycle model for software development (Ragunath et al.,\n2010) includes the phases of requirements, design, implementation, and testing. Similarly,\nNascimento et al. (2019) use four stages of development, i.e., understanding the problem,\ndata handling, model building, and model monitoring. Since our focus is on the deployment of models rather than the design aspect, we omit the requirements and data handling\nstages, which leaves us with three phases: implementation or model building, testing, and\nmonitoring. We add a training or learning stage between the implementation and testing\nstage to represent the ML nature of HITL RL applications based on the phases proposed\nby Amershi et al. (2019) for an ML workflow (feature engineering, model training, model\nevaluation, model deployment, and model monitoring). We merge the deployment and\nmonitoring phases into a single deployment phase to simplify the model and reflect the similarities between both stages. This results in the proposed four steps of agent development,\nagent learning, agent evaluation, and deployment. We also allow reiterations of the agent\ndevelopment phase, as proposed by Amershi et al. (2019), to reflect the cyclic nature of the\nworkflow.\n\n\n363\n\n\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-4-0.png)\n\n\nRetzlaff et al.\n\n\n**1.2 Overview and Goals**\n\n\nWith this paper, we want to shift the discussion about RL to embrace human interaction and\ncooperation. Furthermore, we provide an entry point into this exciting area of contemporary\nresearch at the intersection of explainability in RL, with the goal of also giving non-experts\na starting point for HITL RL research. Throughout the paper, we focus on human-agent\ninteraction [1] and, therefore, center our research on topics concerning embodied intelligence\n(i.e. physical entities such as robots controlled by AI systems). Where appropriate, we\nalso refer to and discuss topics about the superset of human-agent interaction, including\nsoftware-only applications. We recognize that the field of HITL RL is very young and, in\nsome sense, has significant room to improve. We highlight that not all techniques we list in\nthis paper are ready to use in production but rather aim to steer the discussion and research\non approaches we deem most promising.\nThe paper is structured as follows. After the introduction and motivation, Section 2 is an\noverview of background work for explainability and interactive learning. In the background,\nwe review the fundamental and current challenges for embodied intelligence. After the\nbackground, we explore the four phases for the deployment of HITL RL systems and analyze\nwhere to apply explainability in HITL RL:\n\n\n1. Agent Development (Section 3)\n\n\n2. Agent Learning (Section 4)\n\n\n3. Agent Evaluation (Section 5)\n\n\n4. Agent Deployment (Section 6)\n\n\nEach of the phases has specific requirements for the success of human participation.\nFurthermore, we explore different approaches and discuss challenges and possible directions\nfor further research. In order to better illustrate the recommendations we have for these\n\nfour phases, we added the use-case of robot operation in forestry, which we will discuss\nthroughout the paper.\nWe choose forestry as our use-case example because of the various possibilities for applying HITL this field provides. Forests are furthermore of great economic value not only\nbecause they provide renewable raw materials but also for their contribution to CO2 sequestration and, with that, the fight against climate change. The use of robots in forestry,\ntherefore, has enormous economic significance (Holzinger et al., 2022b; Holzinger et al.,\n2022d). Consequently, robot operation in forestry is a use-case that receives more and\nmore attention from the research community (Mikhaylov & Lositskii, 2018; Mowshowitz\net al., 2018; Zhang et al., 2019a) and lends itself to the application of RL, as it involves\nmaking decisions over time in uncertain and dynamic environments. RL is well-suited to\nthese types of problems because agents can learn from experience and adapt to changing\nconditions. Successful autonomous navigation in a forest then lays the groundwork for more\ndemanding tasks such as weeding, species identification, and other forest management applications. While robot operation has been thoroughly explored in simulations, it has seen\nfew real-world applications due to robustness and security issues (Surmann et al., 2020).\n\n\n1. In the case of embodied intelligence approaches, we refer to this also as human-robot interaction.\n\n\n364\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n## **1.**\n\n\nFigure 2: Use-case of robot operations in the forest with navigation to a given target. We\nhighlight three important challenges for a successful application: (1) completing a primary\ntask such as identifying tree species, (2) overcoming difficult terrain or replanning the route,\n(3) interacting with the operator or bystander.\n\n\nThis use-case can therefore strongly profit from robustness and explainability benefits of\nHITL approaches.\nIn Figure 2, we highlight three important challenges in robot operation in forestry: (1)\ncompleting a primary task such as identifying tree species; (2) overcoming difficult terrain;\nand (3) interacting with the operator or bystander while navigating to a designated target.\nIn the beginning of each phase, we list how these challenges apply to this phase. In the\ndiscussion section of each phase, we propose specific methods suited for this phase and\nhow they could be used to overcome the discussed challenges and enhance the agent’s\nperformance, robustness, and trustworthiness.\nSection 7 discusses the general challenges observed and how future solutions can be\nshaped to overcome them. Finally, Section 8 outlines the general problems and goals of the\nfield and proposes future work.\n\n\n365\n\n\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-0.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-1.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-2.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-3.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-4.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-5.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-6.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-7.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-8.png)\n\n![](output/images/d4e0d8645fe6972c1974f01300f7a0ffa8d85fff.pdf-6-9.png)\n\n\nRetzlaff et al.\n\n|Phase|Human<br>Involvement|Explanation<br>Requirements|Explainability|Goals|Metrics|\n|---|---|---|---|---|---|\n|Development|_•_Defne problem<br>_•_Construct<br>state<br>space, action space,<br>and reward function<br>Design model|_•_Comparable<br>to<br>other<br>model<br>versions<br>_•_Fast and simple<br>explanations<br>for<br>shallow<br>inspec-<br>tion<br>_•_Complex and ex-<br>haustive explana-<br>tions for deep in-<br>spection|**Focus:**<br>Interpretable<br>models<br>such<br>as<br>decision<br>trees,<br>causal models,<br>compositional language<br>**Explanations**:<br>Counterfactuals,<br>policy<br>querying, decision rules<br>**Users:**<br>RL experts|_•_Create<br>thorough<br>and<br>comparable<br>model<br>sum-<br>maries<br>_•_Use compositional, rep-<br>resentational language<br>_•_Further integrate causal<br>learning<br>into<br>HITL<br>ap-<br>proaches|**Fidelity**<br>_•_Correctness<br>(correlation<br>between<br>out-<br>put on controlled synthetic<br>data or single feature dele-<br>tion)<br>_•_Explainable Model<br>Discrepancy<br>(error between model pre-<br>dictions)|\n|Agent Learning|_•_Give<br>evaluative<br>feedback<br>_•_Deliver<br>action-<br>advice<br>_•_Select action pref-<br>erences<br>_•_Provide<br>demon-<br>strations|_•_Understandable<br>by<br>domain<br>ex-<br>perts<br>_•_Fluent<br>interac-<br>tions|**Focus:**<br>HITL<br>approaches<br>such<br>as<br>human<br>preferences<br>querying,<br>uncertainty highlighting<br>**Explanations:**<br>Counterfactuals, textual<br>explanation in user lan-<br>guage,<br>saliency maps<br>**Users:**<br>Domain experts,<br>RL experts|_•_Make<br>use<br>of<br>imitation<br>learning and preference-<br>based learning as comple-<br>mentary approaches<br>_•_Adapt<br>xAI<br>approaches<br>to HITL context<br>_•_Apply<br>Human-as-<br>Teacher approach<br>_•_Find hybrid methods of<br>diferent kinds for human<br>interaction|**Fidelity**<br>**Relevancy**<br>_•_Human Feedback<br>(evaluate survey with<br>users on relevance to task)<br>**Performance**<br>_•_Time to Explanation<br>(in milliseconds)|\n|Evaluation|_•_Understand<br>and<br>evaluate<br>learned<br>policies<br>on<br>micro<br>and macro-level<br>_•_Test model bound-<br>aries and safety<br>_•_Decide<br>whether<br>model is ready for<br>deployment|_•_Summarise<br>learned behavior<br>_•_Scalable to large<br>models<br>_•_Comparable<br>to<br>untrained models<br>_•_Understandable<br>by<br>domain<br>ex-<br>perts|**Focus:**<br>Safety<br>evaluation<br>by<br>modeling<br>uncertainty,<br>using<br>shield-based<br>de-<br>fenses<br>**Explanations**:<br>Policy<br>summarization<br>with<br>natural<br>language,<br>rules<br>or<br>code,<br>graph-<br>based explanations<br>**Users:**<br>Domain Experts,<br>RL Experts|_•_Ensure<br>understandabil-<br>ity for domain expert<br>_•_Enable<br>thorough<br>and<br>comparable explanations<br>that scale with model size<br>and complexity<br>_•_Further<br>develop<br>dash-<br>boards for policy inspec-<br>tion from diferent view-<br>points|**Fidelity**<br>**Relevancy**<br>**Cognitive Load**<br>_•_Compactness<br>(absolute size of explana-<br>tion in number of features,<br>path length, percent reduc-<br>tion to complete data)<br>_•_Redundancy<br>(overlap between parts of<br>explanations)|\n|Deployment|_•_Deploy agent<br>_•_Interact<br>with<br>agents<br>_•_Defne<br>agents’<br>real-world<br>appli-<br>cation<br>goal<br>and<br>context|_•_Fast, clear, and<br>concise to reduce<br>cognitive load<br>_•_Understandable<br>by end-users<br>_•_Non-intrusive to<br>prevent detrimen-<br>tal efects on user<br>performance|**Focus:**<br>Building<br>User<br>Trust<br>with intent and uncer-<br>tainty<br>communication,<br>allowing<br>error<br>correc-<br>tions<br>**Explanations:**<br>Saliency maps, dendro-<br>grams,<br>bounding-boxes,<br>textual explanations, vi-<br>sual and auditory indi-<br>cators<br>**Users:**<br>End-users|_•_Develop and apply new<br>approaches beyond image<br>and driving-based expla-<br>nations<br>_•_Use simple and fast ex-<br>planations<br>_•_Implement cohesive er-<br>ror and uncertainty han-<br>dling<br>_•_Communicate agent in-<br>tent via diferent modal-<br>ities|**Fidelity**<br>**Relevancy**<br>**Cognitive Load**<br>**Performance**|\n\n\n\nTable 1: Overview of the different explanations contexts in the four different phases. _Ex-_\n_planation Requirements_ enumerates desirable properties of explanations at this phase, and\n_Human Involvement_ describes how the human is involved in it. The _Explainability_ column\nlists (1) example techniques currently used, (2) directionality of explanations, i.e., agentto-human, human-to-agent, or both, and (3) the types of users interacting with the agent\nat this phase. The _Goals_ column describes targets that help achieve a comprehensive HITL\nRL experience, while the _Metrics_ column lists metrics as per Milani et al. (2022) we recommend focusing on in each phase and how to quantify them with approaches listed by\nNauta et al. (2022). See Section 3 for agent development, 4 for agent learning, 5 for agent\nevaluation, and 6 for agent deployment.\n\n\n366\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nTable 2: Comparing strengths and weaknesses of major xAI approaches with an example\nin the forestry robot use-case.\n\n|x nA iqI eTech-<br>u|Example|Strengths|Weaknesses|\n|---|---|---|---|\n|Counter-<br>factuals|If<br>the<br>robot<br>encounters<br>obstacles,<br>a<br>counterfactual explanation could be pro-<br>vided<br>to<br>show<br>how<br>the<br>robot<br>could<br>change its positioning or path to avoid<br>the obstacle.|_•_Provide direct insights into<br>how changing inputs afects<br>model<br>outputs,<br>facilitating<br>quick feedback from humans<br>(Karalus & Lindner, 2021).<br>_•_Easy to understand as hu-<br>mans may prefer contrastive<br>explanations (Miller, 2019a).|_•_Lower<br>coverage<br>for<br>complex<br>models<br>and<br>high-dimensional<br>data,<br>requiring<br>multiple queries to better understand<br>the overall model behavior (Keane et al.,<br>2021).|\n|Policy<br>Querying|User can query the policy of the naviga-<br>tion system to understand why the robot<br>chose a specifc path to reach a partic-<br>ular forest location, considering factors<br>like terrain conditions and efciency.|_•_Allows<br>users<br>to<br>interac-<br>tively<br>explore<br>the<br>model’s<br>decision-making process.<br>_•_Can<br>be<br>combined<br>with<br>counterfactuals<br>(Madumal<br>et al., 2020a).|_•_Challenging to implement, since the<br>model architecture needs to support the<br>extraction<br>and<br>inspection<br>of<br>human-<br>readable policies (Hayes & Shah, 2017).<br>_•_May<br>not<br>provide<br>a<br>comprehensive<br>understanding<br>of<br>the<br>entire<br>model’s<br>behavior.|\n|Policy<br>Summa-<br>rization|Provide a summarized explanation to<br>the<br>operator,<br>explaining<br>its<br>overall<br>strategy<br>for<br>reaching<br>the<br>target<br>lo-<br>cation<br>while<br>avoiding<br>obstacles,<br>en-<br>suring<br>safety,<br>and<br>optimizing<br>energy<br>consumption.|_•_Provides<br>an<br>overview<br>of<br>model behavior and the over-<br>all set of actions the agent<br>makes.<br>_•_Facilitates<br>the<br>identifca-<br>tion of potential biases or<br>unintended behavior.|_•_Can oversimplify behavior, leading to<br>an unfaithful representation of the real<br>policy.<br>_•_Does<br>not<br>explain<br>individual<br>actions<br>and why they were taken (Wells & Bed-<br>narz, 2021b).|\n|Decision<br>Rules|When<br>the<br>robot<br>needs<br>to<br>replan<br>its<br>route, decision rules can explain the cri-<br>teria (such as path length, time to tar-<br>get and energy expenditure) and corre-<br>sponding thresholds used to decide the<br>next action.|_•_Easy<br>to<br>interpret<br>and<br>understand.<br>_•_Can be used to build sim-<br>ple, transparent models that<br>mimic the original black-box<br>model.|_•_May not capture complex interactions<br>and dependencies present in the data,<br>especially for more complex models.<br>_•_These substitute models have limited<br>expressiveness compared to more com-<br>plex models like neural networks (Liu et<br>al., 2018).|\n|Textual<br>Expla-<br>nations<br>in<br>User<br>Language|Generate natural language explanations<br>to interact with the operator or by-<br>standers, providing context on its ac-<br>tions, intentions, and safety precautions<br>during navigation.|_•_Provides<br>explanations<br>in<br>a natural language format,<br>making them easily under-<br>standable<br>to<br>non-experts<br>(Xu et al., 2023).<br>_•_Enables<br>communication<br>of complex model behavior<br>without<br>requiring<br>technical<br>expertise (Ben-Younes et al.,<br>2022).|_•_Generating high-quality textual expla-<br>nations may require advanced language<br>models (Xu et al., 2023).<br>_•_May require combination with other<br>approaches (such as regional highlight-<br>ing) to increase understandability (Xu<br>et al., 2023).|\n|Saliency<br>Maps|Highlight areas in the robot’s visual per-<br>ception where important obstacles or<br>tree species are detected.|_•_Identify<br>important<br>input<br>features that infuence model<br>predictions.<br>_•_Relatively fast and compu-<br>tationally<br>efcient<br>for<br>fea-<br>ture attribution.|_•_May not capture global interactions<br>and<br>complex<br>relationships<br>between<br>features.<br>_•_Can be misleading and create false<br>trust in a model (Evans et al., 2022;<br>Glanois et al., 2021).|\n|Graph-<br>Based<br>Expla-<br>nations|Illustrate the relationships between dif-<br>ferent navigation paths and the corre-<br>sponding terrain conditions, helping the<br>operator visualize the decision-making<br>process.|_•_Represent<br>complex<br>rela-<br>tionships and dependencies<br>between<br>features<br>and<br>predictions.<br>_•_Provide a holistic view of<br>the model’s decision-making<br>process.|_•_Construction and visualization of com-<br>plex<br>graphs<br>can<br>be<br>challenging<br>and<br>time-consuming,<br>growing<br>worse<br>with<br>scale (Wells & Bednarz, 2021b).<br>_•_Interpretation of graph structures can<br>be<br>subjective<br>and<br>context-dependent.<br>Better suited for subject-matter experts<br>than end-users (Song et al., 2019).|\n\n\n\nContinued on next page\n\n\n367\n\n\n\n\nRetzlaff et al.\n\n|Dendro-<br>grams|G fr ao ficru ep ts tei rm idisi etl na tr t r iche aee s op nne c pi e rts ceb a ss se .bd o n nt h te hi er<br>c eh ia c ic s f, tl ip i g oh r o o t i<br>n i e|•Represent hierarchical<br>relationships between data<br>points or clusters.<br>• tiE nn sa hb il pe t ih nre v ai cns ou ma l pti hz aea cit tri o mrn nao --f<br>d a ot a c l u s t e s d e l<br>s a a<br>ner.|• qI un irt ee srpr de ot mat aio inn ka nn od lu en dd ge ers ata nn ddi sn hg ur le d-<br>w o<br>b lke .s t tb 0ee 0tc )uo .am b ein xped w nait th o nt sh e (Sr ea rp rp adro ila lc ah e es<br>l ai,e 2x l la io t<br>2<br>• KC uea ln ob ne ts &dce .n s gi kt ri adv umee tt oo npd ra,t a 0arp ler e )vdp .ceir ro yc e gs es s-<br>i (n rg, tc h o s ee n n Gu m ob nr ao yf c rl u ss mt 2e 2s 1l, hs at a dn ic fe<br>f u n c kti rn e, a n m o d u c -<br>f e n d in r o s f o n<br>a u a|\n|---|---|---|---|\n|Bounding-<br>Boxes|Highlight the regions of interest and<br>identifed objects, ensuring safe interac-<br>tion and communication.|_•_Localize important regions<br>within<br>images,<br>providing<br>more granular explanations<br>(Kashyap et al., 2020).<br>_•_Useful<br>for<br>object<br>detec-<br>tion and localization tasks in<br>computer vision due to fast<br>computation and being easy<br>to understand (Behl et al.,<br>2017).|_•_Selection of bounding-box size and lo-<br>cation can infuence the interpretation.<br>_•_Best applicable to image data and may<br>not generalize well to other data types.|\n|Visual and<br>Auditory<br>Indicators|Use visual indicators and auditory sig-<br>nals to alert bystanders or the opera-<br>tor of its presence and actions, ensur-<br>ing safety during navigation and inter-<br>actions in the forest environment.|_•_Enable low-level, real-time<br>feedback<br>and<br>interaction<br>with<br>the<br>model<br>in<br>multi-<br>modal environments.<br>_•_Improve<br>user<br>engagement<br>and<br>understanding<br>by<br>in-<br>corporating multiple sensory<br>inputs.<br>_•_Very<br>helpful<br>for<br>commu-<br>nicating<br>an<br>agent’s<br>intent<br>(Dragan, 2015).|_•_Designing efective and informative in-<br>dicators may require domain expertise<br>(Jain et al., 2021).<br>_•_Potential risk of information overload,<br>leading to decreased interpretability.<br>_•_Should be combined with other ap-<br>proaches<br>(textual<br>or<br>visual<br>explana-<br>tions) to communicate complex intents<br>(Ben-Younes et al., 2022).|\n\n\n\nTable 1 gives the reader a comprehensive overview of the main insights of our paper.\nIt highlights the requirements, challenges, and human context for the four phases discussed\nextensively in the following pages. The row corresponding to the agent deployment phase\nis also shown in each section to facilitate reading. Table 2 expands on this by providing\nexamples for each of the explainability approaches mentioned in Table 1 aimed at the\nforestry robot use-case. The table furthermore provides a concise overview of their respective\nstrengths and weaknesses, offering insights into their applicability, possible synergies with\nother xAI techniques, and the potential use in HITL RL.\nIn this paper, we argue that RL greatly benefits from being thought of as a humancentered process and that explainability is required to enable this HITL RL approach.\nWe highlight how current xAI methods can be used to facilitate such HITL approaches\nand identify research gaps for further explainability research, ultimately enabling a more\nproductive interaction of humans and RL agents.\n\n\n**2. Background**\n\n\nIn this section, we establish the technical background for our discussion of the four phases\nof the deployment of HITL RL agents. We detail how to provide insight into ML models\nwith the help of explainability approaches. We then explain how interactive learning allows\nintegrating the HITL into RL. Finally, we summarize current challenges for reinforcement\nlearning in general and, more specifically, in the HITL context.\n\n\n368\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\n**2.1 Explainability**\n\n\nExplainable artificial intelligence (xAI) is a framework for helping human users understand\nthe process and output of machine learning models. As ML models are deployed in a\ngrowing number of applications that affect human life (e.g., agriculture, health, smart home\nscenarios, etc.), the need for such xAI frameworks is ever more apparent. Moreover, xAI\napproaches are essential for many human-AI collaboration scenarios, where understanding\nand trusting model outputs are prerequisites for their use (Holzinger, 2021).\nExplainability has grown from the 1980s and 1990s with researchers aiming to extract\nrules from knowledge or rule-based systems and neural networks (Buchanan & Shortliffe,\n1984; Chandrasekaran et al., 1989; Tickle et al., 1998) towards a dedicated and expanding\nresearch community, as seen in the example of the DARPA initiative (Gunning & Aha,\n2019). xAI efforts have since led to several successful xAI methods (Holzinger et al., 2022c;\nZhou et al., 2021). However, we acknowledge that the terms “explainability” and “interpretability” are not used consistently and sometimes interchangeably in the literature\n(Miller, 2019a). Therefore, we choose to follow the recommendations of Holzinger et al.\n(2022c), which define explainability as the collection of methods highlighting the decisionrelevant components of machine representations and machine models. Interpretability, on\nthe other hand, is defined by Doran et al. (2017) as a system in which the user not only sees\nbut also understands how inputs are mathematically mapped to outputs. This definition of\ninterpretability implies model transparency and requires an understanding of the technical\ndetails of mapping but, as opposed to explainability, does not consider decision-relevant\naspects and external factors. This understanding also aligns with the notion of Glanois\net al. (2021), which defines interpretability as the methods that passively make the model\nunderstandable, whereas explainability actively generates explanations for a model. Therefore, we differentiate interpretability as methods that enable a mechanistic understanding of\nthe model, whereas explainability also encompasses active explanations of decision-relevant\nfactors.\n\nThe following examples show how xAI frameworks and human users can interact:\n\n\n1. Explaining the role of a data source in the final decision, for example, to identify\nwhich data samples were used for a specific action or decision. This is, for example,\nimportant for assigning credit to (and potentially compensating) the individuals who\nproduced the data (Zanzotto, 2019).\n\n\n2. Building trust in human users is especially important when safety is a concern. In\nAI applications in medicine, the human user needs a reliable explanation for the decision made by the AI agent. Therefore, transparency and accountability are essential\n(Schneeberger et al., 2020; Stoeger et al., 2021).\n\n\n3. Enabling humans to provide richer feedback through additional counterfactual examples (Del Ser et al., 2024). AI agents can use feedback in the form of explanations\nprovided by humans, leading to more accurate, robust, and transparent models (Karalus & Lindner, 2021; Puiutta & Veith, 2020).\n\n\nThese examples show the various applications of xAI frameworks. Especially in the\ncontext of human-machine cooperation, the cognitive ability of the human operator paired\n\n\n369\n\n\n\n\nRetzlaff et al.\n\n\nwith the computational power of a machine has the potential to handle complex tasks\n(Buchelt et al., 2024; Liang et al., 2017). Here, it is essential for the machine, as well as the\nhuman operator, to be able to react to the environment and for the operator to understand\nand interpret the actions of the machine correctly. Therefore, the underlying algorithm\nand its decisions must be understandable to different audiences with various goals (Heuillet\net al., 2021), which shows the importance of explainability in the context of human-machine\ncooperation.\nWe use the categorization presented by Glanois et al. (2021) to classify the interpretability approaches surveyed. We extend their classification to also include explainability approaches, which results in the following three categories:\n\n\n1. interpretable/explainable inputs of RL models\n\n\n2. interpretable/explainable transition and reward models for RL\n\n\n3. interpretable/explainable decision-making processes of RL\n\n\nThe first category focuses on the input to the RL model used to make decisions. It\nincludes not only the agent’s state but also other structural information, such as the problem\ndescriptions from human experts (Hasanbeig et al., 2021) and the relational (Battaglia et\nal., 2018; Mart´ınez et al., 2017) or hierarchical structure (Andreas et al., 2017; Lyu et al.,\n2019) of the problem. This context information helps to better understand the decisions\nmade by RL models.\nAn important building block for explaining model inputs is visualizing them as perceived by the model. This visualization is often combined with showing the relevance and\nimportance of a given decision, which helps to evaluate whether the model is focusing on\nthe right aspects of the input, but can also mislead the user if used incorrectly. See Evans\net al. (2022) for a further discussion of this set of problems. Saliency maps are one of\nthe most common examples for visualization approaches and work by highlighting important image regions. Liu et al. (2018) show an example where continuous “super-pixels”\nwith large feature influence are highlighted. Bach et al. (2015) developed the technique\nof “Layer-Wise Relevance Propagation” to iteratively change the model input to find the\nrelative importance of individual (image) parts or features.\nThe second category of explainable transition and reward models leverages understandable models of the task or environment, e.g., a transition model (Mart´ınez et al., 2016; Zhu\net al., 2020) or a preference model (Icarte et al., 2018; Icarte et al., 2019). Such models\nhelp explain both the RL agent’s reasoning about its decision-making and humans’ understanding of the decision-making process.\nThe third category is interpretable/explainable decision-making of RL agents. It consists of approaches to represent decision policies in an intuitively understandable manner.\nSome approaches learn such interpretable policies in the form of decision trees (Likmeta\net al., 2020; Silva et al., 2020; Topin et al., 2021), formulas (Hein et al., 2018, 2019), fuzzy\nrules (Akrour et al., 2019; Hein et al., 2017; Zhang et al., 2021), logic rules (Jiang & Luo,\n2019), or programs (Sun et al., 2019; Verma et al., 2019).\nGenerally, it is challenging to reliably assess the quality and efficacy of xAI solutions, as\nuser cost (that is, cognitive load and other user requirements) is often difficult to objectively\nmeasure (Bruneau et al., 2002). To better assess user cost, Milani et al. (2022) name four\n\n\n370\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nkey metrics for evaluating xAI solutions, which we will also use for evaluating different\nclasses of xAI solutions: _Fidelity_, the truthfulness of the explanation with respect to the\nmodel itself; _Performance_, the default metric used to evaluate the success of the AI solution\nto be explained; _Relevancy_, the relevance of the explanations provided to the task at hand;\nand _Cognitive load_, the mental effort required to understand the explanations provided. In\nthe following paragraphs, we present different approaches to how the metrics of Milani et al.\n(2022) could be quantified. A related paper is presented by Nauta et al. (2022), where the\nauthors conduct an exhaustive survey of quantitative measurements for different properties\nof xAI approaches.\nModel fidelity can be measured with correctness as a key element to ensure that the\nmodel performs correctly and produces accurate outputs (Nauta et al., 2022). Correctness\ncan be quantified by generating synthetic data and then testing the model on data known to\nbe correct to determine how well the model is able to replicate the correct output. Another\nway to measure fidelity is to test the model’s performance on data with single deletions of\nfeatures, which allows determining the correlation between change in output and importance\nscore of the feature. For explainable models, one can also compare the decisions made by\nthe model and the explanation, and quantify correctness with error measurements such as\nthe mean squared error (MSE) between the model’s predictions and the explanation. A\nlower MSE indicates that the model and explanation are in closer agreement (Nauta et al.,\n2022). Intuitively, the computational performance of explanations can be quantified by\nmeasuring (in milliseconds or seconds, if appropriate) how quickly an explanation can be\ngenerated.\nBoth the relevancy metric and cognitive load are difficult to quantify, as they are highly\ndependent on the user’s expertise and other environmental factors (Milani et al., 2022). We,\ntherefore, propose to rely on the qualitative measurement of relevancy by evaluating human\nfeedback for the given explanations as the most promising and comprehensive approach to\ndetermine whether the explanation is relevant to the human.\nThe cognitive load can also be evaluated by measuring the compactness of the explanation as surveyed by Nauta et al. (2022). A compact explanation is one that is small in size\n(i.e., bytes of information) or sparse, meaning it contains only the most important information. Compactness can be measured in different ways depending on the underlying model\nand modality, for example, with the number of features in the explanation, the path length\nin a decision tree, or the reduction in size compared to the complete data. Furthermore, the\nredundancy of explanations can be computed to evaluate and minimize the overlap between\nparts of the explanation.\nIn addition to quantifying explanations with these metrics, they can be classified as\nunidirectional or bidirectional. In the unidirectional case, the system simply provides an\nexplanation to the user. In the bidirectional case, the user can give feedback or additional\nquestions to the system, which can improve model accuracy and allow the system to generate\nupdated explanations (Smith-Renner et al., 2020). Although this provides a more in-depth\nand interactive explanation process, the interactive approach is still in its infancy, with\nchallenges in technical implementation and application scale that still need to be overcome\n(Smith-Renner et al., 2020; Sreedharan et al., 2022). Since we speak from the perspective\nof a position paper, we recommend using a bidirectional explanation in the stages from\n\n\n371\n\n\n\n\nRetzlaff et al.\n\n\nagent learning onward. However, implementing bidirectional explanations will not always\nbe possible or viable due to the added technical complexity.\nAfter the high-level categorization of explainability by Glanois et al. (2021), we focus in\nthe two following paragraphs on approaches that explain model decisions as parameterized\nby their learned policies. As the policies determine the decision-making of a model, they\nbelong to the “explainable decision-making” category of the categorization by Glanois et al.\n(2021). In policy summarization, the general aim is to make the underlying model and its\npolicy tangible. This can be achieved by codifying its decision process as rules, as seen in\nthe linear model U-trees by Liu et al. (2018) or by representing the learned model with\ngenerated code blocks (Verma et al., 2018).\nThe second approach to explain model decisions by learned policies is policy querying.\nIn policy querying, the decision process that leads to a given result is explained. This can\nbe general (“when do you do X”) or specific to a given action. An example of a specific\nexplanation is a natural language explanation for a classification in the ML space (Alonso\net al., 2018) or the generation of a summary of “when do you do X?” type questions in\nnatural language to explain the actions of an agent (Hayes & Shah, 2017).\nAs in the last subsection of explainability approaches, causal models provide a fundamentally different approach to explainability. Methods available within this framework\ngenerally fall into the categories of explainable transition and reward models and decisionmaking — both provide causal explanations for the task, environment, or the policies themselves.\nOne causal approach is graph neural networks (GNN) (Vu & Thai, 2020), which can\ngenerate explanations for a prediction via a probabilistic graphical model (PGM) that identifies crucial graph elements (e.g., nodes and edges) causally responsible for that prediction.\nAlong similar lines, Madumal et al. (2020b) encode causal models using action influence\ngraphs to generate explanations using causal chains. Adding these causal explanations\nyields better explanations as well as improved prediction performance compared with baseline explanation models.\nCausal imitation learning, on the other hand, allows one to learn a structural causal\nmodel (SCM) (Pearl et al., 2000) from policies performed by humans. This is the case even\nif the actual reward is not specified and the environment is not perceived as the same by the\nlearner and the human expert demonstrator. Dynamic SCMs are incorporated to formalize\nthe partially observable Markov decision process (POMDP) (Sutton & Barto, 2018) as perceived by the agent and to take into account human intervention and its implications. The\nso-called counterfactual agent does not blindly take the human’s advice and execute it, but\nrather compares it with other possible actions and decides correspondingly. In cases where\nthe reward and transition functions are the same, human feedback is beneficial, even if the\ninstructions are suboptimal. Counterfactual explanations are an especially powerful explanation approach since they leverage the fact that humans prefer contrastive explanations\n(see Miller (2019a) for a more detailed discussion).\n\n\n**2.2 Interactive Learning in Reinforcement Learning**\n\n\nA fundamental way of learning in nature is parents interactively teaching their offspring.\nSimilar learning dynamics exist between a teacher and a student, where the teacher tries\n\n\n372\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nto guide the student with their experience and knowledge. Following the same rationale,\ninteractive learning (Arzate Cruz & Igarashi, 2020a) in RL aims to include the human as\nteacher to guide the RL agent by using domain knowledge and rich human experience.\nEven without HITL, RL has been successfully applied to solve various real-world problems,\nranging from drug discovery (Popova et al., 2018), navigating high-pressure balloons in the\nstratosphere (Bellemare et al., 2020) to robot manipulation tasks (Nguyen & La, 2019).\nAlthough these are exciting research directions, recent deep RL systems still face many\nchallenges, including sample inefficiency, sim-to-real transfer issues, generalization, exploration exploitation trade-off, etc., to name a few (see Subsection 2.3 for further discussion)\n(Ibarz et al., 2021). In response to these challenges, interactive RL aims to overcome them\nby involving a human prior to training (Guo et al., 2022), during training (Knox & Stone,\n2008), or in the deployment phase of the RL system (Guo et al., 2021). Interactions could\neither be teacher-initiated (Torrey & Taylor, 2013), student-initiated (Da Silva et al., 2020;\nMandel et al., 2017), or jointly initiated by both parties (Amir et al., 2016).\nIn interactive learning, the human is often characterized as a teacher, and the teaching\nloop can contain different types of critique, advice modalities, and guidance that can be\nfed back to the RL algorithm. There can be different modalities of human advice, such as\nbinary evaluative feedback [+1/-1] (Knox & Stone, 2008), action-advice (Torrey & Taylor,\n2013), preference-based feedback (Christiano et al., 2017; Lee et al., 2021), and sub-goal\nspecification (Le et al., 2018). A comprehensive survey of various types of human guidance\nin Deep RL can be found in Zhang et al. (2019b). Thomaz and Breazeal (2006) proposed\none of the earliest works on Interactive RL, which allowed human trainers to give binary\nfeedback for the agent’s behavior and specific objects associated with the task.\nA common approach to modifying the reward function is called reward shaping. In\nreward shaping, the teacher provides useful information to shape the reward function to encourage favourable parts of the state space and penalize unfavourable parts (Ng et al., 1999).\nReward shaping is useful in sparse reward environments and facilitates reward specification\nin complex domains. TAMER (Knox & Stone, 2008; Knox et al., 2013) is a well-known reward shaping framework where a human expert provides evaluative reinforcement (positive\nor negative feedback) signal by observing an agent in action, and the agent maximizes the\nhuman’s feedback with classification. Subsequent variants of this method (Knox & Stone,\n2010, 2012) optimized the human reinforcement with the environment reward function to\nlearn a reward model.\nMethods that consider modifying the agent’s policy are called policy shaping (Cederborg et al., 2015; Griffith et al., 2013; Wu et al., 2021a). These methods augment an agent’s\npolicy directly using human knowledge. This technique does not require a well-formulated\nreward function but assumes that the trainer knows a near-optimal policy to guide the\nagent. Human advice can also be useful in guiding the agent in its exploration phase so\nthat the agent can identify highly rewarding states or trajectories in fewer environmental interactions (Amir et al., 2016). Action pruning is another way HITL RL can guide\nexploration and improve learning (Abel et al., 2017).\nLastly, human-advice-based value functions can be combined with agent value functions\nto effectively guide the agent (Jiang et al., 2021; Kartoun et al., 2010; Taylor et al., 2011;\nWu et al., 2021a). Demonstrations from humans can also increase the value function by\nbiasing it according to the actions taken by the expert (Hester et al., 2018; Nair et al., 2018;\n\n\n373\n\n\n\n\nRetzlaff et al.\n\n\nVecerik et al., 2017). These approaches have been particularly successful in complex robotic\ntasks such as pushing, sliding, etc., which humans can easily demonstrate. Demonstrations,\nby default, may contain human biases that can, in turn, be removed by experts (Wang\net al., 2022).\nThe first indicator of the need for human intervention is poor model performance, since\na bad model is more likely to produce suboptimal policies. However, good models can also\nlead to suboptimal policies in deployment scenarios due to the sim-to-real gap (the task of\ntranslating behavior learned in a simulation to reality (Zagal et al., 2004)). In many cases,\nhuman intervention is simulated from pseudo-agents in development. This approach can\nhelp to evaluate the potential benefits and imperfections of the model before deployment.\nThe designer of the interactive framework must also consider that human interventions\nmight not always be perfect or beneficial; the user might need special training and an\ninformative user interface (UI) to effectively improve the RL algorithm.\nIn turn, the type of UI (hardware-driven or natural interaction) determines the degree of expertise required and can affect the quality of the feedback (Lin et al., 2020).\nKeyboard keys, mouse clicks with sliders, and game controllers are examples of UIs in\nhardware-delivered interactions, and experts or knowledgeable trainers generally use these\nUIs. On the other hand, sound interfaces that use the techniques of audification and sonification [2] (Hermann et al., 2011; Kartoun et al., 2010; Saranti et al., 2009; Scurto et al.,\n2021), cameras to capture facial expressions (Arakawa et al., 2018), etc. are examples of\nUIs for natural interaction that non-expert users prefer.\n\n\n**2.3 Challenges for Reinforcement Learning and HITL Applications**\n\n\nTo conclude the background section, we discuss the underlying challenges for reinforcement learning in general and those more specific to HITL approaches to better understand\nfundamental and current challenges in the field.\nThe first fundamental challenge in RL is the exploration/exploitation trade-off, which\nis defined by the decision of when to continue exploiting a current option and when to\nexplore further for new options. The decision maker must balance between exploring a\nset of unknown options to find the best one (exploration) and exploiting the best option\nalready discovered (exploitation). A more in-depth description of this problem can be found\nin Audibert et al. (2009).\nThe second important challenge is the “sim-to-real gap,” which results from the fact\nthat simulations always under-model the target system (i.e. the real-world), which means\nthat various aspects of reality are missing. This presents agents with unforeseen challenges\nand sometimes even prevents a policy learned in a simulation from being transferable to the\nreal world. However, real-world samples are very expensive regarding cost, complexity, and\ntime, making modeling much more appealing despite its challenges (Kober et al., 2013).\nThe third challenge concerns the difficulties in pixel-based learning. In real-world applications, vision is often the central modality for agents, which makes learning from images\nand videos essential. Current solutions, however, often rely on weak assumptions and, consequently, do not generalize well (Tomar et al., 2021). Refer to Ibarz et al. (2021) for a\n\n\n2. Audification involves visualizing an existing sound, while sonification involves creating a sound to represent data or information.\n\n\n374\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nfurther discussion of the main challenges in RL as identified by them, namely sample efficiency, sim-to-real-gap, exploration challenges, generalization challenges, goal and reward\nshaping, and safety issues.\nFollowing the general challenges for RL, Roy et al. (2021) provide a comprehensive\noverview of challenges specific to reinforcement learning for embodied intelligence. We\nhighlight five particular constraints:\n\n\n  - Interaction with the real world involves safety risks for exploration and hard limits on\nresources like energy.\n\n\n  - Poor alignment of learned models with the real world.\n\n\n  - Require stronger generalizations and adaptation than regular Deep Learning approaches since specifications, goals, and rewards might change.\n\n\n  - Observed data are drawn from a local distribution, but generalization requires the\nagent to learn a reasonable world model beyond what is currently observed.\n\n\n  - Agent morphology defines what can be learned from the environment and has to be\nconsidered when designing agents.\n\n\nFinally, there are challenges specific to HITL approaches. The first challenge is to what\nextent RL should imitate a good human player, and when and how RL can be used to\nsurpass the performance of the human (Abel et al., 2017). Additionally, surpassing human\nperformance often requires the HITL RL agent to discover new action sequences, which is\nhindered by engineering overly specific and complex reward functions (Liu & Abbeel, 2020).\nThis challenge of striking a balance between human intervention and the agent’s ability of\nexploration is another extension of the exploration-exploitation trade-off described above.\nGlanois et al. (2021) discuss challenges with regard to explainability. Due to the close\nconnection between HITL and explainability, we also include challenges for explainability\nin our discussion of HITL challenges. Glanois et al. (2021) state that explanations are not\nreliable and do not make sense when the neural network is not yet fully trained. Therefore,\nthe network does not exhibit a cohesive behavior that could be explained. This can be due\nto lack of training, poor overall performance, lack of generalization capability, misclassified\nexamples, and other underlying errors. Therefore, ML specialists will need to consider at\nwhat point in development the application of different explanation methods is appropriate and capable of providing insights into the model. Glanois et al. (2021) further name\nas open challenges the problem complexes of scalability, performance, and achieving full\ninterpretability in general with RL xAI methods.\nAnother challenge in explainability is that most of the current xAI methods invented\nfor deep neural networks are not created with RL principles in mind. They are driven\nby the mathematical principles of neural networks and typically developed with the intent\nof uncovering a simpler, interpretable model or pinpointing the important elements of a\npotential input. In the example of the convolutional neural network (CNN) that was used\nto process the Atari images (Mnih et al., 2013), layer-wise relevance propagation (LRP)\ncould be used (Alber et al., 2019; Bach et al., 2015). However, this would only provide the\nuser with a heatmap about what is relevant for positive or negative prediction, meaning\n\n\n375\n\n\n\n\nRetzlaff et al.\n\n\nthat it would only characterize (in RL terms) one input state. Those heatmaps are not\njuxtaposed or combined with the possible actions from that state or their expected reward\nas a whole — the human would not know why the RL algorithm decided for the selected\nnext action. Reconstructing the complete strategy of a model, its rules, and the underlying\npurposes of all (or at least the representative) state-action pairs out of those heatmaps\nwould be a very cumbersome task.\nWe argue that the challenges discussed show that a generalizable and performant RL\nis a fundamentally challenging problem. We claim that many of these challenges can be\novercome with the application of HITL approaches, supported by Mathewson and Pilarski\n(2022), who argue that human-centered interactive approaches are essential for designing\nand deploying machine learning systems. While Mathewson and Pilarski (2022) take the\nbird’s-eye perspective on machine learning systems and formulate a high-level guideline for\nhuman-centered design of ML systems, we focus specifically on designing, evaluating and\ndeploying interactive HITL RL systems and associated short-term and long-term challenges.\nAs the second focus of our paper, we argue that xAI approaches are fundamental to\nthe success of HITL approaches, a notion supported by other researchers (Heuillet et al.,\n2021; Milani et al., 2022). Therefore, in each of the four deployment phases we outlined\nabove, xAI is a central component. In the following sections, we aim to show where xAI\ncan be applied in the deployment of HITL RL agents, which solutions exist, and how they\nmight be adapted to allow for a productive HITL interaction. We furthermore discuss how\nexplainability can impact the safety considerations of RL applications in each phase and\nhow this can, in turn, help to build trust and accountability.\nThis section summarizes the background on explainability, interactive learning, and\nchallenges in HITL RL. In the next sections, we will dive into the four phases of deployment,\nstarting in Section 3, discussing how xAI techniques could be applied to the initial phase\nof HITL RL development. In Section 4, we focus on the agent learning phase, followed by\nthe discussion of the subsequent evaluation and deployment of these systems in Sections 5\nand 6, respectively.\n\n\n**3. Initial Agent Development**\n\n\nThe first steps of the HITL RL model deployment entail underlying model development,\nproblem formulation, and pre-learning considerations. These steps make the model understandable to machine learning (ML) specialists who seek detailed insight into their model.\nThe development phase is characterized by the ML specialist laying the technical groundwork. In this phase, the focus is on the RL model itself.\nIn all phases of RL agent development, we distinguish three user types: machine learning\nspecialist, domain expert, and end-user. The ML specialist, also called a developer, has\nexpertise in developing and interpreting ML solutions. The domain expert has experience\nand authority in the field where the AI solution is applied but does not have a technical\nbackground to fully understand the AI model. Finally, end-users represent customers who\nbuy and use a product available on the market. Although the end-user has a background\nin the respective field, they do not have the expertise to decide whether a given policy is\ncorrect and appropriate.\n\n\n376\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nThe ML specialist defines the problem to be solved by the agent and selects the agent’s\nenvironment accordingly. For that, they construct a Markov decision process (MDP), defining the state space, action space, and reward function. All three have a critical impact on\nthe speed of learning, the agent’s final performance, and what policy is learned. The ML\nspecialists also set the agent’s algorithm and hyperparameters and decide whether to incorporate prior knowledge by adding hand-crafted features or transferring knowledge from\nan existing model. Another consideration is whether the agent should pre-train on existing\ndata.\n\nIn the initial agent development phase, explainability can help show the impact of\nalgorithm or hyperparameter selection, how prior knowledge biases the agent, and how\npretraining changes the agent’s learning process. Considering explainability in this stage\nallows ML specialists to start in the right direction instead of being forced to make costly\n(post hoc) changes later on in the learning lifecycle. Many decisions and considerations\nmade in this phase have broad implications for the overall life cycle of RL and are likely to\nbe difficult to change later on. Table 4 gives an overview of key aspects of the initial agent\ndevelopment phase.\n\n|Phase|Human<br>Involvement|Explanation<br>Requirements|Explainability|Goals|Metrics|\n|---|---|---|---|---|---|\n|Development|_•_Defne problem<br>_•_Construct<br>state<br>space,<br>action<br>space,<br>and reward function<br>_•_Design model|_•_Comparable to other<br>model versions<br>_•_Fast and simple ex-<br>planations for shallow<br>inspection<br>_•_Complex and exhaus-<br>tive<br>explanations<br>for<br>deep inspection|**Focus:**<br>Interpretable<br>models<br>such as decision trees,<br>causal models, compo-<br>sitional language<br>**Explanations**:<br>Counterfactuals,<br>pol-<br>icy querying, decision<br>rules<br>**Users:**<br>RL experts|_•_Create<br>thorough<br>and<br>comparable<br>model summaries<br>_•_Use<br>compositional,<br>representational lan-<br>guage<br>_•_Further<br>integrate<br>causal<br>learning<br>into<br>HITL approaches|**Fidelity**<br>_•_Correctness<br>(correlation<br>between<br>output<br>on controlled synthetic data<br>or single feature deletion)<br>_•_Explainable Model<br>Discrepancy<br>(error between model predic-<br>tions)|\n\n\n\nTable 4: Overview of the types of human involvement, the specific requirements for explanations, the focus, and types of explainability approaches for the initial agent development\nphase. Furthermore, we list different goals in this phase as well as metrics and options for\ntheir quantification.\n\n\nWe highlight the importance of making a model understandable for developers, even\nif they implemented the model and have a basic understanding of how it works. First,\nexplainability can help researchers understand the underlying mechanisms of the model\nand how it makes decisions, which can be useful for debugging the model and identifying\npotential issues or improvements. Additionally, explainability can facilitate the development\nof new models and methods. By understanding how a model works, researchers can better\niterate on it and create new models that improve or extend its capabilities.\nIn the agent development phase, working with an understandable model helps ensure\nthat the model is based on reasonable assumptions and is able to reach consistent conclusions. Numerous errors in training data, model initialization, and the initial learning\nprocess can be monitored and limited in this phase. Designing the proper function and\narchitecture of the model in this phase also ensures a suitable baseline for comparison with\n\nthe trained model.\n\nWe regard the agent development phase as the only phase where interactivity is not\nrequired. Bidirectional, interactive explanations are essential when involving experts and\n\n\n377\n\n\n\n\nRetzlaff et al.\n\n\nend-users in the development process, but at this initial stage of development, the low\noverhead of simpler xAI methods is preferable.\nFor this phase, we primarily consider the exploration-exploitation trade-off and sim-toreal gap as relevant challenges (see Subsection 2.3). These constrain the learning algorithm\nand parameter selection and subsequently define which type of explanation is applicable to\nthe model.\n\nIn reference to our forest operation use-case, we highlight the risk of introducing unnoticed, fundamental errors to the model in this phase. This could, for example, be an\nincorrectly specified reward function, which has the potential to impede the agent’s ability\nto perform basic navigation tasks. Since the reward function is the central building block\nof an agent’s behavior, this can hamper the overall progress of the agent’s development. In\nthe discussion subsection of this phase (see 3.5), we discuss how explainability and HITL\napproaches can be used to minimize those risks.\n\n\n**3.1 Requirements**\n\n\nWe propose primary considerations for the development phase and identify two approaches\nfor an explainable development process. The first is a faster and more superficial evaluation\nof the general model behavior, which allows for a high-level inspection of the model behavior.\nThe second is an in-depth assessment that provides a more detailed and complex view of\n\nthe model.\n\nIn the first case, explanations must be computed quickly to enable a feedback loop\nduring training. Xin et al. (2018) explore the implications of a faster feedback loop and\nhighlight aspects such as introspection, the ability to rapidly analyze and compare the\nimpact of changes to reuse intermediate results. They find that a faster feedback loop\nalso enables an easier end-to-end optimization by the ML specialist. With the metrics of\nMilani et al. (2022), we therefore propose high demands for computational performance and\nreduced cognitive load to enable more explanation breadth.\nIn the second case of an in-depth assessment, explanations require more time to compute, understand, and interpret correctly. Therefore, the ML specialist can use these explanations to analyze a small number of snapshots of the model in depth, allowing a better\nunderstanding of the behavior of complex models. Both approaches should complement\neach other, since a thorough assessment of the model behavior requires both depth and\nbreadth.\n\nIn the beginning of the initial agent development phase, the generated explanations\nshould furthermore be comparable to those of other model versions to track the progress of\ndevelopment by contrasting different behaviors and their explanations. This is essential to\nenable researchers to monitor how the model behavior evolves and assess if the development\nprogresses in the desired direction.\nComplex and detailed explanations are best suited for ML specialists, as they require\ndetailed insights into the model and can afford the required cognitive load. In addition,\ncomputational resources are the least constrained in this phase, and explanations do not yet\nneed to scale to large model sizes. In the metrics of Milani et al. (2022), these requirements\nresult in lower demands for computational performance and cognitive load but high fidelity.\n\n\n378\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nIn the following section, we identify different approaches that can help during this\nphase. First, pre-training can help with the groundwork of intelligent behavior and enable\nsensible debugging. Second, interpretability approaches should be considered in this phase.\nThey allow for an inherent understandability, which can benefit all subsequent phases if\nimplemented here. Third, we argue which types of explainability approaches are suitable\nfor this phase.\n\n\n**3.2 Approach: Pre-Training**\n\n\nPre-training models benefit the RL training workflow since they prepare the groundwork for\nthe Human-Robot interaction and benefit HITL in several ways. Pre-training models helps\nto develop useful priors, diverse behaviors, generalized policies/feedback, and efficient initial\nfeedback from human input (Daniel et al., 2016; Eysenbach et al., 2018; Florensa et al.,\n2017; Hazan et al., 2019; Lee et al., 2021). Recently, Parisi et al. (2022) demonstrated the\neffectiveness of pretraining with out-of-domain computer vision data sets for downstream\nrobotics control tasks, which shows that pre-training can learn useful representations even\nfrom out-of-domain datasets. Pre-training involves training a model on a large dataset in an\nunsupervised manner to learn general language representations. These pre-trained models\ncan then be fine-tuned on specific tasks or domains with smaller datasets with transfer\nlearning (Taylor & Stone, 2009). The pre-trained model acts as a knowledge base, capturing\ngeneral language understanding, and the fine-tuning process adapts this knowledge to the\ntarget task, making it more efficient and effective.\nPreference-based learning is a HITL technique that strongly benefits from pre-training,\nin which an embodied agent allows a human to decide which is the preferred option of\ntwo or more possibilities of behavior (for example, two different movement policies). This\nchoice simplifies the difficult reward-selection process, i.e., sidesteps the need to define the\nreward function explicitly. It is advantageous for this approach if the robot already exhibits\ntwo “meaningful” movement policies rather than the normal frenetic behavior found in\nnewly instantiated models. Judging the policies of an already trained model is generally\neasier because, after the initial noise of random initialization, it shows meaningful behavior\n(Akrour et al., 2011). Lee et al. (2021) demonstrated the importance of unsupervised pretraining using intrinsic reward to make the agent learn diverse skills, which further helps\ngenerate informative queries for receiving human preference. Generally, approaches such as\ntransfer learning and lifelong learning for RL agents are promising, as they also alleviate\nthe problem of the noisy warm-up phase of RL (Taylor & Stone, 2009; Yang et al., 2021).\nThe application of pre-training in ChatGPT has demonstrated the effectiveness of leveraging large-scale datasets to initialize Large Language Models (LLMs), leading to a reasonable parameter initialization and, with that, improved performance and more efficient\nlearning in downstream tasks (Zhou et al., 2023). This success showcases the paradigm shift\nthat pre-training can bring to RL, as it enables models to acquire useful knowledge from\nvast amounts of unlabeled data, providing a strong foundation for subsequent fine-tuning\nand learning from specific tasks.\nPre-training holds relevance not only during the development stage but also impacts\nthe other stages due to the superior downstream performance. During the learning stage,\npre-training helps initialize the RL agent, facilitating faster convergence and enabling better\n\n\n379\n\n\n\n\nRetzlaff et al.\n\n\nsample efficiency. In the evaluation stage, pre-training can help improve the model’s performance and generalization capabilities on diverse tasks. Lastly, in the deployment stage,\npre-training ensures that the RL agent is well-equipped with foundational knowledge, enhancing its ability to adapt to real-world scenarios and perform effectively (Yang et al.,\n2023).\n\n\n**3.3 Approach: Explainability**\n\n\nExplanations can provide various benefits for building RL models during the initial agent\ndevelopment phase. Contextual information should be taken into account when defining\nwhat constitutes a “good” explanation for an RL model. This can be background knowledge,\ndifferent levels of expertise, as well as the needs and expectations of the addressee of this\nexplanation. There are various types of explanations, like visual (Atrey et al., 2019; Gupta\net al., 2019), textual (Fukuchi et al., 2017b; Hayes & Shah, 2017), causal (Madumal et al.,\n2020a, 2020b), or decision tree explanations (Bastani et al., 2018). The approach of Liu et al.\n(2018) sets out the decision process as rules to make the influence and learning of the network\nmore transparent. Another approach is to represent the learned model with generated code\nblocks, as presented by Verma et al. (2018). A policy network is codified by learning a neural\npolicy network and searching for the optimal policy, which results in human-readable policies\nand improves generalization, but it also incurs a performance penalty during training. A\nthird approach is to represent network policies in natural language, such as Alonso et al.\n(2018), who showed an example of justifying classifications with a textual explanation of\nthe choice made by a decision tree.\nFurthermore, the subset of policy querying approaches that allow one to look at questions like “When do you do X?” can be used for this phase. Hayes and Shah (2017) provide\nan example of such questions and generate a summary of a “When do you do X?” type\nquestion in natural language to explain the actions of an agent. An important addition to\nthis approach is the use of counterfactuals (see Evans et al. (2022) for a more thorough\nassessment of the importance of counterfactuals). Madumal et al. (2020b) generate a structural causal model for RL agents, which allows one to generate explanations of taken actions\n(see Subsection 2.2). This approach also allows one to respond to counterfactual queries,\nlike: “why did you not do Y?”. Providing such counterfactuals is shown by the authors to\nproduce satisfactory explanations and increase user trust.\nFinally, causal learning strategies can assist in understanding the underlying model\nand move from just interpretability to full explainability. This shift is accomplished by not\nonly making the model understandable, but also by actively explaining decisions and their\ncontext. An example of such xAI methods is probabilistic graphical models (PGM), which\nhelp to construct causal models and are often applied to graph neural networks (GNN)\n(Saranti et al., 2019). Graph neural networks are especially suitable for explainability\nmethods in the initial agent development phase, as they allow for an intuitive and more\ndirect visualization of critical components. Vu and Thai (2020), for example, support the\ninformed creation of a causal model by identifying essential graph components and then\ngenerating PGMs that approximate that prediction. These essential components can help\nto identify cause and effect in neural networks and determine cause-and-effect relations.\n\n\n380\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\n**3.4 Focus: Interpretability**\n\n\nA central approach to adding explainability in the HITL (in the broader sense) is to make\nthe model interpretable, i.e., to provide an inherently understandable AI solution. Interpretable models, in combination with pre-trained or otherwise initialized systems, facilitate\nthe judgment of the model and its decision-making by the ML specialist.\nRoy et al. (2021) enumerate several approaches that would translate to more interpretable models. For example, embedding core knowledge into models (such as physical\nconstraints) could provide agents with innate reasoning capabilities. This ability to reason\ncan, in turn, allow checking and debugging the entire reasoning process (Ha & Schmidhuber, 2018). As second aspect, Roy et al. (2021) highlight the use of compositional language, which can facilitate a high-level understanding of the concepts the model learned.\nKoditschek (2021) suggests that the use of model composition and, with it, compositional\nlanguage are key elements of embodied intelligence.\nIn addition to the approaches listed above, using a representative language could allow\nabstractive reasoning with rigorous generalization (Roy et al., 2021). This can be achieved\nby graph neural networks, natural language, or attention mechanisms in combination with\nsys1/sys2 separation and further the inherent understandability of the learned model. The\nadvantage of a combination of innate reasoning with understandable language could allow\nan intuitive understanding of the model. Intuitive understanding of model behavior can be\none building block for tackling the challenge of adversarial attacks, for example with image\nrecognition approaches. Many image recognition models focus on very different aspects\nthan what humans do and “understand” images on a fundamentally different level. This\nmakes image recognition susceptible to changes that are unnoticeable for a human observer,\nbut drastically alter the classification of an image (Chakraborty et al., 2021). By ensuring\nthe model “understands” images in the same way that humans do, this attack surface can\nbe reduced.\n\n\n**3.5 Discussion, Outlook, and Use-Case**\n\n\nRegarding the current challenges in the development phase, we find many xAI approaches\nintended for this setting. However, we encounter a lack of interactivity and comparability of\nthe explanations, which hinders a thorough evaluation of the model at this phase. Furthermore, we find that current explainability approaches often lack the causality and intuitive\nunderstandability required for a thorough introspection of a newly developed model.\nFirstly, we suggest using compositional and representational language for explainability\nto enhance a model’s intuitive understandability, as mentioned by Roy et al. (2021). Representing the model as a hierarchical, graphical, or topological structure (Battaglia et al.,\n2018; Lyu et al., 2019) is more understandable to humans than a traditional neural network\nmodel. Such structural models are however not as powerful as neural network models, since\ntheir expression and computation ability are limited. An optimal approach would therefore\nbe to integrate the structural models with the neural network model itself without losing\nexplainability.\nSecondly, we think that causal learning approaches should be adapted and integrated\nmuch more deeply into HITL approaches. In addition to generating better explanations,\n\n\n381\n\n\n\n\nRetzlaff et al.\n\n\nthey could also yield improved prediction performance, as exemplified by Madumal et al.\n(2020b).\nThirdly, policy querying approaches such as those presented by Hayes and Shah (2017)\ncould be adapted to allow specific inquiries into the model structure and be expanded with\ncounterfactual structures. We ultimately envision interactive, thorough, and comparable\nmodel summaries. Individual components of such a solution can already be found, but the\nsimplicity of a comprehensive solution could greatly benefit the development process. Roy et\nal. (2021) encourage us to think about the opportunities that other forms of sensors, sensor\nfusion, and new components enable, such as novel interaction approaches, application areas\nand ways of learning (see Subsection 2.3 for a perspective on the respective challenges).\nAnother direction involves human teaching (Kulick et al., 2013) or programs (Penkov &\nRamamoorthy, 2019; Sun et al., 2019) to guide the agent in learning the symbolic structure\nor representations of the task, which greatly reduces the complexity of the task.\nIn the example of robot operation in the forest, core knowledge of physical properties\nhelps develop a more robust model of the agent and the environment (Ha & Schmidhuber,\n2018). The model policy can, for example, be represented and tested with code blocks\n(Verma et al., 2018). The representation with code blocks is chosen since this phase focuses\non the model developers and, therefore, speaks the user’s language. To further enhance the\nmodel debugging process, it can be helpful to allow developers to query the model policies\nand allow inspection of why the robot stopped and what object it considered an obstacle\n(Hayes & Shah, 2017). Finally, the environment could be represented as a graph with points\nof interest to help developers better understand how the agent perceives the landscape (Lyu\net al., 2019) and ensure that this understanding is in line with the real-world conditions.\nWe emphasize that explainability is an essential tool for uncovering model errors at\nthe initial agent development phase. However, explainability alone will not be enough\nto discover all possible errors. We recommend aligning subject-matter experts and ML\nspecialists as well as considering performance metrics and other indicators to ensure that\nthe reward function aligns with the desired objectives. Explainability serves as a valuable\ntool in this process, facilitating the collaboration between experts and the agent in a HITL\nprocess, highlighting where the agent takes incorrect decisions, and providing insights to\nenhance the understanding of the implications of the reward function (see Section 5 for\nfurther discussion).\n\n\n**3.6 Initial Agent Development Summary**\n\n\nTo summarize the development phase, we refer to Table 1. The model explanations should\nbe comparable to each other, potentially allowing the user to understand the differences\nbetween the tested architectures. Explanations should be carefully designed to be broad\nand shallow or detailed and complex, depending on the cognitive resources and goals of the\naudience. Humans are involved in this phase for defining the problem, the overall model\ndesign, as well as the specification of state space, action space and reward function, and\nevaluation metrics of the agent.\nWe recommend using pre-training approaches such transfer learning in combination\nwith preference-based learning to leverage human knowledge effectively. Possible xAI approaches for HITL in this phase are focused on making the underlying model more under\n\n382\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nstandable, for example, by enabling interpretable model policies by representing them as\ntrees, causal models, or with compositional language. Finally, post hoc explanations, such\nas counterfactuals or extracted rules, can be useful to gain insights even at this early stage.\nCausal approaches should also be considered at this stage to help identify cause and effect\nin this phase and those further downstream.\nIn this section, we covered the development phase, the first of the four phases in creating\nHITL RL systems. In the next section, we discuss the subsequent phase of interactively\ntraining the agent.\n\n\n**4. Agent Learning**\n\n\nOnce developed, the agent is trained either autonomously or interactively with the help of\nhuman input. Section 3 discussed the current explanation techniques used mainly during\nthe development phase under the close supervision of the developer. This section focuses\non approaches where humans act as a trainer or teacher (as domain expert), guiding the\nagent learning process.\nIn this phase, the model is trained interactively with a domain expert, under the close\nsupervision of ML specialists. This phase focuses on understanding the model perceptions\nand fundamentals, and assesses the agent’s cooperation capabilities with the end-user. The\nhuman can decide to disallow the agent from selecting invalid or suboptimal actions. The\nagent’s action selection can also be expanded with an expert bias to enable faster learning\nrates. Furthermore, developers can decide to include an interactive paradigm with human\ndemonstration, feedback, advice, or other types of cooperation.\nEventually, the domain expert judges whether the learning process is successful or\nif the MDP (or other agent components) should be revised. This can be either because\nthe agent is learning too slowly or because the policy being learned is otherwise unsuited\nfor the problem. Finding and determining the appropriate role of HITL should be the\nfirst step in developing an optimal mechanism for interaction. We propose that the focus\nshould be on providing interpretable information and clear explanations to the human\nduring this teaching process to enable transparency between the parties. As a consequence,\nhuman trainers can understand how agents perceive the world and provide better feedback.\nExplainability can show the impact of bias through an existing controller or human advice,\nhow learning is progressing, or how the current policy functions.\nAmong the challenges identified in Subsection 2.3, the following are relevant to this\nphase: (1) determining to what extent agents should imitate or follow human advice, (2)\noptimizing reward shaping, (3) choosing explanation methods and complexity according to\nthe background knowledge and expertise of teachers, and (4) reducing the sim-to-real gap.\nTable 5 gives an overview of key aspects of the agent learning phase.\nIn the forest operation use-case, we are now beginning to engage with non-technical\nexperts. During this phase, we will be communicating and interacting with inexperienced\noperators, which may present unexpected scenarios that stress the agent’s ability to generalize. Additionally, it is important to consider the potential risks associated with the\nagent’s interactions with the environment, including navigating unknown obstacles or unstable ground. This emphasizes the importance of thorough testing and validation during\nthis phase, to ensure the agent’s ability to operate effectively in the field.\n\n\n383\n\n\n\n\nRetzlaff et al.\n\n|Phase|Human<br>Involvement|Explanation<br>Requirements|Explainability|Goals|Metrics|\n|---|---|---|---|---|---|\n|Agent Learning|_•_Give<br>evaluative<br>feedback<br>_•_Deliver<br>action-<br>advice<br>_•_Select action pref-<br>erences<br>_•_Provide<br>demon-<br>strations|_•_Understandable by do-<br>main experts<br>_•_Fluent interactions|**Focus:**<br>HITL approaches such as<br>human<br>preferences<br>querying,<br>uncertainty<br>highlighting<br>**Explanations:**<br>Counterfactuals,<br>textual<br>explanation<br>in<br>user<br>language,<br>saliency<br>maps<br>**Users:**<br>Domain experts,<br>RL experts|_•_Make<br>use<br>of<br>imitation<br>learning<br>and<br>preference-based<br>learning<br>as<br>complementary<br>ap-<br>proaches<br>_•_Adapt xAI approaches to<br>HITL context<br>_•_Apply<br>Human-as-<br>Teacher approach<br>_•_Find hybrid methods of<br>diferent<br>kinds for human interac-<br>tion|**Fidelity**<br>**Relevancy**<br>_•_Human Feedback<br>(evaluate<br>survey<br>with<br>users<br>on<br>relevance<br>to task)<br>**Performance**<br>_•_Time to Explana-<br>tion<br>(in milliseconds)|\n\n\n\nTable 5: Overview of the types of human involvement, the specific requirements for explanations, the focus and types of explainability approaches, the goals as well as metrics for\nthe agent learning phase.\n\n\n**4.1 Requirements**\n\n\nThis phase is the first where a novice user may interact with the RL agent, which imposes\ncertain requirements for a reduced complexity of the explanation. This also coincides with\nthe requirement for rapid explanations for the sake of fluency, enabling a truly interactive\ntraining process. Interpretable inputs, such as symbolic representations of the problem\nstructure or visualizations of the agent’s perception, are well suited for these requirements\nsince they give rapid introspection into what the agent perceives and bases its decisions on\n(Glanois et al., 2021). This rapidity only allows for a more shallow introspection, which\nis alleviated to a certain degree by the thorough testing in the development and evaluation phases. Fundamental errors in the model should be taken into account during the\ndevelopment phase, while hidden biases introduced during training are in focus during the\nevaluation phase.\nWe emphasize that even suboptimal explanations by human teachers can be better than\nnone. Current literature focuses on agents using human advice during the learning phase,\ntaking advantage of humans’ a priori knowledge. However, even though human decisions\ncould be less accurate, Zhang and Bareinboim (2020) demonstrate that agents are more\nlikely to learn suboptimal policies if they ignore human advice. To refer to the metrics of\nMilani et al. (2022), we propose the requirements of high fidelity, relevancy, and performance\nto ensure that the model is able to learn and interact with the user effectively.\n\n\n**4.2 Approach: Explainability**\n\n\nAs stated in Subsection 2.2, interactive RL uses human feedback to reduce problems in\nareas such as sample efficiency, sim-to-real transfer problems, and generalization. Like any\ninteraction, interactive RL requires a level of agent-human understanding, and one effective\nway to improve communication involves explaining one’s and others’ behavior (De Graaf &\nMalle, 2017). Therefore, more and more recent approaches augment interactive RL using\nexplainability techniques. For instance, researchers found that most people training an AI\nagent assume that their behavior reveals their knowledge (Habibian et al., 2021).\n\n\n384\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nFukuchi et al. (2017b) proposed a method to explain an agent’s future behavior to\nits trainer using the same expressions used by the trainer. The agent selects the phrases\nassuming that a higher reward means that the agent correctly followed the advice. In later\nwork, they apply the approach to agents that dynamically change policies (Fukuchi et al.,\n2017a).\nFurthermore, simple visualization techniques, like saliency maps or layer-wise relevance\npropagation, could be used to explain the agent’s perception of the world at a glance. These\ntechniques each have their own set of benefits and drawbacks more explicitly discussed in\nLiu et al. (2018) and Bach et al. (2015)\n\n\n**4.3 Approach: Interactive Learning**\n\n\nWhile explanations usually assume a human explainee, interactive RL can also allow the\nhuman to give feedback or explanations to the agent. One common way of providing feedback is to evaluate agent actions as positive or negative (Arakawa et al., 2018; Knox &\nStone, 2008; Knox et al., 2013; MacGlashan et al., 2017). However, this limited feedback\ncould improve if the trainer explains why specific actions are wrong. Guan et al. (2020)\naugment the binary evaluative feedback with visual explanations using saliency maps from\nhumans. In addition to improving the agent’s sample efficiency, the approach also reduces\nthe human input required. Likewise, Karalus and Lindner (2021) enhance evaluative feedback but this time using counterfactual explanations, yielding significant improvements in\nconvergence speed. In case of negative feedback, humans can communicate to the agent\nthat the feedback would have been positive _if_ action _a_ had been performed in a different\nstate _s_ _[′]_ . The authors limit counterfactual feedback only to negative reward cases, where it\nhas the largest impact.\nAnother example of counterfactual application is provided by Pearl (2009), which uses\ndynamical structural causal models (DSCM) to explicitly model the differences in the capabilities of the agent and the human operator as the world states evolve. In this framework,\nthe agent views human feedback as the intended action and adjusts it (using counterfactual\nreasoning) if the action is suboptimal. A trade-off between autonomy and optimality is\ndemonstrated, meaning that fully autonomous agents are likely to be suboptimal and could\nonly achieve optimality if they receive critical feedback from their human operators. The\ncounterfactual approach proposed by the authors improves on standard methods even when\nhuman advice is imperfect.\nFurther, human trainers tend to give more positive feedback and the learning agent\nshould be able to inherently accommodate this feedback bias. Making the agent’s assumptions transparent to the trainer can improve the overall process. Additionally, RL agents\nwho learn from human demonstration, imitation, or querying the trainer’s preferences can\ninherit biased human behavior. In that case, xAI can also shed light on the biases of the\nmodel before deployment. The robot then selects different behaviors and asks people about\ntheir preferences. Habibian et al. (2021) study the influence of robot questions on how their\ntrainers perceive them. In their approach, the robot chooses informative questions that\nsimultaneously reveal its learning. Compared to other approaches that do not account for\nhuman perception, Habibian et al. (2021) found that people prefer revealing and informative\n\n\n385\n\n\n\n\nRetzlaff et al.\n\n\nquestions since they find these questions easier to answer, more revealing about the robot’s\nlearning, and better focused on uncertain aspects of the task.\n\n\n**4.4 Focus: Interaction Design**\n\n\nThe agent learning step is the first phase where interaction with a non-technical user, i.e.,\nthe domain expert, takes place. Therefore, we recommend using this phase to focus on the\ninteraction between user and agent, enabling a fluent and cooperative workflow. Wu et al.\n(2021b) propose that a human can play different roles for interaction with RL agents, such\nas Supervisor, Controller, Assistant, Collaborateur, or Impactfactor. This encourages us\nto take into account how collaboration is framed and what it entails in developing efficient\nteaching approaches. A learning agent will possibly interact with the designer, trainer,\nand final user. Therefore, it is important to consider experts to na¨ıve collaborators. For\nexample, tools for visual explanations that use typical data visualization techniques, such\nas bars, may be useful for people with a scientific background; however, they add mental\nload to others (Anderson et al., 2020).\nWu et al. (2021b) state that the ideal interaction for HITL would be fluent, performant, and reliable. For systems geared at performance, the interaction is usually framed\nas collaboration, while a focus on reliability favors the role of supervisor for the human.\nFor fluency, however, new roles of interaction are proposed and discussed, which would also\nrequire new kinds of interface. An agent could integrate implicit empathic feedback from a\nhuman in the form of gestures, vocalizations, and facial expressions as shown by Cui et al.\n(2020), allowing a more intuitive interaction and richer feedback from the human to the\nlearning agent. To effectively leverage such feedback, appropriate user interfaces should be\ndeveloped, and the underlying model should be able to process multimodal data such as\nspeech or image. Another form of implicitly improving feedback is the approach presented\nby Peng et al. (2016), which makes the agent move slower if it is uncertain, enabling the\nhuman to provide feedback where it is most useful with an intuitive cue. Identifying and\nleveraging useful implicit feedback from humans would facilitate agent learning by moving\nbeyond what the human explicitly mentions as a teacher.\nUltimately, we propose that Agent Learning approaches should consider different approaches in their HITL framework rather than forcing one specific technique, which allows\none to better determine the mentioned sweet spot of interaction.\n\n\n**4.5 Discussion, Outlook, and Use-Case**\n\n\nFor the agent learning phase, we find that many approaches support the Human-as-Teacher\nparadigm itself, but identify a lack of explainability approaches that facilitate this interaction. More generally, xAI approaches in this phase need to evolve to support interactivity\nin the HITL setting, which is currently underdeveloped.\nTo proceed, the various current approaches should be further adapted for the HITL and\nRL contexts. An important adaptation is the development of a suite of tools to facilitate the\nsystematic deployment and comparison of the different approaches to discover sweet-spot\nmixes. An example of an effective combination could be to build fundamental behavior\nvia imitation learning, followed by fine-turning actions by preference-based learning, and\nfinally identifying and solving weak spots using querying approaches. We propose to strive\n\n\n386\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nfor a solution that allows efficient HITL training in the field along with domain experts,\nenabling users to rapidly bootstrap agent behavior and further support this process with\nreplanning and corrections. A combination of such approaches could be very efficient and\nfast in bringing about robust agents suited for real-world applications.\nWith regard to our use-case, we now move to real-world tests with a domain expert.\nSince at this stage we aim to improve the model’s robustness and performance, the focus\nshould be on thorough explainability. We recommend providing visual explanations of\nrobot perception at this point (Glanois et al., 2021), augmented, for example, by easily\nunderstandable saliency maps that highlight important image regions (Liu et al., 2018). In\nthis phase, we also recommend starting to integrate human feedback, which can be used to\nclassify elements in the environment, for example, to determine whether a way is passable or\nnot (Guan et al., 2020). If uncertain, the robot can move slowly and highlight the problem in\nits visual output to communicate with the user (Peng et al., 2016). Additionally, imitation\nlearning could be integrated to allow the robot to simply follow the user path and, with\nthat, learn more about the environment in a safe way (Pearl et al., 2000).\n\n\n**4.6 Agent Learning Summary**\n\n\nTo summarize the agent learning phase according to Table 1, explanations should provide\nthe user with interpretable inputs and allow for a rich interaction to better understand\nthe behavior of the model. This also requires explanations to be in the language of the\ndomain expert, facilitating a productive “Human-as-Teacher” interaction. We recommend\nfocusing on designing and testing the optimal interaction between human and agent, balancing feedback, explainability, and nonintrusiveness. Humans are involved in this phase to\nprovide evaluative feedback, give advice and preferences to the agent, and provide demonstrations for complicated tasks. The main explainability techniques are those explaining the\nagent’s perceptions and evaluating behavior, for example, with counterfactuals or textual\nexplanations.\nFurthermore, techniques which enable the agent to query human preferences or give\ninteractive feedback to instructions from the human teacher are recommendable, while\nvisualization approaches like saliency maps could be used for the sake of their low cognitive\nand computational load. Explainability is interactive at this phase, and RL experts as\nwell as domain experts are involved. These requirements differ from the agent development\nphase in that a domain expert is involved in the process, and explainability should be geared\ntowards interactivity and efficient agent learning, whereas the development phase was more\nfocused on generating insights into the model architecture.\nIn this section, we covered the second phase of development, discussing the training\nof HITL RL agents. In the next section, we discuss the subsequent phase of thoroughly\nevaluating the learned policies and the emerging agent behavior.\n\n\n**5. Agent Evaluation**\n\n\nIn this section, we detail the requirements of the agent evaluation phase and how explainability approaches and safety considerations can help build trust in the learned model. For\nthe success of human-robot teamwork, safety is essential. The robot must meet the innate\n\n\n387\n\n\n\n\nRetzlaff et al.\n\n\nexpectations of the human to be predictable and safe and communicate its intentions (Eder\net al., 2014).\nThe third phase is characterized by testing the behavior of the trained model, which\nrequires tools that enable comparability, quantify the learning progress, and can furthermore\nscale up to thoroughly evaluate larger-scale models. ML specialists and domain experts\nexhaustively test the learned model. Developers need to ensure that no erroneous behavior\nor glitches have emerged during training, focusing mainly on the syntactical level. The\ndomain expert will need to understand and evaluate the learned policies for sensible microand macro-behavior, therefore focusing more on an evaluation of the semantic behavior.\nThe judgment for either moving forward with the deployment to market or engaging\nin another development-learning-evaluation cycle lies in this phase and is made by the\ndevelopers and project owners. The domain expert must decide if the learned policy is\nready for deployment, if more training is needed, or if the problem definition needs to be\nchanged. In the evaluation phase, explainability can help with an in-depth inspection of\nlearned policies and emerging behavior.\nTo ensure this, the trained system has to be tested extensively. It is important to make\na distinction between errors in the underlying model and errors learned during training. In\nthe agent deployment phase described in Section 6, underlying errors in the model architecture should be discovered and fixed. Therefore, the agent evaluation phase can focus on\ndiscovering errors acquired during training and, ultimately, ensuring a safe decision-making\n\nprocess.\nThis type of acquired error becomes apparent in various forms. One is shortcut learning,\nwhere a model finds undesired shortcuts in the training data instead of learning the desired\nconcept. This often prevents a generalization since the shortcut is, in most cases, not\npresent in the application context. For example, an algorithm that learns the hospital\ntoken embedded in an image rather than the targeted signs of pneumonia in X-ray images\nrepresents a case of acquired error (Geirhos et al., 2020).\nAnother symptom of errors acquired during training is adversarial attacks, which show\nthat the model did not learn the desired concept, but rather invisible patterns in the image\n(Goodfellow et al., 2014). There are several approaches to reduce the attack surface for\nadversarial attacks with optimizations in the training process, but we will focus on the\nunderlying issue of models failing to learn concepts. This problem is also part of the\nchallenge of alignment of the learned model and the real world, identified in Subsection 2.3\nand discussed by Roy et al. (2021).\nBoth issues show why it is important to examine the behavior of the trained model. We\npropose that at this phase of development, the focus should be on the evaluation of safety\naspects, such as represented by the decision-making process of the model, which reflects the\nlearned behavior. This decision-making process can be made tangible with approaches such\nas policy summarization, graph-based explanations, and causal models. Furthermore, we\nconsider different safety approaches, which ensure that the agent takes only allowed actions\nand is transparent with regard to its level of uncertainty. Table 6 gives an overview of key\naspects of the agent evaluation phase.\nThe agent evaluation phase of our forest operations use-case is crucial to discover potential pitfalls that could arise. One pitfall is the failure to discover emerging errors in the\nmodel, which impedes performance or prevents application in the field. Failure to consider\n\n\n388\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n|Phase|Human<br>Involvement|Explanation<br>Requirements|Explainability|Goals|Metrics|\n|---|---|---|---|---|---|\n|Evaluation|_•_Understand and evaluate<br>learned policies on micro<br>and macro-level<br>_•_Test<br>model<br>boundaries<br>and safety<br>_•_Decide whether model is<br>ready for deployment|_•_Summarise<br>learned<br>behav-<br>ior<br>_•_Scalable<br>to<br>large<br>models<br>_•_Comparable<br>to<br>untrained models<br>_•_Understandable<br>by domain experts|**Focus:**<br>Safety<br>evalua-<br>tion by<br>modeling uncer-<br>tainty,<br>using<br>shield-<br>based defenses<br>**Explanations**:<br>Policy<br>summa-<br>rization with<br>natural<br>lan-<br>guage,<br>rules<br>or<br>code,<br>graph-<br>based<br>explana-<br>tions<br>**Users:**<br>Domain<br>Experts,<br>RL Experts|_•_Ensure<br>understandability<br>for domain expert<br>_•_Enable thorough and com-<br>parable explanations<br>that scale with model size<br>and complexity<br>_•_Further<br>develop<br>dash-<br>boards for policy inspection<br>from diferent viewpoints|**Fidelity**<br>**Relevancy**<br>**Cognitive Load**<br>_•_Compactness<br>(absolute size of explanation<br>in number of features,<br>path<br>length,<br>percent reduction to<br>complete data)<br>_•_Redundancy<br>(overlap between parts of ex-<br>planations)|\n\n\n\nTable 6: Overview of the types of human involvement, the specific requirements for explanations, the focus and types of explainability approaches, the goals as well as metrics for\nthe agent evaluation phase.\n\n\nhow the agent’s performance scales with increasing complexity and size of the tasks can\nlead to great results in theory and less useful results in practice. This could manifest in,\nfor example, an agent that can only successfully complete ten consecutive tasks for which\nit was trained, but then starts to fail or misbehave after that.\nAnother key consideration is ensuring that the agent is thoroughly tested across a wide\nrange of edge cases and scenarios to ensure that it can handle a diverse range of operating\nconditions. An agent that is not able to handle a wide range of tasks and operate effectively\nunder different conditions will be restricted to a very narrow field of applications, while\npotentially failing under unfavorable conditions. This could, for example, be a forest with a\ndifferent biome, terrain roughness or weather conditions than trained on. This shows why\nit is important to ensure that the agent’s reliability and safety are properly evaluated. This\nincludes both ensuring that the agent is able to detect and avoid potential hazards, such as\nobstacles or unstable ground, and that it can operate in a generalizable and reliable manner\nthat minimizes the risk of injury to both the end-user and other personnel.\n\n\n**5.1 Requirements**\n\n\nDuring the evaluation phase, xAI approaches have to work with large models and complex\ndecision-making processes. This requirement, for example, makes the use of text- or rulebased approaches (Hayes & Shah, 2017; Tabrez & Hayes, 2019) more challenging, since\nthey might be helpful when producing one page of output, while parsing and understanding\nmany pages of model policy explanations will become prohibitive. In a similar vein, visual\napproaches such as trees or DAGs, in general, should not exceed a certain size to still be\nuseful.\nThis requirement is supported by Wells and Bednarz (2021a), who find that the authors\nof several xAI approaches identify scaling up their approaches as a major challenge, which\nalso shows why many xAI approaches are only applied to toy examples. This is especially\nrelevant to text-or graph-based explanations, which can rapidly become unwieldy.\n\n\n389\n\n\n\n\nRetzlaff et al.\n\n\nFurthermore, models should provide explanations that are understandable to domain\nexperts. It will often be the case that only the domain expert, instead of the developer, can\njudge whether a learned policy is consistent, which makes it a requirement that the domain\nexpert can evaluate it.\nA final consideration is that the provided explanations should be able to highlight\ndifferences in the learned behavior with regard to a newly initialized or only pre-trained\nmodel, in order to gain an understanding of what the agent actually learned during the\ndifferent phases of the training process.\nTo refer back to the metrics of (Milani et al., 2022), we propose adding cognitive load\nto the demands on fidelity and relevancy in the previous phases. Cognitive load is added to\nensure explanations scale to the full-size model, which constitutes a major challenge with\napproaches relying on visual or textual summarization (Vu & Thai, 2020).\n\n\n**5.2 Approach: Explainability**\n\n\nPolicy summarization approaches focus on showing and explaining model policies to the\nuser. We refer to examples that codify the decision process of the model as rules (Liu\net al., 2018), as code blocks (Verma et al., 2018), or through natural language. Alonso\net al. (2018) shows an example of justifying classifications with a textual explanation of\nthe choice made by a decision tree, which could in turn be transferred to RL applications.\nOther examples for providing an introspection into LLMs with textual explanations are Zini\nand Awad (2022) and Xu et al. (2023).\nPolicy summarization is well suited for assessing a trained model and checking its\npolicies for unexpected and undesired behavior. Depending on whether and which domain\nexperts are included in the process, different summarization approaches are advisable. Summarizing model policies as code blocks can be intuitive for computer science and adjacent\nfields, but are likely inadvisable for domain experts with a non-technical background. Here,\nspecial care should be given to assessing how a model could best be summarized to be\nintuitively understandable for the explanation target, since the additional cognitive load for\nunderstanding the explanation modality should be kept to a minimum. A second aspect\nis that the scale of the model should be considered. Ten blocks of model policy code can\neasily be evaluated, but a hundred blocks of code will be very difficult to understand and\nthoroughly inspect. Here, the graph-based explanation approach may be a useful addition.\nGraph-based explanations can furthermore be very helpful in providing a quick and\nintuitive overview of model behavior. Holzinger et al. (2021) recommend the use of graphbased explanations for HITL systems, as they can be used to intuitively compare expert\ndomain knowledge with learned model behavior. Song et al. (2019) show how graph-based\nexplanations can be applied in recommender systems, a field where knowledge graphs are\noften used. With their presented system, the user is shown a meaningful path within that\ngraph on how a recommendation was formed, which helps provide effective recommendations and good explanations. The use of graph-based explanations can, however, become\noverwhelming for the user if the model behavior or explained decision becomes too complex. Approaches such as PGExplainer (Vu & Thai, 2020) alleviate this by focusing the\nexplanation graph on relevant parts of the decision graph.\n\n\n390\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\n**5.3 Focus: Safety Evaluation**\n\n\nRL systems can be highly complex and non-transparent, making it difficult to understand\ntheir decision-making processes and identify potential safety issues. We emphasize the\nneed to proactively evaluate the safety of the developed application to ensure that they are\ndesigned and deployed in a way that minimizes risks to the agent itself and others. The\nmentioned approaches help to evaluate the agent’s behavior and test it for unexpected or\nundesirable actions.\n\nHowever, while testing the model is an essential part of the training loop, it should not\nbe the only component to ensure safe operation. An example of how safety can be ensured\nis presented by Xiong et al. (2020), who propose using shield-based defenses, where agents\nlearn to stay within predefined, safe boundaries during training and application, and with\nthat increase robustness.\n\nIn addition, approaches that estimate model uncertainty in different scenarios can be\nuseful. Lutjens et al. (2018) present a collision avoidance policy to provide computationally\ntractable and parallelizable uncertainty estimations in navigation tasks. This can be used\nto ensure that the model is sufficiently confident in the test scenarios employed by the\ndevelopers, but also able to discover blind spots. These blind spots can then be used to test\nhow the model behaves when encountering them.\n\n\n**5.4 Discussion, Outlook, and Use-Case**\n\n\nIn the agent evaluation phase, we identify a major challenge in the scalability of xAI approaches, which are often suited to smaller models but fail to provide explainability for\nlarge trained models and their learned policies. We also find that few approaches allow for\nidentifying variations between different versions of a trained model, which we consider a\ncentral feature for this phase in order to conduct an exhaustive evaluation of the learned\npolicy. Visual approaches such as saliency maps can be applied in this phase to evaluate the\nbehavior of the model, but should be used with caution due to providing only superficial\ninsights about the ”where”, but not the ”how”, and being susceptible to confirmation bias\n(Evans et al., 2022).\nWe highlighted that many explainability approaches that suffice in the development and\nlearning phase need adaptation to the evaluation phase due to model size and complexity.\nApproaches such as code block summarization as provided by Verma et al. (2018) could be\nextended by focusing only on relevant parts of the explanations as illustrated by Vu and\nThai (2020). Furthermore, expert readability must be ensured to allow domain experts to\nhelp test and assess whether the learned policies are sensible, while post hoc explanations\nsuch as visual (Atrey et al., 2019; Gupta et al., 2019) or textual (Fukuchi et al., 2017b;\nHayes & Shah, 2017) explanations would increase overall explainability.\nWe recommend integrating various tools to help evaluate and scrutinize a model from\nmany different points of view. We propose that detailed explanations of the decision of the\nmodel are essential for this process. Furthermore, the focus should be on evaluating model\nsafety by providing explanations in such a way that they are understandable to the domain\nexperts, allowing one not only to debug superficial model behavior, but also to check the\nlearned routines for semantically sensible behavior, which is a task that also calls on the\nknowledge of the domain expert.\n\n\n391\n\n\n\n\nRetzlaff et al.\n\n\nWe suggest that a combination of the explainability methods described can help significantly by ensuring that the model has only learned the desired behavior. The use of\ngraph-based explanations is recommended as complementary to the policy summarization\napproach since the summarization provides a broad overview of model policy, which can\nthen be further inspected by querying specific explanations. The rapidity and intuitiveness of graph-based explanations allow inspection of the learned model policies together\nwith domain experts, and together with safety measures and uncertainty estimation build a\ntrustworthy model which informs the user of its limits. With this in mind, developers, domain experts and project owners should decide whether the model is ready for deployment\nor needs further development.\nTo refer back to the use-case of forest operation, we ensure that the agent is based on\na reliable model capable of autonomously navigating in different environments and safely\nresponding to user errors. To ensure reliability, the model should be tested in different\nenvironments. Furthermore, intentional user errors (such as trying to navigate to an unreachable location or entering malformed data) can be used to test the agent and see how\nit behaves in such edge cases (Xiong et al., 2020). Moreover, the model policy should be\nsummarized and could be presented in a form of a policy graph, which will allow developers\nand domain experts to easily assess the behavior of the robot and see how the trained policy works (Vu & Thai, 2020). This could also be used to compare different model versions\nagainst each other to see what the agent has learned. Finally, the model should provide an\nunderstanding of the uncertainties and blind spots that it may encounter when navigating\nin the forest, for example, by using uncertainty estimates (Lutjens et al., 2018).\n\n\n**5.5 Agent Evaluation Summary**\n\n\nTo summarize the agent evaluation phase as per Table 1, the explanations in this phase\nshould scale to large trained models and be comparable to previous versions of the agent.\nThey must be understandable by the domain experts to allow an exhaustive comparison of\nthe learned model behavior. We also recommend focusing on evaluating the overall safety of\nthe system and the robustness of its behavior. Humans are involved in understanding and\nevaluating these policies and the resulting behavior at the micro level (sensible individual\ndecision) and at the macro level (cohesive overall behavior) and deciding if to deploy the\nmodel in the real world.\n\nUseful explainability techniques include policy summarization, graph-based explanations, and approaches to interpretable decision-making, such as extracting decision trees\nand logic rules, involving domain experts and RL experts in a bidirectional fashion. In\nthis phase, the benefit of utilizing causal models can lead to intrinsic explanation of the\nmodel. The model should also be evaluated with regard to safety aspects to ensure lasting user trust. In comparison to the other phases, this phase has many similarities in the\nxAI techniques used with the initial agent development phase, but requires in addition the\nunderstandability for the domain expert and the scaling of xAI methods to more complex\npolicies.\nIn this section, considerations regarding the evaluation of HITL RL systems were discussed. In the next section, we discuss the final step of deploying the agent in a real-world\n\ncontext.\n\n\n392\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n|Phase|Human<br>Involvement|Explanation<br>Requirements|Explainability|Goals|Metrics|\n|---|---|---|---|---|---|\n|Deployment|_•_Deploy agent<br>_•_Interact with agents<br>_•_Defne agents’ real-<br>world<br>application goal and<br>context|_•_Fast, clear, and concise to<br>reduce cognitive load<br>_•_Understandable<br>by<br>end-<br>users<br>_•_Non-intrusive to prevent<br>detrimental efects on<br>user performance|**Focus:**<br>Building User Trust with<br>intent<br>and<br>uncertainty<br>communication,<br>allow-<br>ing error corrections<br>**Explanations:**<br>Saliency maps,<br>dendro-<br>grams, bounding-boxes,<br>textual explanations,<br>visual and auditory indi-<br>cators<br>**Users:** End-users|_•_Develop and apply new ap-<br>proaches<br>beyond<br>image<br>and<br>driving-<br>based explanations<br>_•_Use simple and fast explana-<br>tions<br>_•_Implement<br>cohesive<br>error<br>and uncertainty handling<br>_•_Communicate agent intent<br>via diferent modalities|**Fidelity**<br>**Relevancy**<br>**Cognitive**<br>**Load**<br>**Performance**|\n\n\n\nTable 7: Overview of the types of human involvement, the specific requirements for explanations, the focus and types of explainability approaches, the goals as well as metrics for\nthe agent deployment phase.\n\n\n**6. Agent Deployment**\n\n\nThe agent is then deployed in the real world and interacts with the end-user. It now has\nto provide an efficient interaction for different users, while also building trust by providing\nexplanations for its behavior. A human typically needs to argue why the agent is safe and\nshould be deployed in the real world from a vendor perspective. The developers will need to\ndetermine if the agent should continually learn, if its policy should be frozen, or if it should\nretrain if an environment change makes this necessary. The customer and the end-user then\nmake the final deployment decision, determining the usage context and specific application\nof the agent in the field. Here, explainability can help users understand the final policy,\nimprove trust, assess safety, and understand the stability of the policy.\nIn this section, we focus on approaches that facilitate an efficient interaction between\nthe trained agent and its human user. This frequent and repeated interaction requires\nfinding a balance between appropriately showing explanations and not hindering the task\nat hand — a trade-off also highlighted by Anderson and Bischof (2013), who state that\nwhile initially guides can be helpful, they can also be detrimental to long-term performance\nand learning. Additionally, challenges in terms of safety and generalization to overcome\nunforeseen situations come into play (see Roy et al. (2021) and Subsection 2.3). Table 7\ngives an overview of key aspects of the agent deployment phase.\nWe propose that the use of HITL agents can lead to significant performance gains during\nthe four phases of development, learning, evaluation, and deployment. In this section, we\nfocus on approaches ensuring that those benefits actually reach the end-user, factoring in\nissues like mental overload and distrust.\n\nWhen it comes to the agent deployment phase for the forest operations use-case, there\nare several challenges that must be considered. One significant challenge is communication.\nIf the agent’s actions and intentions are not effectively communicated to the operator or if\nthe operator is unable to provide commands and feedback to the agent, it may not be able\nto function effectively in the field. Additionally, it is important to ensure that the agent can\nmostly function autonomously and ask for help from the human only when necessary, as\ntoo much dependence on the HITL may reduce the agent’s autonomy and practicality for\nreal-world use. Furthermore, safety should always be of highest priority, as the agent can\n\n\n393\n\n\n\n\nRetzlaff et al.\n\n\nonly be successful if it operates without causing injury to personnel or otherwise damaging\nitself and its ability to operate.\n\n\n**6.1 Requirements**\n\n\nThe xAI systems used by end-users can draw on the vast pool of research on HCI usability.\nTherefore, we adapt considerations and requirements from the well-known “golden rules of\ninterface design” (Shneiderman et al., 2016). It must be taken into account that designing\ninteractive RL systems has special design requirements (Arzate Cruz & Igarashi, 2020b),\nalthough the underlying rules of HCI design still apply.\nTo reduce the memory load, the explanations must be simple and quickly understandable. We aim to facilitate difficult tasks and should avoid complicating the human-robot\ninteraction with overly complex explanations. Since operators are likely to work with rapidly\nchanging perspectives and environments, explanations should be computed in real time to\nensure they correspond to the current situation. An example of this is an autonomous\ncar. If explanations are provided with a delay of several seconds, actions that could require\nintervention will already have occurred.\nRegarding the guideline to allow experienced users to take shortcuts, agents should\nprovide explanations on demand and should be deactivated if desired, as discussed in Anderson and Bischof (2013). This option is essential to prevent information fatigue and to\nallow natural and efficient human-robot collaboration.\nOur third consideration is to simplify error handling and give the user the feeling of\nbeing in control. We propose that a HITL model should provide some means to show\nwhether it is uncertain about a given situation or decision, for example, in the form of\na warning light as seen in cars. Such a mechanism can give the user a notification that\nsomething is wrong or uncertain, and allow us to investigate what causes this. Along\nsimilar lines, we propose some kind of startup check sequence, again based on the warning\nlight startup sequence of a car, where users can ensure that the system is in order and\ncorrectly understands the situational context.\nIn contrast to the other three phases of HITL RL deployment, we do not propose that\nthe major requirements can be lifted at this phase. We rather suggest that this phase is\nthe most demanding of the four enumerated, as it combines constraints on computational\nand cognitive capacities, requiring high fidelity, relevancy, performance, and low cognitive\nload (Milani et al., 2022) to ensure that the model performs well in real world situations\nand can handle a variety of inputs.\n\n\n**6.2 Approach: Explainability**\n\n\nSeveral researchers provide examples of what real-time explanations for different use-cases\ncould look like. Rodriguez et al. (2021) provide feature-based explanations for COVID-19\ncase predictions, while Kulkarni and Gkountouna (2021) developed a classroom dashboard\nthat gives an overview of student performance with dendrograms and text-based explanations. Another example is the utilization of dendrograms for estimating the remaining\nuseful lifetime of industrial machinery by machine learning combined with domain knowledge (Serradilla et al., 2020). The majority of those real-time explainability systems are\ndata/software-based, while for the area of explanations for robotic systems, there are much\n\n\n394\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nfewer examples. Most autonomous driving systems provide explanations in the form of\nbounding-boxes and labels for recognized objects, which is an appropriate option for explaining model perception and helps with localizing important image regions (Behl et al.,\n2017; Kashyap et al., 2020).\nThe next step is decision explanations. Here, Ben-Younes et al. (2022) present a method\nwhere object saliency is combined with a textual explanation of an action. For example,\nthe observed traffic light is highlighted, in combination with the textual explanation of a\n“stop” action. Such an approach is already helpful and quick to evaluate by the end-user.\nStill, it could, for example, be even further refined when using known symbols and signs\ninstead of text along with regional highlighting. Xu et al. (2023) provides an example of\nhow textual explanations could be used to create textual explanations aligned with human\njudgement, increasing their understandability.\nA major component for trusting an agent is the predictability of the agents’ actions.\nTherefore, we suggest that xAI approaches used in the real-world focus on making the\nagents’ decisions and planning transparent for the user by showing intended actions. Strictly,\nsince actions do not have to be explained, just announced, this approach belongs to the\narea of interpretability, which constitutes a subset of explainability (Dragan, 2015). Requiring only interpretability and not explainability simplifies the requirements for such an\nindication, though, of course, HCI principles still have to be taken into account to avoid\nincurring too much mental load. Caltagirone et al. (2017), for example, shows a predicted\ntrajectory for autonomous driving applications, which could easily be translated into other\nmovement-based domains. An open challenge is how these predictions can be communicated in other contexts than autonomous cars and with other modalities. Here, items such\nas smartwatches, headphones, or simple visual indicators could provide familiar and flexible\ninterfaces.\n\n\n**6.3 Focus: Building User Trust**\n\n\nTrust is essential for AI development because it enables effective deployment and adoption\nof AI systems. We established that without trust, people may be hesitant to use or rely\non AI systems, which can limit their potential benefits (see Subsection 2.1). In the agent\ndeployment phase, we therefore recommend to focus on building trust with the end-user\nand highlighted how different explainability approaches can be used for that.\nFurthermore, we suggest that the use of “warning light” alerts could be beneficial,\nrecognizing when the agent is uncertain about a decision and advising the user. This could,\non the one hand, increase the general robustness of the agents’ decisions and also foster\nhuman trust in the agents’ decisions, since the user can now estimate better if the agent is\ncertain about a decision.\n\nSuch a warning light could be based on uncertainty estimation and be activated when\nthe uncertainty rises above a given threshold. Jain et al. (2021) give an example of how\nepistemic uncertainty can be estimated to a certain degree. The introduction of such an\napproach could help the user focus on the given task and interaction with the robot, while\nstill being in control and able to intervene when required.\n\n\n395\n\n\n\n\nRetzlaff et al.\n\n\nSuch an intervention approach is demonstrated by Wu et al. (2021a). They allow the\nHITL operator to intervene when the agent makes erroneous decisions and, furthermore,\nallow the model to learn from those interventions.\n\nThe startup sequence approach could complement this concept of error handling. Liu\net al. (2021) for example proposes an error detection framework, where the HITL operator\nis presented with a list of the most relevant and explainable features to detect unusual or\nnonsensical behavior. This list is evaluated during startup with a quick glance and provides\nconsiderable trust benefits.\n\n\n**6.4 Discussion, Outlook, and Use-Case**\n\n\nIn the agent deployment phase, we find only a few suitable xAI approaches. The application\nof current approaches is most often hindered by the failure to speak the user’s language,\nlimited available computation, or too much complexity to be usable in a real-time context.\nAdditionally, many HITL RL approaches fail to gain (and deserve) the user’s trust, in\naddition to failing to communicate uncertainty when warranted.\nWe propose that the currently available approaches look beyond the use-cases of autonomous driving focused on visual aspects and consider other modalities. An example is\nthe context of credit or policy computations, which require explaining textual facts. Additionally, modalities beyond graphic dashboards must be considered to ensure, such as\nauditory and tactic perspectives. Furthermore, visual perspectives should be explored in\ndifferent form factors such as smartwatches, LED indicators, and image projections.\nWe emphasize the need for simple and fast explanations, as there are only a few such\ncurrent approaches. We furthermore recommend considering explaining agent actions via\ndifferent modalities, such as visual indicators, but also haptic or auditory signals, aspects\nwhich are largely unexplored as of now. Finally, we envision a suite of tools equipped with\nwarning indicators that show when the agent encounters difficult situations, allowing the\nuser to trust the agent when it is within its generalization capabilities, and inform the user\nif that is not the case. Ultimately, a start-up sequence with different checks would allow\nthe user to ensure that the agent is properly initialized and trustworthy.\nWith regard to our use-case, we now similarly recommend relying on simple, robust,\nand nonintrusive indicators to communicate the agent state to the user. For example, the\nrobot may use visual cues to show its operating intentions accompanied by a warning light\nor an audible alarm that communicates uncertainty (Jain et al., 2021). Additionally, a\ndisplay that can be connected to the machine could be used to allow the user to view\nand understand the models’ perception (Glanois et al., 2021). This will provide easy and\nquick explanations for the user, helping them identify and avoid obstacles along the way.\nFurthermore, its actions could be justified with textual explanations (Ben-Younes et al.,\n2022), which ideally also allow the user to provide feedback and correct mistakes (Wu et al.,\n2021a).\n\n\n**6.5 Agent Deployment Summary**\n\n\nTo summarize, the agent deployment phase according to Table 1, the main requirement is\nthat the explanations are understandable by the end-user. Additionally, they will be used in\nthe field and therefore should not incur significant overhead, either with too much cognitive\n\n\n396\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nload or long and costly computation times. We highlight the need to focus on how to best\nhandle errors and communicate model uncertainty to the end-user. Humans are involved\nas end-users in deploying the agent, defining the usage context, and specific agent tasks.\nExplainability can be provided in the form of a combined explainability dashboard,\ncommunicating the agent’s intents, actions, and ways of handling uncertainty and errors\nin a bidirectional fashion. Explanations can visualize important aspects of the agents’\nperception, or provide textual or other explanations for the agent’s decision and intent. For\nvisualization, currently used techniques are saliency maps, dendrograms, and boundingboxes, which, in combination with textual explanations and visual or auditory indicators,\ncould be used to communicate the agent’s intent. This phase is the first entirely focused\non the end-user and on generating user trust in the system, allowing one to accurately\nassess its strengths and weaknesses and enabling efficient real-world cooperation. This does\nnot require providing insights into the model architecture as in the agent development and\ndeployment phase, but rather highlighting relevant aspects of its perception and decisionmaking as in the agent learning phase.\nWe have now covered the four phases that lead to the deployment of HITL RL systems.\nIn the next section, we open up the discussion on propositions for general research directions\nthat emerged in this paper.\n\n\n**7. Research Directions**\n\n\nWe refer to Subsection 2.3 to emphasize that we consider RL to be a challenging problem\nsetting that could greatly benefit from HITL approaches. We highlight that there is no onesize-fits-all solution for explainability and that the requirements for suitable HITL xAI differ\nbetween each phase. We do not propose a strict separation of explainability techniques in\ndifferent phases, but rather recognize the suitability of certain types of xAI for each phase\ndepending on the nature of the human involvement, the aspect of the agent in focus (e.g.,\nmodel architecture, its decision-making process, or its perception of the world), and the task\nto be tackled. The types of xAI approaches we recommend are informed by the explanation\nrequirements listed in Table 1, the strengths and weaknesses of xAI methods as per Table 2,\nas well as the nature of human participation, the types of users, and the directionality of\nthe interaction between the human and the agent. Explainability plays a central role in\neach phase of the agent deployment process, and discussed how it influences safety and trust\nconsiderations at each step.\nOur broader vision is that the HITL RL approaches depicted could, in the future,\nenhance the productivity of a human-robot team. Khatib et al. (1999) stated that the HITL\ncontributes experience, domain knowledge, and is able to ensure the correct execution of\ntasks. The robot, on the other hand, can increase human capabilities in terms of force,\nspeed, and precision. Moreover, the robot should reduce human exposure to harmful and\nhazardous conditions. However, it is essential to allow both a machine and a human operator\nto react to the environment and human beings to correctly understand and interpret the\nmachine’s actions, as the underlying algorithms and their decisions must be understandable\nto a wide variety of different audiences with different goals (Heuillet et al., 2021). Therefore,\nwe highlight the importance of explainability in HITL RL.\n\n\n397\n\n\n\n\nRetzlaff et al.\n\n\nAn exemplary instance of applying human feedback can be seen with RLHF in ChatGPT, where the integration of human feedback for fine-tuning the model plays a pivotal role\nin its success (Li et al., 2023). RLHF allows models to learn from human interactions and\nfeedback, resulting in improved performance and more natural conversation generation, exhibiting impressive capabilities in generating coherent and contextually relevant responses.\nRLHF can furthermore help to enhance sample efficiency, relying on only moderately large\nlabelled sample sets ( [˜] 50,000 samples), thereby also possibly reducing training time (Lambert et al., 2022). This advantage is particularly valuable considering the extensive training\ndurations needed for renowned RL models like AlphaGo and OpenAI Five, which require\none month and ten months of training time, respectively (OpenAI et al., 2019; Silver et al.,\n2017). These instances highlight the ample scope for improvement in terms of both training\ntime and sample efficiency.\nHowever, it is essential to consider the limitations associated with incorporating human\nfeedback. Biases present in training data and introduced through human feedback can\nlead to incorrect or biased responses, and ChatGPT models have been known to generate\nhallucinations and provide factually inaccurate information, posing significant drawbacks\nto their applicability (Peng et al., 2023).\nExplainability techniques can alleviate these limitations by shedding light on the\ndecision-making process, enabling users to identify and address issues. Incorporation of\nHITL and explainability mitigates the risks of biased or incorrect outputs by involving\nhuman oversight and intervention (Peng et al., 2023), underscoring the importance of explainability in comprehending the decision-making process of LLMs. Additionally, lack of\ntransparency, coupled with biased training data, may result in the dissemination of inaccurate or harmful content by users (Ray, 2023).\nVarious explanation approaches, including classic attention mechanisms in NLP\n(Glanois et al., 2021), text-based explanations by LLMs (Xu et al., 2023; Zini & Awad,\n2022), and symbolic representation (Saba, 2023), can enable users to understand the model’s\nreasoning and identify potential flaws. In particular, the use of LLMs to generate explanations has garnered attention recently. For example, Zini and Awad (2022) employ the\nmodel itself to provide transparent explanations for its decision-making. Similarly, Xu et al.\n(2023) introduce an explainable metric that combines human instructions and the implicit\nknowledge of GPT, offering explanations aligned with human judgment for given outputs.\nAnother caveat to explainability is that most of the work on xAI is heavily biased\nby what researchers assumed to be good explanations for a given task or domain (Miller,\n2019b), not taking into account the preferences and expertise of human end-users. Based\non the feedback of the operators, models and approaches must adapt their language and\nmodalities to be effective, which requires a more human-centered development (Puiutta &\nVeith, 2020). To ensure that explainability methods actually align with users’ expectations,\nwe call for a comprehensive set of guidelines and requirements for developing xAI systems.\nBut human-robot collaboration comes with challenges beyond explainability. When the\nagent is deployed, the trust requirement is essential; otherwise, the agent will not be used.\nAccording to De Santis et al. (2008), only trustworthy robots can work in this team. Humans\ntend to anthropomorphize robots (Damiano & Dumouchel, 2018), thus overestimating their\ncognitive capabilities. De Santis et al. (2008) argue that a user’s mental model might\nresult in a fake robot dependability, which exacerbates the problem of safety in human\n\n398\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nrobot collaboration. This collaboration relies on the predictability of the robot’s actions,\ntestability, explainability of the policies, as well as performance increases, which leads us to\nemphasize a research focus on trust and safety issues in HITL RL in the long term.\nWe finally propose future research on the use of explainable AI in safety. One area\nof interest is the development of safety-critical systems that incorporate explainable AI\nmethods to ensure that the agent’s decision-making process is transparent, interpretable,\nand can be easily understood by human decision-makers. This could include the use of\ntechniques such as counterfactuals, which allows for the examination of the factors that\nled to a particular decision, and the use of natural language explanations to communicate\nthe agent’s decision-making process to human users. Another area of research could be\non the development of methods for evaluating and testing the safety and robustness of AI\nagents, such as testing for robustness to distributional shift and adversarial attacks, and\nmethods for detecting and mitigating bias in the agent’s decision-making process. We also\nwant to highlight that both trust and user experience play critical roles in determining\nthe overall success of an AI application. While safety and trust are paramount in many\napplications, especially fields such as healthcare, education, and entertainment rely on user\nexperience for their effectiveness and acceptance. Users must feel comfortable, confident,\nand satisfied with the AI system to fully engage and benefit from it (Holzinger, 2021;\nHolzinger et al., 2022a). Building trust involves ensuring transparency, explainability, and\naccountability in the decision-making process, enhancing user confidence. Simultaneously,\nproviding a positive and seamless user experience through intuitive interfaces, personalized\ninteractions, and effective problem-solving capabilities is essential for user satisfaction and\nadoption (Arzate Cruz & Igarashi, 2020b). Striking a balance by establishing both trust and\nan exceptional user experience is crucial in driving the success and widespread acceptance of\nAI applications in various domains. Further research is needed to explore how to effectively\ncombine both goals in order to avoid the fact that trust-building measures hinder (or are\nsacrificed for the sake of) a fluid user experience. In this section, we have proposed different\nresearch directions. In the next section, we summarize the content and central insights of\nthis paper and conclude with the vision we have for the future of HITL RL.\n\n\n**8. Conclusion and Future Outlook**\n\n\nIn summary, we emphasize that RL is a fundamentally difficult problem setting and could\nbenefit greatly from HITL interactions. A human expert can contribute to the conceptual\nunderstanding gained through many years of experience in the task at hand, thus significantly improving robustness and explainability (Holzinger, 2021). Numerous approaches\nhave shown that RL benefits from human-centered approaches (Li et al., 2019; Mathewson\n& Pilarski, 2022).\nWe further argue that HITL RL in particular benefits greatly from xAI approaches.\nThese are, after all, fundamentally human approaches, which, in turn, ensure successful\ninteractions, acceptance, and trust, as well as conceptual knowledge about the agents’\nlimitations (Heuillet et al., 2021; Milani et al., 2022).\nWe identify the following phases for deploying HITL RL solutions: (1) initial agent\ndevelopment, (2) agent learning, (3) agent evaluation, and (4) agent deployment. In our\nwork, we discuss how xAI can support each of these phases and what are some considerations\n\n\n399\n\n\n\n\nRetzlaff et al.\n\n\nfor a successful deployment. Thus, the HITL combination enables better human-robot\ncollaboration and ultimately increases on-task productivity and efficiency.\nIn the deployment phase, interactive, thorough, and coherent model summaries can\nenable an agile and transparent workflow. During the agent’s interactive learning, xAI\napproaches can enable more efficient training in the field through interactive replanning\nand imitation learning. In the evaluation phase, comprehensive explanations of model\ndecisions can provide detailed insight into the trained model and lead to informed decisions\nabout whether to proceed with the deployment or start another development cycle. In the\ndeployment phase, simple and quick explanations of actions and different explanations for\nerror handling could significantly increase user confidence in the agent and lead to more\nefficient collaboration.\nLast but not least, we propose a vision of an interactive human-robot collaboration that\nenables new use-cases for RL applications and allows both humans and robots to realize their\nfull potential and respective strengths. Such a collaboration requires strong interaction and\ntrust between both parties, which can only be achieved through comprehensive explanation\nand a deep and intuitive understanding of the mental model generated by the agent.\n\n\n**Acknowledgments**\n\n\nParts of this work have been funded by the Austrian Science Fund (FWF), Project: P-32554\n“explainable Artificial Intelligence”. Part of this work has taken place in the Intelligent\nRobot Learning (IRL) Lab at the University of Alberta, which is supported in part by\nresearch grants from the Alberta Machine Intelligence Institute (Amii); a Canada CIFAR\nAI Chair, Amii; Compute Canada; Huawei; Mitacs; and NSERC. We also thank Dr. Antonie\nBodley for her editorial and proofreading assistance.\n\n\n**References**\n\n\nAbel, D., Salvatier, J., Stuhlm¨uller, A., & Evans, O. (2017). Agent-agnostic human-in-theloop reinforcement learning. _arXiv preprint arXiv:1701.04079_ .\nAiyappa, R., An, J., Kwak, H., & Ahn, Y.-Y. (2023, March). Can we trust the evaluation\non ChatGPT? [arXiv:2303.12767 [cs]]. https://doi.org/10.48550/arXiv.2303.12767\nAkanksha, E., Jyoti, Sharma, N., & Gulati, K. Review on Reinforcement Learning, Research\nEvolution and Scope of Application. In: In _2021 5th International Conference on_\n_Computing Methodologies and Communication (ICCMC)_ . 2021, April, 1416–1423.\nhttps://doi.org/10.1109/ICCMC51019.2021.9418283.\nAkrour, R., Schoenauer, M., & Sebag, M. Preference-based policy learning (D. Gunopulos,\nT. Hofmann, D. Malerba, & M. Vazirgiannis, Eds.). In: _Machine learning and knowl-_\n_edge discovery in databases_ (D. Gunopulos, T. Hofmann, D. Malerba, & M. Vazirgiannis, Eds.). Ed. by Gunopulos, D., Hofmann, T., Malerba, D., & Vazirgiannis, M.\nBerlin, Heidelberg: Springer Berlin Heidelberg, 2011, 12–27. https://doi.org/978-3642-23780-5.\n\nAkrour, R., Tateo, D., & Peters, J. Towards reinforcement learning of human readable\npolicies. In: In _Workshop on deep continuous-discrete machine learning_ . 2019.\n\n\n400\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nAlber, M., Lapuschkin, S., Seegerer, P., H¨agele, M., Sch¨utt, K. T., Montavon, G., Samek, W.,\nM¨uller, K.-R., D¨ahne, S., & Kindermans, P.-J. (2019). Innvestigate neural networks!\n_Journal of Machine Learning Research_, _20_ (93), 1–8.\nAlonso, J. M., Ramos-Soto, A., Castiello, C., & Mencar, C. Explainable ai beer style classifier. In: In _Sicsa realx_ . 2018. https://ceur-ws.org/Vol-2151/Paper ~~S~~ 1.pdf\nAmershi, S., Begel, A., Bird, C., DeLine, R., Gall, H., Kamar, E., Nagappan, N., Nushi,\nB., & Zimmermann, T. Software Engineering for Machine Learning: A Case Study.\nen. In: In _2019 IEEE/ACM 41st International Conference on Software Engineering:_\n_Software Engineering in Practice (ICSE-SEIP)_ . Montreal, QC, Canada: IEEE, 2019,\nMay, 291–300. isbn: 978-1-72811-760-7. https://doi.org/10.1109/ICSE-SEIP.2019.\n00042.\n\nAmir, O., Kamar, E., Kolobov, A., & Grosz, B. Interactive teaching strategies for agent\ntraining. In: In _In proceedings of ijcai 2016_ . 2016.\nAnderson, A., Dodge, J., Sadarangani, A., Juozapaitis, Z., Newman, E., Irvine, J., Chattopadhyay, S., Olson, M., Fern, A., & Burnett, M. (2020). Mental models of mere\nmortals with explanations of reinforcement learning. _ACM Transactions on Inter-_\n_active Intelligent Systems (TiiS)_, _10_ (2), 1–37. https://doi.org/10.1145/3366485\nAnderson, F., & Bischof, W. F. Learning and performance with gesture guides. In: In\n_Proceedings of the sigchi conference on human factors in computing systems_ . CHI\n’13. Paris, France: Association for Computing Machinery, 2013, 1109––1118. isbn:\n9781450318990. https://doi.org/10.1145/2470654.2466143.\nAndreas, J., Klein, D., & Levine, S. Modular multitask reinforcement learning with policy\nsketches. In: In _Proceedings of the 34th international conference on machine learning_ .\n_70_ . Proceedings of Machine Learning Research. PMLR. 2017, 166–175.\nArakawa, R., Kobayashi, S., Unno, Y., Tsuboi, Y., & Maeda, S.-i. (2018). Dqn-tamer:\nHuman-in-the-loop reinforcement learning with intractable feedback. _arXiv preprint_\n_arXiv:1810.11748_ .\nArrieta, A. B., D´ıaz-Rodr´ıguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garc´ıa,\nS., Gil-L´opez, S., Molina, D., Benjamins, R., Chatila, R., & Herrera, F. (2019). Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and\nChallenges toward Responsible AI [arXiv: 1910.10045]. _arXiv:1910.10045 [cs]_ . Retrieved January 5, 2022, from http://arxiv.org/abs/1910.10045\nArzate Cruz, C., & Igarashi, T. A survey on interactive reinforcement learning: Design\nprinciples and open challenges. In: In _Proceedings of the 2020 acm designing inter-_\n_active systems conference_ . Association for Computing Machinery, 2020, 1195–1209.\nhttps://doi.org/10.1145/3357236.3395525.\nArzate Cruz, C., & Igarashi, T. A Survey on Interactive Reinforcement Learning: Design\nPrinciples and Open Challenges. In: In _Proceedings of the 2020 ACM Designing In-_\n_teractive Systems Conference_ . DIS ’20. New York, NY, USA: Association for Computing Machinery, 2020, July, 1195–1209. isbn: 978-1-4503-6974-9. https://doi.org/\n10.1145/3357236.3395525.\nAtrey, A., Clary, K., & Jensen, D. D. (2019). Exploratory not explanatory: Counterfactual analysis of saliency maps for deep reinforcement learning. _arXiv preprint_\n_arXiv:1912.05743_ . http://arxiv.org/abs/1912.05743\n\n\n401\n\n\n\n\nRetzlaff et al.\n\n\nAudibert, J.-Y., Munos, R., & Szepesv´ari, C. (2009). Exploration–exploitation tradeoff using\nvariance estimates in multi-armed bandits. _Theoretical Computer Science_, _410_ (19),\n1876–1902. https://doi.org/10.1016/j.tcs.2009.01.016\nBach, S., Binder, A., Montavon, G., Klauschen, F., M¨uller, K.-R., & Samek, W. (2015).\nOn pixel-wise explanations for non-linear classifier decisions by layer-wise relevance\npropagation. _PloS one_, _10_ (7), e0130140. https://doi.org/10.1371/journal.pone.\n0130140\nBaniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., & Biecek, P. (2022). Dalex:\nResponsible machine learning with interactive explainability and fairness in Python.\n_The Journal of Machine Learning Research_, _22_ (1), 214:9759–214:9765. https://doi.\norg/10.5555/3546258.3546472\nBastani, O., Pu, Y., & Solar-Lezama, A. Verifiable reinforcement learning via policy extraction (S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, &\nR. Garnett, Eds.). In: _Advances in neural information processing systems_ (S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, & R. Garnett, Eds.).\nEd. by Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., &\nGarnett, R. _31_ . Curran Associates, Inc., 2018.\nBattaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R., Gulcehre, C., Song,\nF., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A., Allen, K., Nash, C., Langston,\nV., . . . Pascanu, R. (2018). Relational inductive biases, deep learning, and graph\nnetworks. _arXiv preprint arXiv:1806.01261_ .\nBehl, A., Hosseini Jafari, O., Karthik Mustikovela, S., Abu Alhaija, H., Rother, C., &\nGeiger, A. Bounding Boxes, Segmentations and Object Coordinates: How Important\nIs Recognition for 3D Scene Flow Estimation in Autonomous Driving Scenarios? In:\nIn _Proceedings of the IEEE International Conference on Computer Vision_ . 2017,\n2574–2583. Retrieved July 24, 2023, from https://openaccess.thecvf.com/content\niccv ~~2~~ 017/html/Behl ~~B~~ ounding ~~B~~ oxes ~~S~~ egmentations ~~I~~ CCV ~~2~~ 017 ~~p~~ aper.html\nBellemare, M. G., Candido, S., Castro, P. S., Gong, J., Machado, M. C., Moitra, S., Ponda,\nS. S., & Wang, Z. (2020). Autonomous navigation of stratospheric balloons using\nreinforcement learning. _Nature_, _588_ (7836), 77–82. https://doi.org/10.1038/s41586020-2939-8\nBen-Younes, H., Zablocki, E., P´erez, P., & Cord, M. (2022). Driving behavior explanation [´]\nwith multi-level fusion. _Pattern Recognition_, _123_, 108421. https://doi.org/10.1016/\nj.patcog.2021.108421\nBruneau, D., Sasse, M. A., & McCarthy, J. The eyes never lie: The use of eye tracking\ndata in hci research. In: In _Chi 2002: Booktitle=CHI 2002: Conference on Human_\n_Factors in Computing Systems conference on human factors in computing systems_ .\nUniversity College London, 2002.\nBuchanan, B. G., & Shortliffe, E. H. (1984). Rule-based expert systems: The MYCIN experiments of the Stanford Heuristic Programming Project. _Artificial Intelligence_ .\nhttps://doi.org/10.1016/0004-3702(85)90067-0\nBuchelt, A., Adrowitzer, A., Kieseberg, P., Gollob, C., Nothdurft, A., Eresheim, S., Tschiatschek, S., Stampfer, K., & Holzinger, A. (2024). Exploring artificial intelligence\n\n\n402\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nfor applications of drones in forest ecology and management. _Forest Ecology and_\n_Management_, _551_, 121530. https://doi.org/10.1016/j.foreco.2023.121530\nCaltagirone, L., Bellone, M., Svensson, L., & Wahde, M. Lidar-based driving path generation using fully convolutional neural networks. In: In _2017 ieee 20th international_\n_conference on intelligent transportation systems (itsc)_ . IEEE. 2017, 1–6. https://\ndoi.org/10.1109/ITSC.2017.8317618.\nCederborg, T., Grover, I., Isbell, C. L., & Thomaz, A. L. Policy shaping with human teachers. In: In _Twenty-fourth international joint conference on artificial intelligence_ .\n2015, 3366–3372.\nChakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., & Mukhopadhyay, D. (2021).\nA survey on adversarial attacks and defences. _CAAI Transactions on Intelligence_\n_Technology_, _6_ (1), 25–45. https://doi.org/10.1049/cit2.12028\nChandrasekaran, B., Tanner, M. C., & Josephson, J. R. (1989). Explaining Control Strategies in Problem Solving [Publisher: IEEE Computer Society]. _IEEE Intelligent Sys-_\n_tems_, _4_ (01), 9–15, 19. https://doi.org/10.1109/64.21896\nChoi, J. H., Hickman, K. E., Monahan, A., & Schwarcz, D. (2023, January). ChatGPT Goes\nto Law School. https://doi.org/10.2139/ssrn.4335905\nChristiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., & Amodei, D. (2017). Deep\nreinforcement learning from human preferences. _arXiv preprint arXiv:1706.03741_ .\nCui, Y., Zhang, Q., Allievi, A., Stone, P., Niekum, S., & Knox, W. B. (2020). The empathic framework for task learning from implicit human feedback. _arXiv preprint_\n_arXiv:2009.13649_ .\nDa Silva, F. L., Hernandez-Leal, P., Kartal, B., & Taylor, M. E. Uncertainty-aware action\nadvising for deep reinforcement learning agents. In: In _Proceedings of the aaai con-_\n_ference on artificial intelligence_ . _34_ . 2020, 5792–5799. https://doi.org/10.1609/aaai.\nv34i04.6036.\nDamiano, L., & Dumouchel, P. (2018). Anthropomorphism in human–robot co-evolution.\n_Frontiers in psychology_, _9_, 468. https://doi.org/10.3389/fpsyg.2018.00468\nDaniel, C., Neumann, G., Kroemer, O., & Peters, J. (2016). Hierarchical relative entropy\npolicy search. _Journal of Machine Learning Research_, _17_, 1–50.\nDe Graaf, M. M., & Malle, B. F. How people explain action (and autonomous intelligent\nsystems should too). In: In _2017 aaai fall symposium series_ . 2017.\nDe Santis, A., Siciliano, B., De Luca, A., & Bicchi, A. (2008). An atlas of physical human–robot interaction. _Mechanism and Machine Theory_, _43_ (3), 253–270. https:\n//doi.org/10.1016/j.mechmachtheory.2007.03.003\nDel Ser, J., Barredo-Arrieta, A., D´ıaz-Rodr´ıguez, N., Herrera, F., Saranti, A., & Holzinger,\nA. (2024). On generating trustworthy counterfactual explanations. _Information Sci-_\n_ences_, _655_, 119898. https://doi.org/10.1016/j.ins.2023.119898\nDoran, D., Schulz, S., & Besold, T. R. (2017, October). What Does Explainable AI Really\nMean? A New Conceptualization of Perspectives [arXiv:1710.00794 [cs]]. https://\ndoi.org/10.48550/arXiv.1710.00794\nDragan, A. D. (2015, July). _Legible Robot Motion Planning_ (Doctoral dissertation). Carnegie\nMellon University. https://doi.org/10.1184/R1/6720419.v1\nEder, K., Harper, C., & Leonards, U. Towards the safety of human-in-the-loop robotics:\nChallenges and opportunities for safety assurance of robotic co-workers. In: In _The_\n\n\n403\n\n\n\n\nRetzlaff et al.\n\n\n_23rd ieee international symposium on robot and human interactive communication_ .\nIEEE. 2014, 660–665. https://doi.org/10.1109/ROMAN.2014.6926328.\nEuropean Parliament. (2020). European Parliament resolution of 20 October 2020 with\nrecommendations to the Commission on a framework of ethical aspects of artificial\nintelligence, robotics and related technologies. https://eur- lex.europa.eu/legalcontent/EN/TXT/?uri=CELEX:52020IP0275\nEvans, T., Retzlaff, C. O., Geißler, C., Kargl, M., Plass, M., M¨uller, H., Kiehl, T.-R.,\nZerbe, N., & Holzinger, A. (2022). The explainability paradox: Challenges for xai\nin digital pathology. _Future Generation Computer Systems_, _133_ (8), 281–296. https:\n//doi.org/10.1016/j.future.2022.03.009\nEysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2018). Diversity is all you need: Learning\nskills without a reward function. _arXiv preprint arXiv:1802.06070_ .\nFlorensa, C., Duan, Y., & Abbeel, P. (2017). Stochastic neural networks for hierarchical\nreinforcement learning. _arXiv preprint arXiv:1704.03012_ .\nFukuchi, Y., Osawa, M., Yamakawa, H., & Imai, M. Application of instruction-based behavior explanation to a reinforcement learning agent with changing policy (D. Liu,\nS. Xie, Y. Li, D. Zhao, & E.-S. M. El-Alfy, Eds.). In: _International conference on_\n_neural information processing_ (D. Liu, S. Xie, Y. Li, D. Zhao, & E.-S. M. El-Alfy,\nEds.). Ed. by Liu, D., Xie, S., Li, Y., Zhao, D., & El-Alfy, E.-S. M. Springer. 2017,\n100–108. https://doi.org/10.1007/978-3-319-70087-8 ~~1~~ 1.\nFukuchi, Y., Osawa, M., Yamakawa, H., & Imai, M. Autonomous self-explanation of behavior for interactive reinforcement learning agents. In: In _Proceedings of the 5th inter-_\n_national conference on human agent interaction_ . New York, NY, USA: Association\nfor Computing Machinery, 2017, 97–101. https://doi.org/10.1145/3125739.3125746.\nGeirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., & Wichmann, F. A. (2020). Shortcut learning in deep neural networks. _Nature Machine_\n_Intelligence_, _2_ (11), 665–673. https://doi.org/10.1038/s42256-020-00257-z\nGlanois, C., Weng, P., Zimmer, M., Li, D., Yang, T., Hao, J., & Liu, W. (2021). A survey\non interpretable reinforcement learning. _arXiv preprint arXiv:2112.13112_ .\nGoodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and harnessing adversarial\nexamples. _arXiv preprint arXiv:1412.6572_ .\nGriffith, S., Subramanian, K., Scholz, J., Isbell, C. L., & Thomaz, A. L. Policy shaping:\nIntegrating human feedback with reinforcement learning (C. Burges, L. Bottou, M.\nWelling, Z. Ghahramani, & K. Weinberger, Eds.). In: _Advances in neural informa-_\n_tion processing systems_ (C. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K.\nWeinberger, Eds.). Ed. by Burges, C., Bottou, L., Welling, M., Ghahramani, Z., &\nWeinberger, K. _26_ . 2013.\nGuan, L., Verma, M., Guo, S., Zhang, R., & Kambhampati, S. (2020). Explanation\naugmented feedback in human-in-the-loop reinforcement learning. _arXiv preprint_\n_arXiv:2006.14804v3_ .\nGunning, D., & Aha, D. W. (2019). Darpa’s explainable artificial intelligence program. _AI_\n_Magazine_, _40_ (2), 44–58. https://doi.org/10.1609/aimag.v40i2.2850\nGuo, W., Wu, X., Khan, U., & Xing, X. Edge: Explaining deep reinforcement learning policies (M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, & J. W. Vaughan, Eds.).\nIn: _Advances in neural information processing systems_ (M. Ranzato, A. Beygelzimer,\n\n\n404\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nY. Dauphin, P. Liang, & J. W. Vaughan, Eds.). Ed. by Ranzato, M., Beygelzimer,\nA., Dauphin, Y., Liang, P., & Vaughan, J. W. _34_ . 2021, 12222–12236.\nGuo, Z., Yao, C., Feng, Y., & Xu, Y. (2022). Survey of reinforcement learning based on\nhuman prior knowledge. _Journal of Uncertain Systems_, _15_ (01), 2230001. https://\ndoi.org/10.1142/S1752890922300011\nGupta, P., Puri, N., Verma, S., Kayastha, D., Deshmukh, S., Krishnamurthy, B., & Singh,\nS. (2019). Explain your move: Understanding agent actions using focused feature\nsaliency. _arXiv preprint arXiv:1912.12191v2_ .\nHa, D., & Schmidhuber, J. (2018). World models. _arXiv preprint arXiv:1803.10122_ . https:\n//doi.org/10.5281/ZENODO.1207631\nHabibian, S., Jonnavittula, A., & Losey, D. P. (2021). Here’s what i’ve learned: Asking\nquestions that reveal reward learning. _arXiv preprint arXiv:2107.01995_ .\nHasanbeig, M., Jeppu, N. Y., Abate, A., Melham, T., & Kroening, D. Deepsynth: Automata\nsynthesis for automatic task segmentation in deep reinforcement learning. In: In _The_\n_thirty-fifth {aaai} conference on artificial intelligence,{aaai}_ . _2_ . 2021, 36.\nHayes, B., & Shah, J. A. Improving robot controller transparency through autonomous\npolicy explanation. In: In _2017 12th acm/ieee international conference on human-_\n_robot interaction (hri_ . IEEE. 2017, 303–312. https://doi.org/10.1145/2909824.\n3020233.\n\nHazan, E., Kakade, S., Singh, K., & Van Soest, A. Provably efficient maximum entropy\nexploration (K. Chaudhuri & R. Salakhutdinov, Eds.). In: _Proceedings of the 36th_\n_international conference on machine learning_ (K. Chaudhuri & R. Salakhutdinov,\nEds.). Ed. by Chaudhuri, K., & Salakhutdinov, R. _97_ . Proceedings of Machine Learning Research. PMLR. 2019, 2681–2691.\nHein, D., Hentschel, A., Runkler, T., & Udluft, S. (2017). Particle swarm optimization for\ngenerating interpretable fuzzy reinforcement learning policies. _Engineering Applica-_\n_tions of Artificial Intelligence_, _65_, 87–98. https://doi.org/10.1016/j.engappai.2017.\n07.005\nHein, D., Udluft, S., & Runkler, T. A. (2018). Interpretable policies for reinforcement learning by genetic programming. _Engineering Applications of Artificial Intelligence_, _76_,\n158–169. https://doi.org/10.1016/j.engappai.2018.09.007\nHein, D., Udluft, S., & Runkler, T. A. Generating interpretable reinforcement learning policies using genetic programming. In: In _Proceedings of the genetic and evolutionary_\n_computation conference companion_ . Association for Computing Machinery, 2019,\n23–24. https://doi.org/10.1145/3319619.3326755.\nHermann, T., Hunt, A., & Neuhoff, J. G. (2011). _The sonification handbook_ . Logos Verlag\nBerlin.\n\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Horgan, D., Quan,\nJ., Sendonaris, A., Osband, I., Dulac-Arnold, G., Agapiou, J., Leibo, J., & Gruslys,\nA. Deep q-learning from demonstrations. In: In _Proceedings of the aaai conference_\n_on artificial intelligence_ . _32_ . 2018. https://doi.org/10.1609/aaai.v32i1.11757.\nHeuillet, A., Couthouis, F., & D´ıaz-Rodr´ıguez, N. (2021). Explainability in deep reinforcement learning. _Knowledge-Based Systems_, _214_, 106685. https://doi.org/10.1016/j.\nknosys.2020.106685\n\n\n405\n\n\n\n\nRetzlaff et al.\n\n\nHolzinger, A. The next frontier: Ai we can really trust (M. Kamp, Ed.). In: _Proceedings of_\n_the ecml pkdd 2021, ccis 1524_ (M. Kamp, Ed.). Ed. by Kamp, M. Cham: Springer\nNature, 2021, pp. 427–440. https://doi.org/10.1007/978-3-030-93736-2 ~~3~~ 3.\nHolzinger, A., Carrington, A., & Mueller, H. (2020). Measuring the quality of explanations:\nThe system causability scale (scs). comparing human and machine explanations. _KI_\n\n_- K¨unstliche Intelligenz (German Journal of Artificial intelligence), Special Issue on_\n_Interactive Machine Learning, Edited by Kristian Kersting, TU Darmstadt_, _34_ (2),\n193–198. https://doi.org/10.1007/s13218-020-00636-z\nHolzinger, A., Dehmer, M., Emmert-Streib, F., Cucchiara, R., Augenstein, I., Del Ser, J.,\nSamek, W., Jurisica, I., & D´ıaz-Rodr´ıguez, N. (2022a). Information fusion as an\nintegrative cross-cutting enabler to achieve robust, explainable, and trustworthy\nmedical artificial intelligence. _Information Fusion_, _79_ (3), 263–278. https://doi.org/\n10.1016/j.inffus.2021.10.007\nHolzinger, A., Malle, B., Saranti, A., & Pfeifer, B. (2021). Towards multi-modal causability\nwith graph neural networks enabling information fusion for explainable ai. _Informa-_\n_tion Fusion_, _71_ (7), 28–37. https://doi.org/10.1016/j.inffus.2021.01.008\nHolzinger, A., & M¨uller, H. (2021). Toward Human–AI Interfaces to Support Explainability\nand Causability in Medical AI [Conference Name: Computer]. _Computer_, _54_ (10),\n78–86. https://doi.org/10.1109/MC.2021.3092610\nHolzinger, A., Saranti, A., Angerschmid, A., Retzlaff, C. O., Gronauer, A., Pejakovic, V.,\nMedel, F., Krexner, T., Gollob, C., & Stampfer, K. (2022b). Digital transformation\nin smart farm and forest operations needs human-centered ai: Challenges and future\ndirections. _Sensors_, _22_ (8), 3043. https://doi.org/10.3390/s22083043\nHolzinger, A., Saranti, A., Molnar, C., Biececk, P., & Samek, W. Explainable ai methods\n\n    - a brief overview. In: In _Xxai - lecture notes in artificial intelligence lnai 13200_ .\nSpringer, 2022. https://doi.org/10.1007/978-3-031-04083-2 ~~2~~ .\nHolzinger, A., Stampfer, K., Nothdurft, A., Gollob, C., & Kieseberg, P. (2022d). Challenges\nin artificial intelligence for smart forestry. _European Research Consortium for In-_\n_formatics and Mathematics (ERCIM) News_, _130_ (July), 40–41.\nHussein, A., Gaber, M. M., Elyan, E., & Jayne, C. (2017). Imitation Learning: A Survey of\nLearning Methods [Place: New York, NY, USA Publisher: Association for Computing Machinery]. _ACM Computing Surveys_, _50_ (2). https://doi.org/10.1145/3054912\nIbarz, J., Tan, J., Finn, C., Kalakrishnan, M., Pastor, P., & Levine, S. (2021). How to\ntrain your robot with deep reinforcement learning: Lessons we have learned. _The_\n_International Journal of Robotics Research_, _40_ (4-5), 698–721. https://doi.org/10.\n1177/0278364920987859\nIcarte, R. T., Klassen, T., Valenzano, R., & McIlraith, S. Using reward machines for highlevel task specification and decomposition in reinforcement learning (J. Dy & A.\nKrause, Eds.). In: _Proceedings of the 35th international conference on machine learn-_\n_ing_ (J. Dy & A. Krause, Eds.). Ed. by Dy, J., & Krause, A. _80_ . Proceedings of\nMachine Learning Research. PMLR. 2018, 2107–2116.\nIcarte, R. T., Waldie, E., Klassen, T., Valenzano, R., Castro, M., & McIlraith, S. Learning reward machines for partially observable reinforcement learning (H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, & R. Garnett, Eds.). In:\n_Advances in neural information processing systems_ (H. Wallach, H. Larochelle, A.\n\n\n406\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nBeygelzimer, F. d'Alch´e-Buc, E. Fox, & R. Garnett, Eds.). Ed. by Wallach, H.,\nLarochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., & Garnett, R. _32_ . 2019.\nJain, M., Lahlou, S., Nekoei, H., Butoi, V., Bertin, P., Rector-Brooks, J., Korablyov, M.,\n& Bengio, Y. (2021). DEUP: direct epistemic uncertainty prediction. _arXiv preprint_\n_arXiv:2102.08501_ .\n\nJiang, Y., Bharadwaj, S., Wu, B., Shah, R., Topcu, U., & Stone, P. Temporal-logic-based\nreward shaping for continuing reinforcement learning tasks. In: In _Proceedings of the_\n_35th aaai conference on artificial intelligence_ . 2021.\nJiang, Z., & Luo, S. Neural logic reinforcement learning (K. Chaudhuri & R. Salakhutdinov,\nEds.). In: _Proceedings of the 36th international conference on machine learning_ (K.\nChaudhuri & R. Salakhutdinov, Eds.). Ed. by Chaudhuri, K., & Salakhutdinov, R.\n_97_ . Proceedings of Machine Learning Research. PMLR. 2019, 3110–3119.\nKaralus, J., & Lindner, F. (2021). Accelerating the convergence of human-in-theloop reinforcement learning with counterfactual explanations. _arXiv preprint_\n\n_arXiv:2108.01358_ .\nKartoun, U., Stern, H., & Edan, Y. (2010). A human-robot collaborative reinforcement\nlearning algorithm. _Journal of Intelligent & Robotic Systems_, _60_ (2), 217–239. https:\n//doi.org/10.1007/s10846-010-9422-y\nKashyap, S., Karargyris, A., Wu, J., Gur, Y., Sharma, A., Wong, K. C. L., Moradi, M.,\n& Syeda-Mahmood, T. Looking in the Right Place for Anomalies: Explainable Ai\nThrough Automatic Location Learning [ISSN: 1945-8452]. In: In _2020 IEEE 17th_\n_International Symposium on Biomedical Imaging (ISBI)_ . ISSN: 1945-8452. 2020,\nApril, 1125–1129. https://doi.org/10.1109/ISBI45749.2020.9098370.\nKeane, M. T., Kenny, E. M., Delaney, E., & Smyth, B. (2021, February). If Only We Had\nBetter Counterfactual Explanations: Five Key Deficits to Rectify in the Evaluation\nof Counterfactual XAI Techniques [arXiv:2103.01035 [cs]]. https : / / doi . org / 10 .\n48550/arXiv.2103.01035\nKhatib, O., Yokoi, K., Brock, O., Chang, K., & Casal, A. Robots in human environments.\nIn: In _Proceedings of the first workshop on robot motion and control. romoco’99_\n_(cat. no.99ex353)_ . IEEE, 1999, 213–221. https://doi.org/10.1109/ROMOCO.1999.\n791078.\n\nKnox, W. B., & Stone, P. Tamer: Training an agent manually via evaluative reinforcement.\nIn: In _2008 7th ieee international conference on development and learning_ . IEEE,\n2008, 292–297. https://doi.org/10.1109/DEVLRN.2008.4640845.\nKnox, W. B., & Stone, P. Combining manual feedback with subsequent mdp reward signals\nfor reinforcement learning. In: In _9th international conference on autonomous agents_\n_and multiagent systems (aamas) 2010_ . 2010, 5–12.\nKnox, W. B., & Stone, P. Reinforcement learning from simultaneous human and mdp reward. In: In _11th international conference on autonomous agents and multiagent_\n_systems (aamas) 2012_ . _1004_ . Valencia. 2012, 475–482.\nKnox, W. B., Stone, P., & Breazeal, C. Training a robot via human feedback: A case study\n(G. Herrmann, M. J. Pearson, A. Lenz, P. Bremner, A. Spiers, & U. Leonards, Eds.).\nIn: _International conference on social robotics_ (G. Herrmann, M. J. Pearson, A.\nLenz, P. Bremner, A. Spiers, & U. Leonards, Eds.). Ed. by Herrmann, G., Pearson,\n\n\n407\n\n\n\n\nRetzlaff et al.\n\n\nM. J., Lenz, A., Bremner, P., Spiers, A., & Leonards, U. Springer. 2013, 460–470.\nhttps://doi.org/10.1007/978-3-319-02675-6 ~~4~~ 6.\nKober, J., Bagnell, J. A., & Peters, J. (2013). Reinforcement learning in robotics: A survey.\n_The International Journal of Robotics Research_, _32_ (11), 1238–1274. https://doi.\norg/10.1177/0278364913495721\nKoditschek, D. E. (2021). What is robotics? why do we need it and how can we get it?\n_Annual Review of Control, Robotics, and Autonomous Systems_, _4_ (1), 1–33. https:\n//doi.org/10.1146/annurev-control-080320-011601\nKulick, J., Toussaint, M., Lang, T., & Lopes, M. Active learning for teaching a robot\ngrounded relational symbols. In: In _Proceedings of the twenty-third international_\n_joint conference on artificial intelligence_ . IJCAI ’13. Beijing, China: AAAI Press,\n2013, 1451–1457.\nKulkarni, A., & Gkountouna, O. (2021). Demonstrating react: A real-time educational aipowered classroom tool. _arXiv preprint arXiv:2108.07693_ .\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman, S. J. (2017). Building machines\nthat learn and think like people [Publisher: Cambridge University Press]. _Behavioral_\n_and Brain Sciences_, _40_, e253. https://doi.org/10.1017/S0140525X16001837\nLambert, N., Castricato, L., von Werra, L., & Havrilla, A. (2022). Illustrating Reinforcement\nLearning from Human Feedback (RLHF). _Hugging Face Blog_ .\nLe, H., Jiang, N., Agarwal, A., Dudik, M., Yue, Y., & Daum´e III, H. Hierarchical imitation\nand reinforcement learning (J. Dy & A. Krause, Eds.). In: _Proceedings of the 35th_\n_international conference on machine learning_ (J. Dy & A. Krause, Eds.). Ed. by\nDy, J., & Krause, A. _80_ . Proceedings of Machine Learning Research. PMLR, 2018,\n\n2917–2926.\nLee, K., Smith, L., & Abbeel, P. (2021). Pebble: Feedback-efficient interactive reinforcement\nlearning via relabeling experience and unsupervised pre-training. _arXiv preprint_\n\n_arXiv:2106.05091_ .\nLi, B., Fang, G., Yang, Y., Wang, Q., Ye, W., Zhao, W., & Zhang, S. (2023, April). Evaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness [arXiv:2304.11633 [cs]]. https:\n//doi.org/10.48550/arXiv.2304.11633\nLi, G., Gomez, R., Nakamura, K., & He, B. (2019). Human-centered reinforcement learning:\nA survey. _IEEE Transactions on Human-Machine Systems_, _49_ (4), 337–349. https:\n//doi.org/10.1109/THMS.2019.2912447\nLi, Y. (2017). Deep reinforcement learning: An overview. _arXiv preprint arXiv:1701.07274_ .\nLiang, H., Yang, L., Cheng, H., Tu, W., & Xu, M. Human-in-the-loop reinforcement learning.\nIn: In _2017 chinese automation congress (cac)_ . 2017, 4511–4518. https://doi.org/\n10.1109/CAC.2017.8243575.\nLikmeta, A., Metelli, A. M., Tirinzoni, A., Giol, R., Restelli, M., & Romano, D. (2020).\nCombining reinforcement learning with rule-based controllers for transparent and\ngeneral decision-making in autonomous driving. _Robotics and Autonomous Systems_,\n_131_, 103568. https://doi.org/10.1016/j.robot.2020.103568\nLin, J., Ma, Z., Gomez, R., Nakamura, K., He, B., & Li, G. (2020). A review on interactive\nreinforcement learning from human social feedback. _IEEE Access_, _8_, 120757–120765.\nhttps://doi.org/10.1109/ACCESS.2020.3006254\n\n\n408\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nLiu, G., Schulte, O., Zhu, W., & Li, Q. Toward interpretable deep reinforcement learning\nwith linear model u-trees. In: In _Joint european conference on machine learning and_\n_knowledge discovery in databases_ . Springer. 2018, 414–429. https://doi.org/10.1007/\n978-3-030-10928-8 ~~2~~ 5.\nLiu, H., & Abbeel, P. (2020). Unsupervised active pre-training for reinforcement learning\n\n[Paper was rejected due to lack of novelty]. _ICLR 2021_ .\nLiu, Y., Mishra, N., Sieb, M., Shentu, Y., Abbeel, P., & Chen, X. (2022, October). Autoregressive Uncertainty Modeling for 3D Bounding Box Prediction [arXiv:2210.07424\n\n[cs]]. https://doi.org/10.48550/arXiv.2210.07424\nLiu, Z., Guo, Y., & Mahmud, J. (2021). When and why does a model fail? a humanin-the-loop error detection framework for sentiment analysis. _arXiv preprint_\n_arXiv:2106.00954_ .\nLutjens, B., Everett, M., & How, J. P. (2018). Safe reinforcement learning with model\nuncertainty estimates. _arXiv preprint arXiv:1810.08700_ .\nLyu, D., Yang, F., Liu, B., & Gustafson, S. Sdrl: Interpretable and data-efficient deep\nreinforcement learning leveraging symbolic planning. In: In _Proceedings of the aaai_\n_conference on artificial intelligence_ . _33_ . 2019, 2970–2977. https://doi.org/10.1609/\naaai.v33i01.33012970.\n\nMacGlashan, J., Ho, M. K., Loftin, R., Peng, B., Wang, G., Roberts, D. L., Taylor, M. E.,\n& Littman, M. L. Interactive learning from policy-dependent human feedback (D.\nPrecup & Y. W. Teh, Eds.). In: _Proceedings of the 34th international conference on_\n_machine learning_ (D. Precup & Y. W. Teh, Eds.). Ed. by Precup, D., & Teh, Y. W.\n_70_ . Proceedings of Machine Learning Research. PMLR, 2017, 2285–2294.\nMadumal, P., Miller, T., Sonenberg, L., & Vetere, F. (2020a). Distal explanations for modelfree explainable reinforcement learning. _arXiv preprint arXiv:2001.10284_ .\nMadumal, P., Miller, T., Sonenberg, L., & Vetere, F. Explainable reinforcement learning\nthrough a causal lens. In: In _Proceedings of the aaai conference on artificial intelli-_\n_gence_ . _34_ . 2020, 2493–2500. https://doi.org/10.1609/aaai.v34i03.5631.\nMandel, T., Liu, Y.-E., Brunskill, E., & Popovi´c, Z. Where to add actions in human-in-theloop reinforcement learning. In: In _Proceedings of the thirty-first aaai conference on_\n_artificial intelligence_ . AAAI’17. San Francisco, California, USA, 2017, 2322––2328.\nMao, H., Chen, S., Dimmery, D., Singh, S., Blaisdell, D., Tian, Y., Alizadeh, M., & Bakshy, E. (2020, August). Real-world Video Adaptation with Reinforcement Learning\n\n[arXiv:2008.12858 [cs]]. https://doi.org/10.48550/arXiv.2008.12858\nMart´ınez, D., Alenya, G., & Torras, C. (2017). Relational reinforcement learning with guided\ndemonstrations [Special Issue on AI and Robotics]. _Artificial Intelligence_, _247_, 295–\n312. https://doi.org/10.1016/j.artint.2015.02.006\nMart´ınez, D., Alenya, G., Torras, C., Ribeiro, T., & Inoue, K. Learning relational dynamics\nof stochastic domains for planning. In: In _Proceedings of the international conference_\n_on automated planning and scheduling_ . _26_ . 2016, 235–243.\nMathewson, K. W., & Pilarski, P. M. (2022). A brief guide to designing and evaluating\nhuman-centered interactive machine learning. _arXiv preprint arXiv:2204.09622_ .\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A Survey\non Bias and Fairness in Machine Learning [Publisher: ACM New York, NY, USA].\n_ACM Computing Surveys (CSUR)_, _54_ (6), 1–35. https://doi.org/10.1145/3457607\n\n\n409\n\n\n\n\nRetzlaff et al.\n\n\nMikhaylov, M. N., & Lositskii, I. A. Control and navigation of forest robot. In: In _2018_\n_25th Saint Petersburg International Conference on Integrated Navigation Systems_\n_(ICINS)_ . 2018, May, 1–2. https://doi.org/10.23919/ICINS.2018.8405881.\nMilani, S., Topin, N., Veloso, M., & Fang, F. (2022). A survey of explainable reinforcement\nlearning. _arXiv preprint arXiv:2202.08434_ .\nMiller, T. (2019a). Explanation in artificial intelligence: Insights from the social sciences.\n_Artificial Intelligence_, _267_, 1–38. https://doi.org/10.1016/j.artint.2018.07.007\nMiller, T. (2019b). Explanation in artificial intelligence: Insights from the social sciences.\n_Artificial intelligence_, _267_, 1–38. https://doi.org/10.1016/j.artint.2018.07.007\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing atari with deep reinforcement learning. _arXiv preprint_\n_arXiv:1312.5602_ .\nMowshowitz, A., Tominaga, A., & Hayashi, E. (2018). Robot Navigation in Forest Management. _Journal of Robotics and Mechatronics_, _30_ (2), 223–230. https://doi.org/10.\n20965/jrm.2018.p0223\nNair, A., McGrew, B., Andrychowicz, M., Zaremba, W., & Abbeel, P. Overcoming exploration in reinforcement learning with demonstrations. In: In _2018 ieee interna-_\n_tional conference on robotics and automation (icra)_ . IEEE. 2018, 6292–6299. https:\n//doi.org/10.1109/ICRA.2018.8463162.\nNascimento, E. d. S., Ahmed, I., Oliveira, E., Palheta, M. P., Steinmacher, I., & Conte, T.\nUnderstanding Development Process of Machine Learning Systems: Challenges and\nSolutions [ISSN: 1949-3789]. In: In _2019 ACM/IEEE International Symposium on_\n_Empirical Software Engineering and Measurement (ESEM)_ . ISSN: 1949-3789. 2019,\nSeptember, 1–6. https://doi.org/10.1109/ESEM.2019.8870157.\nNauta, M., Trienes, J., Pathak, S., Nguyen, E., Peters, M., Schmitt, Y., Schl¨otterer, J.,\nvan Keulen, M., & Seifert, C. (2022, May). From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI\n\n[arXiv:2201.08164 [cs]]. Retrieved December 6, 2022, from http://arxiv.org/abs/\n2201.08164\n\nNg, A. Y., Harada, D., & Russell, S. J. Policy invariance under reward transformations: Theory and application to reward shaping. In: In _Proceedings of the sixteenth interna-_\n_tional conference on machine learning_ . _99_ . ICML ’99. Morgan Kaufmann Publishers\nInc., 1999, 278–287.\nNguyen, H., & La, H. Review of deep reinforcement learning for robot manipulation. In:\nIn _2019 third ieee international conference on robotic computing (irc)_ . IEEE, 2019,\n590–595. https://doi.org/10.1109/IRC.2019.00120.\nOpenAI, Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi,\nD., Fischer, Q., Hashme, S., Hesse, C., J´ozefowicz, R., Gray, S., Olsson, C., Pachocki,\nJ., Petrov, M., Pinto, H. P. d. O., Raiman, J., Salimans, T., . . . Zhang, S. (2019, December). Dota 2 with Large Scale Deep Reinforcement Learning [arXiv:1912.06680\n\n[cs, stat]]. Retrieved July 13, 2023, from http://arxiv.org/abs/1912.06680\nParisi, S., Rajeswaran, A., Purushwalkam, S., & Gupta, A. The unsurprising effectiveness\nof pre-trained vision models for control. In: In _International conference on machine_\n_learning, ICML 2022, 17-23 july 2022, baltimore, maryland, USA_ . PMLR, 2022,\n17359–17371. https://proceedings.mlr.press/v162/parisi22a.html\n\n\n410\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nPearl, J. (2009). _Causality: Models, reasoning, and inference (2nd edition)_ . Cambridge University Press.\nPearl, J., et al. (2000). Models, reasoning and inference. _Cambridge, UK: CambridgeUni-_\n_versityPress_, _19_, 2.\nPeng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z.,\nChen, W., & Gao, J. (2023, March). Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback\n\n[arXiv:2302.12813 [cs]]. https://doi.org/10.48550/arXiv.2302.12813\nPeng, B., MacGlashan, J., Loftin, R., Littman, M. L., Roberts, D. L., & Taylor, M. E.\nA Need for Speed: Adapting Agent Action Speed to Improve Task Learning from\nNon-Expert Humans. In: In _Proceedings of the 2016 International Conference on_\n_Autonomous Agents and Multiagent Systems ( AAMAS )_ . International Foundation\nfor Autonomous Agents; Multiagent Systems, 2016, 957–965.\nPenkov, S., & Ramamoorthy, S. Learning programmatically structured representations with\nperceptor gradients. In: In _Proceedings of the 7th international conference on learn-_\n_ing representations_ . OpenReview.net, 2019. https://openreview.net/forum?id=\nSJggZnRcFQ\nPopova, M., Isayev, O., & Tropsha, A. (2018). Deep reinforcement learning for de novo drug\ndesign. _Science advances_, _4_ (7), eaap7885. https://doi.org/10.1126/sciadv.aap7885\nPuiutta, E., & Veith, E. M. Explainable reinforcement learning: A survey. In: In _Inter-_\n_national cross-domain conference for machine learning and knowledge extraction_ .\nSpringer. 2020, 77–95. https://doi.org/doi.org/10.1007/978-3-030-57321-8 ~~5~~ .\nRagunath, P., Velmourougan, S, Davachelvan, P, Kayalvizhi, S, & Ravimohan, R. (2010).\nEvolving A New Model (SDLC Model-2010) For Software Development Life Cycle\n(SDLC). _International Journal of Computer Science and Network Security_, 8.\nRay, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key\nchallenges, bias, ethics, limitations and future scope. _Internet of Things and Cyber-_\n_Physical Systems_, _3_, 121–154. https://doi.org/10.1016/j.iotcps.2023.04.003\nRodriguez, A., Tabassum, A., Cui, J., Xie, J., Ho, J., Agarwal, P., Adhikari, B., & Prakash,\nB. A. (2021). Deepcovid: An operational deep learning-driven framework for explainable real-time covid-19 forecasting. _medRxiv_, 2020–09. https://doi.org/10.\n1101/2020.09.28.20203109\nRoy, N., Posner, I., Barfoot, T., Beaudoin, P., Bengio, Y., Bohg, J., Brock, O., Depatie, I.,\nFox, D., Koditschek, D., et al. (2021). From machine learning to robotics: Challenges\nand opportunities for embodied intelligence. _arXiv preprint arXiv:2110.15245_ .\nSaba, W. S. (2023, May). Towards Explainable and Language-Agnostic LLMs: Symbolic\nReverse Engineering of Language at Scale [arXiv:2306.00017 [cs]]. https://doi.org/\n10.48550/arXiv.2306.00017\nSaranti, A., Eckel, G., & Pirr´o, D. Quantum harmonic oscillator sonification (S. Ystad,\nM. Aramaki, R. Kronland-Martinet, & K. Jensen, Eds.). In: _Auditory display_ (S.\nYstad, M. Aramaki, R. Kronland-Martinet, & K. Jensen, Eds.). Ed. by Ystad, S.,\nAramaki, M., Kronland-Martinet, R., & Jensen, K. Springer, 2009, pp. 184–201.\nhttps://doi.org/10.1007/978-3-642-12439-6 ~~1~~ 0.\nSaranti, A., Taraghi, B., Ebner, M., & Holzinger, A. Insights into learning competence\nthrough probabilistic graphical models (A. Holzinger, P. Kieseberg, A. M. Tjoa, &\n\n\n411\n\n\n\n\nRetzlaff et al.\n\n\nE. Weippl, Eds.). In: _International cross-domain conference for machine learning_\n_and knowledge extraction_ (A. Holzinger, P. Kieseberg, A. M. Tjoa, & E. Weippl,\nEds.). Ed. by Holzinger, A., Kieseberg, P., Tjoa, A. M., & Weippl, E. Springer.\nSpringer, 2019, 250–271. https://doi.org/10.1007/978-3-030-29726-8 ~~1~~ 6.\nSchneeberger, D., Stoeger, K., & Holzinger, A. The european legal framework for medical\nai. In: In _International cross-domain conference for machine learning and knowledge_\n_extraction, springer lncs 12279_ . Springer, 2020, pp. 209–226. https://doi.org/10.\n1007/978-3-030-57321-8 12.\nScurto, H., Kerrebroeck, B. V., Caramiaux, B., & Bevilacqua, F. (2021). Designing deep\nreinforcement learning for human parameter exploration. _ACM Transactions on_\n_Computer-Human Interaction (TOCHI)_, _28_ (1), 1–35. https://doi.org/10.1145/\n3414472\n\nSerradilla, O., Zugasti, E., Cernuda, C., Aranburu, A., de Okariz, J. R., & Zurutuza, U.\nInterpreting Remaining Useful Life estimations combining Explainable Artificial Intelligence and domain knowledge in industrial machinery [ISSN: 1558-4739]. In: In\n_2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)_ . ISSN: 15584739. 2020, July, 1–8. https://doi.org/10.1109/FUZZ48607.2020.9177537.\nShneiderman, B., Plaisant, C., Cohen, M. S., Jacobs, S., Elmqvist, N., & Diakopoulos,\nN. (2016). _Designing the user interface: Strategies for effective human-computer_\n_interaction_ . Pearson.\n\nSilva, A., Gombolay, M., Killian, T., Jimenez, I., & Son, S.-H. Optimization methods for\ninterpretable differentiable decision trees applied to reinforcement learning (S. Chiappa & R. Calandra, Eds.). In: _Proceedings of the twenty third international con-_\n_ference on artificial intelligence and statistics_ (S. Chiappa & R. Calandra, Eds.).\nEd. by Chiappa, S., & Calandra, R. PMLR. 2020, 1855–1865.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert,\nT., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui, F., Sifre, L., van\nden Driessche, G., Graepel, T., & Hassabis, D. (2017). Mastering the game of Go\nwithout human knowledge [Number: 7676 Publisher: Nature Publishing Group].\n_Nature_, _550_ (7676), 354–359. https://doi.org/10.1038/nature24270\nSmith-Renner, A., Fan, R., Birchfield, M., Wu, T., Boyd-Graber, J., Weld, D. S., & Findlater, L. No Explainability without Accountability: An Empirical Study of Explanations and Feedback in Interactive ML. In: In _Proceedings of the 2020 CHI Conference_\n_on Human Factors in Computing Systems_ . CHI ’20. New York, NY, USA: Association for Computing Machinery, 2020, April, 1–13. https://doi.org/10.1145/3313831.\n3376624.\nSong, W., Duan, Z., Yang, Z., Zhu, H., Zhang, M., & Tang, J. (2019). Explainable knowledge graph-based recommendation via deep reinforcement learning. _arXiv preprint_\n\n_arXiv:1906.09506_ .\nSreedharan, S., Kulkarni, A., & Kambhampati, S. (2022). Explainable Human–AI Interaction: A Planning Perspective [Publisher: Morgan & Claypool Publishers]. _Synthe-_\n_sis Lectures on Artificial Intelligence and Machine Learning_, _16_ (1), 1–184. https:\n//doi.org/10.2200/S01152ED1V01Y202111AIM050\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D.,\n& Christiano, P. F. Learning to summarize with human feedback (H. Larochelle, M.\n\n\n412\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\nRanzato, R. Hadsell, M. F. Balcan, & H. Lin, Eds.). In: _Advances in Neural Infor-_\n_mation Processing Systems_ (H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,\n& H. Lin, Eds.). Ed. by Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., &\nLin, H. _33_ . Curran Associates, Inc., 2020, 3008–3021. https://proceedings.neurips.\ncc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf\nStoeger, K., Schneeberger, D., & Holzinger, A. (2021). Medical artificial intelligence: The\neuropean legal perspective. _Communications of the ACM_, _64_ (11), 34–36. https :\n//doi.org/10.1145/3458652\nSun, S.-H., Wu, T.-L., & Lim, J. J. Program guided agent. In: In _International conference_\n_on learning representations_ . 2019.\nSurmann, H., Jestel, C., Marchel, R., Musberg, F., Elhadj, H., & Ardani, M. (2020, May).\nDeep Reinforcement learning for real autonomous mobile robot navigation in indoor\nenvironments [arXiv:2005.13857 [cs]]. https://doi.org/10.48550/arXiv.2005.13857\nSutton, R. S., & Barto, A. G. (2018). _Reinforcement learning: An introduction_ . MIT press.\nTabrez, A., & Hayes, B. Improving human-robot interaction through explainable reinforcement learning. In: In _2019 14th acm/ieee international conference on human-robot_\n_interaction (hri)_ . IEEE. 2019, 751–753. https://doi.org/10.1109/HRI.2019.8673198.\nTaylor, M. E., & Stone, P. (2009). Transfer learning for reinforcement learning domains: A\nsurvey. _Journal of Machine Learning Research_, _10_ (7), 1633–1685.\nTaylor, M. E., Suay, H. B., & Chernova, S. Integrating reinforcement learning with human\ndemonstrations of varying ability. In: In _The 10th international conference on au-_\n_tonomous agents and multiagent systems-volume 2_ . AAMAS ’11. Citeseer. Richland,\nSC: International Foundation for Autonomous Agents; Multiagent Systems, 2011,\n\n617–624.\n\nThomaz, A. L., & Breazeal, C. Reinforcement learning with human teachers: Evidence of\nfeedback and guidance with implications for learning performance. In: In _Aaai_ . _6_ .\nBoston, MA. 2006, 1000–1005.\nTickle, A., Andrews, R., Golea, M., & Diederich, J. (1998). The truth will come to light:\nDirections and challenges in extracting the knowledge embedded within trained\nartificial neural networks. _IEEE Transactions on Neural Networks_, _9_ (6), 1057–1068.\nhttps://doi.org/10.1109/72.728352\nTomar, M., Mishra, U. A., Zhang, A., & Taylor, M. E. (2021). Learning representations for\npixel-based control: What matters and why? _arXiv preprint arXiv:2111.07775_ .\nTopin, N., Milani, S., Fang, F., & Veloso, M. Iterative bounding mdps: Learning interpretable policies via non-interpretable methods. In: In _Proceedings of the aaai con-_\n_ference on artificial intelligence_ . _35_ . 2021, 9923–9931.\nTorrey, L., & Taylor, M. Teaching on a budget: Agents advising agents in reinforcement\nlearning. In: In _Proceedings of the 2013 international conference on autonomous_\n_agents and multi-agent systems_ . Richland, SC: International Foundation for Autonomous Agents; Multiagent Systems, 2013, 1053–1060. https://doi.org/10.5555/\n2484920.2485086.\n\nVecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Roth¨orl,\nT., Lampe, T., & Riedmiller, M. (2017). Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. _arXiv preprint_\n\n_arXiv:1707.08817_ .\n\n\n413\n\n\n\n\nRetzlaff et al.\n\n\nVerma, A., Le, H., Yue, Y., & Chaudhuri, S. Imitation-projected programmatic reinforcement learning (H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, &\nR. Garnett, Eds.). In: _Advances in neural information processing systems_ (H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, & R. Garnett, Eds.).\nEd. by Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., &\nGarnett, R. _32_ . Curran Associates, Inc., 2019.\nVerma, A., Murali, V., Singh, R., Kohli, P., & Chaudhuri, S. Programmatically interpretable\nreinforcement learning (J. Dy & A. Krause, Eds.). In: _Proceedings of the 35th inter-_\n_national conference on machine learning_ (J. Dy & A. Krause, Eds.). Ed. by Dy, J.,\n& Krause, A. _80_ . Proceedings of Machine Learning Research. PMLR, 2018, 5045–\n5054. https://proceedings.mlr.press/v80/verma18a.html\nVu, M. N., & Thai, M. T. (2020). Pgm-explainer: Probabilistic graphical model explanations\nfor graph neural networks. _arXiv preprint arXiv:2010.05788_ .\nWang, X., Lee, K., Hakhamaneshi, K., Abbeel, P., & Laskin, M. Skill preferences: Learning\nto extract and execute robotic skills from human feedback. In: In _Conference on_\n_robot learning_ . PMLR. 2022, 1259–1268.\nWells, L., & Bednarz, T. (2021a). Explainable ai and reinforcement learning — a systematic\nreview of current approaches and trends. _Frontiers in artificial intelligence_, _4_, 48.\nhttps://doi.org/10.3389/frai.2021.550030\nWells, L., & Bednarz, T. (2021b). Explainable AI and Reinforcement Learning—A Systematic Review of Current Approaches and Trends. _Frontiers in Artificial Intelligence_,\n_4_ . Retrieved July 15, 2023, from https://www.frontiersin.org/articles/10.3389/frai.\n2021.550030\nWu, J., Huang, Z., Huang, C., Hu, Z., Hang, P., Xing, Y., & Lv, C. (2021a). Human-inthe-loop deep reinforcement learning with application to autonomous driving. _arXiv_\n_preprint arXiv:2104.07246_ .\nWu, X., Xiao, L., Sun, Y., Zhang, J., Ma, T., & He, L. (2021b). A survey of human-in-theloop for machine learning. _arXiv preprint arXiv:2108.00941_ .\nXin, D., Ma, L., Liu, J., Macke, S., Song, S., & Parameswaran, A. Accelerating human-inthe-loop machine learning: Challenges and opportunities. In: In _Proceedings of the_\n_second workshop on data management for end-to-end machine learning_ . New York,\nNY, USA: Association for Computing Machinery, 2018, 1–4. https://doi.org/10.\n1145/3209889.3209897.\nXiong, Z., Eappen, J., Zhu, H., & Jagannathan, S. (2020). Robustness to adversarial attacks\nin learning-enabled controllers. _arXiv preprint arXiv:2006.06861_ .\nXu, W., Wang, D., Pan, L., Song, Z., Freitag, M., Wang, W. Y., & Li, L. (2023, May).\nINSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback [arXiv:2305.14282 [cs]]. https://doi.org/10.48550/arXiv.2305.14282\nYang, S., Nachum, O., Du, Y., Wei, J., Abbeel, P., & Schuurmans, D. (2023, March).\nFoundation Models for Decision Making: Problems, Methods, and Opportunities\n\n[arXiv:2303.04129 [cs]]. https://doi.org/10.48550/arXiv.2303.04129\nYang, T., Hao, J., Meng, Z., Zhang, Z., Hu, Y., Chen, Y., Fan, C., Wang, W., Liu, W., Wang,\nZ., & Peng, J. Efficient deep reinforcement learning via adaptive policy transfer. In:\nIn _Proceedings of the twenty-ninth international conference on international joint_\n\n\n414\n\n\n\n\nHuman-in-the-Loop Reinforcement Learning\n\n\n_conferences on artificial intelligence_ . 2021, 3094–3100. https://doi.org/10.24963/\nijcai.2020/428.\nZagal, J. C., del Solar, J. R., & Vallejos, P. (2004). Back to reality: Crossing the reality\ngap in evolutionary robotics [IFAC/EURON Symposium on Intelligent Autonomous\nVehicles, Lisbon, Portugal, 5-7 July 2004]. _IFAC Proceedings Volumes_, _37_ (8), 834–\n839. https://doi.org/10.1016/S1474-6670(17)32084-0\nZanzotto, F. M. (2019). Viewpoint: Human-in-the-loop artificial intelligence. _Journal of_\n_Artificial Intelligence Research_, _64_, 243–252. https://doi.org/10.1613/jair.1.11345\nZhang, C., Yong, L., Chen, Y., Zhang, S., Ge, L., Wang, S., & Li, W. (2019a). A RubberTapping Robot Forest Navigation and Information Collection System Based on 2D\nLiDAR and a Gyroscope [Number: 9 Publisher: Multidisciplinary Digital Publishing\nInstitute]. _Sensors_, _19_ (9), 2136. https://doi.org/10.3390/s19092136\nZhang, J., & Bareinboim, E. Can humans be out of the loop? In: In _First conference on_\n_causal learning and reasoning (clear 2022_ . 2020. https://openreview.net/forum?id=\nP0f91v5fTK\n\nZhang, P., Hao, J., Wang, W., Tang, H., Ma, Y., Duan, Y., & Zheng, Y. Kogun: Accelerating deep reinforcement learning via integrating human suboptimal knowledge. In:\nIn _Proceedings of the twenty-ninth international conference on international joint_\n_conferences on artificial intelligence_ . 2021, 2291–2297.\nZhang, R., Torabi, F., Guan, L., Ballard, D. H., & Stone, P. Leveraging human guidance for\ndeep reinforcement learning tasks. In: In _Twenty-eighth international joint confer-_\n_ence on artificial intelligence (ijcai-19)_ . International Joint Conferences on Artificial\nIntelligence Organization, 2019, 6339–6346. https://doi.org/10.24963/ijcai.2019/\n884.\n\nZhou, C., Li, Q., Li, C., Yu, J., Liu, Y., Wang, G., Zhang, K., Ji, C., Yan, Q., He, L., Peng,\nH., Li, J., Wu, J., Liu, Z., Xie, P., Xiong, C., Pei, J., Yu, P. S., & Sun, L. (2023, May).\nA Comprehensive Survey on Pretrained Foundation Models: A History from BERT\nto ChatGPT [arXiv:2302.09419 [cs]]. https://doi.org/10.48550/arXiv.2302.09419\nZhou, J., Gandomi, A. H., Chen, F., & Holzinger, A. (2021). Evaluating the quality of\nmachine learning explanations: A survey on methods and metrics. _Electronics_, _10_ (5),\n593. https://doi.org/10.3390/electronics10050593\nZhu, G., Wang, J., Ren, Z., Lin, Z., & Zhang, C. Object-oriented dynamics learning through\nmulti-level abstraction. In: In _Proceedings of the aaai conference on artificial intel-_\n_ligence_ . _34_ . 2020, 6989–6998. https://doi.org/10.1609/aaai.v34i04.6183.\nZini, J. E., & Awad, M. (2022). On the Explainability of Natural Language Processing Deep\nModels. _ACM Computing Surveys_, _55_ (5), 103:1–103:31. https://doi.org/10.1145/\n3529755\n\n\n415\n\n\n",
    "ranking": {
      "relevance_score": 0.6757229798891227,
      "citation_score": 0.9,
      "recency_score": 0.7259729402610394,
      "final_score": 0.7256033799484898
    },
    "citation_key": "Retzlaff2024HumanintheLoopRL",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "498238a3bd5fd322fc3ce1572e33bbe3853a356f",
    "title": "Deep Reinforcement Learning: A Brief Survey",
    "published": "2017-08-19",
    "authors": [
      "Kai Arulkumaran",
      "M. Deisenroth",
      "Miles Brundage",
      "A. Bharath"
    ],
    "summary": "Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.",
    "pdf_url": "https://arxiv.org/pdf/1708.05866",
    "doi": "10.1109/MSP.2017.2743240",
    "fields_of_study": [
      "Computer Science",
      "Mathematics"
    ],
    "venue": "IEEE Signal Processing Magazine",
    "citation_count": 3252,
    "bibtex": "@Article{Arulkumaran2017DeepRL,\n author = {Kai Arulkumaran and M. Deisenroth and Miles Brundage and A. Bharath},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {26-38},\n title = {Deep Reinforcement Learning: A Brief Survey},\n volume = {34},\n year = {2017}\n}\n",
    "markdown_text": "IEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 1\n\n# A Brief Survey of Deep Reinforcement Learning\n\n\nKai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath\n\n\n\n_**Abstract**_ **—Deep reinforcement learning is poised to revolu-**\n**tionise the field of AI and represents a step towards building**\n**autonomous systems with a higher level understanding of the**\n**visual world. Currently, deep learning is enabling reinforcement**\n**learning to scale to problems that were previously intractable,**\n**such as learning to play video games directly from pixels. Deep**\n**reinforcement learning algorithms are also applied to robotics,**\n**allowing control policies for robots to be learned directly from**\n**camera inputs in the real world. In this survey, we begin with**\n**an introduction to the general field of reinforcement learning,**\n**then progress to the main streams of value-based and policy-**\n**based methods. Our survey will cover central algorithms in**\n**deep reinforcement learning, including the deep** _Q_ **-network,**\n**trust region policy optimisation, and asynchronous advantage**\n**actor-critic. In parallel, we highlight the unique advantages of**\n**deep neural networks, focusing on visual understanding via**\n**reinforcement learning. To conclude, we describe several current**\n**areas of research within the field.**\n\n\nI. INTRODUCTION\n\nOne of the primary goals of the field of artificial intelligence\n(AI) is to produce fully autonomous agents that interact with\ntheir environments to learn optimal behaviours, improving over\ntime through trial and error. Crafting AI systems that are\nresponsive and can effectively learn has been a long-standing\nchallenge, ranging from robots, which can sense and react\nto the world around them, to purely software-based agents,\nwhich can interact with natural language and multimedia.\nA principled mathematical framework for experience-driven\nautonomous learning is reinforcement learning (RL) [135]. Although RL had some successes in the past [141, 129, 62, 93],\nprevious approaches lacked scalablity and were inherently\nlimited to fairly low-dimensional problems. These limitations\nexist because RL algorithms share the same complexity issues as other algorithms: memory complexity, computational\ncomplexity, and in the case of machine learning algorithms,\nsample complexity [133]. What we have witnessed in recent\nyears—the rise of deep learning, relying on the powerful\n_function approximation_ and _representation learning_ properties\nof deep neural networks—has provided us with new tools to\novercoming these problems.\nThe advent of deep learning has had a significant impact\non many areas in machine learning, dramatically improving\nthe state-of-the-art in tasks such as object detection, speech\nrecognition, and language translation [70]. The most important\nproperty of deep learning is that deep neural networks can\nautomatically find compact low-dimensional representations\n(features) of high-dimensional data (e.g., images, text and\naudio). Through crafting inductive biases into neural network\narchitectures, particularly that of hierarchical representations,\nmachine learning practitioners have made effective progress\nin addressing the curse of dimensionality [15]. Deep learning\nhas similarly accelerated progress in RL, with the use of\ndeep learning algorithms within RL defining the field of\n“deep reinforcement learning” (DRL). The aim of this survey\n\n\n\nis to cover both seminal and recent developments in DRL,\nconveying the innovative ways in which neural networks can\nbe used to bring us closer towards developing autonomous\nagents. For a more comprehensive survey of recent efforts in\nDRL, including applications of DRL to areas such as natural\nlanguage processing [106, 5], we refer readers to the overview\nby Li [78].\nDeep learning enables RL to scale to decision-making\nproblems that were previously intractable, i.e., settings with\nhigh-dimensional state and action spaces. Amongst recent\nwork in the field of DRL, there have been two outstanding\nsuccess stories. The first, kickstarting the revolution in DRL,\nwas the development of an algorithm that could learn to play\na range of Atari 2600 video games at a superhuman level,\ndirectly from image pixels [84]. Providing solutions for the\ninstability of function approximation techniques in RL, this\nwork was the first to convincingly demonstrate that RL agents\ncould be trained on raw, high-dimensional observations, solely\nbased on a reward signal. The second standout success was\nthe development of a hybrid DRL system, AlphaGo, that\ndefeated a human world champion in Go [128], paralleling the\nhistoric achievement of IBM’s Deep Blue in chess two decades\nearlier [19] and IBM’s Watson DeepQA system that beat the\nbest human Jeopardy! players [31]. Unlike the handcrafted\nrules that have dominated chess-playing systems, AlphaGo\nwas composed of neural networks that were trained using\nsupervised and reinforcement learning, in combination with\na traditional heuristic search algorithm.\nDRL algorithms have already been applied to a wide range\nof problems, such as robotics, where control policies for robots\ncan now be learned directly from camera inputs in the real\nworld [74, 75], succeeding controllers that used to be handengineered or learned from low-dimensional features of the\nrobot’s state. In a step towards even more capable agents,\nDRL has been used to create agents that can meta-learn (“learn\nto learn”) [29, 156], allowing them to generalise to complex\nvisual environments they have never seen before [29]. In\nFigure 1, we showcase just some of the domains that DRL\nhas been applied to, ranging from playing video games [84]\nto indoor navigation [167].\nVideo games may be an interesting challenge, but learning\nhow to play them is not the end goal of DRL. One of the\ndriving forces behind DRL is the vision of creating systems\nthat are capable of learning how to adapt in the real world.\nFrom managing power consumption [142] to picking and\nstowing objects [75], DRL stands to increase the amount\nof physical tasks that can be automated by learning. However, DRL does not stop there, as RL is a general way of\napproaching optimisation problems by trial and error. From\ndesigning state-of-the-art machine translation models [168] to\nconstructing new optimisation functions [76], DRL has already\nbeen used to approach all manner of machine learning tasks.\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 2\n\n\nFig. 1. A range of visual RL domains. **(a)** Two classic Atari 2600 video games, “Freeway” and “Seaquest”, from the Arcade Learning Environment\n(ALE) [10]. Due to the range of supported games that vary in genre, visuals and difficulty, the ALE has become a standard testbed for DRL algorithms\n\n[84, 95, 44, 122, 132, 157, 85]. As we will discuss later, the ALE is one of several benchmarks that are now being used to standardise evaluation in RL.\n**(b)** The TORCS car racing simulator, which has been used to test DRL algorithms that can output continuous actions [64, 79, 85] (as the games from the\nALE only support discrete actions). **(c)** Utilising the potentially unlimited amount of training data that can be amassed in robotic simulators, several methods\naim to transfer knowledge from the simulator to the real world [22, 115, 146]. **(d)** Two of the four robotic tasks designed by Levine et al. [74]: screwing\non a bottle cap and placing a shaped block in the correct hole. Levine et al. [74] were able to train visuomotor policies in an end-to-end fashion, showing\nthat visual servoing could be learned directly from raw camera inputs by using deep neural networks. **(e)** A real room, in which a wheeled robot trained to\nnavigate the building is given a visual cue as input, and must find the corresponding location [167]. **(f)** A natural image being captioned by a neural network\nthat uses reinforcement learning to choose where to look [166]. By processing a small portion of the image for every word generated, the network can focus\nits attention on the most salient points. Figures reproduced from [10, 79, 146, 74, 167, 166], respectively.\n\n\n\n![](output/images/498238a3bd5fd322fc3ce1572e33bbe3853a356f.pdf-1-0.png)\n\nAnd, in the same way that deep learning has been utilised\nacross many branches of machine learning, it seems likely\nthat in the future, DRL will be an important component in\nconstructing general AI systems [68].\n\n\nII. REWARD-DRIVEN BEHAVIOUR\n\n\nBefore examining the contributions of deep neural networks\nto RL, we will introduce the field of RL in general. The\nessence of RL is learning through _interaction_ . An RL agent\ninteracts with its environment and, upon observing the consequences of its actions, can learn to alter its own behaviour in\nresponse to rewards received. This paradigm of trial-and errorlearning has its roots in behaviourist psychology, and is one\nof the main foundations of RL [135]. The other key influence\non RL is optimal control, which has lent the mathematical\nformalisms (most notably dynamic programming [13]) that\nunderpin the field.\n\nIn the RL set-up, an autonomous _agent_, controlled by\na machine learning algorithm, observes a _state_ **s** _t_ from its\n_environment_ at timestep _t_ . The agent interacts with the environment by taking an _action_ **a** _t_ in state **s** _t_ . When the agent\ntakes an action, the environment and the agent transition to\na new state **s** _t_ +1 based on the current state and the chosen\naction. The state is a sufficient statistic of the environment\n\n\n\nand thereby comprises all the necessary information for the\nagent to take the best action, which can include parts of the\nagent, such as the position of its actuators and sensors. In the\noptimal control literature, states and actions are often denoted\nby **x** _t_ and **u** _t_, respectively.\n\n\nThe best sequence of actions is determined by the _rewards_\nprovided by the environment. Every time the environment\ntransitions to a new state, it also provides a scalar reward\n_rt_ +1 to the agent as feedback. The goal of the agent is to\nlearn a _policy_ (control strategy) _π_ that maximises the expected\n_return_ (cumulative, discounted reward). Given a state, a policy\nreturns an action to perform; an _optimal policy_ is any policy\nthat maximises the expected return in the environment. In\nthis respect, RL aims to solve the same problem as optimal\ncontrol. However, the challenge in RL is that the agent needs\nto learn about the consequences of actions in the environment\nby trial and error, as, unlike in optimal control, a model of the\nstate transition dynamics is not available to the agent. Every\ninteraction with the environment yields information, which the\nagent uses to update its knowledge. This _perception-action-_\n_learning loop_ is illustrated in Figure 2.\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 3\n\n\nFig. 2. The perception-action-learning loop. At time _t_, the agent receives state **s** _t_ from the environment. The agent uses its policy to choose an action **a** _t_ .\nOnce the action is executed, the environment transitions a step, providing the next state **s** _t_ +1 as well as feedback in the form of a reward _rt_ +1. The agent\nuses knowledge of state transitions, of the form ( **s** _t,_ **a** _t,_ **s** _t_ +1 _, rt_ +1), in order to learn and improve its policy.\n\n\n\n![](output/images/498238a3bd5fd322fc3ce1572e33bbe3853a356f.pdf-2-0.png)\n\n_A. Markov Decision Processes_\n\nFormally, RL can be described as a Markov decision process\n(MDP), which consists of:\n\n_•_ A set of states _S_, plus a distribution of starting states\n_p_ ( **s** 0).\n\n_•_ A set of actions _A_ .\n\n_•_ Transition dynamics _T_ ( **s** _t_ +1 _|_ **s** _t,_ **a** _t_ ) that map a stateaction pair at time _t_ onto a distribution of states at time\n_t_ + 1.\n\n\n_•_ An immediate/instantaneous reward function\n_R_ ( **s** _t,_ **a** _t,_ **s** _t_ +1).\n\n_•_ A discount factor _γ ∈_ [0 _,_ 1], where lower values place\nmore emphasis on immediate rewards.\nIn general, the policy _π_ is a mapping from states to a\nprobability distribution over actions: _π_ : _S →_ _p_ ( _A_ = **a** _|S_ ). If\nthe MDP is _episodic_, i.e., the state is reset after each episode of\nlength _T_, then the sequence of states, actions and rewards in an\nepisode constitutes a _trajectory_ or _rollout_ of the policy. Every\nrollout of a policy accumulates rewards from the environment,\nresulting in the return _R_ = [�] _[T]_ _t_ =0 _[ −]_ [1] _[γ][t][r][t]_ [+1][. The goal of RL is]\nto find an optimal policy, _π_ _[∗]_, which achieves the maximum\nexpected return from all states:\n\n\n_π_ _[∗]_ = argmax E[ _R|π_ ] _._ (1)\n\n_π_\n\n\nIt is also possible to consider non-episodic MDPs, where\n_T_ = _∞_ . In this situation, _γ <_ 1 prevents an infinite sum\nof rewards from being accumulated. Furthermore, methods\nthat rely on complete trajectories are no longer applicable,\nbut those that use a finite set of transitions still are.\nA key concept underlying RL is the Markov property—\nonly the current state affects the next state, or in other words,\nthe future is conditionally independent of the past given\nthe present state. This means that any decisions made at **s** _t_\ncan be based solely on **s** _t−_ 1, rather than _{_ **s** 0 _,_ **s** 1 _, . . .,_ **s** _t−_ 1 _}_ .\nAlthough this assumption is held by the majority of RL\nalgorithms, it is somewhat unrealistic, as it requires the states\nto be _fully observable_ . A generalisation of MDPs are partially\nobservable MDPs (POMDPs), in which the agent receives an\n\n\n\nobservation **o** _t ∈_ Ω, where the distribution of the observation\n_p_ ( **o** _t_ +1 _|_ **s** _t_ +1 _,_ **a** _t_ ) is dependent on the current state and the\nprevious action [56]. In a control and signal processing context, the observation would be described by a measurement/\nobservation mapping in a state-space-model that depends on\nthe current state and the previously applied action.\n\nPOMDP algorithms typically maintain a _belief_ over the\ncurrent state given the previous belief state, the action taken\nand the current observation. A more common approach in\ndeep learning is to utilise recurrent neural networks (RNNs)\n\n[163, 44, 45, 85, 96], which, unlike feedforward neural\nnetworks, are dynamical systems. This approach to solving\nPOMDPs is related to other problems using dynamical systems\nand state space models, where the true state can only be\nestimated [16].\n\n\n_B. Challenges in RL_\n\n\nIt is instructive to emphasise some challenges faced in RL:\n\n\n_•_ The optimal policy must be inferred by trial-and-error\ninteraction with the environment. The only learning signal\nthe agent receives is the reward.\n\n_•_ The observations of the agent depend on its actions and\ncan contain strong temporal correlations.\n\n_•_ Agents must deal with long-range time dependencies:\nOften the consequences of an action only materialise after\nmany transitions of the environment. This is known as the\n(temporal) _credit assignment problem_ [135].\n\n\nWe will illustrate these challenges in the context of an\nindoor robotic visual navigation task: if the goal location is\nspecified, we may be able to estimate the distance remaining\n(and use it as a reward signal), but it is unlikely that we will\nknow exactly what series of actions the robot needs to take\nto reach the goal. As the robot must choose where to go as it\nnavigates the building, its decisions influence which rooms it\nsees and, hence, the statistics of the visual sequence captured.\nFinally, after navigating several junctions, the robot may find\nitself in a dead end. There is a range of problems, from\nlearning the consequences of actions to balancing exploration\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 4\n\n\n\nagainst exploitation, but ultimately these can all be addressed\nformally within the framework of RL.\n\nIII. REINFORCEMENT LEARNING ALGORITHMS\n\nSo far, we have introduced the key formalism used in RL,\nthe MDP, and briefly noted some challenges in RL. In the\nfollowing, we will distinguish between different classes of\nRL algorithms. There are two main approaches to solving\nRL problems: methods based on _value functions_ and methods\nbased on _policy search_ . There is also a hybrid, _actor-critic_\napproach, which employs both value functions and policy\nsearch. We will now explain these approaches and other useful\nconcepts for solving RL problems.\n\n_A. Value Functions_\n\nValue function methods are based on estimating the value\n(expected return) of being in a given state. The _state-value_\n_function V_ _[π]_ ( **s** ) is the expected return when starting in state **s**\nand following _π_ henceforth:\n\n\n_V_ _[π]_ ( **s** ) = E[ _R|_ **s** _, π_ ] (2)\n\n\nThe optimal policy, _π_ _[∗]_, has a corresponding state-value\nfunction _V_ _[∗]_ ( **s** ), and vice-versa, the optimal state-value function can be defined as\n\n\n_V_ _[∗]_ ( **s** ) = max _V_ _[π]_ ( **s** ) _∀_ **s** _∈S._ (3)\n_π_\n\n\nIf we had _V_ _[∗]_ ( **s** ) available, the optimal policy could be retrieved by choosing among all actions available at **s** _t_ and picking the action **a** that maximises E **s** _t_ +1 _∼T_ ( **s** _t_ +1 _|_ **s** _t,_ **a** )[ _V_ _[∗]_ ( **s** _t_ +1)].\nIn the RL setting, the transition dynamics _T_ are unavailable.\nTherefore, we construct another function, the _state-action-_\n_value_ or _quality function Q_ _[π]_ ( **s** _,_ **a** ), which is similar to _V_ _[π]_,\nexcept that the initial action **a** is provided, and _π_ is only\nfollowed from the succeeding state onwards:\n\n\n_Q_ _[π]_ ( **s** _,_ **a** ) = E[ _R|_ **s** _,_ **a** _, π_ ] _._ (4)\n\n\nThe best policy, given _Q_ _[π]_ ( **s** _,_ **a** ), can be found by choosing **a**\ngreedily at every state: argmax **a** _Q_ _[π]_ ( **s** _,_ **a** ). Under this policy,\nwe can also define _V_ _[π]_ ( **s** ) by maximising _Q_ _[π]_ ( **s** _,_ **a** ): _V_ _[π]_ ( **s** ) =\nmax **a** _Q_ _[π]_ ( **s** _,_ **a** ).\n**Dynamic Programming:** To actually learn _Q_ _[π]_, we exploit\nthe Markov property and define the function as a Bellman\nequation [13], which has the following recursive form:\n\n\n_Q_ _[π]_ ( **s** _t,_ **a** _t_ ) = E **s** _t_ +1[ _rt_ +1 + _γQ_ _[π]_ ( **s** _t_ +1 _, π_ ( **s** _t_ +1))] _._ (5)\n\n\nThis means that _Q_ _[π]_ can be improved by _bootstrapping_, i.e.,\nwe can use the current values of our estimate of _Q_ _[π]_ to improve\nour estimate. This is the foundation of _Q_ -learning [159] and\nthe state-action-reward-state-action (SARSA) algorithm [112]:\n\n\n_Q_ _[π]_ ( **s** _t,_ **a** _t_ ) _←_ _Q_ _[π]_ ( **s** _t,_ **a** _t_ ) + _αδ,_ (6)\n\n\nwhere _α_ is the learning rate and _δ_ = _Y −Q_ _[π]_ ( **s** _t,_ **a** _t_ ) the temporal difference (TD) error; here, _Y_ is a target as in a standard\nregression problem. SARSA, an _on-policy_ learning algorithm,\nis used to improve the estimate of _Q_ _[π]_ by using transitions\ngenerated by the behavioural policy (the policy derived from\n\n\n\n_Q_ _[π]_ ), which results in setting _Y_ = _rt_ + _γQ_ _[π]_ ( **s** _t_ +1 _,_ **a** _t_ +1). _Q_ learning is _off-policy_, as _Q_ _[π]_ is instead updated by transitions\nthat were not necessarily generated by the derived policy.\nInstead, _Q_ -learning uses _Y_ = _rt_ + _γ_ max **a** _Q_ _[π]_ ( **s** _t_ +1 _,_ **a** ), which\ndirectly approximates _Q_ _[∗]_ .\nTo find _Q_ _[∗]_ from an arbitrary _Q_ _[π]_, we use _generalised_\n_policy iteration_, where policy iteration consists of _policy eval-_\n_uation_ and _policy improvement_ . Policy evaluation improves\nthe estimate of the value function, which can be achieved\nby minimising TD errors from trajectories experienced by\nfollowing the policy. As the estimate improves, the policy can\nnaturally be improved by choosing actions greedily based on\nthe updated value function. Instead of performing these steps\nseparately to convergence (as in policy iteration), generalised\npolicy iteration allows for interleaving the steps, such that\nprogress can be made more rapidly.\n\n\n_B. Sampling_\n\nInstead of bootstrapping value functions using dynamic\nprogramming methods, Monte Carlo methods estimate the\nexpected return (2) from a state by averaging the return from\nmultiple rollouts of a policy. Because of this, pure Monte Carlo\nmethods can also be applied in non-Markovian environments.\nOn the other hand, they can only be used in episodic MDPs,\nas a rollout has to terminate for the return to be calculated.\n\nIt is possible to get the best of both methods by combining\nTD learning and Monte Carlo policy evaluation, as in done in\nthe TD( _λ_ ) algorithm [135]. Similarly to the discount factor,\nthe _λ_ in TD( _λ_ ) is used to interpolate between Monte Carlo\nevaluation and bootstrapping. As demonstrated in Figure 3,\nthis results in an entire spectrum of RL methods based around\nthe amount of sampling utilised.\nAnother major value-function based method relies on learning the _advantage_ function _A_ _[π]_ ( **s** _,_ **a** ) [6, 43]. Unlike producing\nabsolute state-action values, as with _Q_ _[π]_, _A_ _[π]_ instead represents\nrelative state-action values. Learning relative values is akin\nto removing a baseline or average level of a signal; more\nintuitively, it is easier to learn that one action has better\nconsequences than another, than it is to learn the actual return\nfrom taking the action. _A_ _[π]_ represents a relative advantage\nof actions through the simple relationship _A_ _[π]_ = _Q_ _[π]_ _−_ _V_ _[π]_,\nand is also closely related to the baseline method of variance\nreduction within gradient-based policy search methods [164].\nThe idea of advantage updates has been utilised in many recent\nDRL algorithms [157, 40, 85, 123].\n\n\n_C. Policy Search_\n\nPolicy search methods do not need to maintain a value\nfunction model, but directly search for an optimal policy\n_π_ _[∗]_ . Typically, a parameterised policy _πθ_ is chosen, whose\nparameters are updated to maximise the expected return E[ _R|θ_ ]\nusing either gradient-based or gradient-free optimisation [26].\nNeural networks that encode policies have been successfully\ntrained using both gradient-free [37, 23, 64] and gradientbased [164, 163, 46, 79, 122, 123, 74] methods. Gradient-free\noptimisation can effectively cover low-dimensional parameter\nspaces, but despite some successes in applying them to large\nnetworks [64], gradient-based training remains the method of\nchoice for most DRL algorithms, being more sample-efficient\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 5\n\n\n\n![](output/images/498238a3bd5fd322fc3ce1572e33bbe3853a356f.pdf-4-0.png)\n\nFig. 3. Two dimensions of RL algorithms, based on the _backups_ used to learn\nor construct a policy. At the extremes of these dimensions are (a) dynamic\nprogramming, (b) exhaustive search, (c) one-step TD learning and (d) pure\nMonte Carlo approaches. Bootstrapping extends from (c) 1-step TD learning\nto _n_ -step TD learning methods [135], with (d) pure Monte Carlo approaches\nnot relying on bootstrapping at all. Another possible dimension of variation\nis choosing to (c, d) sample actions versus (a, b) taking the expectation over\nall choices. Recreated from [135].\n\n\nwhen policies possess a large number of parameters.\n\n\nWhen constructing the policy directly, it is common to\noutput parameters for a probability distribution; for continuous\nactions, this could be the mean and standard deviations of\nGaussian distributions, whilst for discrete actions this could\nbe the individual probabilities of a multinomial distribution.\nThe result is a stochastic policy from which we can directly\nsample actions. With gradient-free methods, finding better\npolicies requires a heuristic search across a predefined class\nof models. Methods such as evolution strategies essentially\nperform hill-climbing in a subspace of policies [116], whilst\nmore complex methods, such as compressed network search,\nimpose additional inductive biases [64]. Perhaps the greatest\nadvantage of gradient-free policy search is that they can also\noptimise non-differentiable policies.\n\n\n**Policy Gradients:** Gradients can provide a strong learning\nsignal as to how to improve a parameterised policy. However,\nto compute the expected return (1) we need to average over\nplausible trajectories induced by the current policy parameterisation. This averaging requires either deterministic approximations (e.g., linearisation) or stochastic approximations via\nsampling [26]. Deterministic approximations can only be applied in a model-based setting where a model of the underlying\ntransition dynamics is available. In the more common modelfree RL setting, a Monte Carlo estimate of the expected return\nis determined. For gradient-based learning, this Monte Carlo\napproximation poses a challenge since gradients cannot pass\nthrough these samples of a stochastic function. Therefore, we\nturn to an estimator of the gradient, known in RL as the REINFORCE rule [164], elsewhere known as the score function [34]\nor likelihood-ratio estimator [36]. The latter name is telling as\nusing the estimator is similar to the practice of optimising\n\n\n\n![](output/images/498238a3bd5fd322fc3ce1572e33bbe3853a356f.pdf-4-1.png)\n\nFig. 4. Actor-critic set-up. The actor (policy) receives a state from the\nenvironment and chooses an action to perform. At the same time, the critic\n(value function) receives the state and reward resulting from the previous\ninteraction. The critic uses the TD error calculated from this information to\nupdate itself and the actor. Recreated from [135].\n\n\nthe log-likelihood in supervised learning. Intuitively, gradient\nascent using the estimator increases the log probability of the\nsampled action, weighted by the return. More formally, the\nREINFORCE rule can be used to compute the gradient of an\nexpectation over a function _f_ of a random variable _X_ with\nrespect to parameters _θ_ :\n\n\n_∇θ_ E _X_ [ _f_ ( _X_ ; _θ_ )] = E _X_ [ _f_ ( _X_ ; _θ_ ) _∇θ_ log _p_ ( _X_ )] _._ (7)\n\n\nAs this computation relies on the empirical return of a\ntrajectory, the resulting gradients possess a high variance.\nBy introducing unbiased estimates that are less noisy it is\npossible to reduce the variance. The general methodology\nfor performing this is to subtract a baseline, which means\nweighting updates by an advantage rather than the pure return.\nThe simplest baseline is the average return taken over several\nepisodes [164], but many more options are available [123].\n**Actor-critic Methods:** It is possible to combine value\nfunctions with an explicit representation of the policy, resulting\nin actor-critic methods, as shown in Figure 4. The “actor”\n(policy) learns by using feedback from the “critic” (value\nfunction). In doing so, these methods trade off variance\nreduction of policy gradients with bias introduction from value\nfunction methods [63, 123].\n\nActor-critic methods use the value function as a baseline\n\nfor policy gradients, such that the only fundamental difference\nbetween actor-critic methods and other baseline methods are\n\nthat actor-critic methods utilise a _learned_ value function. For\n\nthis reason, we will later discuss actor-critic methods as a\nsubset of policy gradient methods.\n\n_D. Planning and Learning_\nGiven a model of the environment, it is possible to use\ndynamic programming over all possible actions (Figure 3\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 6\n\n\n\n(a)), sample trajectories for heuristic search (as was done by\nAlphaGo [128]), or even perform an exhaustive search (Figure\n3 (b)). Sutton and Barto [135] define _planning_ as any method\nwhich utilises a model to produce or improve a policy. This\nincludes _distribution models_, which include _T_ and _R_, and\n_sample models_, from which only samples of transitions can\nbe drawn.\n\nIn RL, we focus on learning without access to the underlying model of the environment. However, interactions with the\nenvironment could be used to learn value functions, policies,\nand also a model. Model-free RL methods learn directly\nfrom interactions with the environment, but model-based RL\nmethods can simulate transitions using the learned model,\nresulting in increased sample efficiency. This is particularly\nimportant in domains where each interaction with the environment is expensive. However, learning a model introduces extra\ncomplexities, and there is always the danger of suffering from\nmodel errors, which in turn affects the learned policy; a common but partial solution in this latter scenario is to use model\npredictive control, where planning is repeated after small\nsequences of actions in the real environment [16]. Although\ndeep neural networks can potentially produce very complex\nand rich models [95, 132, 32], sometimes simpler, more dataefficient methods are preferable [40]. These considerations\nalso play a role in actor-critic methods with learned value\nfunctions [63, 123].\n\n\n_E. The Rise of DRL_\n\nMany of the successes in DRL have been based on scaling\nup prior work in RL to high-dimensional problems. This is\ndue to the learning of low-dimensional feature representations\nand the powerful function approximation properties of neural\nnetworks. By means of representation learning, DRL can deal\nefficiently with the curse of dimensionality, unlike tabular and\ntraditional non-parametric methods [15]. For instance, convolutional neural networks (CNNs) can be used as components\nof RL agents, allowing them to learn directly from raw, highdimensional visual inputs. In general, DRL is based on training\ndeep neural networks to approximate the optimal policy _π_ _[∗]_,\nand/or the optimal value functions _V_ _[∗]_, _Q_ _[∗]_ and _A_ _[∗]_ .\nAlthough there have been DRL successes with gradientfree methods [37, 23, 64], the vast majority of current works\nrely on gradients and hence the backpropagation algorithm\n\n[162, 111]. The primary motivation is that when available,\ngradients provide a strong learning signal. In reality, these\ngradients are estimated based on approximations, through\nsampling or otherwise, and as such we have to craft algorithms\nwith useful inductive biases in order for them to be tractable.\n\nThe other benefit of backpropagation is to view the optimisation of the expected return as the optimisation of a\nstochastic function [121, 46]. This function can comprise of\nseveral parts—models, policies and value functions—which\ncan be combined in various ways. The individual parts, such as\nvalue functions, may not directly optimise the expected return,\nbut can instead embody useful information about the RL\ndomain. For example, using a differentiable model and policy,\nit is possible to forward propagate and backpropagate through\nentire rollouts; on the other hand, innacuracies can accumulate\n\n\n\nover long time steps, and it may be be pertinent to instead use\na value function to summarise the statistics of the rollouts [46].\nWe have previously mentioned that representation learning and\nfunction approximation are key to the success of DRL, but it\nis also true to say that the field of deep learning has inspired\nnew ways of thinking about RL.\nFollowing our review of RL, we will now partition the\nnext part of the survey into value function and policy search\n\n                                      methods in DRL, starting with the well-known deep _Q_\nnetwork (DQN) [84]. In these sections, we will focus on stateof-the-art techniques, as well as the historical works they are\nbuilt upon. The focus of the state-of-the-art techniques will be\non those for which the state space is conveyed through visual\ninputs, e.g., images and video. To conclude, we will examine\nongoing research areas and open challenges.\n\n\nIV. VALUE FUNCTIONS\n\nThe well-known function approximation properties of neural\nnetworks led naturally to the use of deep learning to regress\nfunctions for use in RL agents. Indeed, one of the earliest\nsuccess stories in RL is TD-Gammon, a neural network that\nreached expert-level performance in Backgammon in the early\n90s [141]. Using TD methods, the network took in the state of\nthe board to predict the probability of black or white winning.\nAlthough this simple idea has been echoed in later work\n\n[128], progress in RL research has favoured the explicit use\nof value functions, which can capture the structure underlying\nthe environment. From early value function methods in DRL,\nwhich took simple states as input [109], current methods\nare now able to tackle visually and conceptually complex\nenvironments [84, 122, 85, 96, 167].\n\n\n_A. Function Approximation and the DQN_\n\nWe begin our survey of value-function-based DRL algorithms with the DQN [84], pictured in Figure 5, which\nachieved scores across a wide range of classic Atari 2600 video\ngames [10] that were comparable to that of a professional\nvideo games tester. The inputs to the DQN are four greyscale\nframes of the game, concatenated over time, which are initially\nprocessed by several convolutional layers in order to extract\nspatiotemporal features, such as the movement of the ball\nin “Pong” or “Breakout.” The final feature map from the\nconvolutional layers is processed by several fully connected\nlayers, which more implicitly encode the effects of actions.\nThis contrasts with more traditional controllers that use fixed\npreprocessing steps, which are therefore unable to adapt their\nprocessing of the state in response to the learning signal.\nA forerunner of the DQN—neural fitted _Q_ iteration\n\n                                      (NFQ)—involved training a neural network to return the _Q_\nvalue given a state-action pair [109]. NFQ was later extended\nto train a network to drive a slot car using raw visual inputs\nfrom a camera over the race track, by combining a deep\nautoencoder to reduce the dimensionality of the inputs with\na separate branch to predict _Q_ -values [69]. Although the previous network could have been trained for both reconstruction\n\nand RL tasks simultaneously, it was both more reliable and\ncomputationally efficient to train the two parts of the network\nsequentially.\nThe DQN [84] is closely related to the model proposed\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 7\n\n\nFig. 5. The deep _Q_ -network [84]. The network takes the state—a stack of greyscale frames from the video game—and processes it with convolutional and\nfully connected layers, with ReLU nonlinearities in between each layer. At the final layer, the network outputs a discrete action, which corresponds to one of\nthe possible control inputs for the game. Given the current state and chosen action, the game returns a new score. The DQN uses the reward—the difference\nbetween the new score and the previous one—to learn from its decision. More precisely, the reward is used to update its estimate of _Q_, and the error between\nits previous estimate and its new estimate is backpropagated through the network.\n\n\n\n![](output/images/498238a3bd5fd322fc3ce1572e33bbe3853a356f.pdf-6-0.png)\n\nby Lange et al. [69], but was the first RL algorithm that\nwas demonstrated to work directly from raw visual inputs\nand on a wide variety of environments. It was designed such\nthat the final fully connected layer outputs _Q_ _[π]_ ( **s** _, ·_ ) for all\naction values in a discrete set of actions—in this case, the\nvarious directions of the joystick and the fire button. This not\nonly enables the best action, argmax **a** _Q_ _[π]_ ( **s** _,_ **a** ), to be chosen\nafter a single forward pass of the network, but also allows the\nnetwork to more easily encode action-independent knowledge\nin the lower, convolutional layers. With merely the goal of\nmaximising its score on a video game, the DQN learns to\nextract salient visual features, jointly encoding objects, their\nmovements, and, most importantly, their interactions. Using\ntechniques originally developed for explaining the behaviour\nof CNNs in object recognition tasks, we can also inspect what\nparts of its view the agent considers important (see Figure 6).\n\n\nFig. 6. Saliency map of a trained DQN [84] playing “Space Invaders” [10].\nBy backpropagating the training signal to the image space, it is possible to\nsee what a neural-network-based agent is attending to. In this frame, the\nmost salient points—shown with the red overlay—are the laser that the agent\nrecently fired, and also the enemy that it anticipates hitting in a few time\nsteps.\n\n\nThe true underlying state of the game is contained within\n128 bytes of Atari 2600 RAM. However, the DQN was\ndesigned to directly learn from visual inputs (210 _×_ 160pixel\n8-bit RGB images), which it takes as the state **s** . It is\nimpractical to represent _Q_ _[π]_ ( **s** _,_ **a** ) exactly as a lookup table:\n\n                                      When combined with 18 possible actions, we obtain a _Q_\ntable of size _|S| × |A|_ = 18 _×_ 256 [3] _[×]_ [210] _[×]_ [160] . Even if it were\nfeasible to create such a table, it would be sparsely populated,\nand information gained from one state-action pair cannot be\npropagated to other state-action pairs. The strength of the DQN\n\n\n\nlies in its ability to compactly represent both high-dimensional\nobservations and the _Q_ -function using deep neural networks.\nWithout this ability, tackling the discrete Atari domain from\nraw visual inputs would be impractical.\nThe DQN addressed the fundamental instability problem\nof using function approximation in RL [145] by the use of\ntwo techniques: experience replay [80] and target networks.\nExperience replay memory stores transitions of the form\n( **s** _t,_ **a** _t,_ **s** _t_ +1 _, rt_ +1) in a cyclic buffer, enabling the RL agent\nto sample from and train on previously observed data offline.\nNot only does this massively reduce the amount of interactions\nneeded with the environment, but batches of experience can\nbe sampled, reducing the variance of learning updates. Furthermore, by sampling uniformly from a large memory, the\ntemporal correlations that can adversely affect RL algorithms\nare broken. Finally, from a practical perspective, batches\nof data can be efficiently processed in parallel by modern\nhardware, increasing throughput. Whilst the original DQN\nalgorithm used uniform sampling [84], later work showed\nthat prioritising samples based on TD errors is more effective\nfor learning [118]. We note that although experience replay\nis typically thought of as a model-free technique, it could\nactually be considered a simple model [150].\nThe second stabilising method, introduced by Mnih et al.\n\n[84], is the use of a target network that initially contains the\nweights of the network enacting the policy, but is kept frozen\nfor a large period of time. Rather than having to calculate the\nTD error based on its own rapidly fluctuating estimates of the\n_Q_ -values, the policy network uses the fixed target network.\nDuring training, the weights of the target network are updated\nto match the policy network after a fixed number of steps.\nBoth experience replay and target networks have gone on to\nbe used in subsequent DRL works [40, 79, 158, 89].\n\n_B. Q-Function Modifications_\n\nConsidering that one of the key components of the DQN is\na function approximator for the _Q_ -function, it can benefit from\nfundamental advances in RL. van Hasselt [148] showed that\nthe single estimator used in the _Q_ -learning update rule overestimates the expected return due to the use of the maximum\naction value as an approximation of the maximum _expected_\naction value. Double- _Q_ learning provides a better estimate\nthrough the use of a double estimator [148]. Whilst double_Q_ learning requires an additional function to be learned, later\nwork proposed using the already available target network from\nthe DQN algorithm, resulting in significantly better results\nwith only a small change in the update step [149]. A more\n\n\n\n![](output/images/498238a3bd5fd322fc3ce1572e33bbe3853a356f.pdf-6-1.png)\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 8\n\n\n\nradical proposal by Bellemare et al. [12] was to actually learn\nthe full _value distribution_, rather than just the expectation; this\nprovides additional information, such as whether the potential\nrewards come from a skewed or multimodal distribution. Al\nthough the resulting algorithm—based on learning categorical\ndistributions—was used to construct the Categorical DQN, the\nbenefits can potentially be applied to any RL algorithm that\nutilises learned value functions.\n\nYet another way to adjust the DQN architecture is to\ndecompose the _Q_ -function into meaningful functions, such\nas constructing _Q_ _[π]_ by adding together separate layers that\ncompute the state-value function _V_ _[π]_ and advantage function\n_A_ _[π]_ [157]. Rather than having to come up with accurate _Q_ values for all actions, the duelling DQN [157] benefits from a\nsingle baseline for the state in the form of _V_ _[π]_, and easier-tolearn relative values in the form of _A_ _[π]_ . The combination of the\n\nduelling DQN with prioritised experience replay [118] is one\nof the state-of-the-art techniques in discrete action settings.\nFurther insight into the properties of _A_ _[π]_ by Gu et al. [40]\nled them to modify the DQN with a convex advantage layer\nthat extended the algorithm to work over sets of continuous\nactions, creating the normalised advantage function (NAF)\nalgorithm. Benefiting from experience replay, target networks\nand advantage updates, NAF is one of several state-of-the-art\ntechniques in continuous control problems [40].\n\nSome RL domains, such as recommender systems, have\nvery large discrete action spaces, and hence may be difficult to\ndirectly deal with. Dulac-Arnold et al. [30] proposed learning\n“action embeddings” over the large set of original actions,\nand then using _k_ -nearest neighbors to produce “proto-actions”\nwhich can be used with traditional RL methods. The idea of\n\nusing representation learning to create distributed embeddings\nis a particular strength of DRL, and has been successfully\nutilised for other purposes [161, 100]. Another related scenario\nin RL is when many actions need to be made simultaneously,\nsuch as specifying the torques in a many-jointed robot, which\nresults in the action space growing exponentially. A naive but\nreasonable approach is to factorise the policy, treating each\naction independently [115]. An alternative is to construct an\nautoregressive policy, where each action in a single timestep\nis predicted conditionally on the state and previously chosen\nactions from the same timestep [106, 5, 168]. Metz et al.\n\n[81] used this idea in order to construct the sequential DQN,\nallowing them to discretise a large action space and outperform\nNAF—which is limited by its quadratic advantage function—\nin continous control problems. In a broader context, rather\nthan dealing directly with primitive actions directly, one may\nchoose to invoke “subpolicies” from higher-level policies\n\n[136]; this concept, known as hierarchical reinforcement learning (HRL), will be discussed later.\n\n\nV. POLICY SEARCH\n\n\nPolicy search methods aim to directly find policies by means\nof gradient-free or gradient-based methods. Prior to the current\nsurge of interest in DRL, several successful methods in DRL\neschewed the commonly used backpropagation algorithm in\nfavour of evolutionary algorithms [37, 23, 64], which are\ngradient-free policy search algorithms. Evolutionary methods\n\n\n\nrely on evaluating the performance of a population of agents.\nHence, they are expensive for large populations or agents with\nmany parameters. However, as black-box optimisation methods they can be used to optimise arbitrary, non-differentiable\nmodels and naturally allow for more exploration in parameter\nspace. In combination with a compressed representation of\nneural network weights, evolutionary algorithms can even be\nused to train large networks; such a technique resulted in the\nfirst deep neural network to learn an RL task, straight from\nhigh-dimensional visual inputs [64]. Recent work has reignited\ninterest in evolutionary methods for RL as they can potentially\nbe distributed at larger scales than techniques that rely on\ngradients [116].\n\n\n_A. Backpropagation through Stochastic Functions_\n\nThe workhorse of DRL, however, remains backpropagation\n\n[162, 111]. The previously discussed REINFORCE rule [164]\nallows neural networks to learn stochastic policies in a taskdependent manner, such as deciding where to look in an\nimage to track [120], classify [83] or caption objects [166].\nIn these cases, the stochastic variable would determine the\ncoordinates of a small crop of the image, and hence reduce\nthe amount of computation needed. This usage of RL to make\ndiscrete, stochastic decisions over inputs is known in the deep\nlearning literature as _hard attention_, and is one of the more\ncompelling uses of basic policy search methods in recent years,\nhaving many applications outside of traditional RL domains.\nMore generally, the ability to backpropagate through stochastic\nfunctions, using techniques such as REINFORCE [164] or the\n“reparameterisation trick” [61, 108], allows neural networks\nto be treated as stochastic computation graphs that can be\noptimised over [121], which is a key concept in algorithms\nsuch as stochastic value gradients (SVGs) [46].\n\n\n_B. Compounding Errors_\n\nSearching directly for a policy represented by a neural\nnetwork with very many parameters can be difficult and can\nsuffer from severe local minima. One way around this is to\nuse guided policy search (GPS), which takes a few sequences\nof actions from another controller (which could be constructed\nusing a separate method, such as optimal control). GPS learns\nfrom them by using supervised learning in combination with\nimportance sampling, which corrects for off-policy samples\n\n[73]. This approach effectively biases the search towards a\ngood (local) optimum. GPS works in a loop, by optimising\npolicies to match sampled trajectories, and optimising trajectory distributions to match the policy and minimise costs.\nInitially, GPS was used to train neural networks on simulated\ncontinuous RL problems [72], but was later utilised to train\na policy for a real robot based on visual inputs [74]. This\nresearch by Levine et al. [74] showed that it was possible\nto train visuomotor policies for a robot “end-to-end”, straight\nfrom the RGB pixels of the camera to motor torques, and,\nhence, is one of the seminal works in DRL.\nA more commonly used method is to use a trust region, in\nwhich optimisation steps are restricted to lie within a region\nwhere the approximation of the true cost function still holds.\nBy preventing updated policies from deviating too wildly\nfrom previous policies, the chance of a catastrophically bad\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 9\n\n\n\nupdate is lessened, and many algorithms that use trust regions\nguarantee or practically result in monotonic improvement in\npolicy performance. The idea of constraining each policy\ngradient update, as measured by the Kullback-Leibler (KL)\ndivergence between the current and proposed policy, has a long\nhistory in RL [57, 4, 59, 103]. One of the newer algorithms in\nthis line of work, trust region policy optimisation (TRPO),\nhas been shown to be relatively robust and applicable to\ndomains with high-dimensional inputs [122]. To achieve this,\nTRPO optimises a _surrogate_ objective function—specifically,\nit optimises an (importance sampled) advantage estimate, constrained using a quadratic approximation of the KL divergence.\nWhilst TRPO can be used as a pure policy gradient method\nwith a simple baseline, later work by Schulman et al. [123]\nintroduced generalised advantage estimation (GAE), which\nproposed several, more advanced variance reduction baselines.\nThe combination of TRPO and GAE remains one of the state\nof-the-art RL techniques in continuous control. However, the\nconstrained optimisation of TRPO requires calculating secondorder gradients, limiting its applicability. In contrast, the\nnewer proximal policy optimisation (PPO) algorithm performs\nunconstrained optimisation, requiring only first-order gradient\ninformation [1, 47, 125]. The two main variants include an\nadaptive penalty on the KL divergence, and a heuristic clipped\nobjective which is independent of the KL divergence [125].\nBeing less expensive whilst retaining the performance of\nTRPO means that PPO (with or without GAE) is gaining\npopularity for a range of RL tasks [47, 125].\n\n\n_C. Actor-Critic Methods_\n\n\nInstead of utilising the average of several Monte Carlo\nreturns as the baseline for policy gradient methods, actorcritic approaches have grown in popularity as an effective\nmeans of combining the benefits of policy search methods\nwith learned value functions, which are able to learn from full\nreturns and/or TD errors. They can benefit from improvements\nin both policy gradient methods, such as GAE [123], and value\nfunction methods, such as target networks [84]. In the last few\nyears, DRL actor-critic methods have been scaled up from\nlearning simulated physics tasks [46, 79] to real robotic visual\nnavigation tasks [167], directly from image pixels.\n\nOne recent development in the context of actor-critic algorithms are deterministic policy gradients (DPGs) [127], which\nextend the standard policy gradient theorems for stochastic\npolicies [164] to deterministic policies. One of the major\nadvantages of DPGs is that, whilst stochastic policy gradients integrate over both state and action spaces, DPGs only\nintegrate over the state space, requiring fewer samples in\nproblems with large action spaces. In the initial work on\nDPGs, Silver et al. [127] introduced and demonstrated an\noff-policy actor-critic algorithm that vastly improved upon\na stochastic policy gradient equivalent in high-dimensional\ncontinuous control problems. Later work introduced deep DPG\n(DDPG), which utilised neural networks to operate on highdimensional, visual state spaces [79]. In the same vein as\nDPGs, Heess et al. [46] devised a method for calculating\ngradients to optimise stochastic policies, by “reparameterising”\n\n[61, 108] the stochasticity away from the network, thereby\n\n\n\nallowing standard gradients to be used (instead of the highvariance REINFORCE estimator [164]). The resulting SVG\nmethods are flexible, and can be used both with (SVG(0) and\nSVG(1)) and without (SVG( _∞_ )) value function critics, and\nwith (SVG( _∞_ ) and SVG(1)) and without (SVG(0)) models.\nLater work proceeded to integrate DPGs and SVGs with\nRNNs, allowing them to solve continuous control problems\nin POMDPs, learning directly from pixels [45].\nValue functions introduce a broadly applicable benefit in\nactor-critic methods—the ability to use off-policy data. Onpolicy methods can be more stable, whilst off-policy methods\ncan be more data efficient, and hence there have been several\nattempts to merge the two [158, 94, 41, 39, 42]. Earlier\nwork has either utilised a mix of on-policy and off-policy\ngradient updates [158, 94, 39], or used the off-policy data\nto train a value function in order to reduce the variance of\n\non-policy gradient updates [41]. The more recent work by\nGu et al. [42] unified these methods under interpolated policy\ngradients (IPGs), resulting in one of the newest state-of-theart continuous DRL algorithms, and also providing insights for\nfuture research in this area. Together, the ideas behind IPGs\nand SVGs (of which DPGs can be considered a special case)\nform algorithmic approaches for improving learning efficiency\nin DRL.\n\nAn orthogonal approach to speeding up learning is to\nexploit parallel computation. In particular, methods for training\nnetworks through asynchronous gradient updates have been\ndeveloped for use on both single machines [107] and distributed systems [25]. By keeping a canonical set of parameters\nthat are read by and updated in an asynchronous fashion\nby multiple copies of a single network, computation can be\nefficiently distributed over both processing cores in a single\nCPU, and across CPUs in a cluster of machines. Using a\ndistributed system, Nair et al. [91] developed a framework\nfor training multiple DQNs in parallel, achieving both better\nperformance and a reduction in training time. However, the\nsimpler asynchronous advantage actor-critic (A3C) algorithm\n\n[85], developed for both single and distributed machine settings, has become one of the most popular DRL techniques\nin recent times. A3C combines advantage updates with the\nactor-critic formulation, and relies on asynchronously updated\npolicy and value function networks trained in parallel over\nseveral processing threads. The use of multiple agents, situated\nin their own, independent environments, not only stabilises\nimprovements in the parameters, but conveys an additional\nbenefit in allowing for more exploration to occur. A3C has\nbeen used as a standard starting point in many subsequent\nworks, including the work of Zhu et al. [167], who applied it\nto robotic navigation in the real world through visual inputs.\nFor simplicity, the underlying algorithm may be used with\njust one agent, termed advantage actor-critic (A2C) [156].\nAlternatively, segments from the trajectories of multiple agents\ncan be collected and processed together in a batch, with\nbatch processing more efficiently enabled by GPUs; this\nsynchronous version also goes by the name of A2C [125].\nThere have been several major advancements on the original\nA3C algorithm that reflect various motivations in the field of\nDRL. The first is actor-critic with experience replay [158, 39],\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 10\n\n\n\nwhich adds Retrace( _λ_ ) off-policy bias correction [88] to a\n_Q_ -value-based A3C, allowing it to use experience replay in\norder to improve sample complexity. Others have attempted to\nbridge the gap between value and policy-based RL, utilising\ntheoretical advancements to improve upon the original A3C\n\n[89, 94, 124]. Finally, there is a growing trend towards exploiting auxiliary tasks to improve the representations learned\nby DRL agents, and, hence, improve both the learning speed\nand final performance of these agents [77, 54, 82].\n\n\nVI. CURRENT RESEARCH AND CHALLENGES\n\nTo conclude, we will highlight some current areas of research in DRL, and the challenges that still remain. Previously,\nwe have focused mainly on model-free methods, but we will\nnow examine a few model-based DRL algorithms in more\ndetail. Model-based RL algorithms play an important role in\nmaking RL data-efficient and in trading off exploration and\nexploitation. After tackling exploration strategies, we shall\nthen address HRL, which imposes an inductive bias on the\nfinal policy by explicitly factorising it into several levels. When\navailable, trajectories from other controllers can be used to\nbootstrap the learning process, leading us to imitation learning\nand inverse RL (IRL). For the final topic specific to RL, we\nwill look at multi-agent systems, which have their own special\nconsiderations. We then bring to attention two broader areas—\nthe use of RNNs, and transfer learning—in the context of\nDRL. We then examine the issue of evaluating RL, and current\nbenchmarks for DRL.\n\n\n_A. Model-based RL_\n\nThe key idea behind model-based RL is to learn a transition model that allows for simulation of the environment\n\nwithout interacting with the environment directly. Model-based\nRL does not assume specific prior knowledge. However, in\npractice, we can incorporate prior knowledge (e.g., physicsbased models [58]) to speed up learning. Model learning\nplays an important role in reducing the amount of required\ninteractions with the (real) environment, which may be limited\nin practice. For example, it is unrealistic to perform millions of\nexperiments with a robot in a reasonable amount of time and\nwithout significant hardware wear and tear. There are various\napproaches to learn predictive models of dynamical systems\nusing pixel information. Based on the deep dynamical model\n\n[154], where high-dimensional observations are embedded\ninto a lower-dimensional space using autoencoders, several\nmodel-based DRL algorithms have been proposed for learning\nmodels and policies from pixel information [95, 160, 155]. If a\nsufficiently accurate model of the environment can be learned,\nthen even simple controllers can be used to control a robot\ndirectly from camera images [32]. Learned models can also\nbe used to guide exploration purely based on simulation of the\nenvironment, with deep models allowing these techniques to\nbe scaled up to high-dimensional visual domains [132].\nA compelling insight on the benefits of neural-networkbased models is that they can overcome some of the problems\nincurred by planning with imperfect models; in effect, by\n_embedding_ the activations and predictions (outputs) of these\nmodels into a vector, a DRL agent can not only obtain more\ninformation than just the final result of any model rollouts, but\n\n\n\nit can also learn to downplay this information if it believes\nthat the model is inaccurate [161]. This can be more efficient,\nthough less principled, than Bayesian methods for propagating\nuncertainty [52]. Another way to make use of the flexiblity\nof neural-network-based models is to let them decide when to\n\nplan, that is, given a finite amount of computation, whether it is\nworth modelling one long trajectory, several short trajectories,\nanything in-between, or simply to take an action in the real\nenvironment [100].\n\nAlthough deep neural networks can make reasonable predictions in simulated environments over hundreds of timesteps\n\n[21], they typically require many samples to tune the large\namount of parameters they contain. Training these models\noften requires more samples (interaction with the environment)\nthan simpler models. For this reason, Gu et al. [40] train\nlocally linear models for use with the NAF algorithm—\nthe continuous equivalent of the DQN [84]—to improve the\nalgorithm’s sample complexity in the robotic domain where\nsamples are expensive. In order to spur the adoption of deep\nmodels in model-based DRL, it is necessary to find strategies\nthat can be used in order to improve their data efficiency [90].\nA less common but potentially useful paradigm exists\nbetween model-free and model-based methods—the successor\n\nrepresentation (SR) [24]. Rather than picking actions directly\nor performing planning with models, learning _T_ is replaced\nwith learning expected (discounted) future occupancies (SRs),\nwhich can be linearly combined with _R_ in order to calculate\nthe optimal action; this decomposition makes SRs more robust\nthan model-free methods when the reward structure changes\n(but still fallible when _T_ changes). Work extending SRs to\ndeep neural networks has demonstrated its usefulness in multitask settings, whilst within a complex visual environment [66].\n\n\n_B. Exploration vs. Exploitation_\n\nOne of the greatest difficulties in RL is the fundamental\ndilemma of _exploration versus exploitation_ : When should the\nagent try out (perceived) non-optimal actions in order to\nexplore the environment (and potentially improve the model),\nand when should it exploit the optimal action in order to make\nuseful progress? Off-policy algorithms, such as the DQN [84],\ntypically use the simple _ϵ_ -greedy exploration policy, which\nchooses a random action with probability _ϵ ∈_ [0 _,_ 1], and the\noptimal action otherwise. By decreasing _ϵ_ over time, the agent\nprogresses towards exploitation. Although adding independent\nnoise for exploration is usable in continuous control problems,\nmore sophisticated strategies inject noise that is correlated\nover time (e.g., from stochastic processes) in order to better\npreserve momentum [79].\nThe observation that temporal correlation is important led\nOsband et al. [97] to propose the bootstrapped DQN, which\nmaintains several _Q_ -value “heads” that learn different values\nthrough a combination of different weight initialisations and\nbootstrapped sampling from experience replay memory. At\nthe beginning of each training episode, a different head is\nchosen, leading to temporally-extended exploration. Usunier\net al. [147] later proposed a similar method that performed\nexploration in policy space by adding noise to a single output\nhead, using zero-order gradient estimates to allow backpropa\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 11\n\n\n\ngation through the policy.\nOne of the main principled exploration strategies is the\n_upper confidence bound_ (UCB) algorithm, based on the principle of “optimism in the face of uncertainty” [67]. The idea\nbehind UCB is to pick actions that maximise E[R] + _κσ_ [ _R_ ],\nwhere _σ_ [ _R_ ] is the standard deviation of the return and\n_κ >_ 0. UCB therefore encourages exploration in regions with\nhigh uncertainty and moderate expected return. Whilst easily\nachievable in small tabular cases, the use of powerful density\nmodels [11], or conversely, hashing [139], has allowed this\nalgorithm to scale to high-dimensional visual domains with\nDRL. UCB is only one technique for trading off exploration\nand exploitation in the context of Bayesian optimisation [126];\nfuture work in DRL may benefit from investigating other\nsuccessful techniques that are used in Bayesian optimisation.\nUCB can also be considered one way of implementing\n_intrinsic motivation_, which is a general concept that advocates\ndecreasing uncertainty/making progress in learning about the\nenvironment [119]. There have been several DRL algorithms\nthat try to implement intrinsic motivation via minimising\nmodel prediction error [132, 101] or maximising information\ngain [86, 52].\n\n\n_C. Hierarchical RL_\n\nIn the same way that deep learning relies on hierarchies\nof features, HRL relies on hierarchies of policies. Early work\nin this area introduced _options_, in which, apart from _primi-_\n_tive actions_ (single-timestep actions), policies could also run\nother policies (multi-timestep “actions”) [136]. This approach\nallows top-level policies to focus on higher-level _goals_, whilst\n_subpolicies_ are responsible for fine control. Several works in\nDRL have attempted HRL by using one top-level policy that\nchooses between subpolicies, where the division of states or\ngoals in to subpolicies is achieved either manually [2, 143, 65]\nor automatically [3, 151, 152]. One way to help construct\nsubpolicies is to focus on discovering and reaching goals,\nwhich are specific states in the environment; they may often be\nlocations, which an agent should navigate to. Whether utilised\nwith HRL or not, the discovery and generalisation of goals is\nalso an important area of ongoing research [117, 66, 152].\n\n\n_D. Imitation Learning and Inverse RL_\n\nOne may ask why, if given a sequence of “optimal” actions\nfrom expert demonstrations, it is not possible to use supervised\nlearning in a straightforward manner—a case of “learning\nfrom demonstration”. This is indeed possible, and is known\nas _behavioural cloning_ in traditional RL literature. Taking\nadvantage of the stronger signals available in supervised learning problems, behavioural cloning enjoyed success in earlier\nneural network research, with the most notable success being\nALVINN, one of the earliest autonomous cars [104]. However,\nbehavioural cloning cannot adapt to new situations, and small\ndeviations from the demonstration during the execution of the\nlearned policy can compound and lead to scenarios where the\npolicy is unable to recover. A more generalisable solution is\nto use provided trajectories to guide the learning of suitable\nstate-action pairs, but fine-tune the agent using RL [49].\nAlternatively, if the expert is still available to query during\ntraining, the agent can use active learning to gather extra data\n\n\n\nwhen it is unsure, allowing it to learn from states away from\nthe optimal trajectories [110]. This has been applied to a deep\nlearning setting, where a CNN trained in a visual navigation\ntask with active learning significantly improved upon a pure\nimitation learning baseline [53].\nThe goal of IRL is to estimate an unknown reward function\nfrom observed trajectories that characterise a desired solution\n\n[92]; IRL can be used in combination with RL to improve\nupon demonstrated behaviour. Using the power of deep neural\nnetworks, it is now possible to learn complex, nonlinear reward\nfunctions for IRL [165]. Ho and Ermon [51] showed that policies are uniquely characterised by their _occupancies_ (visited\nstate and action distributions) allowing IRL to be reduced to\nthe problem of measure matching. With this insight, they were\nable to use generative adversarial training [38] to facilitate\nreward function learning in a more flexible manner, resulting in\nthe generative adversarial imitation learning (GAIL) algorithm.\nGAIL was later extended to allow IRL to be applied even when\nreceiving expert trajectories from a different visual viewpoint\nto that of the RL agent [131]. In complementary work, Baram\net al. [7] exploit gradient information that was not used in\nGAIL to learn models within the IRL process.\n\n\n_E. Multi-agent RL_\n\nUsually, RL considers a single learning agent in a stationary environment. In contrast, multi-agent RL (MARL)\nconsiders multiple agents learning through RL, and often the\nnon-stationarity introduced by other agents changing their\nbehaviours as they learn [18]. In DRL, the focus has been\non enabling (differentiable) communication between agents,\nwhich allows them to co-operate. Several approaches have\nbeen proposed for this purpose, including passing messages\nto agents sequentially [33], using a bidirectional channel\n(providing ordering with less signal loss) [102], and an allto-all channel [134]. The addition of communication channels\nis a natural strategy to apply to MARL in complex scenarios\nand does not preclude the usual practice of modelling cooperative or competing agents as applied elsewhere in the\nMARL literature [18]. Other DRL works of note in MARL\ninvestigate the effects of learning and sequential decision\nmaking in game theory [48, 71].\n\n\n_F. Memory and Attention_\n\nAs one of the earliest works in DRL the DQN spawned\nmany extensions. One of the first extensions was converting\nthe DQN into an RNN, which allows the network to better\ndeal with POMDPs by integrating information over long time\nperiods. Like recursive filters, recurrent connections provide an\nefficient means of acting conditionally on temporally distant\nprior observations. By using recurrent connections between\nits hidden units, the deep recurrent _Q_ -network (DRQN) introduced by Hausknecht and Stone [44] was able to successfully infer the velocity of the ball in the game “Pong,”\neven when frames of the game were randomly blanked out.\nFurther improvements were gained by introducing _attention_ —\na technique where additional connections are added from the\nrecurrent units to lower layers—to the DRQN, resulting in the\ndeep attention recurrent _Q_ -network (DARQN) [130]. Attention\ngives a network the ability to choose which part of its next\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 12\n\n\n\ninput to focus on, and allowed the DARQN to beat both\nthe DQN and DRQN on games, which require longer-term\nplanning. However, the DQN outperformed the DRQN and\nDARQN on games requiring quick reactions, where _Q_ -values\ncan fluctuate more rapidly.\nTaking recurrent processing further, it is possible to add a\ndifferentiable memory to the DQN, which allows it to more\nflexibly process information in its “working memory” [96]. In\ntraditional RNNs, recurrent units are responsible for both performing calculations and storing information. Differentiable\nmemories add large matrices that are purely used for storing\ninformation, and can be accessed using differentiable read\nand write operations, analagously to computer memory. With\ntheir key-value-based memory _Q_ -network (MQN), Oh et al.\n\n[96] constructed an agent that could solve a simple maze\nbuilt in Minecraft, where the correct goal in each episode\nwas indicated by a coloured block shown near the start of\nthe maze. The MQN, and especially its more sophisticated\nvariants, significantly outperformed both DQN and DRQN\nbaselines, highlighting the importance of using decoupled\nmemory storage. More recent work, where the memory was\ngiven a 2D structure in order to resemble a spatial map, hints\nat future research where more specialised memory structures\nwill be developed to address specific problems, such as 2D or\n3D navigation [98]. Alternatively, differentiable memories can\nbe used as approximate hash tables, allowing DRL algorithms\nto store and retrieve successful experiences to facilitate rapid\nlearning [105].\n\nNote that RNNs are not restricted to value-function-based\n\nmethods but have also been successfully applied to policy\nsearch [163] and actor-critic methods [45, 85].\n\n\n_G. Transfer Learning_\n\nEven though DRL algorithms can process high-dimensional\ninputs, it is rarely feasible to train RL agents directly on\nvisual inputs in the real world, due to the large number of\nsamples required. To speed up learning in DRL, it is possible\nto exploit previously acquired knowledge from related tasks,\nwhich comes in several guises: transfer learning, multitask\nlearning [20] and curriculum learning [14] to name a few.\nThere is much interest in transferring learning from one task to\nanother, particularly from training in physics simulators with\nvisual renderers and fine-tuning the models in the real world.\nThis can be achieved in a naive fashion, directly using the\nsame network in both the simulated and real phases [167], or\nwith more sophisticated training procedures that directly try\nto mitigate the problem of neural networks “catastrophically\nforgetting” old knowledge by adding extra layers when transferring domain [114, 115]. Other approaches involve directly\nlearning an alignment between simulated and real visuals\n\n[146], or even between two different camera viewpoints [131].\nA different form of transfer can be utilised to help RL in\nthe form of multitask training [77, 54, 82]. Especially with\nneural networks, supervised and unsupervised learning tasks\ncan help train features that can be used by RL agents, making\noptimising the RL objective easier to achieve. For example,\nthe “unsupervised reinforcement and auxiliary learning” A3Cbased agent is additionally trained with “pixel control” (maxi\n\n\nmally changing pixel inputs), plus reward prediction and value\nfunction learning from experience replay [54]. Meanwhile, the\nA3C-based agent of Mirowski et al. [82] was additionally\ntrained to construct a depth map given RGB inputs, which\nhelps it in its task of learning to navigate a 3D environment.\nIn an ablation study, Mirowski et al. [82] showed the predicting\ndepth was more useful than receiving depth as an extra input,\nlending further support to the idea that gradients induced by\nauxiliary tasks can be extremely effective at boosting DRL.\n\nTransfer learning can also be used to construct more\ndata- and parameter-efficient policies. In the student-teacher\nparadigm in machine learning, one can first train a more\npowerful “teacher” model, and then use it to guide the training\nof a less powerful “student” model. Whilst originally applied\nto supervised learning, the neural network knowledge transfer\ntechnique known as _distillation_ [50] has been utilised to both\ntransfer policies learned by large DQNs to smaller DQNs, and\ntransfer policies learned by several DQNs trained on separate\ngames to one single DQN [99, 113]. Together, the combination\nof multitask and transfer learning can improve the sample\nefficiency and robustness of current DRL algorithms [140].\nThese are important topics if we wish to construct agents that\ncan accomplish a wide range of tasks, since naively training\non multiple RL objectives at once may be infeasible.\n\n\n_H. Benchmarks_\n\n\nOne of the challenges in any field in machine learning is\ndeveloping a standardised way to evaluate new techniques.\nAlthough much early work focused on simple, custom MDPs,\nthere shortly emerged control problems that could be used as\nstandard benchmarks for testing new algorithms, such as the\nCartpole [8] and Mountain Car [87] domains.\n\nHowever, these problems were limited to relatively small\nstate spaces, and therefore failed to capture the complexities\nthat would be encountered in most realistic scenarios. Ar\nguably the initial driver of DRL, the ALE provided an interface\nto Atari 2600 video games, with code to access over 50 games\nprovided with the initial release [10]. As video games can vary\ngreatly, but still present interesting and challenging objectives\nfor humans, they provide an excellent testbed for RL agents.\nAs the first algorithm to successfully play a range of these\ngames directly from their visuals, the DQN [84] has secured\nits place as a milestone in the development of RL algorithms.\nThis success story has started a trend of using video games\nas standardised RL testbeds, with several interesting options\nnow available. ViZDoom provides an interface to the Doom\nfirst-person shooter [60], and echoing the popularity of esports competitions, ViZDoom competitions are now held at\nthe yearly IEEE Conference on Computational Intelligence\nin Games. Facebook’s TorchCraft [137] and DeepMind’s\nStarCraft II Learning Environment [153] respectively provide\ninterfaces to the StarCraft and StarCraft II real-time strategy\ngames, presenting challenges in both micromanagement and\nlong-term planning. In an aim to provide more flexible environments, DeepMind Lab was developed on top of the Quake\nIII Arena first-person shooter engine [9], and Microsoft’s\nProject Malmo exposed an interface to the Minecraft sandbox\ngame [55]. Both environments provide customisable platforms\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 13\n\n\n\nfor RL agents in 3D environments.\nMost DRL approaches focus on discrete actions, but some\nsolutions have also been developed for continuous control\nproblems. Many DRL papers in continuous control [122, 46,\n79, 85, 7, 131] have used the MuJoCo physics engine to\nobtain relatively realistic dynamics for multi-joint continuous\ncontrol problems [144], and there has now been some effort\nto standardise these problems [28].\nTo help with standardisation and reproducibility, most of\nthe aforementioned RL domains and more have been made\n\navailable in the OpenAI Gym, a library and online service\nthat allows people to easily interface with and publicly share\nthe results of RL algorithms on these domains [17].\n\n\nVII. CONCLUSION: BEYOND PATTERN RECOGNITION\n\nDespite the successes of DRL, many problems need to be\naddressed before these techniques can be applied to a wide\nrange of complex real-world problems [68]. Recent work with\n(non-deep) generative causal models demonstrated superior\ngeneralisation over standard DRL algorithms [85, 114] in\nsome benchmarks [10], achieved by reasoning about causes\nand effects in the environment [58]. For example, the schema\nnetworks of Kanksy et al. [58] trained on the game “Breakout”\nimmediately adapted to a variant where a small wall was\nplaced in front of the target blocks, whilst progressive (A3C)\nnetworks [114] failed to match the performance of the schema\nnetworks even after training on the new domain. Although\nDRL has already been combined with AI techniques, such as\nsearch [128] and planning [138], a deeper integration with\nother traditional AI approaches promises benefits such as\nbetter sample complexity, generalisation and interpretability\n\n[35]. In time, we also hope that our theoretical understanding\nof the properties of neural networks (particularly within DRL)\nwill improve, as it currently lags far behind practice.\nTo conclude, it is worth revisiting the overarching goal\nof all of this research: the creation of general-purpose AI\nsystems that can interact with and learn from the world around\nthem. Interaction with the environment is simultaneously the\nadvantage and disadvantage of RL. Whilst there are many\nchallenges in seeking to understand our complex and everchanging world, RL allows us to choose how we explore\nit. In effect, RL endows agents with the ability to perform\nexperiments to better understand their surroundings, enabling\nthem to learn even high-level causal relationships. The availability of high-quality visual renderers and physics engines\nnow enables us to take steps in this direction, with works that\ntry to learn intuitive models of physics in visual environments\n\n[27]. Challenges remain before this will be possible in the real\nworld, but steady progress is being made in agents that learn\nthe fundamental principles of the world through observation\nand action. Perhaps, then, we are not too far away from\nAI systems that learn and act in more human-like ways in\nincreasingly complex environments.\n\n\nACKNOWLEDGMENTS\n\nThe authors would like to thank the reviewers and broader\n\ncommunity for their feedback on this survey; in particular,\nwe would like to thank Nicolas Heess for clarifications on\nseveral points. Kai Arulkumaran would like to acknowledge\n\n\n\nPhD funding from the Department of Bioengineering, Imperial\nCollege London. This research has been partially funded by a\nGoogle Faculty Research Award to Marc Deisenroth.\n\n\nREFERENCES\n\n\n[1] Pieter Abbeel and John Schulman. Deep Reinforcement Learning\nthrough Policy Optimization, 2016. Tutorial at NIPS 2016.\n\n[2] Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, and Anil Anthony Bharath. Classifying Options for Deep Reinforcement Learning.\nIn _IJCAI Workshop on Deep Reinforcement Learning: Frontiers and_\n_Challenges_, 2016.\n\n[3] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The Option-Critic\nArchitecture. In _AAAI_, 2017.\n\n[4] J Andrew Bagnell and Jeff Schneider. Covariant Policy Search. In\n_IJCAI_, 2003.\n\n[5] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan\nLowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An ActorCritic Algorithm for Sequence Prediction. In _ICLR_, 2017.\n\n[6] Leemon C Baird III. Advantage Updating. Technical report, DTIC,\n1993.\n\n[7] Nir Baram, Oron Anschel, and Shie Mannor. Model-Based Adversarial Imitation Learning. In _NIPS Workshop on Deep Reinforcement_\n_Learning_, 2016.\n\n[8] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike Adaptive Elements That Can Solve Difficult Learning Control\nProblems. _IEEE Trans. on Systems, Man, and Cybernetics_, (5):834–\n846, 1983.\n\n[9] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus\nWainwright, Heinrich K¨uttler, Andrew Lefrancq, Simon Green, V´ıctor\nVald´es, Amir Sadik, et al. DeepMind Lab. _arXiv:1612.03801_, 2016.\n\n[10] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An Evaluation Platform for\nGeneral Agents. In _IJCAI_, 2015.\n\n[11] Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul,\nDavid Saxton, and R´emi Munos. Unifying Count-Based Exploration\nand Intrinsic Motivation. In _NIPS_, 2016.\n\n[12] Marc G Bellemare, Will Dabney, and R´emi Munos. A Distributional\nPerspective on Reinforcement Learning. In _ICML_, 2017.\n\n[13] Richard Bellman. On the Theory of Dynamic Programming. _PNAS_,\n38(8):716–719, 1952.\n\n[14] Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum Learning. In _ICML_, 2009.\n\n[15] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation\nLearning: A Review and New Perspectives. _IEEE Trans. on Pattern_\n_Analysis and Machine Intelligence_, 35(8):1798–1828, 2013.\n\n[16] Dimitri P Bertsekas. Dynamic Programming and Suboptimal Control:\nA Survey from ADP to MPC. _European Journal of Control_, 11(4-5):\n310–334, 2005.\n\n[17] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym.\n_arXiv:1606.01540_, 2016.\n\n[18] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A Comprehensive survey of Multiagent Reinforcement Learning. _IEEE Trans. on_\n_Systems, Man, And Cybernetics_, 2008.\n\n[19] Murray Campbell, A Joseph Hoane, and Feng-hsiung Hsu. Deep Blue.\n_Artificial Intelligence_, 134(1-2):57–83, 2002.\n\n[20] Rich Caruana. Multitask Learning. _Machine Learning_, 28(1):41–75,\n1997.\n\n[21] Silvia Chiappa, S´ebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent Environment Simulators. In _ICLR_, 2017.\n\n[22] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor\nBlackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba. Transfer from Simulation to Real World through Learning Deep Inverse\nDynamics Model. _arXiv:1610.03518_, 2016.\n\n[23] Giuseppe Cuccu, Matthew Luciw, J¨urgen Schmidhuber, and Faustino\nGomez. Intrinsically Motivated Neuroevolution for Vision-Based\nReinforcement Learning. In _ICDL_, volume 2, 2011.\n\n[24] Peter Dayan. Improving Generalization for Temporal Difference\nLearning: The Successor Representation. _Neural Computation_, 5(4):\n613–624, 1993.\n\n[25] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin,\nMark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al.\nLarge Scale Distributed Deep Networks. In _NIPS_, 2012.\n\n[26] Marc P Deisenroth, Gerhard Neumann, and Jan Peters. A Survey on\nPolicy Search for Robotics. _Foundations and Trends_ R _⃝_ _in Robotics_, 2\n(1–2), 2013.\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 14\n\n\n\n\n[27] Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter\nBattaglia, and Nando de Freitas. Learning to Perform Physics Experiments via Deep Reinforcement Learning. In _ICLR_, 2017.\n\n[28] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter\nAbbeel. Benchmarking Deep Reinforcement Learning for Continuous\nControl. In _ICML_, 2016.\n\n[29] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever,\nand Pieter Abbeel. RL [2] : Fast Reinforcement Learning via Slow\nReinforcement Learning. In _NIPS Workshop on Deep Reinforcement_\n_Learning_, 2016.\n\n[30] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep Reinforcement Learning\nin Large Discrete Action Spaces. _arXiv:1512.07679_, 2015.\n\n[31] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David\nGondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric\nNyberg, John Prager, et al. Building Watson: An Overview of the\nDeepQA Project. _AI Magazine_, 31(3):59–79, 2010.\n\n[32] Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine,\nand Pieter Abbeel. Deep Spatial Autoencoders for Visuomotor Learning. In _ICRA_, 2016.\n\n[33] Jakob Foerster, Yannis M Assael, Nando de Freitas, and Shimon Whiteson. Learning to Communicate with Deep Multi-Agent Reinforcement\nLearning. In _NIPS_, 2016.\n\n[34] Michael C Fu. Gradient Estimation. _Handbooks in Operations_\n_Research and Management Science_, 13:575–616, 2006.\n\n[35] Marta Garnelo, Kai Arulkumaran, and Murray Shanahan. Towards\nDeep Symbolic Reinforcement Learning. In _NIPS Workshop on Deep_\n_Reinforcement Learning_, 2016.\n\n[36] Peter W Glynn. Likelihood Ratio Gradient Estimation for Stochastic\nSystems. _Communications of the ACM_, 33(10):75–84, 1990.\n\n[37] Faustino Gomez and J¨urgen Schmidhuber. Evolving Modular FastWeight Networks for Control. In _ICANN_, 2005.\n\n[38] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative Adversarial Nets. In _NIPS_, 2014.\n\n[39] Audrunas Gruslys, Mohammad Gheshlaghi Azar, Marc G Bellemare,\nand R´emi Munos. The Reactor: A Sample-Efficient Actor-Critic\nArchitecture. _arXiv:1704.04651_, 2017.\n\n[40] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine.\nContinuous Deep Q-Learning with Model-Based Acceleration. In\n_ICLR_, 2016.\n\n[41] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E\nTurner, and Sergey Levine. Q-Prop: Sample-Efficient Policy Gradient\nwith an Off-Policy Critic. In _ICLR_, 2017.\n\n[42] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E\nTurner, Bernhard Sch¨olkopf, and Sergey Levine. Interpolated Policy\nGradient: Merging On-Policy and Off-Policy Gradient Estimation for\nDeep Reinforcement Learning. In _NIPS_, 2017.\n\n[43] Mance E Harmon and Leemon C Baird III. Multi-Player Residual\nAdvantage Learning with General Function Approximation. Technical\nreport, DTIC, 1996.\n\n[44] Matthew Hausknecht and Peter Stone. Deep Recurrent Q-Learning for\nPartially Observable MDPs. In _AAAI Fall Symposium Series_, 2015.\n\n[45] Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, and David Silver.\nMemory-Based Control with Recurrent Neural Networks. In _NIPS_\n_Workshop on Deep Reinforcement Learning_, 2015.\n\n[46] Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez,\nand Yuval Tassa. Learning Continuous Control Policies by Stochastic\nValue Gradients. In _NIPS_, 2015.\n\n[47] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg\nWayne, Yuval Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin\nRiedmiller, et al. Emergence of Locomotion Behaviours in Rich\nEnvironments. _arXiv:1707.02286_, 2017.\n\n[48] Johannes Heinrich and David Silver. Deep Reinforcement Learning\nfrom Self-Play in Imperfect-Information Games. 2016.\n\n[49] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom\nSchaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian\nOsband, John Agapiou, et al. Learning from Demonstrations for Real\nWorld Reinforcement Learning. _arXiv:1704.03732_, 2017.\n\n[50] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. 2014.\n\n[51] Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation\nLearning. In _NIPS_, 2016.\n\n[52] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck,\nand Pieter Abbeel. VIME: Variational Information Maximizing Exploration. In _NIPS_, 2016.\n\n\n\n\n[53] Ahmed Hussein, Mohamed Medhat Gaber, and Eyad Elyan. Deep\nActive Learning for Autonomous Navigation. In _EANN_, 2016.\n\n[54] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom\nSchaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement Learning with Unsupervised Auxiliary Tasks. In _ICLR_,\n2017.\n\n[55] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell.\nThe Malmo Platform for Artificial Intelligence Experimentation. In\n_IJCAI_, 2016.\n\n[56] Leslie P Kaelbling, Michael L Littman, and Anthony R Cassandra.\nPlanning and Acting in Partially Observable Stochastic Domains.\n_Artificial Intelligence_, 101(1):99–134, 1998.\n\n[57] Sham M Kakade. A Natural Policy Gradient. In _NIPS_, 2002.\n\n[58] Ken Kansky, Tom Silver, David A M´ely, Mohamed Eldawy, Miguel\nL´azaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott\nPhoenix, and Dileep George. Schema Networks: Zero-Shot Transfer\nwith a Generative Causal Model of Intuitive Physics. In _ICML_, 2017.\n\n[59] Hilbert J Kappen. Path Integrals and Symmetry Breaking for Optimal\nControl Theory. _JSTAT_, 2005(11):P11011, 2005.\n\n[60] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and\nWojciech Ja´skowski. ViZDoom: A Doom-Based AI Research Platform\nfor Visual Reinforcement Learning. In _CIG_, 2016.\n\n[61] Diederik P Kingma and Max Welling. Auto-Encoding Variational\nBayes. In _ICLR_, 2014.\n\n[62] Nate Kohl and Peter Stone. Policy Gradient Reinforcement Learning\nfor Fast Quadrupedal Locomotion. In _ICRA_, volume 3, 2004.\n\n[63] Vijay R Konda and John N Tsitsiklis. On Actor-Critic Algorithms.\n_SICON_, 42(4):1143–1166, 2003.\n\n[64] Jan Koutn´ık, Giuseppe Cuccu, J¨urgen Schmidhuber, and Faustino\nGomez. Evolving Large-Scale Neural Networks for Vision-Based\nReinforcement Learning. In _GECCO_, 2013.\n\n[65] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh\nTenenbaum. Hierarchical Deep Reinforcement Learning: Integrating\nTemporal Abstraction and Intrinsic Motivation. In _NIPS_, 2016.\n\n[66] Tejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J Gershman. Deep Successor Reinforcement Learning. In _NIPS Workshop_\n_on Deep Reinforcement Learning_, 2016.\n\n[67] Tze Leung Lai and Herbert Robbins. Asymptotically Efficient Adaptive\nAllocation Rules. _Advances in Applied Mathematics_, 6(1):4–22, 1985.\n\n[68] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and\nSamuel J Gershman. Building Machines That Learn and Think Like\nPeople. _The Behavioral and Brain Sciences_, page 1, 2016.\n\n[69] Sascha Lange, Martin Riedmiller, and Arne Voigtlander. Autonomous\nReinforcement Learning on Raw Visual Input Data in a Real World\nApplication. In _IJCNN_, 2012.\n\n[70] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep Learning.\n_Nature_, 521(7553):436–444, 2015.\n\n[71] Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and\nThore Graepel. Multi-Agent Reinforcement Learning in Sequential\nSocial Dilemmas. In _AAMAS_, 2017.\n\n[72] Sergey Levine and Pieter Abbeel. Learning Neural Network Policies\nwith Guided Policy Search under Unknown Dynamics. In _NIPS_, 2014.\n\n[73] Sergey Levine and Vladlen Koltun. Guided Policy Search. In _ICLR_,\n2013.\n\n[74] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. Endto-End Training of Deep Visuomotor Policies. _JMLR_, 17(39):1–40,\n2016.\n\n[75] Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen.\nLearning Hand-Eye Coordination for Robotic Grasping with Deep\nLearning and Large-Scale Data Collection. In _ISER_, 2016.\n\n[76] Ke Li and Jitendra Malik. Learning to Optimize. 2017.\n\n[77] Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen,\nLi Deng, and Ji He. Recurrent Reinforcement Learning: A Hybrid\nApproach. _arXiv:1509.03044_, 2015.\n\n[78] Yuxi Li. Deep Reinforcement Learning: An Overview.\n_arXiv:1701.07274_, 2017.\n\n[79] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,\nTom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous\nControl with Deep Reinforcement Learning. In _ICLR_, 2016.\n\n[80] Long-Ji Lin. Self-Improving Reactive Agents Based on Reinforcement\nLearning, Planning and Teaching. _Machine Learning_, 8(3–4):293–321,\n1992.\n\n[81] Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete Sequential Prediction of Continuous Actions for Deep RL.\n_arXiv:1705.05035_, 2017.\n\n[82] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 15\n\n\n\nKavukcuoglu, et al. Learning to Navigate in Complex Environments.\nIn _ICLR_, 2017.\n\n[83] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray\nKavukcuoglu. Recurrent Models of Visual Attention. In _NIPS_,\n2014.\n\n[84] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,\nJoel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller,\nAndreas K Fidjeland, Georg Ostrovski, et al. Human-Level Control\nthrough Deep Reinforcement Learning. _Nature_, 518(7540):529–533,\n2015.\n\n[85] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex\nGraves, Timothy P Lillicrap, Tim Harley, David Silver, and Koray\nKavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In _ICLR_, 2016.\n\n[86] Shakir Mohamed and Danilo Jimenez Rezende. Variational Information\nMaximisation for Intrinsically Motivated Reinforcement Learning. In\n_NIPS_, 2015.\n\n[87] Andrew William Moore. Efficient Memory-Based Learning for Robot\nControl. Technical report, University of Cambridge, Computer Laboratory, 1990.\n\n[88] R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G Bellemare. Safe and Efficient Off-Policy Reinforcement Learning. In _NIPS_,\n2016.\n\n[89] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.\nBridging the Gap Between Value and Policy Based Reinforcement\nLearning. _arXiv:1702.08892_, 2017.\n\n[90] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey\nLevine. Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. _arXiv:1708.02596_, 2017.\n\n[91] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory\nFearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa\nSuleyman, Charles Beattie, Stig Petersen, et al. Massively Parallel\nMethods for Deep Reinforcement Learning. In _ICML Workshop on_\n_Deep Learning_, 2015.\n\n[92] Andrew Y Ng and Stuart J Russell. Algorithms for Inverse Reinforcement Learning. In _ICML_, 2000.\n\n[93] Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie\nSchulte, Ben Tse, Eric Berger, and Eric Liang. Autonomous Inverted\nHelicopter Flight via Reinforcement Learning. _Experimental Robotics_,\npages 363–372, 2006.\n\n[94] Brendan O’Donoghue, R´emi Munos, Koray Kavukcuoglu, and\nVolodymyr Mnih. PGQ: Combining Policy Gradient and Q-Learning.\nIn _ICLR_, 2017.\n\n[95] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and\nSatinder Singh. Action-Conditional Video Prediction using Deep\nNetworks in Atari Games. In _NIPS_, 2015.\n\n[96] Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, and Honglak\nLee. Control of Memory, Active Perception, and Action in Minecraft.\nIn _ICLR_, 2016.\n\n[97] Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin\nVan Roy. Deep Exploration via Bootstrapped DQN. In _NIPS_, 2016.\n\n[98] Emilio Parisotto and Ruslan Salakhutdinov. Neural Map: Structured\nMemory for Deep Reinforcement Learning. _arXiv:1702.08360_, 2017.\n\n[99] Emilio Parisotto, Jimmy L Ba, and Ruslan Salakhutdinov. ActorMimic: Deep Multitask and Transfer Reinforcement Learning. In _ICLR_,\n2016.\n\n[100] Razvan Pascanu, Yujia Li, Oriol Vinyals, Nicolas Heess, Lars Buesing,\nSebastien Racani`ere, David Reichert, Th´eophane Weber, Daan Wierstra, and Peter Battaglia. Learning Model-Based Planning from Scratch.\n_arXiv:1707.06170_, 2017.\n\n[101] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell.\nCuriosity-Driven Exploration by Self-supervised Prediction. In _ICML_,\n2017.\n\n[102] Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang,\nHaitao Long, and Jun Wang. Multiagent Bidirectionally-Coordinated\nNets: Emergence of Human-level Coordination in Learning to Play\nStarCraft Combat Games. _arXiv:1703.10069_, 2017.\n\n[103] Jan Peters, Katharina M¨ulling, and Yasemin Altun. Relative Entropy\nPolicy Search. In _AAAI_, 2010.\n\n[104] Dean A Pomerleau. ALVINN, an Autonomous Land Vehicle in\na Neural Network. Technical report, Carnegie Mellon University,\nComputer Science Department, 1989.\n\n[105] Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adri`a Puigdom`enech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles\nBlundell. Neural Episodic Control. In _ICML_, 2017.\n\n[106] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech\nZaremba. Sequence Level Training with Recurrent Neural Networks.\n\n\n\nIn _ICLR_, 2016.\n\n[107] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.\nHogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient\nDescent. In _NIPS_, 2011.\n\n[108] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.\nStochastic Backpropagation and Approximate Inference in Deep Generative Models. In _ICML_, 2014.\n\n[109] Martin Riedmiller. Neural Fitted Q Iteration—First Experiences with\na Data Efficient Neural Reinforcement Learning Method. In _ECML_,\n2005.\n\n[110] St´ephane Ross, Geoffrey J Gordon, and Drew Bagnell. A Reduction\nof Imitation Learning and Structured Prediction to No-Regret Online\nLearning. In _AISTATS_, 2011.\n\n[111] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning Representations by Back-Propagating Errors. _Cognitive Modeling_,\n5(3):1, 1988.\n\n[112] Gavin A Rummery and Mahesan Niranjan. _On-line Q-learning using_\n_Connectionist Systems_ . University of Cambridge, Department of\nEngineering, 1994.\n\n[113] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr\nMnih, Koray Kavukcuoglu, and Raia Hadsell. Policy Distillation. In\n_ICLR_, 2016.\n\n[114] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert\nSoyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and\nRaia Hadsell. Progressive Neural Networks. _arXiv:1606.04671_, 2016.\n\n[115] Andrei A Rusu, Matej Vecerik, Thomas Roth¨orl, Nicolas Heess, Razvan\nPascanu, and Raia Hadsell. Sim-to-Real Robot Learning from Pixels\nwith Progressive Nets. In _CoRL_, 2017.\n\n[116] Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution Strategies as a Scalable Alternative to Reinforcement Learning.\n_arXiv:1703.03864_, 2017.\n\n[117] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal\nValue Function Approximators. In _ICML_, 2015.\n\n[118] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experience Replay. In _ICLR_, 2016.\n\n[119] J¨urgen Schmidhuber. A Possibility for Implementing Curiosity and\nBoredom in Model-Building Neural Controllers. In _SAB_, 1991.\n\n[120] J¨urgen Schmidhuber and Rudolf Huber. Learning to Generate Artificial\nFovea Trajectories for Target Detection. _IJNS_, 2(01n02):125–134,\n1991.\n\n[121] John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel.\nGradient Estimation using Stochastic Computation Graphs. In _NIPS_,\n2015.\n\n[122] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and\nPhilipp Moritz. Trust Region Policy Optimization. In _ICML_, 2015.\n\n[123] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and\nPieter Abbeel. High-Dimensional Continuous Control using Generalized Advantage Estimation. In _ICLR_, 2016.\n\n[124] John Schulman, Pieter Abbeel, and Xi Chen. Equivalence Between\nPolicy Gradients and Soft Q-Learning. _arXiv:1704.06440_, 2017.\n\n[125] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\nand Oleg Klimov. Proximal Policy Optimization Algorithms.\n_arXiv:1707.06347_, 2017.\n\n[126] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and\nNando de Freitas. Taking the Human out of the Loop: A Review\nof Bayesian Optimization. _Proc. of the IEEE_, 104(1):148–175, 2016.\n\n[127] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic Policy Gradient Algorithms.\nIn _ICML_, 2014.\n\n[128] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis\nAntonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe Game of Go with Deep Neural Networks and Tree Search. _Nature_,\n529(7587):484–489, 2016.\n\n[129] Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker.\nOptimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. _JAIR_, 16:105–133, 2002.\n\n[130] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov,\nand Anastasiia Ignateva. Deep Attention Recurrent Q-Network. In\n_NIPS Workshop on Deep Reinforcement Learning_, 2015.\n\n[131] Bradley C Stadie, Pieter Abbeel, and Ilya Sutskever. Third Person\nImitation Learning. In _ICLR_, 2017.\n\n[132] Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing\nExploration in Reinforcement Learning with Deep Predictive Models.\nIn _NIPS Workshop on Deep Reinforcement Learning_, 2015.\n\n[133] Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and\n\n\n\n\nIEEE SIGNAL PROCESSING MAGAZINE, SPECIAL ISSUE ON DEEP LEARNING FOR IMAGE UNDERSTANDING (ARXIV EXTENDED VERSION) 16\n\n\n\nMichael L Littman. PAC Model-Free Reinforcement Learning. In\n_ICML_, 2006.\n\n[134] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning\nMultiagent Communication with Backpropagation. In _NIPS_, 2016.\n\n[135] Richard S Sutton and Andrew G Barto. _Reinforcement Learning: An_\n_Introduction_ . MIT Press, 1998.\n\n[136] Richard S Sutton, Doina Precup, and Satinder Singh. Between\nMDPs and Semi-MDPs: A Framework for Temporal Abstraction in\nReinforcement Learning. _Artificial Intelligence_, 112(1–2):181–211,\n1999.\n\n[137] Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala,\nTimoth´ee Lacroix, Zeming Lin, Florian Richoux, and Nicolas Usunier.\nTorchCraft: A Library for Machine Learning Research on Real-Time\nStrategy Games. _arXiv:1611.00625_, 2016.\n\n[138] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel.\nValue Iteration Networks. In _NIPS_, 2016.\n\n[139] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen,\nYan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. #Exploration: A Study of Count-Based Exploration for Deep Reinforcement\nLearning. In _NIPS_, 2017.\n\n[140] Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan,\nJames Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu.\nDistral: Robust Multitask Reinforcement Learning. In _NIPS_, 2017.\n\n[141] Gerald Tesauro. Temporal Difference Learning and TD-Gammon.\n_Communications of the ACM_, 38(3):58–68, 1995.\n\n[142] Gerald Tesauro, Rajarshi Das, Hoi Chan, Jeffrey Kephart, David\nLevine, Freeman Rawson, and Charles Lefurgy. Managing Power Consumption and Performance of Computing Systems using Reinforcement\nLearning. In _NIPS_, 2008.\n\n[143] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and\nShie Mannor. A Deep Hierarchical Approach to Lifelong Learning in\nMinecraft. In _AAAI_, 2017.\n\n[144] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A Physics\nEngine for Model-Based Control. In _IROS_, 2012.\n\n[145] John N Tsitsiklis and Benjamin Van Roy. Analysis of TemporalDifference Learning with Function Approximation. In _NIPS_, 1997.\n\n[146] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao\nPeng, Sergey Levine, Kate Saenko, and Trevor Darrell. Towards\nAdapting Deep Visuomotor Representations from Simulated to Real\nEnvironments. In _WAFR_, 2016.\n\n[147] Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala.\nEpisodic Exploration for Deep Deterministic Policies: An Application\nto StarCraft Micromanagement Tasks. In _ICLR_, 2017.\n\n[148] Hado van Hasselt. Double Q-Learning. In _NIPS_, 2010.\n\n[149] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement\nLearning with Double Q-Learning. In _AAAI_, 2016.\n\n[150] Harm Vanseijen and Rich Sutton. A Deeper Look at Planning as\nLearning from Replay. In _ICML_, 2015.\n\n[151] Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex\nGraves, Oriol Vinyals, John Agapiou, and Koray Kavukcuoglu. Strategic Attentive Writer for Learning Macro-Actions. In _NIPS_, 2016.\n\n[152] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas\nHeess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. FeUdal\nNetworks for Hierarchical Reinforcement Learning. In _ICML_, 2017.\n\n[153] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich\nK¨uttler, John Agapiou, Julian Schrittwieser, et al. StarCraft II: A New\nChallenge for Reinforcement Learning. _arXiv:1708.04782_, 2017.\n\n[154] Niklas Wahlstr¨om, Thomas B Sch¨on, and Marc P Deisenroth. Learning\nDeep Dynamical Models from Image Pixels. _IFAC SYSID_, 48(28),\n2015.\n\n[155] Niklas Wahlstr¨om, Thomas B Sch¨on, and Marc P Deisenroth. From\nPixels to Torques: Policy Learning with Deep Dynamical Models. In\n_ICML Workshop on Deep Learning_, 2015.\n\n[156] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer,\nJoel Z Leibo, R´emi Munos, Charles Blundell, Dharshan Kumaran, and\nMatt Botvinick. Learning to Reinforcement Learn. In _CogSci_, 2017.\n\n[157] Ziyu Wang, Nando de Freitas, and Marc Lanctot. Dueling Network\nArchitectures for Deep Reinforcement Learning. In _ICLR_, 2016.\n\n[158] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, R´emi\nMunos, Koray Kavukcuoglu, and Nando de Freitas. Sample Efficient\nActor-Critic with Experience Replay. In _ICLR_, 2017.\n\n[159] Christopher JCH Watkins and Peter Dayan. Q-Learning. _Machine_\n_Learning_, 8(3-4):279–292, 1992.\n\n[160] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin\nRiedmiller. Embed to Control: A Locally Linear Latent Dynamics\nModel for Control from Raw Images. In _NIPS_, 2015.\n\n\n\n\n[161] Th´eophane Weber, S´ebastien Racani`ere, David P Reichert, Lars\nBuesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdom`enech\nBadia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. ImaginationAugmented Agents for Deep Reinforcement Learning. In _NIPS_, 2017.\n\n[162] Paul John Werbos. Beyond Regression: New Tools for Prediction\nand Analysis in the Behavioral Sciences. Technical report, Harvard\nUniversity, Applied Mathematics, 1974.\n\n[163] Daan Wierstra, Alexander F¨orster, Jan Peters, and J¨urgen Schmidhuber.\nRecurrent Policy Gradients. _Logic Journal of the IGPL_, 18(5):620–634,\n2010.\n\n[164] Ronald J Williams. Simple Statistical Gradient-Following Algorithms\nfor Connectionist Reinforcement Learning. _Machine Learning_, 8(3-4):\n229–256, 1992.\n\n[165] Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum\nEntropy Deep Inverse Reinforcement Learning. In _NIPS Workshop on_\n_Deep Reinforcement Learning_, 2015.\n\n[166] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C\nCourville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio.\nShow, Attend and Tell: Neural Image Caption Generation with Visual\nAttention. In _ICML_, volume 14, 2015.\n\n[167] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav\nGupta, Li Fei-Fei, and Ali Farhadi. Target-Driven Visual Navigation\nin Indoor Scenes using Deep Reinforcement Learning. In _ICRA_, 2017.\n\n[168] Barret Zoph and Quoc V Le. Neural Architecture Search with\nReinforcement Learning. In _ICLR_, 2017.\n\n\n**Kai Arulkumaran** (ka709@imperial.ac.uk) is a Ph.D. candidate in the\nDepartment of Bioengineering at Imperial College London. He received a\nB.A. in Computer Science at the University of Cambridge in 2012, and an\nM.Sc. in Biomedical Engineering at Imperial College London in 2014. He\nwas a Research Intern in Twitter Magic Pony and Microsoft Research in\n2017. His research focus is deep reinforcement learning and transfer learning\nfor visuomotor control.\n\n\n**Marc Peter Deisenroth** (m.deisenroth@imperial.ac.uk) is a Lecturer in\nStatistical Machine Learning in the Department of Computing at Imperial\nCollege London and with PROWLER.io. He received an M.Eng. in Computer\nScience at the University of Karlsruhe in 2006 and a Ph.D. in Machine\nLearning at the Karlsruhe Institute of Technology in 2009. He has been\nawarded an Imperial College Research Fellowship in 2014 and received Best\nPaper Awards at ICRA 2014 and ICCAS 2016. He is a recipient of a Google\nFaculty Research Award and a Microsoft Ph.D. Scholarship. His research\nis centred around data-efficient machine learning for autonomous decision\nmaking.\n\n\n**Miles Brundage** (miles.brundage@philosophy.ox.ac.uk) is a Ph.D. candidate\nin Human and Social Dimensions of Science and Technology at Arizona\nState University, and a Research Fellow at the University of Oxford’s Future\nof Humanity Institute. He received a B.A. in Political Science at George\nWashington University in 2010. His research focuses on governance issues\nrelated to artificial intelligence.\n\n\n**Anil Anthony Bharath** (a.bharath@imperial.ac.uk) is a Reader in the Department of Bioengineering at Imperial College London and a Fellow of the\nInstitution of Engineering and Technology. He received a B.Eng. in Electronic\nand Electrical Engineering from University College London in 1988, and\na Ph.D. in Signal Processing from Imperial College London in 1993. He\nwas an academic visitor in the Signal Processing Group at the University of\nCambridge in 2006. He is a co-founder of Cortexica Vision Systems. His\nresearch interests are in deep architectures for visual inference.\n\n\n",
    "ranking": {
      "relevance_score": 0.7419432789997495,
      "citation_score": 0.9,
      "recency_score": 0.237512941355269,
      "final_score": 0.7231115894353516
    },
    "citation_key": "Arulkumaran2017DeepRL",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "10f7f74473a3bb998b97fdcf7305c85e8d4ff000",
    "title": "Solving Robotic Manipulation With Sparse Reward Reinforcement Learning Via Graph-Based Diversity and Proximity",
    "published": "2023-03-01",
    "authors": [
      "Zhenshan Bing",
      "Hongkuan Zhou",
      "Rui Li",
      "Xiaojie Su",
      "F. O. Morin",
      "Kai Huang",
      "Alois Knoll"
    ],
    "summary": "In multigoal reinforcement learning (RL), algorithms usually suffer from inefficiency in the collection of successful experiences in tasks with sparse rewards. By utilizing the ideas of relabeling hindsight experience and curriculum learning, some prior works have greatly improved the sample efficiency in robotic manipulation tasks, such as hindsight experience replay (HER), hindsight goal generation (HGG), graph-based HGG (G-HGG), and curriculum-guided HER (CHER). However, none of these can learn efficiently to solve challenging manipulation tasks with distant goals and obstacles, since they rely either on heuristic or simple distance-guided exploration. In this article, we introduce graph-curriculum-guided HGG (GC-HGG), an extension of CHER and G-HGG, which works by selecting hindsight goals on the basis of graph-based proximity and diversity. We evaluated GC-HGG in four challenging manipulation tasks involving obstacles in both simulations and real-world experiments, in which significant enhancements in both sample efficiency and overall success rates over prior works were demonstrated. Videos and codes can be viewed at this link: https://videoviewsite.wixsite.com/gc-hgg.",
    "pdf_url": "https://mediatum.ub.tum.de/doc/1657383/document.pdf",
    "doi": "10.1109/TIE.2022.3172754",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "IEEE transactions on industrial electronics (1982. Print)",
    "citation_count": 36,
    "bibtex": "@Article{Bing2023SolvingRM,\n author = {Zhenshan Bing and Hongkuan Zhou and Rui Li and Xiaojie Su and F. O. Morin and Kai Huang and Alois Knoll},\n booktitle = {IEEE transactions on industrial electronics (1982. Print)},\n journal = {IEEE Transactions on Industrial Electronics},\n pages = {2759-2769},\n title = {Solving Robotic Manipulation With Sparse Reward Reinforcement Learning Via Graph-Based Diversity and Proximity},\n volume = {70},\n year = {2023}\n}\n",
    "markdown_text": "This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n## Solving Robotic Manipulation with Sparse Reward Reinforcement Learning via Graph-Based Diversity and Proximity\n\n\nZhenshan Bing, Hongkuan Zhou, Rui Li, Xiaojie Su, _Senior Member, IEEE_\nFabrice O. Morin, Kai Huang, Alois Knoll, _Senior Member, IEEE_\n\n\n\n_**Abstract**_ **—In multi-goal reinforcement learning (RL), al-**\n**gorithms usually suffer from inefficiency in the collection**\n**of successful experiences in tasks with sparse rewards.**\n**By utilizing the ideas of relabeling hindsight experience**\n**and curriculum learning, some prior works have greatly**\n**improved the sample efficiency in robotic manipulation**\n**tasks, such as hindsight experience replay (HER), hind-**\n**sight goal generation (HGG), graph-based hindsight goal**\n**generation (G-HGG), and curriculum-guided hindsight ex-**\n**perience replay (CHER). However, none of these can learn**\n**efficiently for solving challenging manipulation tasks with**\n**distant goals and obstacles, since they rely either on**\n**heuristic or simple distance-guided exploration. In this**\n**work, we introduce graph-curriculum-guided hindsight goal**\n**generation (GC-HGG), an extension of CHER and G-HGG,**\n**that works by selecting hindsight goals on the basis of**\n**graph-based proximity and diversity. We evaluated GC-**\n**HGG in four challenging manipulation tasks involving ob-**\n**stacles in both simulations and real-world experiments,**\n**in which significant enhancements in both sample effi-**\n**ciency and overall success rates over prior works were**\n**demonstrated. Videos and codes can be viewed at this link:**\n**[https://videoviewsite.wixsite.com/gc-hgg.](https://videoviewsite.wixsite.com/gc-hgg)**\n\n\n_**Index Terms**_ **—Reinforcement learning, hindsight experi-**\n**ence replay, robotic arm manipulation, path planning.**\n\n\nI. INTRODUCTION\n# D EEP reinforcement learning has successfully revolution-ized the process of solving decision-making problems\n\nin many areas ranging from robotics, for example, solving\na Rubik’s cube or enabling autonomous driving, to mind\ngames such as AlphaGo, Atari games, and Starcraft [1]–[6].\nA recurrent problem of RL, however, is that it requires handcrafted reward functions that are tailored to individual tasks,\nwhich usually feature complex and as yet unknown behaviors\nin most real-world applications. Therefore, the design of a\nproper reward is challenging and a major impediment to the\nwidespread adoption of RL for use in real-world applications.\nOn the one hand, recent work has shown that learning\nwith sparse rewards, such as binary signals indicating a task’s\ncompletion, can enable better applicability to multi-goal RL\ntasks than engineered dense rewards, since sparse rewards can\n\n\nZ. Bing, H. Zhou, F. Morin, and A. Knoll are with the Department of Informatics, Technical University of Munich, Germany. E-mail:\nbing@in.tum.de. K. Huang is with the School of Computer Science,\nSun Yat-sen University, China. R. Li and X. Su are with the School of\nAutomation, Chongqing University, China.\n\n\n\neasily be derived from the task definition without any further\nmanual engineering effort. On the other hand, the sparsity of\nsuch rewards hinders the ability to collect sufficient successful\nexperience, usually resulting in a long learning period and\na low success rate. To tackle this issue, researchers have\nproposed several ideas to improve the sample efficiency, such\nas relabeling hindsight experience with different tasks [7] or\nlearning on a designed sequence of training samples, such as\na curriculum, to progress from easy to difficult [8].\nOne such relabeling approach is hindsight experience replay\n(HER) [9], which greatly improves the success rate and sample\nefficiency of RL algorithms in multi-goal RL tasks with sparse\nrewards. HER first learns with hand-crafted heuristic hindsight\ngoals from previously achieved states that are easy to reach\nand then continues with difficult goals. However, it is limited\nin that it is only applicable when goals can be easily reached\nby heuristic explorations and fails in environments with distant\ngoals. In such environments, the agent only explores around\nthe initial states and will never be able to reach real goals, since\nit experiences no positive reward during random explorations.\nCurriculum-guided HER (CHER) [8] extended the idea\nof HER by adaptively prioritizing the replay buffer entries\naccording to the diversities with respect to other replay goals\nand the proximities to target goals. The diversity metric is formulated as the intra-distances among hindsight goals selected\nfor training and is maximized to encourage exploration. The\nproximity metric is modeled as the Euclidean distance between\nhindsight goals and target goals and is minimized to encourage\nexploitation. However, CHER is not applicable to tasks with\nobstacles that are able to mislead the Euclidean distance.\n\nMoreover, it fails to provide a mechanism for designing\na proper trade-off with which to balance the diversity and\nproximity, which leads to low sample efficiency in complex\ntasks where delicate exploration strategies are required.\nAnother idea for improving the sample efficiency is based\non curriculum learning, which aims to design a proper curriculum for guiding the exploration step by step towards final\ngoals. Hindsight goal generation (HGG) [10] is an automatic\ncurriculum generation approach that selects proper intermediate goals that can lead the agent to the target goals and are\nalso easy to reach at the same time. HGG is better able to\nsolve tasks with long-distant goals than HER or CHER, since\nit can experience positive rewards when reaching intermediate\ngoals generated by the curriculum. Similar to CHER, HGG\nuses the Euclidean distance to define the Wasserstein distance\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\nto measure the distance between a set of intermediate goals\nand the final goal distribution. Therefore, HGG is also not\napplicable in environments with obstacles, in which the shortest obstacle-avoiding distance between two goals cannot be\ncomputed using the Euclidean metric. On the basis of HGG,\nour prior work, graph-based hindsight goal generation (GHGG), solves this problem by selecting hindsight goals based\non shortest distances in an obstacle-avoiding graph, which is\na discrete representation of the environment [11]. Although\nG-HGG is applicable to environments with obstacles, it only\nlearns from randomly sampled hindsight goals from replaybuffer, in which not all the experiences are equally useful to\ndifferent learning stages. Furthermore, the sample efficiency\nof G-HGG highly depends on the gird size. A larger grid-size\nmeans a higher error in the shortest obstacle-avoiding distance\nand therefore miss-lead the selection of intermediate goals.\nTo overcome the aforementioned limitations, we propose graph-curriculum-guided hindsight goal generation (GCHGG), an extension of CHER and G-HGG, to improve the\nsampling efficiency of solving complex robotic manipulation\ntasks with obstacles within the framework of sparse-reward\nRL. First, we create an obstacle-aware graph representation\nof the environment as pre-training steps. Second, we define\na graph-based diversity that is lightweight to compute and a\ngraph-based proximity that can guide the object through obstacles. Third, we design a trade-off mechanism that automatically balances diversity and proximity exploration. Finally,\nby comparing performances in four challenging manipulation\nenvironments, we show that GC-HGG provides a significant\nenhancement in both sample efficiency and success rate over\nHER, CHER, HGG, and G-HGG. By transferring learned policies from simulations, we were able to successfully perform\nGC-HGG on the four tasks in the real world.\n\nOur main contribution to the literature is an algorithm that\nbridges graph-based planning, diversity and proximity-based\nexploration, and automatic curriculum generation for solving\ncomplex manipulation tasks. First, we upgrade the design of\nthe graph-based representation of an environment proposed in\nG-HGG [11] by eliminating the possible interference between\nthe obstacles and the body of the robotic arm. Second, the new\nlightweight graph-based diversity and objective optimization\napproach can greatly reduce the computation burden, while\nthe graph-based proximity can guide the agent through environments with obstacles. By introducing this graph-based\ndiversity, the negative effect of large gird-size on G-HGG\ncan be largely reduced because the agent can still explore\nthe environment, even intermediate goals are not selected\nappropriately. Third, our automatic mechanism for balancing\nproximity and diversity is a general solution that can be\napplied to different environments at different training stages.\n\n\nII. RELATED WORK\n\n\nThere are a number of previous studies that aim to develop\ninformative and effective exploration strategies for solving\ngoal-conditioned RL tasks with sparse rewards. We briefly\nintroduce them on the basis of two main ideas.\n\n\n\n_A. Hindsight Experience Relabeling and Prioritization_\n\nHindsight experience relabeling is a data enrichment\nmethod, and its intuitive idea is that some trajectory that is\nless informative for the current task is likely to be a rich\ninformation source for other tasks. By relabeling one trajectory\nwith a different task that the behavior is better suited to, the\nknowledge gained can be used for different tasks and thus\nimproves the sample efficiency. HER achieves its success by\nrelabeling past experience with a heuristic choice of hindsight\ngoals from achieved states, but it suffers from its inefficient\nrandom replay of experience. Prioritized sampling is a method\nof enforcing exploration on valuable experiences, and some\nof its ideas are based on the temporal-difference error [12],\nreward-weighted entropy [13], transition energy [14], and\ndiversity-proximity of achieved goals [8].\n\n\n_B. Curriculum Learning and Automatic Goal Generation_\n\nCurriculum learning is another method of tackling the\nexploration problem in sparse-reward multi-goal RL. It aims\nto create a curriculum that enforces the agent to learn skills\nthat are suitable for the current learning stage, according to\nthe current ability of the agent. There are several ways of\ncreating such a curriculum, for example, using an intrinsic\nmotivation to improve exploration [15]–[20]. Another way of\nconstructing a meaningful curriculum is to predict high-reward\nstates and to generate goals close to these meaningful states\n\n[8], [21]–[23]. Another idea for improving exploration that is\nsimilar to curriculum learning is automatic goal generation,\nwhich aims to first solve some sub-problems (easy problems)\nthat will be helpful later on for solving the complex problems.\nThe generation of these sub-problems (which are often intermediate goals) is expected to be automatic. One approach for\nselecting appropriate goals for the current training stage is to\nuse a goal generative adversarial network (Goal GAN) [24].\nA goal discriminator is trained to evaluate whether a goal is at\nthe appropriate level of difficulty for the current policy. Some\nother ideas are based on making some certain characteristics of\na goal [25], inverse dynamics [26], and imitation learning [7].\nAlthough all of the above methods have demonstrated\nimproved exploration, they share one significant drawback:\nexploration is not well guided towards distant target goals.\nIn other words, when the goals are “hidden” from the agent,\ne.g., when the target goals are far away from the initial goals\nor blocked by obstacles, the unguided exploration process may\ntake too long to learn the task or may even fail to do so.\n\n\nIII. PRELIMINARY\n\n_A. Curriculum-Guided Hindsight Experience Replay_\n\nC-HER [8] samples hindsight goals guided by the diversity\nand proximity. The diversity demonstrates an agent’s curiosity\nto explore an environment. A set of goals with high diversity\nmeans that there is more exploration in different states and\ndifferent areas within the environment. The proximity denotes\nhow close these goals are to the desired goals. A large\nproximity enforces training towards the desired goals.\nConsider that we have already sampled all the achieved\ntrajectories in a replay buffer _B_ . A set of _B_ contains all goals\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\nachieved in previous exploration. In each iteration, a minibatch needs to be sampled to train the policy. In contrast to\nuniform sampling, C-HER proposes selecting a subset _A_ of _B_\n( _A ⊂_ _B_ ), according to the diversity and proximity of _A_ . The\nproximity of _A_ can be defined as,\n\n\n_Fprox_ ≜ _C_ 1 _−_ � ( _dis_ ( _gi, g_ )) (1)\n\n_gi∈A_\n\n\nwhere _dis_ ( _gi, g_ ) is the Euclidean distance between a hindsight\ngoal _gi_ and the target goal _g_ . _C_ 1 is a constant to ensure\n_Fprox ≥_ 0. Apparently, as the distance _dis_ ( _gi, g_ ) decreases,\nthe proximity of set _A_ increases. The diversity is defined as\n\n\n_Fdiv_ ( _A_ ) ≜ _C_ 2 _−_ � min _i∈A_ _[dis]_ [(] _[g][i][, g][j]_ [)][,] (2)\n\n_j∈B/A_\n\n\nwhere _C_ 2 is a constant to ensure _Fdiv ≥_ 0. It should be\nnoted that the values of _C_ 1 and _C_ 2 do not require any prior\nknowledge, since they can simply be set with large values\nin practice. Then the selection of _A_ from _B_ can be solved\nby following combinatorial optimization that maximizes both\nproximity and diversity:\n\n\nmax (3)\n_A⊂B_ _[F]_ [(] _[A]_ [)][ ≜] _[λF][prox]_ [(] _[A]_ [) +] _[ F][div]_ [(] _[A]_ [)]\n\n\nwhere _λ_ is a trade-off weight that balances the proximity and\ndiversity, which controls the proportion of the diversity and\nproximity during training. It increase gradually in line with\n\n\n_λk_ = (1 + _τ_ ) _[k]_ _λ_ 0 (4)\n\n\nwhere _τ_ determines the increasing rate of the proximity.\n\n\n_B. Hindsight Goal Generation (HGG)_\n\n\nHGG [10] extends HER to tasks with distant goal distributions that are far away from the initial state distribution\nand cannot be solved by heuristic exploration. These target\ngoals _GT_ belong to a goal space _G_ and the initial states _S_ 0\nbelong to the state space _S_ . The distribution _T_ _[∗]_ : _G × S →_ R\ndetermines how they are sampled. Instead of optimizing _V_ _[π]_\n\nwith the difficult target goal-initial state distribution _T_ _[∗]_, which\ncarries the risk of being too far from the know goals, HGG\ntries to optimize with a set of intermediate goals sampled from\n_T_ . On the one hand, the goals contained in _T_ should be easy\nto reach, which requires a high _V_ _[π]_ ( _T_ ). On the other hand,\ngoals in _T_ should be close enough to _T_ _[∗]_ to be challenging\nfor the agent. This trade-off can be formalized as\n\n\nmax _T,π_ _[V][ π]_ [(] _[T]_ [ )] _[ −]_ _[L][ · D]_ [(] _[T][ ∗][,][ T]_ [ )][.] (5)\n\n\nThe Lipschitz constant _L_ is treated as a hyper-parameter. In\npractice, to select these goals, HGG first approximates _T_ _[∗]_ by\ntaking _K_ samples from _T_ _[∗]_ and storing them in _T_ [ˆ] _[∗]_ . Then,\nfor an initial state and goal (ˆ _s_ _[i]_ 0 _[,]_ [ ˆ] _[g][i]_ [)] _[ ∈]_ _[T]_ [ˆ] _[ ∗]_ [, HGG selects a]\ntrajectory _τ_ = _{st}_ _[T]_ _t_ =1 [that minimizes the following function:]\n\n\n_w_ (ˆ _s_ _[i]_ 0 _[,]_ [ ˆ] _[g][i][, τ]_ [) :=] _[ c][∥][m]_ [(ˆ] _[s][i]_ 0 [)] _[ −]_ _[m]_ [(] _[s]_ [0][)] _[∥]_\n\n\n\n_m_ ( _·_ ) is a state abstraction that maps from the state space to\nthe goal space. _||_ is a symbol of concatenation. _c >_ 0 provides\na trade-off between 1) the distance between target goals and\n2) the distance between the goal representation of the initial\nstates. Finally, from each of the _K_ selected trajectories _τ_ _[i]_,\nthe hindsight goal _g_ _[i]_ is selected from the state _s_ _[i]_ _t_ _[∈]_ _[τ][ i]_ [, that]\nminimized (6).\n\n\n\n_g_ _[i]_ := arg min\n_st∈τ_\n\n\n\nˆ\n_∥g_ _[i]_ _−_ _m_ ( _st_ ) _∥−_ [1] . (7)\n� _L_ _[V][ π]_ [((] _[s]_ [0] _[∥][m]_ [(] _[s][t]_ [))] �\n\n\n\n_C. Graph-based Hindsight Goal Generation (G-HGG)_\n\n\nOur prior work G-HGG replaces the Euclidean-distance\nwith Graph-based distance to get the appropriate obstacleavoiding shortest distance to guide the exploration. To do that,\nwe create a bounded goal space _GA_ containing all the goals\nthat the object can reach. Then we create a representation of\n_GA_ with a graph _G_ = ( _P, E_ ), which consists of a set of\nvertices _P_, weighted edges _E_, and an assigned weight _w_ .\n\n\n_E ⊂{_ ( _p_ 1 _, p_ 2 _, w_ ) _|_ ( _p_ 1 _, p_ 2) _∈_ _P_ [2] _, p_ 1 _̸_ = _p_ 2 _, w ∈_ R _},_ (8)\n\n\nwhere _p_ 1 and _p_ 2 are two possible vertices.\nIn environments with obstacles, any goals _gobs ∈G_ lying\nwithin an obstacle that are blocked from being reached are not\nelements of the accessible goal space _gobs /∈GA_ . Since _GA_ is\nbounded, it can be enclosed in a parallelepipedic bounding\nbox defined by values _xmin_, _xmax_, _ymin_, _ymax_, _zmin_, _zmax_\n_∈_ R, which describes the span of the box in each coordinate\ndirection. We then use this box to generate a finite set of\nvertices _P_ [ˆ], spatially arranged in an orthorhombic lattice. _P_ [ˆ] is\ndefined by the total number of vertices _n_ = _nx · ny · nz_, with\n_nx, ny, nz ∈_ N in each direction of _GA_, or alternatively by the\ndistance between two adjacent grid-points in each coordinate\ndirection given by\n\n\n\n_nx_ _[−]_ _−_ _[x]_ 1 _[min]_ _,_ ∆ _y_ = _[y][max]_ _n_ _[−]_ _−_ _[y]_ 1 _[min]_\n\n\n\n∆ _x_ = _[x][max][−][x][min]_\n\n\n\n_ny_ _[−]_ _−_ _[y]_ 1 _[min]_ _,_ ∆ _z_ = _[z][max]_ _nz_ _[−]_ _−_ _[z]_ 1 _[min]_\n\n\n\n(6)\n\n\n\n+ min\n_st∈τ_\n\n\n\n_∥g_ ˆ _[i]_ _−_ _m_ ( _st_ ) _∥−_ [1]\n� _L_\n\n\n\n\n[1] .\n\n_L_ _[V][ π]_ [((] _[s]_ [0] _[||][m]_ [(] _[s][t]_ [))] �\n\n\n\n_x_ _nx−_ 1 _,_ _y_ _ny−_ 1 _,_ _z_ _nz−_ 1 _._\n\n(9)\nFinally, the set of vertices are defined as _P_ = _P_ [ˆ] _∩GA_, where\n\n\n_P_ ˆ := _{_ ( _xmin_ + ∆ _x · i, ymin_ + ∆ _y · j, zmin_ + ∆ _z · k_ ) _|_\n_i ∈_ [0 _, nx −_ 1] _, j ∈_ [0 _, ny −_ 1] _, k ∈_ [0 _, nz −_ 1] _}._\n(10)\n\n\nIn the next step, we connect two adjacent vertices _p_ 1 =\n(ˆ _x_ 1 _,_ ˆ _y_ 1 _,_ ˆ _z_ 1) _∈_ _P, p_ 2 = (ˆ _x_ 2 _,_ ˆ _y_ 2 _,_ ˆ _z_ 2) _∈_ _P_ with an edge of\nweight _w_ considering the following:\n\n( _p_ 1 _, p_ 2 _, w_ ) _∈_ _E ⇐⇒|x_ ˆand2 _− |z_ ˆ _x_ ˆ21 _−| ≤z_ ˆ1∆ _| ≤x_, _|y_ ∆ˆ2 _−z,_ _y_ ˆ1 _| ≤_ ∆ _y_ (11)\n\n\nwhere _w_ := ~~�~~ (ˆ _x_ 2 _−_ _x_ ˆ1) [2] + (ˆ _y_ 2 _−_ _y_ ˆ1) [2] + _·_ (ˆ _z_ 2 _−_ _z_ ˆ1) [2] .\n\nIn environments with obstacles, it is important to ensure\nthat no edge in the graph cuts through an obstacle. To achieve\nthat, we define the space of one of the cuboids with the\nedges _α, β, γ_ . The graph must therefore satisfy the graph\ndensity criterion (12) for every convex sub-obstacle, i.e., the\ncontinuous set goals not included in _GA_ :\n\n\n∆ _x < αmin_ _[obs]_ [; ∆] _[y]_ _[< β]_ _min_ _[obs]_ [; ∆] _[z]_ _[< γ]_ _min_ _[obs]_ _[,]_ (12)\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\nwhere _αmin_ _[obs]_ [,] _[ β]_ _min_ _[obs]_ [,] _[ γ]_ _min_ _[obs]_ _[∈]_ [R][, describing the infimum length]\nof edges of all the obstacles.\nConsidering the graph that represents the environment,\nwe employ a shortest path algorithm, such as Dijkstra’s\nalgorithm [27], to calculate the shortest paths and shortest\ndistances _d_ [ˆ] _G_ between every possible pair of vertices ( _p_ 1 _, p_ 2) =\n((ˆ _x_ 1 _,_ ˆ _y_ 1 _,_ ˆ _z_ 1) _,_ (ˆ _x_ 2 _,_ ˆ _y_ 2 _,_ ˆ _z_ 2)) _∈_ _P_ [2] in a graph _G_ = ( _P, E_ ).\nAll possible combinations of the resulting shortest distance\nfunction _d_ [ˆ] _G_ can be efficiently pre-computed and stored in an\n_n × n_ table, where _n_ denotes the number of vertices in _P_ .\nGiven two goals _g_ 1 = ( _x_ 1 _, y_ 1 _, z_ 1) _∈G_, _g_ 2 = ( _x_ 2 _, y_ 2 _, z_ 2) _∈_\n_G_ and a graph _G_ = ( _P, E_ ) representing the approximate goal\nspace _GA ⊂G_ where _xmin_, _xmax_, _ymin_, _ymax_, _zmin_, _zmax_,\n∆ _x_, ∆ _y_, and ∆ _z_, the graph-based distance _d_ : _G_ [2] _→_ R is\ndefined such that\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n|**_m_(****_s_)**|sho|rtest p|ath|**_g_**|\n|**_m_(****_s_)**|sho||||\n||||||\n||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n|**_m_(****_s_)**||||**_g_**|\n||||||\n||sho|rtest p|ath||\n\n\n\n(b) Graph-based distance.\n\n|Col1|Col2|Col3|\n|---|---|---|\n|**_m_(****_s_)**||**_g_**|\n|invalid|graph-bas|ed path|\n\n\n|d) In ance|nvalid|Col3|d gr|raph-|Col6|-based|\n|---|---|---|---|---|---|---|\n|N<br>interfe|o<br>rence|o<br>rence||Safety<br>region|Safety<br>region||\n||||**_g_**||||\n||||||||\n||||||||\n||||||||\n|||||**(****_s_)**|**(****_s_)**||\n||||**_m_**|**_m_**|**_m_**|**_m_**|\n\n\n\n(f) No interference.\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|**_m_(****_s_)**||**_g_**|\n|s|hortest pat|h|\n\n\n\n_dG_ ( _g_ 1 _, g_ 2) =\n\n\n\nˆ\n_dG_ _ν_ ( _g_ 1) _, ν_ ( _g_ 2) _,_ if _g_ 1 _∈GA ∧_ _g_ 2 _∈GA_\n� �\n� _∞,_ otherwise\n\n\n\n_∞,_ otherwise\n(13)\nwhere _ν_ maps goals in _GA_ to the closest vertex in _P_ :\n\n\n\n_x −_ _xmin_\n_ν_ ( _g_ ) = _ν_ ( _x, y, z_ ) = (ˆ _x,_ ˆ _y,_ ˆ _z_ ) = � _xmin_ + ∆ _x ·_ � ∆ _x_\n\n\n\n_,_\n�\n\n\n\n_y −_ _ymin_\n_ymin_ + ∆ _y ·_\n� ∆\n\n\n\n_, zmin_ + ∆ _x ·_ _z −_ _zmin_\n� � ∆ _z_\n\n\n\n��\n\n\n\n∆ _y_\n\n\n\n(14)\n_⌊a⌉_ rounds any _a ∈_ R to the closest integer value. Mathematically, _d_ ( _g_ 1 _, g_ 2) = _d_ [ˆ] _G_ ( _ν_ ( _g_ 1) _, ν_ ( _g_ 2)).\n\n\n\n(a) Euclidean distance.\n\n\n(c) Graph-based distance with\nlarger grid size.\n\n|Interf<br>witho|erenc<br>bstalc|e<br>e|Col4|Col5|\n|---|---|---|---|---|\n|||**_g_**|||\n||||||\n||||||\n||||**(****_s_)**||\n|||**_m_**|**_m_**|**_m_**|\n\n\n\n(e) Potential interference.\n\n\n\nIV. METHODOLOGY\n\n\n_A. Problem Statement_\n\n\nOur method aims to solve robotic manipulation tasks with\nsparse rewards by making it applicable to environments with\nobstacles and improving its sample efficiency on the basis of\nG-HGG.\n\nFirst, even G-HGG is capable of guiding exploration in\nenvironments with obstacle, the sample efficiency of G-HGG\nis sensitive to the grid size. In Fig. 1, we show an example\nin a 2D environment, in which the task is to push an object\nfrom its initial position _m_ ( _s_ ) to the goal _g_ . Figure 1a shows a\ncase that Euclidean distance may select an invalid path that cut\nthrough an obstacle, while Fig. 1b shows a properly designed\ngraph to generate a valid graph-based distance to bypass the\nobstacle. As shown in Fig. 1c and 1d, a larger grid size\nwill lead to great distant error or even totally failure, since\nit might mislead the selection of intermediate goals. However,\na much fine-tuned grid will lead to great computation burden\nor manual parameter tuning. One way to reduce the influence\nof large distance error is to ensure the diversity of hindsight\ngoals selection. Second, G-HGG uniformly selects hindsight\ngoals from all achieved goals with different significance for a\nsuccess training, but it is not properly designed to control the\nadaptive exploration-exploitation trade-off in selecting suitable\nexperiences for different environments, which is extremely\nimportant when there is a lack of distance-based guidance\nfrom the environment. Third, G-HGG simply considers the\nmovement of the robotic arm as segments of lines, while it\nshould be modeled as a region of space due to its body size.\n\n\n\nFig. 1: The shortest Euclidean distance and graph-based distance between two points in a 2D environment with obstacles.\n\n\nAs illustrated in Fig. 1e, the space covered by the body of the\nrobot is represented as light red circles. Although the trajectory\n(red lines) of the robotic arm has no intersection with the\nobstacle, its body still has interference with the obstacle during\nthe movement, which may cause collision in the real world.\nIn this paper, we propose an algorithm named graphcurriculum-based hindsight goal generation (GC-HGG), in\nwhich graph-based distance is used to select suitable hindsight\ngoals using graph-based diversity and proximity metrics. The\narchitecture of GC-HGG is shown in Fig. 2. We first create a\ngraph representation of the environment that can be used to\ncalculate graph-based distances. We then reformulate (6) and\n(7) to select intermediate goals by replacing the Euclidean\n\nˆ\nmetric _||g_ [2] _−_ _m_ ( _s_ _[i]_ _t_ [)] _[||]_ [ with the graph-based distance] _[ d][G]_ [.]\nFinally, we reformulate (1), (2), and (4) to prioritize hindsight\nexperiences so as to improve the sample efficiency, replacing\nthem with graph-based proximity and diversity, as well as a\ndynamic trade-off mechanism.\n\n\n_B. Graph Creation With Collision Tolerance_\n\n\nAs illustrated in Fig. 1f, we improve the graph creation by\nincorporating the idea of the safety region, which is designed\nto avoid the potential collision between the obstacle and the\nbody of the robotic arm. In theory, the best way to avoid the\ncollision is to calculate any interference at every time step,\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\nEnvironment\n\n\n\nAgent\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-4-0.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-4-1.png)\n\nFig. 2: Architecture of GC-HGG. GC-HGG creates a graph representation to define two metrics: graph-based diversity for\nselecting proper hindsight goals and graph-based proximity for selecting proper intermediate goals.\n\n\n\nwhich consumes a great amount of computation. In this work,\nwe simply tackle this by virtually increasing the size of the\nobstacle as a safety region, which is not accessible to the agent.\nThe increased size can be determined by the radius _ϵ_ of the\nspace occupied by the robotic arm (the flange). Therefore,\n_xmin_, _xmax_, _ymin_, _ymax_, _zmin_, _zmax_ used in Section III-C\nare updated as _xmin_ _−ϵ_, _xmax_ + _ϵ_, _ymin_ _−ϵ_, _ymax_ + _ϵ_, _zmin_ _−ϵ_,\n_zmax_ + _ϵ_, respectively. Then, the graph can be created by\nfollowing the steps in Section III-C.\n\n\n_C. Graph-Based Proximity_\n\n\nwe can replace the Euclidean distance (1) in CHER by our\ngraph-based distance given as\n\n\n_Fprox_ ≜ _C_ 1 _−_ � ( _dG_ ( _gi, g_ )), (15)\n\n_gi∈A_\n\n\nwhere _dG_ is the graph-based distance as defined in (13). An\nillustration of such graph-based distance is given in Fig. 1b.\n\n\n_D. Graph-Based Diversity_\n\n\nThe diversity (2) in CHER is a metric with the evaluation\ntime complexity around _O_ ( _|A||B|_ ), where _|A|_ and _|B|_ are\nthe sizes of the mini-batch and replay buffer, respectively.\nThis large time complexity makes it infeasible when the task\nrequires a large batch size for training. In this work, we\nredesign it by proposing a graph-based diversity that can\nsufficiently encourage the exploration and yet with a much\nsmaller complexity of _O_ ( _|A|log_ 2 _|A|_ ).\nAs illustrated in Fig. 3a, we first define _B_ 1 _, B_ 2 _, ..., B|A| ∈_\n_B_, and _Bi_ contains those goals that are close to _gi ∈_ _A_ . Then\nthe diversity in (2) can be approximated as\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-4-2.png)\n\n(a) Points (green) in _B_ connects\nwith the closest point(red) in _A_ .\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-4-3.png)\n\n(b) the distances between any\ntwo points in _A_ .\n\n\n\nFig. 3: Inter-class distance and intra-class distance.\n\n\n\nTo compute (17), we need 2 nested loops to iterate over all\npossible combinations of ( _gi, gj_ ) in subset _A_ and sum up the\ndistances of each goal pair. Therefore the time complexity is\n_O_ ( _|A|_ [2] ). To reduce this time complexity, we derive the lower\nbound as\n\n� � _dG_ ( _gi, gj_ ) _≥_ � ( _|A| −_ 1) _·_ min _[d][G]_ [(] _[g][i][, g][j]_ [)]\n\n\n\n�\n\n\n\n_g_ � _i∈A_ ( _|A| −_ 1) _·_ min _gj_ _∈A_ _[d][G]_ [(] _[g][i][, g][j]_ [)]\n\n\n\n, (18)\n_g_ � _i∈A_ _g_ min _j_ _∈A_ _[d][G]_ [(] _[g][i][, g][j]_ [)]\n\n\n\n_gi∈A_\n\n\n\n� _dG_ ( _gi, gj_ ) _≥_ �\n\n_gj_ _∈A_ _gi∈_\n\n\n\n_≥_ ( _|A| −_ 1) _·_ �\n\n\n\n� _dG_ ( _g, gi_ ), (16)\n\n_g∈Bi_\n\n\n\nwhere _∀gi, gj ∈_ _A_, _gi ̸_ = _gj_, _dG_ ( _gi, gj_ ) _≥_ min _g∈A dG_ ( _gi, g_ ).\nThe advantage of using this lower bound as a metric for\ndiversity is that we only need to calculate the distance between\neach goal in the subset _A_ and its nearest neighbor rather than\nall other goals in _A_ . By leveraging the k-nearest-neighbor algorithm with _k_ -d tree structure, we can complete the calculation\nof diversity within time complexity of _O_ ( _|A|log|A|_ ), because\nfor each goal in _A_, querying its nearest neighbor only needs\n_O_ ( _log|A|_ ) and there exists _|A|_ goals in the set _A_ . Finally,\nafter normalization, the diversity _Fdiv_ ( _A_ ) from (2) can be\nreformulated as\n\n_Fdiv_ ( _A_ ) = _g_ � _i∈A_ _g_ min _j_ _∈A_ _[d][G]_ [(] _[g][i][, g][j]_ [)][.] (19)\n\n\n_E. Trade-off Between Curiosity and Proximity_\n\n\nTo increase the sample efficiency, an agent has to sample\nfrom a large diversity to explore the environment and gradually\nfocus on specific goals. In CHER, the trade-off _λ_ was defined\n\n\n\n_Fdiv_ ≜ _C_ 2 _−_\n\n\n\n_|A|_\n�\n\n\n_i_ =1\n\n\n\nwhere _B_ 1 _...B|A|_ are _|A|_ clusters of goals which have relatively\nsmall intra-class distances. Since _Fdiv_ is designed to encourage\nthe diversity of goal distributions and we know that a good\nclustering should have both large inter-class distances and\nsmall intra-class distances, we can redesign the diversity on\nthe basis of the inter-class distance, which is formulated as\n\n� � _dG_ ( _gi, gj_ ) (17)\n\n\n\n�\n\n\n\n_gi∈A_\n\n\n\n� _dG_ ( _gi, gj_ ) (17)\n\n_gj_ _∈A_\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\n**Algorithm 1** Curriculum-guided sampling\n\n\n1: Given: the number of random samples K, the trade-off _λ_\ncalculated as (4)\n\n2: _Fmax_ = _−inf_, _Aselect_ = _∅_\n\n3: **for** 1 to K **do**\n\n\n4: randomly sample _A_ from replay buffer _B_ using EBP\n\n5: find K-nearest-neighbour of _A_\n\n6: calculate _F_ ( _A_ ) = _Fdiv_ ( _A_ ) + _λFprox_ ( _A_ )\n\n7: **if** _F_ ( _A_ ) _> Fmax_ **then**\n\n8: _Aselect_ = _A_, _Fmax_ = _F_ ( _A_ )\n\n\n9: return _Aselect_\n\n\nonly as a simple exponential function in (4). This naive design\ncannot adapt to tasks with different difficulties and results in\npoor sample efficiency.\nBased on the Wasserstein distance of the desired goal\ndistribution _T_ _[∗]_ and the current intermediate goal distribution\n_T_, we propose a new and adaptive trade-off as\n\n\n_[,][ T][ ∗]_ [)]\n_λ_ = _ηexp_ ( _[−][D][G]_ [(] _[T]_ ), (20)\n\n_σ_ [2]\n\n\nwhere _η_ and _σ_ are hyper-parameters and _DG_ ( _·, ·_ ) is the graphbased Wasserstein distance, given as\n\n_DG_ ( _T_ [(1)] _, T_ [(2)] ) := inf _µ∈_ Γ( _T_ (1) _,T_ (2)) �E _µ_ � _dG_ ( _s_ [(1)] 0 _[||][g]_ [(1)] _[, s]_ [(2)] 0 _[||][g]_ [(2)][)] � [�] .\n\n(21)\nThis trade-off mechanism will encourage the agent to explore\nthe environment with a large degree of curiosity in the initial\nlearning state or at a stage at which the intermediate goals\n(achieved hindsight goals) are far from the desired goals.\nAlso, as _DG_ ( _·, ·_ ) decreases (intermediate goals approach the\ndesired goals), _λ_ will gradually and adaptively increase, so as\nto enlarge the weight of the proximity.\n\n\n_F. Objective Optimization_\n\n\nOptimizing the final objective (3) can be regarded as the\nfacility location problem, which is NP-hardness with high\ncomputational complexity. CHER uses a greedy algorithm to\nobtain an approximate solution, which largely reduces the\ncomplexity of this NP-hard problem. Meanwhile, it still needs\n_O_ ( _|B||A|_ ) times _Fdiv_ and _Fprox_ evaluations, where _|A|_ and\n_|B|_ are the sizes of mini-batch and replay buffer, respectively.\nHowever, considering the millions of samples throughout the\nwhole training process, it is not necessary to calculate the maximum value of _F_ ( _A_ ) from the entire replay buffer. A sampled\nminibatch _A_ with relatively higher _F_ ( _A_ ) would have a positive\nimpact on our training. In this work, we reduce this computation complexity by randomly sampling _A_ 1 _, A_ 2 _, ..., AK_ from _B_\nand only selecting _Ai_ = arg max _Ai∈{A_ 1 _,A_ 2 _,...,AK_ _} F_ ( _Ai_ ) as\nthe mini-batch (See Algorithm 1). Therefore, the computation\ntime of the sampling is reduced to _O_ ( _K|A|_ log2 _|A|_ ), since\nthe computation time of _Fprox_ ( _A_ ) is _O_ ( _|A|_ ) and _Fdiv_ ( _A_ ) is\n_O_ ( _|A|_ log2 _|A|_ ), using the k-nearest-neighbor algorithm. With\nsuch a complexity reduction, each iteration in the training\nprocess costs only about 180 seconds with these sizes of\nminibatch (256) and replay buffer (10000) in our experiment,\n\n\n\n**Algorithm 2** GC-HGG\n\n\n1: **Given** : an off-policy RL algorithm A, a strategy S for\nsampling goals for replay, a sparse reward function _rg_ .\n\n2: Construct a graph representation _G_ _▷_ Sec. III-C\n\n3: Pre-compute the shortest distance _d_ [ˆ] _G_ between every pair\nof vertices ( _p_ 1 _, p_ 2) _∈_ _P_ [2] with Dijkstra\n\n4: Initialize A and replay buffer _R_\n\n5: **for** _iteration_ **do**\n\n6: Construct a set of _M_ intermediate tasks _{_ (ˆ _s_ _[i]_ 0 _[, g][i]_ [)] _[}][M]_ _i_ =1 [:]\n\n_•_ Sample target tasks _{_ (ˆ _s_ _[i]_ 0 _[,]_ [ ˆ] _[g][i]_ [)] _[}][K]_ _i_ =1 _[∼T][ ∗]_\n\n_•_ Find _K_ distinct trajectories _{τ_ _[i]_ _}_ _[K]_ _i_ =1 [that together]\nminimize (6) _▷_ weighted bipartite matching\n\n_•_ Find _M_ intermediate tasks (ˆ _s_ _[i]_ 0 _[, g][i]_ [)][ by selecting an]\nintermediate goal _g_ _[i]_ from each _τ_ _[i]_\n\n_•_ Calculate the balance of proximity and diversity based\non the Wasserstein distance between the intermediate\n\nand the target goal distribution _▷_ (20)\n\n\n7: **for** _episode_ = 1 _, M_ **do**\n\n8: ( _s_ 0 _, g_ ) _←_ (ˆ _s_ _[i]_ 0 _[, g][i]_ [)]\n9: **for** _t_ = 0 _, T −_ 1 **do**\n10: Sample _at_ using the policy from A with noise:\n\n\n_at ←_ _π_ ( _st ∥_ _g_ ) + _Nt_ (22)\n\n\n11: Execute _at_ and observe a new state _st_ +1\n\n\n12: **for** _t_ = 0 _, T −_ 1 **do**\n13: _rt_ := _rg_ ( _st, at_ )\n14: Store transition ( _st||g, at, rt, st_ +1 _||g_ ) in _R_\n15: Sample a set of additional goals for replay\n_G_ := S( _current episode_ )\n16: **for** _g_ _[′]_ _∈_ _G_ **do**\n\n17: _r_ _[′]_ := _rg_ _[′]_ ( _st, at_ ); Store the transition\n( _st||g_ _[′]_ _, at, r_ _[′]_ _, st_ +1 _||g_ _[′]_ ) in _R_ _▷_ HER\n\n\n18: **for** _t_ = 1 _, N_ **do**\n19: _B_ = curriculum-guided sampling _▷_ Alg. 1\n20: One optimization step using A and _B_ _▷_ DDPG\n\n\nwhile CHER takes around 5 hours with the same setting. The\nentire algorithm is provided as Algorithm 2.\n\n\nV. EXPERIMENTS\n\n\n_A. Environments_\n\n\nTo show the advantages of GC-HGG over HGG, HER, and\nCHER, we create new experimental environments inspired\nby the widely adopted Fetch-gripper benchmark [9]. All\nour environments are MuJoCo environments, which feature\na modeled Kuka robot with a gripper. They all adopt the\nfollowing control strategy. The state space _S_ contains positions\nand velocities for all the joints of the robotic arm, the position\nof the end-effector, and the position and orientation of the\nobject. Depending on whether gripper control is enabled or\ndisabled, the action space is three- or four-dimensional. An\naction consists of the end effector’s position for the next time\nstep and the gripper’s opening control parameter.\n**KukaReach** (Figure 4a): The goal is simply to control the\narm to reach a goal position. The gripper remains permanently\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n(a) KukaReachEnv (b) KukaPushNewEnv (c) KukaPickNoObstacleEnv (d) KukaPickObstacleEnv\n\n\nFig. 4: Robotic manipulation environments in simulations and the real world setup.\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-6-0.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-6-4.png)\n\n|1.0<br>ae<br>0.8<br>uccess<br>0.6<br>0.4<br>an<br>0.2 e<br>0.0<br>0 20 40<br>I|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|0<br>20<br>40<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae||||||||\n|0<br>20<br>40<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae|||||||~~HE~~|\n|0<br>20<br>40<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae|||||||HG<br>C-H|\n|0<br>20<br>40<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae|||||||G-H<br>GC-|\n|0<br>20<br>40<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae||||60<br>80<br>teration|60<br>80<br>teration|60<br>80<br>teration|60<br>80<br>teration|\n\n\n(a) KukaReach\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-6-1.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-6-5.png)\n\n|1.0<br>ae<br>0.8<br>uccess<br>0.6<br>0.4<br>an<br>0.2 e<br>0.0<br>0 25 50 75<br>I|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n|0<br>25<br>50<br>75<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae|||||\n|0<br>25<br>50<br>75<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae||||~~HER~~|\n|0<br>25<br>50<br>75<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae||||HG<br>C-H|\n|0<br>25<br>50<br>75<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae||||G-H<br>GC-|\n|0<br>25<br>50<br>75<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae||||100<br>125<br>150<br>175<br>teration|\n\n\n(b) KukaPushNew\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-6-2.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-6-6.png)\n\n|1.0 HER<br>HGG ae<br>0.8 C-HER<br>G-HGG uccess<br>0.6 GC-HGG<br>0.4<br>an<br>0.2 e<br>0.0<br>0 50 100 150<br>It|HER<br>HG|G|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|0<br>50<br>100<br>150<br>It<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~|~~C-H~~<br>G-H<br>~~GC~~|~~ER~~<br>GG<br>~~HGG~~||||\n|0<br>50<br>100<br>150<br>It<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~|~~-~~|||||\n|0<br>50<br>100<br>150<br>It<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~||||||\n|0<br>50<br>100<br>150<br>It<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~||||||\n|0<br>50<br>100<br>150<br>It<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~|||200<br>250<br>300<br>350<br>400<br>ration|200<br>250<br>300<br>350<br>400<br>ration|200<br>250<br>300<br>350<br>400<br>ration|\n\n\n(c) KukaPickNoObstacle\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-6-3.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-6-7.png)\n\n|1.0 HER<br>HGG ae<br>0.8 C-HER<br>G-HGG uccess<br>0.6 GC-HGG<br>0.4<br>an<br>0.2 e<br>0.0<br>0 50 100 1|HER<br>HGG|Col3|Col4|Col5|\n|---|---|---|---|---|\n|0<br>50<br>100<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~|~~C-HER~~<br>G-HGG<br>~~GCHGG~~||||\n|0<br>50<br>100<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~|~~-~~||||\n|0<br>50<br>100<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~|||||\n|0<br>50<br>100<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~|||||\n|0<br>50<br>100<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>HER<br>HGG<br>~~C-HER~~<br>G-HGG<br>~~GC-HGG~~||50<br>200<br>250<br>300<br>350<br>400<br>Iteration|50<br>200<br>250<br>300<br>350<br>400<br>Iteration|50<br>200<br>250<br>300<br>350<br>400<br>Iteration|\n\n\n(d) KukaPickObstacle\n\n\n\nFig. 5: Median success rate (line) and interquartile range (shaded) in different environments. One iteration contains 50 episodes.\n\n\n\nclosed and the gripper control is disabled, leading to a 3D\naction space. This environment is the easiest task that is\ndesigned as a benchmark task to examine the usability of each\nalgorithm. **KukaPushNew** (Figure 4b): The goal is to push the\npuck from its initial position around the blue obstacle towards\na goal. The gripper remains permanently closed and the\ngripper control for picking up the object is disabled, leading\nto a three-dimensional action space. **KukaPickNoObstacle**\n(Figure 4c): The goal is to pick up the cube from its initial\nposition, lift it up, and place it at a goal position. No obstacle\nis present in this scenario, but the target goals are located in\nthe air. Gripper control is enabled, and the gripper can be\nsymmetrically opened and closed by a single actuator, leading\nto a 4D action space. This task is designed to compare the\nexploration capability of different algorithms in an obstaclefree environment that involves grasping. **KukaPickObstacle**\n(Figure 4d): The goal is to pick up the cube from its initial\nposition, lift it over the obstacle, and place it at a goal position.\nGripper control is enabled, the gripper can be symmetrically\nopened and closed leading to a 4-D action space. Note that all\nfour scenarios are multi-goal tasks and therefore the goal is\nrandomly picked in each episode within the predefined range.\nThe sparse reward used in each scenario is defined as\n\n\n\nthe other algorithms in all complex environments, in terms of\nboth sample efficiency and maximum success rate.\n**KukaReach** : In KukaReach, there exists no much difference\nin performance among HER, HGG, CHER, GC-HGG (See\nFig. 5a). Since no complex exploration is required and no\nobstacle is present, FetchReach can be easily solved by all\nalgorithms within 10 iterations. In most instances, the success\nrate peaks after around 10 iterations. It should be noted that, in\na scenario that no obstacle is present, the graph-based distance\ncan simply be replaced with the Euclidean distance.\n**KukaPushNew** : In KukaPushNew, GC-HGG performs far\nbetter than the other algorithms (Fig. 5b) in terms of their\nsuccess rates. While CHER, HGG, and HER are not be able to\nsolve this task over 200 training iterations, GC-HGG achieves\na success rate over 90% after 70 iterations. The reasons\n\nare obvious. First, HGG and CHER repeatedly choose the\nhindsight goals which are close to the target position, guided\nby the sampled intermediate goals using the Euclidean metric.\nThese hindsight goals will be blocked by an obstacle, which\nwould have limited effects on exploration. Second, due to\na poor trade-off between exploration and exploitation, some\nhindsight goals can even bypass the obstacle, they are rarely\nselected as suitable intermediate goals, and resulting the failure\nof the training. Since KukaPushNew is not as challenging as\nthe KukaPickNoObstalce and KukaPickObstacle, in which the\nagent has to explore to learn to grasp the object, G-HGG is\nalso able to solve the task with slightly worse sample efficiency\nand much higher variance than GC-HGG.\n**KukaPickNoObstacle** : In this task, GC-HGG displays a\nremarkable performance. Within 70 iterations, the success rate\nof GC-HGG reaches over 90%. In terms of HGG and G-HGG,\nthey need more iterations to complete this task. This shows\nthat a diverse exploration at the beginning and a gradual focus\non the target have distinctly positive effects on the training.\n**KukaPickObstacle** : In this task, GC-HGG is able to complete the task in 250 iterations. Although G-HGG can also\n\n\n\n_rg_ ( _s, a_ ) :=\n\n\n\n0, if _||g −_ _m_ ( _s_ ) _|| ≤_ 0 _._ 05\n, (23)\n_−_ 1, otherwise\n�\n\n\n\nwhere _||g −_ _m_ ( _s_ ) _||_ represents the Euclidean distance between\nthe gripper or the puck to the goal. 0 _._ 05 is a distance threshold.\n\n\n_B. Results_\n\n\nWe tested GC-HGG in the four environments with that of\n\ncompare its performance to HGG, HER, and CHER. Since\nwe know that EBP [14] enhances the performance of both\nHER and HGG, we used EBP in all training of HER and\nHGG. The results clearly show that GC-HGG far outperforms\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\ncomplete the task, GC-HGG demonstrates better sample efficiency and robustness. The other methods HER, CHER and\nHGG fail to finish the task. We can clearly see from the figure\nthat a large diversity at the beginning of the training process\nand increasing the proximity based on the current learning\nstage is helpful to both exploration efficiency and the median\n\nsuccess rate.\n\nThe computation time of each algorithm is demonstrated\nin Table I. All experiments are performed on the Intel Core\ni9 9880H CPU and the AMD Radeon Pro 5300 GPU. The\n\ntraining time of HER for each iteration is the shortest because\nof its simple structure. HGG requires more time compared\nto HER due to the selection of intermediate goals. G-HGG\nspends more computation time than HGG, while most of the\ntime is used on reading out the graph-based distance metric\nduring the training process. The computational time for GCHGG is slightly higher than G-HGG due to the _K_ time random\nsampling algorithm to select the mini-batch. In our set up,\nwe set _K_ equals 8 and implemented parallel computation.\nCHER faces time issue, especially when the size of the replybuffer is large. It costs approximately 5 hours to finish one\niteration. The table also indicates that the total computational\ncost of GC-HGG is even less than G-HGG in the environ\nments KukaPickNoObstacle and KukaPickObstacle, because\nwe implemented the same stop condition check [11] of HGG\nor G-HGG in our GC-HGG. The algorithm will continue with\nHER if the selected intermediate goals are close enough to\nthe original target goals. Since GC-HGG has a better sample\nefficiency, the agent can learn faster compared with G-HGG\nso that it takes fewer iterations to switch to HER. Note that\n\nTable I only shows the training time, while all the tasks are\nperformed in real time when deployed in the real world.\n\n\nTABLE I: Computation time of experimental runs (* indicates\nthat the algorithm can not complete the task in 400 iterations)\n\n\nEnvironment HER HGG G-HGG GC-HGG CHER\n\nKukaReach 7(h) 7(h) 7(h) 7(h) 8(h)\nKukaPushNew 7(h)* 9(h)* 11(h) 11.5(h) _>_ 10(d)*\nKukaPickNoObstacle 7(h)* 8.5(h) 15.5(h) 8(h) _>_ 10(d)*\nKukaPickObstacle 7(h)* 7(h)* 15.5(h) 12.5(h) _>_ 10(d)*\nSingle Iteration 52(s) 87(s) 153(s) 179(s) 5(h)\n\n\n_C. Ablation Study_\n\n\nWe present the ablation studies of _η_ and _σ_ for the tradeoff coefficient _λ_, the trade-off method (ours and CHER’s),\nand distance metrics (graph-based distance and Euclidean\ndistance). Due to the page limit, we only show the ablation\nstudy results in the environment KukaPickNoObstacle. The\nfull ablation study in other scenarios can be found at here [1] .\nFigure 6a illustrates the success rates for various values of _η_ in\n(20) in the environment KukaPickNoObstacle. We set _σ_ = 0 _._ 3\nand keep it unchanged during the experiments. The plot shows\nthat GC-HGG performs best when _η_ = 1000. Other choices of\n_η_ only show a slight degradation in sample efficiency. Figure\n6b demonstrates the success rates for various of _σ_, where\n_σ_ = 0 _._ 3 gives the best performance. We set _η_ = 1000 and\nit remains the same during the experiments. Figure 6c shows\n\n\n1https://videoviewsite.wixsite.com/gc-hgg\n\n\n\nthe success rates of different trade-off methods. The blue\n\ncurve indicates our trade-off based on (20). The other three\ndemonstrate the success rates of GC-HGG with CHER’s trade\noff which based on (4). As we can seen from the figure, our\ntrade-off has a better sample efficiency and more robust than\nCHER’s, since our trade-off method is based on the current\nlearning progress rather a naive designed exponential function.\nFigure 6d is an ablation study of distance metrics. There is no\nmuch difference between graph-based distance and euclidean\ndistance since the environment KukaPickNoObstacle does not\n\ncontain an obstacle. The ablation study of distance metrics for\nother environments can be found on our website.\n\n\n_D. Real-World Experiments_\n\n\nThe real-world experiment setup comprised with a KUKA\nLBR iiwa R800 robotic arm, a Robotiq 2F85 gripper, and\nan XBOX 360 Kinect, used to obtain the coordinates of the\nmanipulatable object. Since the robotic arm is controlled via\nits default Java interface and our RL controller is programmed\nwith Python, we use JPype, a python module that provides\nfull access to Java, to exchange data between the robot and\nthe RL controller. The control rate has the same value as the\n\nsimulation, which is 20 Hz across all experiments.\nAs in the four environments in the simulation, we also\ncreate four environments featuring the KUKA robotic arm with\nthe gripper (See Fig. 4). It should be noted that unlike the\nsimulation that obtains the coordinates of the object directly,\nwe use a camera to gather this information when the object\nis on the table in the real world. Specifically, we use four\ngreen markers to create a global coordinate and obtain the\ncoordinates of the object by tracking its red color. However,\nwhen the object is picked up, we can directly calculate its\nposition from the kinematics of the robotic arm.\nWe took the policy directly from each of the tasks trained in\nthe simulation and deployed it in the real world without any\nfine-tuning. Inspired by the experiment performed by HER,\nwe also add Gaussian noise to the observed object’s position\nduring policy training to compensate for the small errors\nintroduced by the camera, which can increase the success\nrate of these tasks. The performances of these four tasks\ndemonstrate that the policy can be successfully transferred to\nthe corresponding tasks in the real world.\nAs shown in the video, the robot arm can always successfully approach the target position in most trajectories.\nKukaReach has the highest success rate among these four tasks\nsince it is the easiest one and the position of the end effector\nobtained from the Kuka interface is accurate. Even the position\nof the object is obtained using images, the robotic arm can still\nsolve KuakPush, KukaPickNoObstacle, and KukaPickObstacle\nwith no significant drop in the success rate compared to the\nperformances in the simulation. The median success of the\nreal-world experiments are listed in Table II. KukaReach has\nthe highest success rate among these four tasks, since it is\nthe easiest one and the position of the end effector obtained\nfrom the Kuka interface is accurate. For KukaPushNew, there\nexists only two cases that the robotic arm can not push the\ncube to the right position. For one case, it pushed the cube\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\n|1.0<br>ae<br>0.8<br>uccess<br>0.6<br>0.4<br>an<br>0.2 e<br>0.0<br>0 50 10|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|0<br>50<br>10<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae||||||\n|0<br>50<br>10<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae|||GC-H<br>GC-H<br>GC-H|GG (<br>=<br>GG (<br>=<br>GG (<br>=|1000<br> 500)<br> 5000|\n|0<br>50<br>10<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae|||0<br>150<br>200<br>250<br>300<br>350<br>40<br>Iteration|0<br>150<br>200<br>250<br>300<br>350<br>40<br>Iteration|0<br>150<br>200<br>250<br>300<br>350<br>40<br>Iteration|\n\n\n(a) Ablation study of _η_ .\n\n\n\n|1.0<br>ae<br>0.8<br>uccess<br>0.6<br>0.4<br>an<br>0.2 e<br>0.0<br>0 50 100|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|0<br>50<br>100<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae||||||\n|0<br>50<br>100<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae|||GC-<br>GC-<br>GC-|HGG (<br> <br>HGG (<br> <br>HGG (<br>|= 0.3)<br>= 0.37<br>= 0.4)|\n|0<br>50<br>100<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae|||150<br>200<br>250<br>300<br>350<br>4<br>Iteration|150<br>200<br>250<br>300<br>350<br>4<br>Iteration|150<br>200<br>250<br>300<br>350<br>4<br>Iteration|\n\n\n(b) Ablation study of _σ_ .\n\n\n\n|1.0 GC-HGG<br>withCHERtrad ae<br>0.8 w wi it th C CH HE ER t tr ra ad<br>h R d uccess<br>0.6<br>0.4<br>an<br>0.2 e<br>0.0<br>0 50 100 150<br>I|GC-HGG<br>withCHERtrad<br>ith CHER t d|e-of ff ( = 0 0.0 00 15)<br>f|Col4|\n|---|---|---|---|\n|0<br>50<br>100<br>150<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>GC-HGG<br>with CHER trad<br>~~with~~ ~~CHER~~ ~~trad~~<br>with CHER trad|~~w~~ ~~ra~~<br>with CHER trad|~~e-o~~ ~~( = .)~~<br>e-off ( = 0.05)||\n|0<br>50<br>100<br>150<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>GC-HGG<br>with CHER trad<br>~~with~~ ~~CHER~~ ~~trad~~<br>with CHER trad||||\n|0<br>50<br>100<br>150<br>I<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>GC-HGG<br>with CHER trad<br>~~with~~ ~~CHER~~ ~~trad~~<br>with CHER trad||200<br>250<br>300<br>350<br>400<br>teration|200<br>250<br>300<br>350<br>400<br>teration|\n\n\n(c) Ablation study of trade-off\nmethods.\n\n\n\n|1.0<br>ae<br>0.8<br>uccess<br>0.6<br>0.4<br>an<br>0.2 GC-HGG e<br>0.0 GC-HGG<br>0 50 100 150 200<br>Iterat|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n|0<br>50<br>100<br>150<br>200<br>Iterat<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>GC-HGG<br>GC-HGG|||||\n|0<br>50<br>100<br>150<br>200<br>Iterat<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>GC-HGG<br>GC-HGG|||GC-HGG<br>GC-HGG|(with Euclidean dista|\n|0<br>50<br>100<br>150<br>200<br>Iterat<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>ean uccess ae<br>GC-HGG<br>GC-HGG|||GC-HGG<br>GC-HGG|250<br>300<br>350<br>ion|\n\n\n(d) Ablation study of distance\nmetrics.\n\n\n\nFig. 6: Ablation studies in the environment of KukaPickNoObstacle.\n\n\nout of its reaching range by accident. For the other, the end\neffector blocked the view of the camera so that the camera\n\ncould not detect the position of the cube. These unexpected\ncases sometimes also happened in KukaPickObstacle and\nKukaPickNoObestacle. The median success rates for those two\n\ntasks are therefore decreased compared to the performance\nin corresponding simulation environments. Videos of these\nfailure cases on the project page.\n\n\nTABLE II: The median success rates of real-world envi\nronments and simulation environments (- indicates that the\nalgorithm can not solve the task.)\n\n\nHER 1.0 1.0(15/15)  -  - Fig. 7: Illustration of potential collision between the robotic\n\n\n|Col1|KukaReach<br>sim. real|KukaPushNew<br>sim. real|\n|---|---|---|\n|HER<br>HGG<br>CHER<br>G-HGG<br>GC-HGG|1.0<br>1.0(15/15)<br>1.0<br>1.0(15/15)<br>1.0<br>1.0(15/15)<br>1.0<br>1.0(15/15)<br>1.0<br>1.0(15/15)|-<br>-<br>-<br>-<br>-<br>-<br>0.97_ ±_ 0.03<br>0.80(12/15)<br>**0.99**_ ±_** 0.01**<br>**0.86(13/15)**|\n||KukaPickNoObstacle<br>sim.<br>real|KukaPickObstacle<br>sim.<br>real|\n|HER<br>HGG<br>CHER<br>G-HGG<br>GC-HGG|-<br>-<br>0.99_ ±_ 0.01<br>0.80(12/15)<br>-<br>-<br>0.99_ ±_ 0.01<br>0.73(11/15)<br>**0.99**_ ±_** 0.01**<br>**0.8(12/15)**|-<br>-<br>-<br>-<br>-<br>-<br>0.82_ ±_ 0.18<br>0.60(10/15)<br>**0.99**_ ±_** 0.01**<br>**0.80(12/15)**)|\n\n\n\n_E. Discussion_\n\n\nTo summary the main differences between GC-HGG and\nthe other algorithms, we provide a brief discussion. The main\ndifference between GC-HGG and HGG lies in the intermediate\n\ngoal selection strategy, with which those intermediate goals are\nselected and used for the optimization of the policy. GC-HGG\nenables the agent to select intermediate goals only from the\naccessible goal space, in which all the obstacles are eliminated.\nTherefore, GC-HGG is applicable in scenarios with obstacles.\nHowever, HGG only selects intermediate goals by replying on\nthe Euclidean distance, which is not applicable in scenarios\nwith obstacles. There is no mechanism for CHER or HER\n\nto select proper intermediate goals. GC-HGG proposes an\nautomated trade-off mechanism to balance the diversity and\nthe proximity during training, which is designed on the basis\nof graph-based distance and leads to superior performance.\nTo discuss the potential collision between the body of the\narm and the obstacle, a push task is illustrated in Figure 7, in\nwhich the arm is controlled to push one object from its left\nside to its right side by bypassing the obstacle. As explained\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-8-2.png)\n\nbefore, the action space for this task is 3 dimensional, namely,\nthe position of the gripper in _x, y, z_ direction. Therefore, the\ngripper can only have translational movement in the _x−y_ plane\nin this task. Since the obstacle is not higher than the 7 _[th]_ joint\nof the robotic arm in the goal space, there is no possibility that\njoint 1-5 will have potential collision with the obstacle. The\nonly possible collision can happen between the gripper and\nthe labyrinth obstacle. As explained in Figure 1.e and Figure\n1.f in the revision paper, the safety region can make sure that\nthere is no collision between the gripper and the obstacle in\nthe _x −_ _y_ plane by setting a safety threshold.\n\n\nVI. CONCLUSION\n\nWe introduced a novel automatic hindsight goal generation\nand exploration algorithm GC-HGG on the basis of G-HGG\nand CHER for complex object manipulation in environments\nwith obstacles, in which the selection of valuable hindsight\ngoals is generated by balancing the graph-based diversity\nand proximity metrics. We schemed GC-HGG as a graph\nconstruction as pretraining steps, and the graph-based diversity\nand proximity computations as critical steps, during the training. Simulations and real-world experiments on four different\nchallenging object manipulation tasks demonstrated superior\nperformance by GC-HGG over HER, CHER, or HGG in terms\nof both the maximum success rate and sample efficiency.\n\n\nACKNOWLEDGMENT\n\nThis project/research has received funding from the European Union’s Horizon 2020 Framework Programme for\n\n\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\nResearch and Innovation under the Specific Grant Agreement\nNo.945539 (Human Brain Project SGA3). We thank Prof.\nAlin Albu-Sch¨affer and Mr. Daniel Seidel from the German\n\nAerospace Center for providing the robotic arm.\n\n\nREFERENCES\n\n\n[1] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath,\n“Deep reinforcement learning: A brief survey,” _IEEE Signal Processing_\n_Magazine_, vol. 34, no. 6, pp. 26–38, 2017.\n\n[2] Z. Hou, J. Fei, Y. Deng, and J. Xu, “Data-efficient hierarchical reinforcement learning for robotic assembly control applications,” _IEEE_\n_Transactions on Industrial Electronics_, vol. 68, no. 11, pp. 11 565–\n11 575, 2021.\n\n[3] L. Wen, X. Li, and L. Gao, “A new reinforcement learning based learning\nrate scheduler for convolutional neural network in fault classification,”\n_IEEE Transactions on Industrial Electronics_, 2020.\n\n[4] W. Bai, T. Li, Y. Long, and C. L. P. Chen, “Event-triggered multigradient recursive reinforcement learning tracking control for multiagent\nsystems,” _IEEE Transactions on Neural Networks and Learning Systems_,\npp. 1–14, 2021.\n\n[5] T. Li, W. Bai, Q. Liu, Y. Long, and C. L. P. Chen, “Distributed faulttolerant containment control protocols for the discrete-time multiagent\nsystems via reinforcement learning method,” _IEEE Transactions on_\n_Neural Networks and Learning Systems_, pp. 1–13, 2021.\n\n[6] Y. Li, J. Zhang, W. Liu, and S. Tong, “Observer-based adaptive optimized control for stochastic nonlinear systems with input and state\nconstraints,” _IEEE Transactions on Neural Networks and Learning_\n_Systems_, pp. 1–15, 2021.\n\n[7] B. Eysenbach, X. GENG, S. Levine, and R. R. Salakhutdinov, “Rewriting history with inverse rl: Hindsight inference for policy improvement,”\nin _Advances in Neural Information Processing Systems_, H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33, pp.\n14 783–14 795. Curran Associates, Inc., 2020.\n\n[8] M. Fang, T. Zhou, Y. Du, L. Han, and Z. Zhengyou, “Curriculum-guided\nHindsight Experience Replay,” _NeurIPS 2019_, 2019.\n\n[9] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder,\nB. McGrew, J. Tobin, P. Abbeel, and W. Zaremba, “Hindsight experience\nreplay,” in _Neurips_, pp. 5049–5059, 2017.\n\n[10] Z. Ren, K. Dong, Y. Zhou, Q. Liu, and J. Peng, “Exploration via\nHindsight Goal Generation,” _33rd Conference on Neural Information_\n_Processing Systems, NeurIPS 2019_, 2019.\n\n[11] Z. Bing, M. Brucker, F. O. Morin, R. Li, X. Su, K. Huang, and\nA. Knoll, “Complex robotic manipulation via graph-based hindsight\ngoal generation,” _IEEE Transactions on Neural Networks and Learning_\n_Systems_, pp. 1–14, 2021.\n\n[12] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\nreplay,” in _International Conference on Learning Representations, 2016_ .\n\n[13] R. Zhao, X. Sun, and V. Tresp, “Maximum entropy-regularized multigoal reinforcement learning,” in _36th International Conference on Ma-_\n_chine Learning, ICML 2019_, vol. 2019-June, pp. 13 022–13 035, 2019.\n\n[14] R. Zhao and V. Tresp, “Energy-Based Hindsight Experience Prioritization,” _2nd Conference on Robot Learning, CoRL 2018_, 2018.\n\n[15] S. Singh, A. G. Barto, and N. Chentanez, “Intrinsically motivated\nreinforcement learning,” _Neurips_, 2005.\n\n[16] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven exploration by self-supervised prediction,” _34th International Conference_\n_on Machine Learning, ICML 2017_, vol. 6, 2017.\n\n[17] S. Forestier, Y. Mollard, and P.-Y. Oudeyer, “Intrinsically Motivated\nGoal Exploration Processes with Automatic Curriculum Learning,”\n[2017. [Online]. Available: http://arxiv.org/abs/1708.02190](http://arxiv.org/abs/1708.02190)\n\n[18] A. P´er´e, S. Forestier, O. Sigaud, and P. Y. Oudeyer, “Unsupervised\nlearning of goal spaces for intrinsically motivated goal exploration,” _6th_\n_International Conference on Learning Representations, ICLR 2018_ .\n\n[19] C. Colas, P. Founder, O. Sigaud, M. Chetouani, and P. Y. Oudeyer,\n“CURIOUS: Intrinsically motivated modular multi-goal reinforcement\nlearning,” in _International Conference on Machine Learning_, 2019.\n\n[20] A. Aubret, L. Matignon, and S. Hassas, “A survey on intrinsic motivation\nin reinforcement learning,” 2019.\n\n[21] A. Goyal, P. Brakel, W. Fedus, S. Singhal, T. Lillicrap, S. Levine,\nH. Larochelle, and Y. Bengio, “Recall traces: Backtracking models\nfor efficient reinforcement learning,” _7th International Conference on_\n_Learning Representations, ICLR 2019_, pp. 1–19, 2019.\n\n[22] C. Florensa, D. Held, X. Geng, and P. Abbeel, “Automatic Goal\nGeneration for Reinforcement Learning Agents,” _Proceedings of the 35th_\n_International Conference on Machine Learning_, 2018.\n\n\n\n\n[23] C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel, “Reverse\nCurriculum Generation for Reinforcement Learning,” _1st Conference on_\n_Robot Learning, CoRL_, 2017.\n\n[24] C. Florensa, D. Held, X. Geng, and P. Abbeel, “Automatic\ngoal generation for reinforcement learning agents,” in _Proceedings_\n_of the 35th International Conference on Machine Learning_, ser.\nProceedings of Machine Learning Research, J. Dy and A. Krause,\nEds., vol. 80, pp. 1515–1528, 10–15 Jul 2018. [Online]. Available:\n[http://proceedings.mlr.press/v80/florensa18a.html](http://proceedings.mlr.press/v80/florensa18a.html)\n\n[25] M. Eppe, S. Magg, and S. Wermter, “Curriculum goal masking for continuous deep reinforcement learning,” _2019 Joint IEEE 9th International_\n_Conference on Development and Learning and Epigenetic Robotics,_\n_2019_, pp. 183–188, 2019.\n\n[26] H. Sun, Z. Li, X. Liu, B. Zhou, and D. Lin, “Policy continuation\nwith hindsight inverse dynamics,” in _Advances in Neural Information_\n_Processing Systems_, vol. 32. Curran Associates, Inc., 2019.\n\n[27] E. Dijkstra, “A Note on Two Problems in Connexion with Graphs,”\n_Numerische Mathematik 1_, 1959.\n\n\n**Zhenshan Bing** received his doctorate degree\nin Computer Science from the Technical University of Munich, Germany, in 2019. He received\nhis B.S degree in Mechanical Design Manufacturing and Automation from Harbin Institute\nof Technology, China, in 2013, and his M.Eng\ndegree in Mechanical Engineering in 2015, at\nthe same university. Dr. Bing is currently a postdoctroal researcher with Informatics 6, Technical\nUniversity of Munich, Munich, Germany. His research investigates the snake-like robot which is\ncontrolled by artificial neural networks and its related applications.\n\n\n**Hongkuan Zhou** received his B. Sc. Degree in\nGames 2Engineering at Technical University of\nMunich, Germany, in 2021. His research interest\nis artificial intelligence on robotics implementations, especially with reinforcement learning\nalgorithms.\n\n\n**Rui Li** received the B.Eng degree in automation\nengineering from the University of Electronic\nScience and Technology of China (UESTC),\nChengdu, China, in 2013 and the Ph.D. degree from the Institute of Automation, Chinese\nAcademy of Science (CASIA), Beijing, China\nin 2018, respectively. He visited Informatics 6,\nTechnical University of Munich as a guest postdoc researcher in 2019. Currently he is a assitant researcher with School of Automation,\nChongqing University. His research interests include intelligent robot system and high-precision assembly.\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-9-0.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-9-1.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-9-2.png)\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n\n\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIE.2022.3172754, IEEE\n\nTransactions on Industrial Electronics\n\n\nIEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS\n\n\n\n**Fabrice O. Morin** received an engineering degree from the Ecole Nationale Sup´erieure des\nMines de Nancy (Nancy, France) in 1999, a\nMaster’s Degree in Bioengineering from the University of Strathclyde (Glasgow, United Kingdom) in 2000, and a Ph.D. in Materials Science\nfrom the Japanese Advanced Institute of Science and Technology (Nomi-Shi, Japan) in 2004.\nAfter several post-docs at the University of Tokyo\n(Japan) and the IMS laboratory (University of\nBordeaux, France), in 2008 he joined Tecnalia,\na nonprofit RTO in San Sebasti´an (Spain), first as a senior researcher,\nthen as a group leader. There, he worked on various projects in\nNeurotechnology and Biomaterials, funded both by public programs and\nprivate research contracts. Since 2017, he has worked as a scientific coordinator at the Technical University of Munich (Germany) where, in the\nframework of the Human Brain Project, he oversees the development\nof software tools for embodied simulation applied to Neuroscience and\nArtificial Intelligence.\n\n\n**Kai Huang** joined Sun Yat-Sen University as\na Professor in 2015. He was appointed as the\ndirector of the Institute of Unmanned Systems of\nSchool of Data and Computer Science in 2016.\nHe was a senior researcher in the Computer\nScience Department, the Technical University of\nMunich, Germany from 2012 to 2015 and a research group leader at fortiss GmbH in Munich,\nGermany, in 2011. He earned his Ph.D. degree\nat ETH Zurich, Switzerland, in 2010, his MSc\nfrom University of Leiden, the Netherlands, in\n2005, and his BSc from Fudan University, China, in 1999. His research\ninterests include techniques for the analysis, design, and optimization of\nembedded systems, particularly in the automotive and robotic domains.\nHe was awarded the Program of Chinese Global Youth Experts 2014\nand was granted the Chinese Government Award for Outstanding SelfFinanced Students Abroad 2010. He was the recipient of Best Paper\nAwards ESTC 2017, ESTIMedia 2013, SAMOS 2009, Best Paper Candidate ROBIO 2017, ESTMedia 2009, and General Chairs’ Recognition\nAward for Interactive Papers in CDC 2009. He has served as a member\nof the technical committee on Cybernetics for Cyber-Physical Systems\nof IEEE SMC Society since 2015.\n\n\n\n**Xiaojie Su** (SM’18) received the PhD degree\nin Control Theory and Control Engineering from\nHarbin Institute of Technology, China in 2013.\nHe is currently a professor and the associate\ndean with the College of Automation, Chongqing\nUniversity, Chongqing, China. He has published\n2 research monographs and more than 50 research papers in international referred journals.\nHis current research interests include intelligent control systems, advanced control, and\nunmanned system control. He currently serves\nas an Associate Editor for a number of journals, including IEEE Systems Journal, IEEE Control Systems Letters, Information Sciences, and\nSignal Processing. He is also an Associate Editor for the Conference\nEditorial Board, IEEE Control Systems Society. He was named to the\n2017, 2018 and 2019 Highly Cited Researchers list, Clarivate Analytics.\n\n\n**Alois** **Knoll** (Senior Member) received\nhis diploma (M.Sc.) degree in\nElectrical/Communications Engineering\nfrom the University of Stuttgart, Germany, in\n1985 and his Ph.D. ( _summa cum laude_ ) in\nComputer Science from Technical University\nof Berlin, Germany, in 1988. He served on the\nfaculty of the Computer Science department at\nTU Berlin until 1993. He joined the University\nof Bielefeld, Germany as a full professor\nand served as the director of the Technical\nInformatics research group until 2001. Since 2001, he has been a\nprofessor at the Department of Informatics, Technical University of\nMunich (TUM), Germany. He was also on the board of directors of\nthe Central Institute of Medical Technology at TUM (IMETUM). From\n2004 to 2006, he was Executive Director of the Institute of Computer\nScience at TUM. Between 2007 and 2009, he was a member of the\nEU’s highest advisory board on information technology, ISTAG, the\nInformation Society Technology Advisory Group, and a member of its\nsubgroup on Future and Emerging Technologies (FET). In this capacity,\nhe was actively involved in developing the concept of the EU’s FET\nFlagship projects. His research interests include cognitive, medical\nand sensor-based robotics, multi-agent systems, data fusion, adaptive\nsystems, multimedia information retrieval, model-driven development\nof embedded systems with applications to automotive software and\nelectric transportation, as well as simulation systems for robotics and\ntraffic.\n\n\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-10-0.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-10-1.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-10-2.png)\n\n![](output/images/10f7f74473a3bb998b97fdcf7305c85e8d4ff000.pdf-10-3.png)\n\n0278-0046 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\n\nAuthorized licensed use limited to: Technische Universitaet Muenchen. Downloaded on May 19,2022 at 06:06:28 UTC from IEEE Xplore. Restrictions apply.\n\n\n",
    "ranking": {
      "relevance_score": 0.7311915582492787,
      "citation_score": 0.7301881188118812,
      "recency_score": 0.6192924128038613,
      "final_score": 0.7198009558172574
    },
    "citation_key": "Bing2023SolvingRM",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "129983331ca874142a3e8eb2d93d820bdf1f9aca",
    "title": "Deep Reinforcement Learning for Autonomous Driving: A Survey",
    "published": "2020-02-02",
    "authors": [
      "B. R. Kiran",
      "Ibrahim Sobh",
      "V. Talpaert",
      "P. Mannion",
      "A. A. Sallab",
      "S. Yogamani",
      "P. P'erez"
    ],
    "summary": "With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.",
    "pdf_url": "https://arxiv.org/pdf/2002.00444",
    "doi": "10.1109/TITS.2021.3054625",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "IEEE transactions on intelligent transportation systems (Print)",
    "citation_count": 2019,
    "bibtex": "@Article{Kiran2020DeepRL,\n author = {B. R. Kiran and Ibrahim Sobh and V. Talpaert and P. Mannion and A. A. Sallab and S. Yogamani and P. P'erez},\n booktitle = {IEEE transactions on intelligent transportation systems (Print)},\n journal = {IEEE Transactions on Intelligent Transportation Systems},\n pages = {4909-4926},\n title = {Deep Reinforcement Learning for Autonomous Driving: A Survey},\n volume = {23},\n year = {2020}\n}\n",
    "markdown_text": "1\n\n\n# Deep Reinforcement Learning for Autonomous Driving: A Survey\n\nB Ravi Kiran [1], Ibrahim Sobh [2], Victor Talpaert [3], Patrick Mannion [4],\nAhmad A. Al Sallab [2], Senthil Yogamani [5], Patrick Pérez [6]\n\n\n\n_**Abstract**_ **—With the development of deep representation learn-**\n**ing, the domain of reinforcement learning (RL) has become**\n**a powerful learning framework now capable of learning com-**\n**plex policies in high dimensional environments. This review**\n**summarises deep reinforcement learning (DRL) algorithms and**\n**provides a taxonomy of automated driving tasks where (D)RL**\n**methods have been employed, while addressing key computa-**\n**tional challenges in real world deployment of autonomous driving**\n**agents. It also delineates adjacent domains such as behavior**\n**cloning, imitation learning, inverse reinforcement learning that**\n**are related but are not classical RL algorithms. The role of**\n**simulators in training agents, methods to validate, test and**\n**robustify existing solutions in RL are discussed.**\n\n\n_**Index Terms**_ **—Deep reinforcement learning, Autonomous driv-**\n**ing, Imitation learning, Inverse reinforcement learning, Con-**\n**troller learning, Trajectory optimisation, Motion planning, Safe**\n**reinforcement learning.**\n\n\nI. INTRODUCTION\n\n\nAutonomous driving (AD) [1] systems constitute of multiple\nperception level tasks that have now achieved high precision\non account of deep learning architectures. Besides the perception, autonomous driving systems constitute of multiple\ntasks where classical supervised learning methods are no more\napplicable. First, when the prediction of the agent’s action\nchanges future sensor observations received from the environment under which the autonomous driving agent operates,\nfor example the task of optimal driving speed in an urban\narea. Second, supervisory signals such as time to collision\n(TTC), lateral error w.r.t to optimal trajectory of the agent,\nrepresent the dynamics of the agent, as well uncertainty in\nthe environment. Such problems would require defining the\nstochastic cost function to be maximized. Third, the agent is\nrequired to learn new configurations of the environment, as\nwell as to predict an optimal decision at each instant while\ndriving in its environment. This represents a high dimensional\nspace given the number of unique configurations under which\nthe agent & environment are observed, this is combinatorially\nlarge. In all such scenarios we are aiming to solve a sequential\n\n\n1Navya, Paris. � ravi.kiran@navya.tech\n2Valeo Cairo AI team, Egypt. � ibrahim.sobh, ahmad.elsallab@{valeo.com}\n3U2IS, ENSTA Paris, Institut Polytechnique de Paris & AKKA Technologies, France. � victor.talpaert@ensta.fr\n4School of Computer Science, National University of Ireland, Galway. �\npatrick.mannion@nuigalway.ie\n5Valeo Vision Systems. � senthil.yogamani@valeo.com\n6Valeo.ai. � patrick.perez@valeo.com\n1For easy reference, the main acronyms used in this article are listed in\nAppendix (Table IV).\n\n\n\ndecision process, which is formalized under the classical\nsettings of Reinforcement Learning (RL), where the agent is\nrequired to learn and represent its environment as well as\nact optimally given at each instant [1]. The optimal action\nis referred to as the policy.\nIn this review we cover the notions of reinforcement\n\nlearning, the taxonomy of tasks where RL is a promising\nsolution especially in the domains of driving policy, predictive\nperception, path and motion planning, and low level controller\ndesign. We also focus our review on the different real world\ndeployments of RL in the domain of autonomous driving\nexpanding our conference paper [2] since their deployment has\nnot been reviewed in an academic setting. Finally, we motivate\nusers by demonstrating the key computational challenges and\nrisks when applying current day RL algorithms such imitation\nlearning, deep Q learning, among others. We also note from\nthe trends of publications in figure 2 that the use of RL or\nDeep RL applied to autonomous driving or the self driving\ndomain is an emergent field. This is due to the recent usage of\nRL/DRL algorithms domain, leaving open multiple real world\nchallenges in implementation and deployment. We address the\nopen problems in VI.\nThe main contributions of this work can be summarized as\n\nfollows:\n\n\n_•_\nSelf-contained overview of RL background for the automotive community as it is not well known.\n\n\n_•_\nDetailed literature review of using RL for different autonomous driving tasks.\n\n\n_•_\nDiscussion of the key challenges and opportunities for\nRL applied to real world autonomous driving.\n\nThe rest of the paper is organized as follows. Section II\nprovides an overview of components of a typical autonomous\ndriving system. Section III provides an introduction to reinforcement learning and briefly discusses key concepts. Section\nIV discusses more sophisticated extensions on top of the basic\nRL framework. Section V provides an overview of RL applications for autonomous driving problems. Section VI discusses\nchallenges in deploying RL for real-world autonomous driving\nsystems. Section VII concludes this paper with some final\nremarks.\n\n\nII. COMPONENTS OF AD SYSTEM\n\n\nFigure 1 comprises of the standard blocks of an AD system\ndemonstrating the pipeline from sensor stream to control\nactuation. The sensor architecture in a modern autonomous\n\ndriving system notably includes multiple sets of cameras,\n\n\n\n\n2\n\n\nFig. 1. Standard components in a modern autonomous driving systems pipeline listing the various tasks. The key problems addressed by these modules are\nScene Understanding, Decision and Planning.\n\n\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-1-0.png)\n\nradars and LIDARs as well as a GPS-GNSS system for\nabsolute localisation and inertial measurement Units (IMUs)\nthat provide 3D pose of the vehicle in space.\nThe goal of the perception module is the creation of an\nintermediate level representation of the environment state (for\nexample bird-eye view map of all obstacles and agents) that is\nto be later utilised by a decision making system that ultimately\nproduces the driving policy. This state would include lane\nposition, drivable zone, location of agents such as cars &\npedestrians, state of traffic lights and others. Uncertainties\nin the perception propagate to the rest of the information\nchain. Robust sensing is critical for safety thus using redundant\nsources increases confidence in detection. This is achieved\nby a combination of several perception tasks like semantic\nsegmentation [3], [4], motion estimation [5], depth estimation\n\n[6], soiling detection [7], etc which can be efficiently unified\ninto a multi-task model [8], [9].\n\n\n_A. Scene Understanding_\n\nThis key module maps the abstract mid-level representation\nof the perception state obtained from the perception module to\nthe high level action or decision making module. Conceptually,\nthree tasks are grouped by this module: Scene understanding,\nDecision and Planning as seen in figure 1 module aims to\nprovide a higher level understanding of the scene, it is built\non top of the algorithmic tasks of detection or localisation.\nBy fusing heterogeneous sensor sources, it aims to robustly\ngeneralise to situations as the content becomes more abstract.\nThis information fusion provides a general and simplified\ncontext for the Decision making components.\nFusion provides a sensor agnostic representation of the environment and models the sensor noise and detection uncertain\nties across multiple modalities such as LIDAR, camera, radar,\nultra-sound. This basically requires weighting the predictions\nin a principled way.\n\n\n_B. Localization and Mapping_\n\nMapping is one of the key pillars of automated driving [10].\nOnce an area is mapped, current position of the vehicle can\n\n\n\nbe localized within the map. The first reliable demonstrations\nof automated driving by Google were primarily reliant on\nlocalisation to pre-mapped areas. Because of the scale of\nthe problem, traditional mapping techniques are augmented\nby semantic object detection for reliable disambiguation. In\naddition, localised high definition maps (HD maps) can be\nused as a prior for object detection.\n\n\n_C. Planning and Driving policy_\n\n\nTrajectory planning is a crucial module in the autonomous\ndriving pipeline. Given a route-level plan from HD maps or\nGPS based maps, this module is required to generate motionlevel commands that steer the agent.\nClassical motion planning ignores dynamics and differential\nconstraints while using translations and rotations required to\nmove an agent from source to destination poses [11]. A robotic\nagent capable of controlling 6-degrees of freedom (DOF) is\nsaid to be holonomic, while an agent with fewer controllable\nDOFs than its total DOF is said to be non-holonomic. Classical\n\nalgorithms such as _A_ _[∗]_ algorithm based on Djisktra’s algorithm\ndo not work in the non-holonomic case for autonomous\n\ndriving. Rapidly-exploring random trees (RRT) [12] are nonholonomic algorithms that explore the configuration space by\nrandom sampling and obstacle free path generation. There are\nvarious versions of RRT currently used in for motion planning\nin autonomous driving pipelines.\n\n\n_D. Control_\n\n\nA controller defines the speed, steering angle and braking\nactions necessary over every point in the path obtained from\na pre-determined map such as Google maps, or expert driving\nrecording of the same values at every waypoint. Trajectory\ntracking in contrast involves a temporal model of the dynamics\nof the vehicle viewing the waypoints sequentially over time.\nCurrent vehicle control methods are founded in classical optimal control theory which can be stated as a minimisation of\na cost function ˙ _x = f_ ( _x_ ( _t_ ), _u_ ( _t_ )) defined over a set of states _x_ ( _t_ )\nand control actions _u_ ( _t_ ). The control input is usually defined\n\n\n\n\n3\n\n\nFig. 2. Trend of publications for keywords 1. \"reinforcement learning\", 2.\"deep reinforcement\", and 3.\"reinforcement learning\" AND (\"autonomous cars\" OR\n\"autonomous vehicles\" OR \"self driving\") for academic publication trends from this [13].\n\n\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-2-0.png)\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-2-1.png)\n\nover a finite time horizon and restricted on a feasible state\nspace _x ∈_ _X_ free [14]. The velocity control are based on classical\nmethods of closed loop control such as PID (proportionalintegral-derivative) controllers, MPC (Model predictive control). PIDs aim to minimise a cost function constituting of\nthree terms current error with proportional term, effect of\npast errors with integral term, and effect of future errors with\nthe derivative term. While the family of MPC methods aim\nto stabilize the behavior of the vehicle while tracking the\nspecified path [15]. A review on controllers, motion planning\nand learning based approaches for the same are provided in\nthis review [16] for interested readers. Optimal control and\nreinforcement learning are intimately related, where optimal\ncontrol can be viewed as a model based reinforcement learning\nproblem where the dynamics of the vehicle/environment are\nmodeled by well defined differential equations. Reinforcement\nlearning methods were developed to handle stochastic control\nproblems as well ill-posed problems with unknown rewards\nand state transition probabilities. Autonomous vehicle stochastic control is large domain, and we advise readers to read the\nsurvey on this subject by authors in [17].\n\n\nIII. REINFORCEMENT LEARNING\n\n\nMachine learning (ML) is a process whereby a computer\nprogram learns from experience to improve its performance at\na specified task [18]. ML algorithms are often classified under\none of three broad categories: supervised learning, unsupervised learning and reinforcement learning (RL). Supervised\nlearning algorithms are based on inductive inference where\nthe model is typically trained using labelled data to perform\nclassification or regression, whereas unsupervised learning encompasses techniques such as density estimation or clustering\napplied to unlabelled data. By contrast, in the RL paradigm\nan autonomous agent learns to improve its performance at\nan assigned task by interacting with its environment. Russel\nand Norvig define an agent as “anything that can be viewed\nas perceiving its environment through sensors and acting\nupon that environment through actuators” [19]. RL agents\nare not told explicitly how to act by an expert; rather an\nagent’s performance is evaluated by a reward function _R_ .\nFor each state experienced, the agent chooses an action and\nreceives an occasional reward from its environment based on\n\nthe usefulness of its decision. The goal for the agent is to\nmaximize the cumulative rewards received over its lifetime.\n\nGradually, the agent can increase its long-term reward by\n\n\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-2-2.png)\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-2-3.png)\n\n**simulator-to-real gap**\n\n\nFig. 3. A graphical decomposition of the different components of an RL\nalgorithm. It also demonstrates the different challenges encountered while\ntraining a D(RL) algorithm.\n\n\nexploiting knowledge learned about the expected utility (i.e.\ndiscounted sum of expected future rewards) of different stateaction pairs. One of the main challenges in reinforcement\nlearning is managing the trade-off between exploration and\nexploitation. To maximize the rewards it receives, an agent\nmust exploit its knowledge by selecting actions which are\nknown to result in high rewards. On the other hand, to\ndiscover such beneficial actions, it has to take the risk of\ntrying new actions which may lead to higher rewards than\nthe current best-valued actions for each system state. In other\nwords, the learning agent has to exploit what it already knows\nin order to obtain rewards, but it also has to explore the\nunknown in order to make better action selections in the future.\n\nExamples of strategies which have been proposed to manage\nthis trade-off include _ϵ_ -greedy and softmax. When adopting\nthe ubiquitous _ϵ_ -greedy strategy, an agent either selects an\naction at random with probability 0 _< ϵ <_ 1, or greedily\nselects the highest valued action for the current state with\nthe remaining probability 1 _−_ _ϵ_ . Intuitively, the agent should\nexplore more at the beginning of the training process when\nlittle is known about the problem environment. As training\nprogresses, the agent may gradually conduct more exploitation\nthan exploration. The design of exploration strategies for RL\nagents is an area of active research (see _e.g._ [20]).\n\n\n\n\nMarkov decision processes (MDPs) are considered the de\nfacto standard when formalising sequential decision making\nproblems involving a single RL agent [21]. An MDP consists\nof a set _S_ of states, a set _A_ of actions, a transition function\n_T_ and a reward function _R_ [22], i.e. a tuple _< S_, _A_, _T_, _R >_ .\nWhen in any state _s ∈_ _S_, selecting an action _a ∈_ _A_ will result\nin the environment entering a new state _s_ _[′]_ _∈_ _S_ with a transition\nprobability _T_ ( _s_, _a_, _s_ _[′]_ ) _∈_ (0, 1), and give a reward _R_ ( _s_, _a_ ). This\nprocess is illustrated in Fig. 3. The stochastic policy _π_ : _S →_ _D_\nis a mapping from the state space to a probability over the set\nof actions, and _π_ ( _a|s_ ) represents the probability of choosing\naction _a_ at state _s_ . The goal is to find the optimal policy\n_π_ _[∗]_, which results in the highest expected sum of discounted\nrewards [21]:\n\n\n\n_π_ _[∗]_ _=_ argmax E _π_\n\n_π_\n\n\n\n_H−_ 1\n�\n�\n\n\n\n_−_\n� _γ_ _[k]_ _r_ _k+_ 1 _| s_ 0 _= s_\n\n_k=_ 0 �\n\n\n\n, (1)\n\n\n\n4\n\n\ndirectly. Dynamic programming (DP) refers to a collection of\nalgorithms that can be used to compute optimal policies given\na perfect model of the environment in terms of reward and\ntransition functions. Unlike DP, in Monte Carlo methods there\nis no assumption of complete environment knowledge. Monte\nCarlo methods are incremental in an episode-by-episode sense.\nUpon the completion of an episode, the value estimates and\npolicies are updated. Temporal Difference (TD) methods, on\nthe other hand, are incremental in a step-by-step sense, making\nthem applicable to non-episodic scenarios. Like Monte Carlo\nmethods, TD methods can learn directly from raw experience\nwithout a model of the environment’s dynamics. Like DP, TD\nmethods learn their estimates based on other estimates.\n\n\n_A. Value-based methods_\n\n\nQ-learning is one of the most commonly used RL algorithms. It is a model-free TD algorithm that learns estimates of\nthe utility of individual state-action pairs (Q-functions defined\nin Eqn. 2). Q-learning has been shown to converge to the\noptimum state-action values for a MDP with probability 1,\nso long as all actions in all states are sampled infinitely\noften and the state-action values are represented discretely\n\n[23]. In practice, Q-learning will learn (near) optimal stateaction values provided a sufficient number of samples are\nobtained for each state-action pair. If a Q-learning agent has\nconverged to the optimal Q values for a MDP and selects\nactions greedily thereafter, it will receive the same expected\nsum of discounted rewards as calculated by the value function\nwith _π_ _[∗]_ (assuming that the same arbitrary initial starting state\nis used for both). Agents implementing Q-learning update their\nQ values according to the following update rule:\n\n\n_Q_ ( _s_, _a_ ) _←_ _Q_ ( _s_, _a_ ) _+_ _α_ [ _r +_ _γ_ max (3)\n_a_ _[′]_ _∈A_ _[Q]_ [(] _[s][′]_ [,] _[a][′]_ [)] _[−]_ _[Q]_ [(] _[s]_ [,] _[a]_ [)]][,]\n\n\nwhere _Q_ ( _s_, _a_ ) is an estimate of the utility of selecting action\n_a_ in state _s_, _α ∈_ [0, 1] is the learning rate which controls the\ndegree to which Q values are updated at each time step, and\n_γ ∈_ [0, 1] is the same discount factor used in Eqn. 1. The\ntheoretical guarantees of Q-learning hold with any arbitrary\ninitial Q values [23]; therefore the optimal Q values for a MDP\ncan be learned by starting with any initial action value function\nestimate. The initialisation can be optimistic (each _Q_ ( _s_, _a_ )\nreturns the maximum possible reward), pessimistic (minimum)\nor even using knowledge of the problem to ensure faster\nconvergence. Deep Q-Networks (DQN) [24] incorporates a\nvariant of the Q-learning algorithm [25], by using deep neural\nnetworks (DNNs) as a non-linear Q function approximator\nover high-dimensional state spaces (e.g. the pixels in a frame\nof an Atari game). Practically, the neural network predicts the\nvalue of all actions without the use of any explicit domainspecific information or hand-designed features. DQN applies\nexperience replay technique to break the correlation between\nsuccessive experience samples and also for better sample\nefficiency. For increased stability, two networks are used where\nthe parameters of the target network for DQN are fixed for\na number of iterations while updating the parameters of the\nonline network. Readers are directed to sub-section III-E for\n\na more detailed introduction to the use of DNNs in Deep RL.\n\n\n\n~~�~~ � ~~�~~ ~~�~~\n: _=Vπ_ ( _s_ )\n\n\nfor all states _s ∈_ _S_, where _r_ _k = R_ ( _sk_, _ak_ ) is the reward at time\n_k_ and _Vπ_ ( _s_ ), the ‘value function’ at state _s_ following a policy\n_π_, is the expected ‘return’ (or ‘utility’) when starting at _s_ and\nfollowing the policy _π_ thereafter [1]. An important, related\nconcept is the action-value function, a.k.a.‘Q-function’ defined\n\nas:\n\n\n\n_Qπ_ ( _s_, _a_ ) _=_ E _π_\n\n\n\n_H−_ 1\n� _γ_ _[k]_ _r_ _k+_ 1 _| s_ 0 _= s_, _a_ 0 _= a_ . (2)\n� _k=_ 0 �\n\n\n\nThe discount factor _γ ∈_ [0, 1] controls how an agent regards\nfuture rewards. Low values of _γ_ encourage myopic behaviour\nwhere an agent will aim to maximise short term rewards,\nwhereas high values of _γ_ cause agents to be more forwardlooking and to maximise rewards over a longer time frame.\nThe horizon _H_ refers to the number of time steps in the\nMDP. In infinite-horizon problems _H = ∞_, whereas in episodic\ndomains _H_ has a finite value. Episodic domains may terminate\nafter a fixed number of time steps, or when an agent reaches\na specified goal state. The last state reached in an episodic\ndomain is referred to as the terminal state. In finite-horizon\nor goal-oriented domains discount factors of (close to) 1 may\nbe used to encourage agents to focus on achieving the goal,\nwhereas in infinite-horizon domains lower discount factors\nmay be used to strike a balance between short- and longterm rewards. If the optimal policy for a MDP is known,\nthen _Vπ_ _[∗]_ may be used to determine the maximum expected\ndiscounted sum of rewards available from any arbitrary initial\nstate. A rollout is a trajectory produced in the state space\nby sequentially applying a policy to an initial state. A MDP\nsatisfies the Markov property, i.e. system state transitions are\ndependent only on the most recent state and action, not on\nthe full history of states and actions in the decision process.\nMoreover, in many real-world application domains, it is not\npossible for an agent to observe all features of the environment\nstate; in such cases the decision-making problem is formulated\nas a partially-observable Markov decision process (POMDP).\nSolving a reinforcement learning task means finding a policy\n_π_ that maximises the expected discounted sum of rewards\nover trajectories in the state space. RL agents may learn\nvalue function estimates, policies and/or environment models\n\n\n\n\n_B. Policy-based methods_\n\n\nThe difference between value-based and policy-based methods is essentially a matter of where the burden of optimality\nresides. Both method types must propose actions and evaluate\nthe resulting behaviour, but while value-based methods focus\non evaluating the optimal cumulative reward and have a policy\nfollows the recommendations, policy-based methods aim to\nestimate the optimal policy directly, and the value is a secondary if calculated at all. Typically, a policy is parameterised\nas a neural network _πθ_ . Policy gradient methods use gradient\ndescent to estimate the parameters of the policy that maximise\nthe expected reward. The result can be a stochastic policy\nwhere actions are selected by sampling, or a deterministic\npolicy. Many real-world applications have continuous action\nspaces. Deterministic policy gradient (DPG) algorithms [26]\n\n[1] allow reinforcement learning in domains with continuous\nactions. Silver _et al._ [26] proved that a deterministic policy\ngradient exists for MDPs satisfying certain conditions, and\nthat deterministic policy gradients have a simple model-free\nform that follows the gradient of the action-value function.\nAs a result, instead of integrating over both state and action\nspaces in stochastic policy gradients, DPG integrates over\nthe state space only leading to fewer samples in problems\nwith large action spaces. To ensure sufficient exploration,\nactions are chosen using a stochastic policy, while learning a\ndeterministic target policy. The REINFORCE [27] algorithm\nis a straight forward policy-based method. The discounted\ncumulative reward _gt =_ [�] _k_ _[H]_ _=_ _[−]_ 0 [1] _[γ][k]_ _[r]_ _[k][+][t][+]_ [1][ at one time step is]\ncalculated by playing the entire episode, so no estimator is\nrequired for policy evaluation. The parameters are updated into\nthe direction of the performance gradient:\n\n\n_θ ←_ _θ +_ _αγ_ _[t]_ _g∇_ log _πθ_ ( _a|s_ ), (4)\n\n\nwhere _α_ is the learning rate for a stable incremental update.\nIntuitively, we want to encourage state-action pairs that result\nin the best possible returns. Trust Region Policy Optimization\n(TRPO) [28], works by preventing the updated policies from\ndeviating too much from previous policies, thus reducing the\nchance of a bad update. TRPO optimises a surrogate objective\nfunction where the basic idea is to limit each policy gradient\nupdate as measured by the Kullback-Leibler (KL) divergence\nbetween the current and the new proposed policy. This method\nresults in monotonic improvements in policy performance.\nWhile Proximal Policy Optimization (PPO) [29] proposed a\nclipped surrogate objective function by adding a penalty for\nhaving a too large policy change. Accordingly, PPO policy\noptimisation is simpler to implement, and has better sample\ncomplexity while ensuring the deviation from the previous\npolicy is relatively small.\n\n\n_C. Actor-critic methods_\n\n\nActor-critic methods are hybrid methods that combine the\nbenefits of policy-based and value-based algorithms. The\npolicy structure that is responsible for selecting actions is\nknown as the ‘actor’. The estimated value function criticises\n\nthe actions made by the actor and is known as the ‘critic’.\nAfter each action selection, the critic evaluates the new state to\n\n\n\n5\n\n\ndetermine whether the result of the selected action was better\n\nor worse than expected. Both networks need their gradient to\nlearn. Let _J_ ( _θ_ ) : _=_ E _πθ_ [ _r_ ] represent a policy objective function,\nwhere _θ_ designates the parameters of a DNN. Policy gradient\nmethods search for local maximum of _J_ ( _θ_ ). Since optimization\nin continuous action spaces could be costly and slow, the\nDPG (Direct Policy Gradient) algorithm represents actions\nas parameterised function _µ_ ( _s|θ_ _[µ]_ ), where _θ_ _[µ]_ refers to the\nparameters of the actor network. Then the unbiased estimate\nof the policy gradient gradient step is given as:\n\n_∇θ_ _J = −_ E _πθ_ �( _g −_ _b_ )log _πθ_ ( _a|s_ )�, (5)\n\n\nwhere _b_ is the baseline. While using _b ≡_ 0 is the simplification\nthat leads to the REINFORCE formulation. Williams [27]\nexplains a well chosen baseline can reduce variance leading\nto a more stable learning. The baseline, _b_ can be chosen\nas _Vπ_ ( _s_ ), _Qπ_ ( _s_, _a_ ) or ‘Advantage’ _Aπ_ ( _s_, _a_ ) based methods.\nDeep Deterministic Policy Gradient (DDPG) [30] is a modelfree, off-policy (please refer to subsection III-D for a detailed\ndistinction), actor-critic algorithm that can learn policies for\ncontinuous action spaces using deep neural net based function\napproximation, extending prior work on DPG to large and\nhigh-dimensional state-action spaces. When selecting actions,\nexploration is performed by adding noise to the actor policy.\nLike DQN, to stabilise learning a replay buffer is used to\nminimize data correlation. A separate actor-critic specific\ntarget network is also used. Normal Q-learning is adapted with\na restricted number of discrete actions, and DDPG also needs\na straightforward way to choose an action. Starting from Qlearning, we extend Eqn. 2 to define the optimal Q-value and\noptimal action as _Q_ _[∗]_ and _a_ _[∗]_ .\n\n\n_Q_ _[∗]_ ( _s_, _a_ ) _=_ max _Qπ_ ( _s_, _a_ ), _a_ _[∗]_ _=_ argmax _Q_ _[∗]_ ( _s_, _a_ ). (6)\n_π_ _a_\n\n\nIn the case of Q-learning, the action is chosen according to\nthe Q-function as in Eqn. 6. But DDPG chains the evaluation\nof Q after the action has already been chosen according to the\npolicy. By correcting the Q-values towards the optimal values\nusing the chosen action, we also update the policy towards the\noptimal action proposition. Thus two separate networks work\nat estimating _Q_ _[∗]_ and _π_ _[∗]_ .\nAsynchronous Advantage Actor Critic (A3C) [31] uses\nasynchronous gradient descent for optimization of deep neural\nnetwork controllers. Deep reinforcement learning algorithms\nbased on experience replay such as DQN and DDPG have\ndemonstrated considerable success in difficult domains such\nas playing Atari games. However, experience replay uses a\nlarge amount of memory to store experience samples and\nrequires off-policy learning algorithms. In A3C, instead of\nusing an experience replay buffer, agents asynchronously\nexecute on multiple parallel instances of the environment. In\naddition to the reducing correlation of the experiences, the\nparallel actor-learners have a stabilizing effect on training\nprocess. This simple setup enables a much larger spectrum\nof on-policy as well as off-policy reinforcement learning\nalgorithms to be applied robustly using deep neural networks.\nA3C exceeded the performance of the previous state-of-theart at the time on the Atari domain while training for half\n\n\n\n\nthe time on a single multi-core CPU instead of a GPU by\ncombining several ideas. It also demonstrates how using an\nestimate of the value function as the previously explained\nbaseline _b_ reduces variance and improves convergence time.\nBy defining the _advantage_ as _Aπ_ ( _a_, _s_ ) _= Qπ_ ( _s_, _a_ ) _−_ _Vπ_ ( _s_ ), the\nexpression of the policy gradient from Eqn. 5 is rewritten\nas _∇θ_ _L = −_ E _πθ_ { _Aπ_ ( _a_, _s_ )log _πθ_ ( _a|s_ )}. The critic is trained to\nminimize 12 �� _Aπθ_ ( _a_, _s_ )��2. The intuition of using advantage\nestimates rather than just discounted returns is to allow the\nagent to determine not just how good its actions were, but also\nhow much better they turned out to be than expected, leading\nto reduced variance and more stable training. The A3C model\nalso demonstrated good performance in 3D environments such\nas labyrinth exploration. Advantage Actor Critic (A2C) is\na synchronous version of the asynchronous advantage actor\ncritic model, that waits for each agent to finish its experience\nbefore conducting an update. The performance of both A2C\nand A3C is comparable. Most greedy policies must alternate\nbetween exploration and exploitation, and good exploration\nvisits the states where the value estimate is uncertain. This\n\nway, exploration focuses on trying to find the most _uncertain_\nstate paths as they bring valuable information. In addition to\nadvantage, explained earlier, some methods use the entropy as\nthe uncertainty quantity. Most A3C implementations include\nthis as well. Two methods with common authors are energybased policies [32] and more recent and with widespread use,\nthe Soft Actor Critic (SAC) algorithm [33], both rely on adding\nan entropy term to the reward function, so we update the policy\nobjective from Eqn. 1 to Eqn. 7. We refer readers to [33] for\nan in depth explanation of the expression\n\n\n_π_ _[∗]_ _MaxEnt_ _[=]_ [ argmax] E _π_ ��[ _r_ ( _st_, _at_ ) _+_ _αH_ ( _π_ (. _|st_ ))]�, (7)\n_π_ _t_\n\n\nshown here for illustration of how the entropy _H_ is added.\n\n\n_D. Model-based (vs. Model-free) & On/Off Policy methods_\n\n\nIn practical situations, interacting with the real environment\ncould be limited due to many reasons including safety and\ncost. Learning a model for environment dynamics may reduce\nthe amount of interactions required with the real environment. Moreover, exploration can be performed on the learned\nmodels. In the case of model-based approaches ( _e.g._ DynaQ [34], R-max [35]), agents attempt to learn the transition\nfunction _T_ and reward function _R_, which can be used when\nmaking action selections. Keeping a model approximation of\nthe environment means storing knowledge of its dynamics, and\nallows for fewer, and sometimes, costly environment interactions. By contrast, in model-free approaches such knowledge\nis not a requirement. Instead, model-free learners sample the\nunderlying MDP directly in order to gain knowledge about\nthe unknown model, in the form of value function estimates\nfor example. In Dyna-2 [36], the learning agent stores longterm and short-term memories, where a memory is defined\nas the set of features and corresponding parameters used by\nan agent to estimate the value function. Long-term memory\nis for general domain knowledge which is updated from real\nexperience, while short-term memory is for specific local\n\n\n\n6\n\n\nknowledge about the current situation, and the value function\nis a linear combination of long and short term memories.\nLearning algorithms can be on-policy or off-policy depending on whether the updates are conducted on fresh trajectories\ngenerated by the policy or by another policy, that could be\ngenerated by an older version of the policy or provided by an\nexpert. On-policy methods such as SARSA [37], estimate the\nvalue of a policy while using the same policy for control.\nHowever, off-policy methods such as Q-learning [25], use\ntwo policies: the behavior policy, the policy used to generate\nbehavior; and the target policy, the one being improved on. An\nadvantage of this separation is that the target policy may be\ndeterministic (greedy), while the behavior policy can continue\nto sample all possible actions, [1].\n\n\n_E. Deep reinforcement learning (DRL)_\n\n\nTabular representations are the simplest way to store learned\nestimates (of _e.g._ values, policies or models), where each stateaction pair has a discrete estimate associated with it. When\nestimates are represented discretely, each additional feature\ntracked in the state leads to an exponential growth in the\nnumber of state-action pair values that must be stored [38].\nThis problem is commonly referred to in the literature as the\n“curse of dimensionality”, a term originally coined by Bellman\n\n[39]. In simple environments this is rarely an issue, but it\nmay lead to an intractable problem in real-world applications,\ndue to memory and/or computational constraints. Learning\nover a large state-action space is possible, but may take an\nunacceptably long time to learn useful policies. Many realworld domains feature continuous state and/or action spaces;\nthese can be discretised in many cases. However, large discretisation steps may limit the achievable performance in a domain,\nwhereas small discretisation steps may result in a large stateaction space where obtaining a sufficient number of samples\nfor each state-action pair is impractical. Alternatively, function\napproximation may be used to generalise across states and/or\nactions, whereby a function approximator is used to store and\nretrieve estimates. Function approximation is an active area\nof research in RL, offering a way to handle continuous state\nand/or action spaces, mitigate against the state-action space\nexplosion and generalise prior experience to previously unseen\nstate-action pairs. Tile coding is one of the simplest forms\nof function approximation, where one tile represents multiple\nstates or state-action pairs [38]. Neural networks are also\ncommonly used to implement function approximation, one of\nthe most famous examples being Tesuaro’s application of RL\nto backgammon [40]. Recent work has applied deep neural\nnetworks as a function approximation method; this emerging\nparadigm is known as deep reinforcement learning (DRL).\nDRL algorithms have achieved human level performance (or\nabove) on complex tasks such as playing Atari games [24] and\nplaying the board game Go [41].\nIn DQN [24] it is demonstrated how a convolutional neural\nnetwork can learn successful control policies from just raw\nvideo data for different Atari environments. The network\n\nwas trained end-to-end and was not provided with any game\nspecific information. The input to the convolutional neural\n\n\n\n\nnetwork consists of a 84 _×_ 84 _×_ 4 tensor of 4 consecutive stacked\n\nframes used to capture the temporal information. Through\nconsecutive layers, the network learns how to combine features\nin order to identify the action most likely to bring the best\noutcome. One layer consists of several convolutional filters.\nFor instance, the first layer uses 32 filters with 8 _×_ 8 kernels\nwith stride 4 and applies a rectifier non linearity. The second\nlayer is 64 filters of 4 _×_ 4 with stride 2, followed by a rectifier\nnon-linearity. Next comes a third convolutional layer of 64\nfilters of 3 _×_ 3 with stride 1 followed by a rectifier. The last\nintermediate layer is composed of 512 rectifier units fully\nconnected. The output layer is a fully-connected linear layer\nwith a single output for each valid action. For DQN training\nstability, two networks are used while the parameters of the\ntarget network are fixed for a number of iterations while\nupdating the online network parameters. For practical reasons,\nthe _Q_ ( _s_, _a_ ) function is modeled as a deep neural network\nthat predicts the value of all actions given the input state.\nAccordingly, deciding what action to take requires performing\na single forward pass of the network. Moreover, in order\nto increase sample efficiency, experiences of the agent are\nstored in a replay memory (experience replay), where the Qlearning updates are conducted on randomly selected samples\nfrom the replay memory. This random selection breaks the\ncorrelation between successive samples. Experience replay\nenables reinforcement learning agents to _remember_ and reuse\nexperiences from the past where observed transitions are stored\nfor some time, usually in a queue, and sampled uniformly from\nthis memory to update the network. However, this approach\nsimply replays transitions at the same frequency that they were\noriginally experienced, regardless of their significance. An\nalternative method is to use two separate experience buckets,\none for positive and one for negative rewards [42]. Then a fixed\nfraction from each bucket is selected to replay. This method\nis only applicable in domains that have a natural notion of\nbinary experience. Experience replay has also been extended\nwith a framework for prioritising experience [43], where\nimportant transitions, based on the TD error, are replayed\nmore frequently, leading to improved performance and faster\ntraining when compared to the standard experience replay\napproach.\nThe max operator in standard Q-learning and DQN uses the\nsame values both to select and to evaluate an action resulting\nin over optimistic value estimates. In Double DQN (D-DQN)\n\n[44] the over estimation problem in DQN is tackled where the\ngreedy policy is evaluated according to the online network and\nuses the target network to estimate its value. It was shown that\nthis algorithm not only yields more accurate value estimates,\nbut leads to much higher scores on several games.\nIn Dueling network architecture [45] the state value function\nand associated advantage function are estimated, and then\ncombined together to estimate action value function. The\nadvantage of the dueling architecture lies partly in its ability\nto learn the state-value function efficiently. In a single-stream\narchitecture only the value for one of the actions is updated.\nHowever in dueling architecture, the value stream is updated\nwith every update, allowing for better approximation of the\nstate values, which in turn need to be accurate for temporal\n\n\n\n7\n\n\ndifference methods like Q-learning.\nDRQN [46] applied a modification to the DQN by combining a Long Short Term Memory (LSTM) with a Deep QNetwork. Accordingly, the DRQN is capable of integrating information across frames to detect information such as velocity\nof objects. DRQN showed to generalize its policies in case of\ncomplete observations and when trained on Atari games and\nevaluated against flickering games, it was shown that DRQN\ngeneralizes better than DQN.\n\n\nIV. EXTENSIONS TO REINFORCEMENT LEARNING\n\n\nThis section introduces and discusses some of the main\n\nextensions to the basic single-agent RL paradigms which\nhave been introduced over the years. As well as broadening\nthe applicability of RL algorithms, many of the extensions\ndiscussed here have been demonstrated to improve scalability,\nlearning speed and/or converged performance in complex\nproblem domains.\n\n\n_A. Reward shaping_\n\n\nAs noted in Section III, the design of the reward function\nis crucial: RL agents seek to maximise the return from the\nreward function, therefore the optimal policy for a domain\nis defined with respect to the reward function. In many realworld application domains, learning may be difficult due to\nsparse and/or delayed rewards. RL agents typically learn how\nto act in their environment guided merely by the reward signal.\nAdditional knowledge can be provided to a learner by the\naddition of a shaping reward to the reward naturally received\nfrom the environment, with the goal of improving learning\nspeed and converged performance. This principle is referred\nto as reward shaping. The term shaping has its origins in the\nfield of experimental psychology, and describes the idea of\nrewarding all behaviour that leads to the desired behaviour.\nSkinner [47] discovered while training a rat to push a lever that\nany movement in the direction of the lever had to be rewarded\nto encourage the rat to complete the task. Analogously to\nthe rat, a RL agent may take an unacceptably long time to\ndiscover its goal when learning from delayed rewards, and\nshaping offers an opportunity to speed up the learning process.\nReward shaping allows a reward function to be engineered in a\nway to provide more frequent feedback signal on appropriate\nbehaviours [48], which is especially useful in domains with\nsparse rewards. Generally, the return from the reward function\nis modified as follows: _r_ _[′]_ _= r+ f_ where _r_ is the return from the\noriginal reward function _R_, _f_ is the additional reward from a\nshaping function _F_, and _r_ _[′]_ is the signal given to the agent\nby the augmented reward function _R_ _[′]_ . Empirical evidence\nhas shown that reward shaping can be a powerful tool to\nimprove the learning speed of RL agents [49]. However, it\ncan have unintended consequences. The implication of adding\na shaping reward is that a policy which is optimal for the\naugmented reward function _R_ _[′]_ may not in fact also be optimal\nfor the original reward function _R_ . A classic example of\nreward shaping gone wrong for this exact reason is reported\nby [49] where the experimented bicycle agent would turn in\ncircle to stay upright rather than reach its goal. Difference\n\n\n\n\nrewards ( _D_ ) [50] and potential-based reward shaping ( _PBRS_ )\n\n[51] are two commonly used shaping approaches. Both _D_\nand _PBRS_ have been successfully applied to a wide range of\napplication domains and have the added benefit of convenient\ntheoretical guarantees, meaning that they do not suffer from\nthe same issues as the unprincipled reward shaping approaches\ndescribed above (see _e.g._ [51]–[55]).\n\n\n_B. Multi-agent reinforcement learning (MARL)_\n\n\nIn multi-agent reinforcement learning, multiple RL agents\nare deployed into a common environment. The single-agent\nMDP framework becomes inadequate when multiple autonomous agents act simultaneously in the same domain.\nInstead, the more general stochastic game (SG) may be used in\nthe case of a Multi-Agent System (MAS) [56]. A SG is defined\nas a tuple _< S_, _A_ 1... _N_, _T_, _R_ 1... _N >_, where _N_ is the number of\nagents, _S_ is the set of system states, _A_ _i_ is the set of actions\nfor agent _i_ (and _A_ is the joint action set), _T_ is the transition\nfunction, and _Ri_ is the reward function for agent _i_ . The SG\nlooks very similar to the MDP framework, apart from the\naddition of multiple agents. In fact, for the case of _N =_ 1 a SG\nthen becomes a MDP. The next system state and the rewards\nreceived by each agent depend on the joint action _a_ of all of the\nagents in a SG, where _a_ is derived from the combination of the\nindividual actions _ai_ for each agent in the system. Each agent\nmay have its own local state perception _si_, which is different\nto the system state _s_ (i.e. individual agents are not assumed to\nhave full observability of the system). Note also that each\nagent may receive a different reward for the same system\nstate transition, as each agent has its own separate reward\nfunction _Ri_ . In a SG, the agents may all have the same goal\n(collaborative SG), totally opposing goals (competitive SG),\nor there may be elements of collaboration and competition\nbetween agents (mixed SG). Whether RL agents in a MAS\nwill learn to act together or at cross-purposes depends on the\nreward scheme used for a specific application.\n\n\n_C. Multi-objective reinforcement learning_\n\n\nIn multi-objective reinforcement learning (MORL) the reward signal is a vector, where each component represents the\nperformance on a different objective. The MORL framework\nwas developed to handle sequential decision making problems\nwhere tradeoffs between conflicting objective functions must\nbe considered. Examples of real-world problems with multiple\nobjectives include selecting energy sources (tradeoffs between\nfuel cost and emissions) [57] and watershed management\n(tradeoffs between generating electricity, preserving reservoir\nlevels and supplying drinking water) [58]. Solutions to MORL\nproblems are often evaluated using the concept of Pareto\ndominance [59] and MORL algorithms typically seek to learn\nor approximate the set of non-dominated solutions. MORL\nproblems may be defined using the MDP or SG framework as\nappropriate, in a similar manner to single-objective problems.\nThe main difference lies in the definition of the reward\nfunction: instead of returning a single scalar value _r_, the\nreward function **R** in multi-objective domains returns a vector\n**r** consisting of the rewards for each individual objective\n\n\n\n8\n\n\n_c ∈_ _C_ . Therefore, a regular MDP or SG can be extended to\na Multi-Objective MDP (MOMDP) or Multi-Objective SG\n(MOSG) by modifying the return of the reward function. For a\nmore complete overview of MORL beyond the brief summary\npresented in this section, the interested reader is referred to\nrecent surveys [60], [61].\n\n\n_D. State Representation Learning (SRL)_\n\n\nState Representation Learning refers to feature extraction\n& dimensionality reduction to represent the state space with\nits history conditioned by the actions and environment of the\nagent. A complete review of SRL for control is discussed\nin [62]. In the simplest form SRL maps a high dimensional\nvector _ot_ into a small dimensional latent space _st_ . The inverse\noperation decodes the state back into an estimate of the\noriginal observation ˆ _ot_ . The agent then learns to map from\nthe latent space to the action. Training the SRL chain is\nunsupervised in the sense that no labels are required. Reducing\nthe dimension of the input effectively simplifies the task as\nit removes noise and decreases the domain’s size as shown\n\nin [63]. SRL could be a simple auto-encoder (AE), though\nvarious methods exist for observation reconstruction such as\n\nVariational Auto-Encoders (VAE) or Generative Adversarial\nNetworks (GANs), as well as forward models for predicting\nthe next state or inverse models for predicting the action given\na transition. A good learned state representation should be\nMarkovian; i.e. it should encode all necessary information to\nbe able to select an action based on the current state only, and\nnot any previous states or actions [62], [64].\n\n\n_E. Learning from Demonstrations_\n\n\nLearning from Demonstrations (LfD) is used by humans\nto acquire new skills in an expert to learner knowledge\ntransmission process. LfD is important for initial exploration\nwhere reward signals are too sparse or the input domain is\ntoo large to cover. In LfD, an agent learns to perform a\ntask from demonstrations, usually in the form of state-action\npairs, provided by an expert without any feedback rewards.\nHowever, high quality and diverse demonstrations are hard to\ncollect, leading to learning sub-optimal policies. Accordingly,\nlearning merely from demonstrations can be used to initialize\nthe learning agent with a good or safe policy, and then\nreinforcement learning can be conducted to enable the discovery of a better policy by interacting with the environment.\nCombining demonstrations and reinforcement learning has\nbeen conducted in recent research. AlphaGo [41], combines\nsearch tree with deep neural networks, initializes the policy\nnetwork by supervised learning on state-action pairs provided\nby recorded games played by human experts. Additionally, a\nvalue network is trained to tell how desirable a board state is.\n\nBy conducting self-play and reinforcement learning, AlphaGo\nis able to discover new stronger actions and learn from its\nmistakes, achieving super human performance. More recently,\nAlphaZero [65], developed by the same team, proposed a\ngeneral framework for self-play models. AlphaZero is trained\nentirely using reinforcement learning and self play, starting\nfrom completely random play, and requires no prior knowledge\n\n\n\n\nof human players. AlphaZero taught itself from scratch how\nto master the games of chess, shogi, and Go game, beating a\nworld-champion program in each case. In [66] it is shown that\ngiven the initial demonstration, no explicit exploration is necessary, and we can attain near-optimal performance. Measuring\nthe divergence between the current policy and the expert policy\nfor optimization is proposed in [67]. DQfD [68] pre-trains the\nagent and uses expert demonstrations by adding them into\nthe replay buffer with additional priority. Moreover, a training\nframework that combines learning from both demonstrations\nand reinforcement learning is proposed in [69] for fast learning\nagents. Two policies close to maximizing the reward function\ncan still have large differences in behaviour. To avoid degenerating a solution which would fit the reward but not the original\nbehaviour, authors [70] proposed a method for enforcing that\nthe optimal policy learnt over the rewards should still match\nthe observed policy in behavior. Behavior Cloning (BC) is\napplied as a supervised learning that maps states to actions\nbased on demonstrations provided by an expert. On the other\nhand, Inverse Reinforcement Learning (IRL) is about inferring\nthe reward function that justifies demonstrations of the expert.\nIRL is the problem of extracting a reward function given\nobserved, optimal behavior [71]. A key motivation is that the\nreward function provides a succinct and robust definition of\na task. Generally, IRL algorithms can be expensive to run,\nrequiring reinforcement learning in an inner loop between\ncost estimation to policy training and evaluation. Generative\nAdversarial Imitation Learning (GAIL) [72] introduces a way\nto avoid this expensive inner loop. In practice, GAIL trains a\npolicy close enough to the expert policy to fool a discriminator.\nThis process is similar to GANs [73], [74]. The resulting\npolicy must travel the same MDP states as the expert, or the\ndiscriminator would pick up the differences. The theory behind\nGAIL is an equation simplification: qualitatively, if IRL is\ngoing from demonstrations to a cost function and RL from a\ncost function to a policy, then we should altogether be able\nto go from demonstration to policy in a single equation while\navoiding the cost function estimation.\n\n\nV. REINFORCEMENT LEARNING FOR AUTONOMOUS\n\n\nDRIVING TASKS\n\n\nAutonomous driving tasks where RL could be applied\ninclude: controller optimization, path planning and trajectory\noptimization, motion planning and dynamic path planning,\ndevelopment of high-level driving policies for complex navigation tasks, scenario-based policy learning for highways,\nintersections, merges and splits, reward learning with inverse\nreinforcement learning from expert data for intent prediction\nfor traffic actors such as pedestrian, vehicles and finally\nlearning of policies that ensures safety and perform risk\nestimation. Before discussing the applications of DRL to AD\ntasks we briefly review the state space, action space and\nrewards schemes in autonomous driving setting.\n\n\n_A. State Spaces, Action Spaces and Rewards_\n\n\nTo successfully apply DRL to autonomous driving tasks,\ndesigning appropriate state spaces, action spaces, and reward\n\n\n\n9\n\n\nfunctions is important. Leurent _et al._ [75] provided a comprehensive review of the different state and action representations\nwhich are used in autonomous driving research. Commonly\nused state space features for an autonomous vehicle include:\nposition, heading and velocity of ego-vehicle, as well as other\nobstacles in the sensor view extent of the ego-vehicle. To avoid\nvariations in the dimension of the state space, a Cartesian or\nPolar occupancy grid around the ego vehicle is frequently\nemployed. This is further augmented with lane information\nsuch as lane number (ego-lane or others), path curvature,\npast and future trajectory of the ego-vehicle, longitudinal\ninformation such as Time-to-collision (TTC), and finally scene\ninformation such as traffic laws and signal locations.\nUsing raw sensor data such as camera images, LiDAR,\nradar, etc. provides the benefit of finer contextual information,\nwhile using condensed abstracted data reduces the complexity\nof the state space. In between, a mid-level representation such\nas 2D bird eye view (BEV) is sensor agnostic but still close to\nthe spatial organization of the scene. Fig. 4 is an illustration\nof a top down view showing an occupancy grid, past and\nprojected trajectories, and semantic information about the\nscene such as the position of traffic lights. This intermediary\nformat retains the spatial layout of roads when graph-based\nrepresentations would not. Some simulators offer this view\nsuch as Carla or Flow (see Table V-C).\nA vehicle policy must control a number of different actuators. Continuous-valued actuators for vehicle control include\n\nsteering angle, throttle and brake. Other actuators such as\ngear changes are discrete. To reduce complexity and allow\nthe application of DRL algorithms which work with discrete\naction spaces only ( _e.g._ DQN), an action space may be\ndiscretised uniformly by dividing the range of continuous\nactuators such as steering angle, throttle and brake into equalsized bins (see Section VI-C). Discretisation in log-space\nhas also been suggested, as many steering angles which are\nselected in practice are close to the centre [76]. Discretisation\ndoes have disadvantages however; it can lead to jerky or\nunstable trajectories if the step values between actions are too\nlarge. Furthermore, when selecting the number of bins for an\nactuator there is a trade-off between having enough discrete\nsteps to allow for smooth control, and not having so many\nsteps that action selections become prohibitively expensive to\nevaluate. As an alternative to discretisation, continuous values\nfor actuators may also be handled by DRL algorithms which\nlearn a policy directly, ( _e.g._ DDPG). Temporal abstractions\noptions framework [77]) may also be employed to simplify\nthe process of selecting actions, where agents select _options_\ninstead of low-level actions. These options represent a subpolicy that could extend a primitive action over multiple time\n\nsteps.\nDesigning reward functions for DRL agents for autonomous\ndriving is still very much an open question. Examples of\ncriteria for AD tasks include: distance travelled towards a\n\ndestination [78], speed of the ego vehicle [78]–[80], keeping\nthe ego vehicle at a standstill [81], collisions with other road\nusers or scene objects [78], [79], infractions on sidewalks [78],\nkeeping in lane, and maintaining comfort and stability while\navoiding extreme acceleration, braking or steering [80], [81],\n\n\n\n\nand following traffic rules [79].\n\n\n_B. Motion Planning & Trajectory optimization_\n\n\nMotion planning is the task of ensuring the existence of a\npath between target and destination points. This is necessary\nto plan trajectories for vehicles over prior maps usually augmented with semantic information. Path planning in dynamic\nenvironments and varying vehicle dynamics is a key problem in autonomous driving, for example negotiating right to\npass through in an intersection [87], merging into highways.\nRecent work by authors [89] contains real world motions by\nvarious traffic actors, observed in diverse interactive driving\nscenarios. Recently, authors demonstrated an application of\nDRL (DDPG) for AD using a full-sized autonomous vehicle\n\n[90]. The system was first trained in simulation, before being\ntrained in real time using on board computers, and was able\nto learn to follow a lane, successfully completing a realworld trial on a 250 metre section of road. Model-based deep\nRL algorithms have been proposed for learning models and\npolicies directly from raw pixel inputs [91], [92]. In [93],\ndeep neural networks have been used to generate predictions in\nsimulated environments over hundreds of time steps. RL is also\nsuitable for Control. Classical optimal control methods like\nLQR/iLQR are compared with RL methods in [94]. Classical\nRL methods are used to perform optimal control in stochastic\nsettings, for example the Linear Quadratic Regulator (LQR)\nin linear regimes and iterative LQR (iLQR) for non-linear\nregimes are utilized. A recent study in [95] demonstrates that\nrandom search over the parameters for a policy network can\nperform as well as LQR.\n\n\n_C. Simulator & Scenario generation tools_\n\n\nAutonomous driving datasets address supervised learning\nsetup with training sets containing image, label pairs for\nvarious modalities. Reinforcement learning requires an environment where state-action pairs can be recovered while\nmodelling dynamics of the vehicle state, environment as well\nas the stochasticity in the movement and actions of the\nenvironment and agent respectively. Various simulators are\nactively used for training and validating reinforcement learning algorithms. Table V-C summarises various high fidelity\nperception simulators capable of simulating cameras, LiDARs\nand radar. Some simulators are also capable of providing the\nvehicle state and dynamics. A complete review of sensors and\nsimulators utilised within the autonomous driving community\nis available in [105] for readers. Learned driving policies are\nstress tested in simulated environments before moving on to\ncostly evaluations in the real world. Multi-fidelity reinforcement learning (MFRL) framework is proposed in [106] where\nmultiple simulators are available. In MFRL, a cascade of\nsimulators with increasing fidelity are used in representing\nstate dynamics (and thus computational cost) that enables\nthe training and validation of RL algorithms, while finding\nnear optimal policies for the real world with fewer expensive\nreal world samples using a remote controlled car. CARLA\nChallenge [107] is a Carla simulator based AD competition\nwith pre-crash scenarios characterized in a National Highway\n\n\n\n10\n\n\nTraffic Safety Administration report [108]. The systems are\nevaluated in critical scenarios such as: Ego-vehicle loses\ncontrol, ego-vehicle reacts to unseen obstacle, lane change\nto evade slow leading vehicle among others. The scores of\nagents are evaluated as a function of the aggregated distance\ntravelled in different circuits, and total points discounted due\nto infractions.\n\n\n_D. LfD and IRL for AD applications_\n\n\nEarly work on Behavior Cloning (BC) for driving cars in\n\n[109], [110] presented agents that learn form demonstrations\n(LfD) that tries to mimic the behavior of an expert. BC is\ntypically implemented as a supervised learning, and accordingly, it is hard for BC to adapt to new, unseen situations.\nAn architecture for learning a convolutional neural network,\nend to end, in self-driving cars domain was proposed in\n\n[111], [112]. The CNN is trained to map raw pixels from\na single front facing camera directly to steering commands.\nUsing a relatively small training dataset from humans/experts,\nthe system learns to drive in traffic on local roads with or\nwithout lane markings and on highways. The network learns\nimage representations that detect the road successfully, without\nbeing explicitly trained to do so. Authors of [113] proposed\nto learn comfortable driving trajectories optimization using\nexpert demonstration from human drivers using Maximum\nEntropy Inverse RL. Authors of [114] used DQN as the\nrefinement step in IRL to extract the rewards, in an effort\nlearn human-like lane change behavior.\n\n\nVI. REAL WORLD CHALLENGES AND FUTURE\n\nPERSPECTIVES\n\n\nIn this section, challenges for conducting reinforcement\nlearning for real-world autonomous driving are presented\nand discussed along with the related research approaches for\nsolving them.\n\n\n_A. Validating RL systems_\n\n\nHenderson _et al._ [115] described challenges in validating\nreinforcement learning methods focusing on policy gradient\nmethods for continuous control algorithms such as PPO,\nDDPG and TRPO as well as in reproducing benchmarks. They\ndemonstrate with real examples that implementations often\nhave varying code-bases and different hyper-parameter values,\nand that unprincipled ways to estimate the top-k rollouts\ncould lead to incoherent interpretations on the performance\nof the reinforcement learning algorithms, and further more on\nhow well they generalize. Authors concluded that evaluation\ncould be performed either on a well defined common setup\nor on real-world tasks. Authors in [116] proposed automated\ngeneration of challenging and rare driving scenarios in highfidelity photo-realistic simulators. These adversarial scenarios\nare automatically discovered by parameterising the behavior\nof pedestrians and other vehicles on the road. Moreover, it is\nshown that by adding these scenarios to the training data of\nimitation learning, the safety is increased.\n\n\n\n\n11\n\n\nFig. 4. Bird Eye View (BEV) 2D representation of a driving scene. Left demonstrates an occupancy grid. Right shows the combination of semantic information\n(traffic lights) with past (red) and projected (green) trajectories. The ego car is represented by a green rectangle in both images.\n\n\nAD Task (D)RL method & description Improvements & Tradeoffs\n\n\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-10-0.png)\n\nLane Keep 1. Authors [82] propose a DRL system for discrete actions 1. This study concludes that using continuous actions provide\n(DQN) and continuous actions (DDAC) using the TORCS smoother trajectories, though on the negative side lead to\nsimulator (see Table V-C) 2. Authors [83] learn discretised more restricted termination conditions & slower convergence\nand continuous policies using DQNs and Deep Deterministic time to learn. 2. Removing memory replay in DQNs help\nActor Critic (DDAC) to follow the lane and maximize average for faster convergence & better performance. The one hot\nvelocity. encoding of action space resulted in abrupt steering control.\n\nWhile DDAC’s continuous policy helps smooth the actions\nand provides better performance.\nLane Change Authors [84] use Q-learning to learn a policy for ego- This approach is more robust compared to traditional apvehicle to perform no operation, lane change to left/right, proaches which consist in defining fixed way points, velocity\naccelerate/decelerate.\n\n\n\nLane Keep 1. Authors [82] propose a DRL system for discrete actions\n(DQN) and continuous actions (DDAC) using the TORCS\nsimulator (see Table V-C) 2. Authors [83] learn discretised\nand continuous policies using DQNs and Deep Deterministic\nActor Critic (DDAC) to follow the lane and maximize average\nvelocity.\n\n\n\nLane Change Authors [84] use Q-learning to learn a policy for ego- This approach is more robust compared to traditional apvehicle to perform no operation, lane change to left/right, proaches which consist in defining fixed way points, velocity\naccelerate/decelerate. profiles and curvature of path to be followed by the ego\n\nvehicle.\n\nRamp Merging Authors [85] propose recurrent architectures namely LSTMs Past history of the state information is used to perform the\nto model longterm dependencies for ego vehicles ramp merg- merge more robustly.\ning into a highway.\n\n\n\nPast history of the state information is used to perform the\nmerge more robustly.\n\n\n\nOvertaking Authors [86] propose Multi-goal RL policy that is learnt by\nQ-Learning or Double action Q-Learning(DAQL) is employed\nto determine individual action decisions based on whether the\nother vehicle interacts with the agent for that particular goal.\n\n\n\nImproved speed for lane keeping and overtaking with collision\navoidance.\n\n\n\nIntersections Authors use DQN to evalute the Q-value for state-action pairs Creep-Go actions defined by authors enables the vehicle to\nto negotiate intersection [87], maneuver intersections with restricted spaces and visibility\nmore safely\nMotion Planning Authors [88] propose an improved _A_ ~~_[∗]_~~ algorithm to learn a Smooth control behavior of vehicle and better peformance\nheuristic function using deep neural netowrks over image- compared to multi-step DQN\nbased input obstacle map\n\n\nTABLE I\n\nLIST OF AD TASKS THAT REQUIRE D(RL) TO LEARN A POLICY OR BEHAVIOR.\n\n\n\n_B. Bridging the simulation-reality gap_\n\n\nSimulation-to-real-world transfer learning is an active domain, since simulations are a source large & cheap data with\nperfect annotations. Authors [117] train a robot arm to grasp\nobjects in the real world by performing domain adaption\nfrom simulation-to-reality, at both feature-level and pixellevel. The vision-based grasping system achieved comparable\nperformance with 50 times fewer real-world samples. Authors\nin [118], randomized the dynamics of the simulator during\ntraining. The resulting policies were capable of generalising to\ndifferent dynamics without requiring retraining on real system.\nIn the domain of autonomous driving, authors [119] train\nan A3C agent using simulation-real translated images of the\ndriving environment. Following which, the trained policy was\nevaluated on a real world driving dataset.\n\nAuthors in [120] addressed the issue of performing imitation\nlearning in simulation that transfers well to images from real\nworld. They achieved this by unsupervised domain translation\nbetween simulated and real world images, that enables learning\n\n\n\nthe prediction of steering in the real world domain with only\nground truth from the simulated domain. Authors remark that\nthere were no pairwise correspondences between images in the\nsimulated training set and the unlabelled real-world image set.\nSimilarly, [121] performs domain adaptation to map real world\nimages to simulated images. In contrast to sim-to-real methods\nthey handle the reality gap during deployment of agents in real\nscenarios, by adapting the real camera streams to the synthetic\nmodality, so as to map the unfamiliar or unseen features of\nreal images back into the simulated environment and states.\nThe agents have already learnt a policy in simulation.\n\n\n_C. Sample efficiency_\n\n\nAnimals are usually able to learn new tasks in just a few\ntrials, benefiting from their prior knowledge about the environment. However, one of the key challenges for reinforcement\nlearning is sample efficiency. The learning process requires too\nmany samples to learn a reasonable policy. This issue becomes\nmore noticeable when collection of valuable experience is\n\n\n\n\n12\n\n\n\nSimulator Description\n\n\nCARLA [78] Urban simulator, Camera & LIDAR streams, with depth & semantic segmentation, Location information\nTORCS [96] Racing Simulator, Camera stream, agent positions, testing control policies for vehicles\nAIRSIM [97] Camera stream with depth and semantic segmentation, support for drones\nGAZEBO (ROS) [98] Multi-robot physics simulator employed for path planning & vehicle control in complex 2D & 3D maps\nSUMO [99] Macro-scale modelling of traffic in cities motion planning simulators are used\nDeepDrive [100] Driving simulator based on unreal, providing multi-camera (eight) stream with depth\nConstellation [101] NVIDIA DRIVE Constellation [TM] simulates camera, LIDAR & radar for AD (Proprietary)\nMADRaS [102] Multi-Agent Autonomous Driving Simulator built on top of TORCS\nFlow [103] Multi-Agent Traffic Control Simulator built on top of SUMO\nHighway-env [104] A gym-based environment that provides a simulator for highway based road topologies\nCarcraft Waymo’s simulation environment (Proprietary)\n\n\nTABLE II\n\nSIMULATORS FOR RL APPLICATIONS IN ADVANCED DRIVING ASSISTANCE SYSTEMS (ADAS) AND AUTONOMOUS DRIVING.\n\n\n\nexpensive or even risky to acquire. In the case of robot control\nand autonomous driving, sample efficiency is a difficult issue\ndue to the delayed and sparse rewards found in typical settings,\nalong with an unbalanced distribution of observations in a\nlarge state space.\n**Reward shaping** enables the agent to learn intermediate\ngoals by designing a more frequent reward function to encourage the agent to learn faster from fewer samples. Authors\nin [122] design a second \"trauma\" replay memory that contains\nonly collision situations in order to pool positive and negative\nexperiences at each training step.\n**IL boostrapped RL** : Further efficiency can be achieved\nwhere the agent first learns an initial policy offline performing imitation learning from roll-outs provided by an expert.\nFollowing which, the agent can self-improve by applying RL\nwhile interacting with the environment.\n**Actor Critic** with Experience Replay (ACER) [123], is a\nsample-efficient policy gradient algorithm that makes use of a\nreplay buffer, enabling it to perform more than one gradient\nupdate using each piece of sampled experience, as well as a\ntrust region policy optimization method.\n**Transfer learning** is another approach for sample efficiency, which enables the reuse of previously trained policy for\na source task to initialize the learning of a target task. Policy\ncomposition presented in [124] propose composing previously\nlearned basis policies to be able to reuse them for a novel task,\nwhich leads to faster learning of new policies. A survey on\ntransfer learning in RL is presented in [125]. Multi-fidelity\nreinforcement learning (MFRL) framework [106] showed to\ntransfer heuristics to guide exploration in high fidelity simulators and find near optimal policies for the real world with\nfewer real world samples. Authors in [126] transferred policies\nlearnt to handle simulated intersections to real world examples\nbetween DQN agents.\n**Meta-learning** algorithms enable agents adapt to new tasks\nand learn new skills rapidly from small amounts of experiences, benefiting from their prior knowledge about the\nworld. Authors of [127] addressed this issue through training\na recurrent neural network on a training set of interrelated\ntasks, where the network input includes the action selected\nin addition to the reward received in the previous time step.\nAccordingly, the agent is trained to learn to exploit the\nstructure of the problem dynamically and solve new problems\nby adjusting its hidden state. A similar approach for designing\n\n\n\nRL algorithms is presented in [128]. Rather than designing a\n“fast” reinforcement learning algorithm, it is represented as a\nrecurrent neural network, and learned from data. In ModelAgnostic Meta-Learning (MAML) proposed in [129], the\nmeta-learner seeks to find an initialisation for the parameters\nof a neural network, that can be adapted quickly for a new\ntask using only few examples. Reptile [130] includes a similar\nmodel. Authors [131] present simple gradient-based metalearning algorithm.\n**Efficient state representations** : World models proposed in\n\n[132] learn a compressed spatial and temporal representation\nof the environment using VAEs. Further on a compact and\nsimple policy directly from the compressed state representation.\n\n\n_D. Exploration issues with Imitation_\n\n\nIn imitation learning, the agent makes use of trajectories\nprovided by an expert. However, the distribution of states\nthe expert encounters usually does not cover all the states\nthe trained agent may encounter during testing. Furthermore\nimitation assumes that the actions are independent and identically distributed (i.i.d.). One solution consists in using the\nData Aggregation (DAgger) methods [133] where the endto-end learned policy is executed, and extracted observationaction pairs are again labelled by the expert, and aggregated\nto the original expert observation-action dataset. Thus, iteratively collecting training examples from both reference and\ntrained policies explores more valuable states and solves this\nlack of exploration. Following work on Search-based Structured Prediction (SEARN) [133], Stochastic Mixing Iterative\nLearning (SMILE) trains a stochastic stationary policy over\nseveral iterations and then makes use of a geometric stochastic\nmixing of the policies trained. In a standard imitation learning\nscenario, the demonstrator is required to cover sufficient states\nso as to avoid unseen states during test. This constraint is\ncostly and requires frequent human intervention. More recently, Chauffeurnet [134] demonstrated the limits of imitation\nlearning where even 30 million state-action samples were\ninsufficient to learn an optimal policy that mapped bird-eye\nview images (states) to control (action). The authors propose\nthe use of simulated examples which introduced perturbations,\nhigher diversity of scenarios such as collisions and/or going\noff the road. The _featurenet_ includes an agent RNN that\n\n\n\n\noutputs the way point, agent box position and heading at each\niteration. Authors [135] identify limits of imitation learning,\nand train a DNN end-to-end using the ego vehicles on input\nraw image, and 2d and 3d locations of neighboring vehicles\nto simultaneously predict the ego-vehicle action as well as\nneighbouring vehicle trajectories.\n\n\n_E. Intrinsic Reward functions_\n\n\nIn controlled simulated environments such as games, an\nexplicit reward signal is given to the agent along with its sensor\nstream. However, in real-world robotics and autonomous driving deriving, designing a _good_ reward functions is essential\nso that the desired behaviour may be learned. The most\ncommon solution has been reward shaping [136] and consists\nin supplying additional well designed rewards to the agent to\nencourage the optimization into the direction of the optimal\npolicy. Rewards as already pointed earlier in the paper, could\nbe estimated by inverse RL (IRL) [137], which depends on\nexpert demonstrations. In the absence of an explicit reward\nshaping and expert demonstrations, agents can use intrinsic\nrewards or intrinsic motivation [138] to evaluate if their actions\nwere good or not. Authors of [139] define _curiosity_ as the error\nin an agent’s ability to predict the consequence of its own\nactions in a visual feature space learned by a self-supervised\ninverse dynamics model. In [140] the agent learns a next state\npredictor model from its experience, and uses the error of\nthe prediction as an intrinsic reward. This enables that agent\nto determine what could be a useful behavior even without\n\nextrinsic rewards.\n\n\n_F. Incorporating safety in DRL_\n\n\nDeploying an autonomous vehicle in real environments after\ntraining directly could be dangerous. Different approaches to\nincorporate safety into DRL algorithms are presented here.\nFor imitation learning based systems, Safe DAgger [141]\nintroduces a safety policy that learns to predict the error\nmade by a primary policy trained initially with the supervised\nlearning approach, without querying a reference policy. An\nadditional safe policy takes both the partial observation of\na state and a primary policy as inputs, and returns a binary\nlabel indicating whether the primary policy is likely to deviate\nfrom a reference policy without querying it. Authors of [142]\naddressed safety in multi-agent Reinforcement Learning for\nAutonomous Driving, where a balance is maintained between\nunexpected behavior of other drivers or pedestrians and not\nto be too defensive, so that normal traffic flow is achieved.\nWhile hard constraints are maintained to guarantee the safety\nof driving, the problem is decomposed into a composition of\na policy for desires to enable comfort driving and trajectory\nplanning. The deep reinforcement learning algorithms for control such as DDPG and safety based control are combined in\n\n[143], including artificial potential field method that is widely\nused for robot path planning. Using TORCS environment,\nthe DDPG is applied first for learning a driving policy in\na stable and familiar environment, then policy network and\nsafety-based control are combined to avoid collisions. It was\nfound that combination of DRL and safety-based control\n\n\n\n13\n\n\nperforms well in most scenarios. In order to enable DRL to\nescape local optima, speed up the training process and avoid\ndanger conditions or accidents, Survival-Oriented Reinforcement Learning (SORL) model is proposed in [144], where\nsurvival is favored over maximizing total reward through\nmodeling the autonomous driving problem as a constrained\nMDP and introducing Negative-Avoidance Function to learn\nfrom previous failure. The SORL model was found to be\nnot sensitive to reward function and can use different DRL\n\nalgorithms like DDPG. Furthermore, a comprehensive survey\non safe reinforcement learning can be found in [145] for\ninterested readers.\n\n\n_G. Multi-agent reinforcement learning_\n\n\nAutonomous driving is a fundamentally multi-agent task;\nas well as the ego vehicle being controlled by an agent,\nthere will also be many other actors present in simulated\nand real world autonomous driving settings, such as pedestrians, cyclists and other vehicles. Therefore, the continued\ndevelopment of explicitly multi-agent approaches to learning\nto drive autonomous vehicles is an important future research\ndirection. Several prior methods have already approached the\nautonomous driving problem using a MARL perspective, e.g.\n\n[142], [146]–[149].\nOne important area where MARL techniques could be very\nbeneficial is in high-level decision making and coordination\nbetween groups of autonomous vehicles, in scenarios such\nas overtaking in highway scenarios [149], or negotiating\nintersections without signalised control. Another area where\nMARL approaches could be of benefit is in the development\nof adversarial agents for testing autonomous driving policies\nbefore deployment [148], i.e. agents controlling other vehicles\nin a simulation that learn to expose weaknesses in the behaviour of autonomous driving policies by acting erratically or\nagainst the rules of the road. Finally, MARL approaches could\npotentially have an important role to play in developing safe\npolicies for autonomous driving [142], as discussed earlier.\n\n\nVII. CONCLUSION\n\n\nReinforcement learning is still an active and emerging area\nin real-world autonomous driving applications. Although there\nare a few successful commercial applications, there is very\nlittle literature or large-scale public datasets available. Thus we\nwere motivated to formalize and organize RL applications for\nautonomous driving. Autonomous driving scenarios involve interacting agents and require negotiation and dynamic decision\nmaking which suits RL. However, there are many challenges to\nbe resolved in order to have mature solutions which we discuss\n\nin detail. In this work, a detailed theoretical reinforcement\nlearning is presented, along with a comprehensive literature\nsurvey about applying RL for autonomous driving tasks.\nChallenges, future research directions and opportunities\nare discussed in section VI. This includes : validating the\nperformance of RL based systems, the simulation-reality gap,\nsample efficiency, designing good reward functions, incorporating safety into decision making RL systems for autonomous\n\nagents.\n\n\n\n\n14\n\n\n\nFramework Description\n\n\nOpenAI Baselines [150] Set of high-quality implementations of different RL and DRL algorithms. The main goal for these\nBaselines is to make it easier for the research community to replicate, refine and create reproducible\nresearch.\n\nUnity ML Agents Toolkit [151] Implements core RL algorithms, games, simulations environments for training RL or IL based agents .\nRL Coach [152] Intel AI Lab’s implementation of modular RL algorithms implementation with simple integration of new\nenvironments by extending and reusing existing components.\nTensorflow Agents [153] RL algorithms package with Bandits from TF.\nrlpyt [154] implements deep Q-learning, policy gradients algorithm families in a single python package\nbsuite [155] DeepMind Behaviour Suite for Reinforcement Learning aims at defining metrics for RL agents.\nAutomating evaluation and analysis.\n\n\nTABLE III\n\nOPEN-SOURCE FRAMEWORKS AND PACKAGES FOR STATE OF THE ART RL/DRL ALGORITHMS AND EVALUATION.\n\n\n\nReinforcement learning results are usually difficult to reproduce and are highly sensitive to hyper-parameter choices,\nwhich are often not reported in detail. Both researchers and\npractitioners need to have a reliable starting point where\nthe well known reinforcement learning algorithms are implemented, documented and well tested. These frameworks have\n\nbeen covered in table VI-G.\n\nThe development of explicitly multi-agent reinforcement\nlearning approaches to the autonomous driving problem is\nalso an important future challenge that has not received a lot\nof attention to date. MARL techniques have the potential to\nmake coordination and high-level decision making between\ngroups of autonomous vehicles easier, as well as providing\nnew opportunities for testing and validating the safety of\nautonomous driving policies.\nFurthermore, implementation of RL algorithms is a challenging task for researchers and practitioners. This work\npresents examples of well known and active open-source RL\nframeworks that provide well documented implementations\nthat enables the opportunity of using, evaluating and extending\ndifferent RL algorithms. Finally, We hope that this overview\npaper encourages further research and applications.\n\n\nA2C, A3C Advantage Actor Critic, Asynchronous A2C\nBC Behavior Cloning\nDDPG Deep DPG\nDP Dynamic Programming\nDPG Deterministic PG\nDQN Deep Q-Network\nDRL Deep RL\nIL Imitation Learning\nIRL Inverse RL\nLfD Learning from Demonstration\nMAML Model-Agnostic Meta-Learning\nMARL Multi-Agent RL\nMDP Markov Decision Process\nMOMDP Multi-Objective MDP\nMOSG Multi-Objective SG\nPG Policy Gradient\nPOMDP Partially Observed MDP\nPPO Proximal Policy Optimization\nQL Q-Learning\nRRT Rapidly-exploring Random Trees\nSG Stochastic Game\n\nSMDP Semi-Markov Decision Process\nTDL Time Difference Learning\nTRPO Trust Region Policy Optimization\n\n\nTABLE IV\n\nACRONYMS RELATED TO REINFORCEMENT LEARNING (RL).\n\n\n\nREFERENCES\n\n\n[1] R. S. Sutton and A. G. Barto, _Reinforcement Learning: An Introduction_\n_(Second Edition)_ . MIT Press, 2018. 1, 4, 5, 6\n\n[2] V. Talpaert., I. Sobh., B. R. Kiran., P. Mannion., S. Yogamani., A. ElSallab., and P. Perez., “Exploring applications of deep reinforcement\nlearning for real-world autonomous driving systems,” in _Proceedings of_\n_the 14th International Joint Conference on Computer Vision, Imaging_\n_and Computer Graphics Theory and Applications - Volume 5 VISAPP:_\n_VISAPP,_, INSTICC. SciTePress, 2019, pp. 564–572. 1\n\n[3] M. Siam, S. Elkerdawy, M. Jagersand, and S. Yogamani, “Deep\nsemantic segmentation for automated driving: Taxonomy, roadmap and\nchallenges,” in _2017 IEEE 20th international conference on intelligent_\n_transportation systems (ITSC)_ . IEEE, 2017, pp. 1–8. 2\n\n[4] K. El Madawi, H. Rashed, A. El Sallab, O. Nasr, H. Kamel, and\nS. Yogamani, “Rgb and lidar fusion based 3d semantic segmentation for\nautonomous driving,” in _2019 IEEE Intelligent Transportation Systems_\n_Conference (ITSC)_ . IEEE, 2019, pp. 7–12. 2\n\n[5] M. Siam, H. Mahgoub, M. Zahran, S. Yogamani, M. Jagersand, and\nA. El-Sallab, “Modnet: Motion and appearance based moving object\ndetection network for autonomous driving,” in _2018 21st International_\n_Conference on Intelligent Transportation Systems (ITSC)_ . IEEE, 2018,\npp. 2859–2864. 2\n\n[6] V. R. Kumar, S. Milz, C. Witt, M. Simon, K. Amende, J. Petzold,\nS. Yogamani, and T. Pech, “Monocular fisheye camera depth estimation\nusing sparse lidar supervision,” in _2018 21st International Conference_\n_on Intelligent Transportation Systems (ITSC)_ . IEEE, 2018, pp. 2853–\n2858. 2\n\n[7] M. Uˇriˇcáˇr, P. Kˇrížek, G. Sistu, and S. Yogamani, “Soilingnet: Soiling\ndetection on automotive surround-view cameras,” in _2019 IEEE Intel-_\n_ligent Transportation Systems Conference (ITSC)_ . IEEE, 2019, pp.\n67–72. 2\n\n[8] G. Sistu, I. Leang, S. Chennupati, S. Yogamani, C. Hughes, S. Milz, and\nS. Rawashdeh, “Neurall: Towards a unified visual perception model for\nautomated driving,” in _2019 IEEE Intelligent Transportation Systems_\n_Conference (ITSC)_ . IEEE, 2019, pp. 796–803. 2\n\n[9] S. Yogamani, C. Hughes, J. Horgan, G. Sistu, P. Varley, D. O’Dea,\nM. Uricár, S. Milz, M. Simon, K. Amende _et al._, “Woodscape: A\nmulti-task, multi-camera fisheye dataset for autonomous driving,” in\n_Proceedings of the IEEE International Conference on Computer Vision_,\n2019, pp. 9308–9318. 2\n\n[10] S. Milz, G. Arbeiter, C. Witt, B. Abdallah, and S. Yogamani, “Visual\nslam for automated driving: Exploring the applications of deep learning,” in _Proceedings of the IEEE Conference on Computer Vision and_\n_Pattern Recognition Workshops_, 2018, pp. 247–257. 2\n\n[11] S. M. LaValle, _Planning Algorithms_ . New York, NY, USA: Cambridge\nUniversity Press, 2006. 2\n\n[12] S. M. LaValle and J. James J. Kuffner, “Randomized kinodynamic\nplanning,” _The International Journal of Robotics Research_, vol. 20,\nno. 5, pp. 378–400, 2001. 2\n\n[13] T. D. Team. Dimensions publication trends. [Online]. Available:\n\n[https://app.dimensions.ai/discover/publication 3](https://app.dimensions.ai/discover/publication)\n\n[14] Y. Kuwata, J. Teo, G. Fiore, S. Karaman, E. Frazzoli, and J. P. How,\n“Real-time motion planning with applications to autonomous urban\ndriving,” _IEEE Transactions on Control Systems Technology_, vol. 17,\nno. 5, pp. 1105–1118, 2009. 3\n\n[15] B. Paden, M. Cáp, S. Z. Yong, D. Yershov, and E. Frazzoli, “A survey of [ˇ]\nmotion planning and control techniques for self-driving urban vehicles,”\n\n\n\n\n_IEEE Transactions on intelligent vehicles_, vol. 1, no. 1, pp. 33–55,\n2016. 3\n\n[16] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and decisionmaking for autonomous vehicles,” _Annual Review of Control, Robotics,_\n_and Autonomous Systems_, no. 0, 2018. 3\n\n[17] S. Kuutti, R. Bowden, Y. Jin, P. Barber, and S. Fallah, “A survey\nof deep learning applications to autonomous vehicle control,” _IEEE_\n_Transactions on Intelligent Transportation Systems_, 2020. 3\n\n[18] T. M. Mitchell, _Machine learning_, ser. McGraw-Hill series in computer\nscience. Boston (Mass.), Burr Ridge (Ill.), Dubuque (Iowa): McGrawHill, 1997. 3\n\n[19] S. J. Russell and P. Norvig, _Artificial intelligence: a modern approach_\n_(3rd edition)_ . Prentice Hall, 2009. 3\n\n[20] Z.-W. Hong, T.-Y. Shann, S.-Y. Su, Y.-H. Chang, T.-J. Fu, and C.Y. Lee, “Diversity-driven exploration strategy for deep reinforcement\nlearning,” in _Advances in Neural Information Processing Systems 31_,\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,\nand R. Garnett, Eds., 2018, pp. 10 489–10 500. 3\n\n[21] M. Wiering and M. van Otterlo, Eds., _Reinforcement Learning: State-_\n_of-the-Art_ . Springer, 2012. 4\n\n[22] M. L. Puterman, _Markov Decision Processes: Discrete Stochastic_\n_Dynamic Programming_, 1st ed. New York, NY, USA: John Wiley\n& Sons, Inc., 1994. 4\n\n[23] C. J. Watkins and P. Dayan, “Technical note: Q-learning,” _Machine_\n_Learning_, vol. 8, no. 3-4, 1992. 4\n\n[24] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\n_et al._, “Human-level control through deep reinforcement learning,”\n_Nature_, vol. 518, 2015. 4, 6\n\n[25] C. J. C. H. Watkins, “Learning from delayed rewards,” Ph.D. dissertation, King’s College, Cambridge, 1989. 4, 6\n\n[26] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, “Deterministic policy gradient algorithms,” in _ICML_, 2014. 5\n\n[27] R. J. Williams, “Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning,” _Machine Learning_, vol. 8, pp.\n229–256, 1992. 5\n\n[28] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in _International Conference on Machine_\n_Learning_, 2015, pp. 1889–1897. 5\n\n[29] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” _arXiv preprint arXiv:1707.06347_,\n2017. 5\n\n[30] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning.” in _4th International Conference on Learning Represen-_\n_tations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference_\n_Track Proceedings_, Y. Bengio and Y. LeCun, Eds., 2016. 5\n\n[31] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in _International Conference on Machine Learning_,\n2016. 5\n\n[32] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine, “Reinforcement\nlearning with deep energy-based policies,” in _Proceedings of the 34th_\n_International Conference on Machine Learning - Volume 70_, ser.\nICML’17. JMLR.org, 2017, pp. 1352–1361. 6\n\n[33] T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan,\nV. Kumar, H. Zhu, A. Gupta, P. Abbeel _et al._, “Soft actor-critic\nalgorithms & applications,” _arXiv:1812.05905_, 2018. 6\n\n[34] R. S. Sutton, “Integrated architectures for learning, planning, and\nreacting based on approximating dynamic programming,” in _Machine_\n_Learning Proceedings 1990_ . Elsevier, 1990. 6\n\n[35] R. I. Brafman and M. Tennenholtz, “R-max-a general polynomial time\nalgorithm for near-optimal reinforcement learning,” _Journal of Machine_\n_Learning Research_, vol. 3, no. Oct, 2002. 6\n\n[36] D. Silver, R. S. Sutton, and M. Müller, “Sample-based learning and\nsearch with permanent and transient memories,” in _Proceedings of the_\n_25th international conference on Machine learning_ . ACM, 2008, pp.\n968–975. 6\n\n[37] G. A. Rummery and M. Niranjan, “On-line Q-learning using connectionist systems,” Cambridge University Engineering Department,\nCambridge, England, Tech. Rep. TR 166, 1994. 6\n\n[38] R. S. Sutton and A. G. Barto, “Reinforcement learning an introduction–\nsecond edition, in progress (draft),” 2015. 6\n\n[39] R. Bellman, _Dynamic Programming_ . Princeton, NJ, USA: Princeton\nUniversity Press, 1957. 6\n\n\n\n15\n\n\n[40] G. Tesauro, “Td-gammon, a self-teaching backgammon program,\nachieves master-level play,” _Neural Computing_, vol. 6, no. 2, Mar. 1994.\n6\n\n[41] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot _et al._, “Mastering the game of go with deep neural networks\nand tree search,” _nature_, vol. 529, no. 7587, pp. 484–489, 2016. 6, 8\n\n[42] K. Narasimhan, T. Kulkarni, and R. Barzilay, “Language understanding\nfor text-based games using deep reinforcement learning,” in _Pro-_\n_ceedings of the 2015 Conference on Empirical Methods in Natural_\n_Language Processing_ . Association for Computational Linguistics,\n2015, pp. 1–11. 7\n\n[43] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\nreplay,” _arXiv preprint arXiv:1511.05952_, 2015. 7\n\n[44] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double q-learning.” in _AAAI_, vol. 16, 2016, pp. 2094–2100. 7\n\n[45] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and\nN. De Freitas, “Dueling network architectures for deep reinforcement\nlearning,” _arXiv preprint arXiv:1511.06581_, 2015. 7\n\n[46] M. Hausknecht and P. Stone, “Deep recurrent q-learning for partially\nobservable mdps,” _CoRR, abs/1507.06527_, 2015. 7\n\n[47] B. F. Skinner, _The behavior of organisms: An experimental analysis._\nAppleton-Century, 1938. 7\n\n[48] E. Wiewiora, “Reward shaping,” in _Encyclopedia of Machine Learning_\n_and Data Mining_, C. Sammut and G. I. Webb, Eds. Boston, MA:\nSpringer US, 2017, pp. 1104–1106. 7\n\n[49] J. Randløv and P. Alstrøm, “Learning to drive a bicycle using reinforcement learning and shaping,” in _Proceedings of the Fifteenth_\n_International Conference on Machine Learning_, ser. ICML ’98. San\nFrancisco, CA, USA: Morgan Kaufmann Publishers Inc., 1998, pp.\n463–471. 7\n\n[50] D. H. Wolpert, K. R. Wheeler, and K. Tumer, “Collective intelligence\nfor control of distributed dynamical systems,” _EPL (Europhysics Let-_\n_ters)_, vol. 49, no. 6, p. 708, 2000. 8\n\n[51] A. Y. Ng, D. Harada, and S. J. Russell, “Policy invariance under\nreward transformations: Theory and application to reward shaping,”\nin _Proceedings of the Sixteenth International Conference on Machine_\n_Learning_, ser. ICML ’99, 1999, pp. 278–287. 8\n\n[52] S. Devlin and D. Kudenko, “Theoretical considerations of potentialbased reward shaping for multi-agent systems,” in _Proceedings of the_\n_10th International Conference on Autonomous Agents and Multiagent_\n_Systems (AAMAS)_, 2011. 8\n\n[53] P. Mannion, S. Devlin, K. Mason, J. Duggan, and E. Howley, “Policy\ninvariance under reward transformations for multi-objective reinforcement learning,” _Neurocomputing_, vol. 263, 2017. 8\n\n[54] M. Colby and K. Tumer, “An evolutionary game theoretic analysis of\ndifference evaluation functions,” in _Proceedings of the 2015 Annual_\n_Conference on Genetic and Evolutionary Computation_ . ACM, 2015,\npp. 1391–1398. 8\n\n[55] P. Mannion, J. Duggan, and E. Howley, “A theoretical and empirical\nanalysis of reward transformations in multi-objective stochastic games,”\nin _Proceedings of the 16th International Conference on Autonomous_\n_Agents and Multiagent Systems (AAMAS)_, 2017. 8\n\n[56] L. Bu¸soniu, R. Babuška, and B. Schutter, “Multi-agent reinforcement\nlearning: An overview,” in _Innovations in Multi-Agent Systems and Ap-_\n_plications - 1_, ser. Studies in Computational Intelligence, D. Srinivasan\nand L. Jain, Eds. Springer Berlin Heidelberg, 2010, vol. 310. 8\n\n[57] P. Mannion, K. Mason, S. Devlin, J. Duggan, and E. Howley, “Multiobjective dynamic dispatch optimisation using multi-agent reinforcement learning,” in _Proceedings of the 15th International Conference_\n_on Autonomous Agents and Multiagent Systems (AAMAS)_, 2016. 8\n\n[58] K. Mason, P. Mannion, J. Duggan, and E. Howley, “Applying multiagent reinforcement learning to watershed management,” in _Proceed-_\n_ings of the Adaptive and Learning Agents workshop (at AAMAS 2016)_,\n2016. 8\n\n[59] V. Pareto, _Manual of political economy_ . OUP Oxford, 1906. 8\n\n[60] D. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley, “A survey\nof multi-objective sequential decision-making,” _Journal of Artificial_\n_Intelligence Research_, vol. 48, pp. 67–113, 2013. 8\n\n[61] R. R˘adulescu, P. Mannion, D. M. Roijers, and A. Nowé, “Multiobjective multi-agent decision making: a utility-based analysis and\nsurvey,” _Autonomous Agents and Multi-Agent Systems_, vol. 34, no. 1,\np. 10, 2020. 8\n\n[62] T. Lesort, N. Diaz-Rodriguez, J.-F. Goudou, and D. Filliat, “State\nrepresentation learning for control: An overview,” _Neural Networks_,\nvol. 108, pp. 379 – 392, 2018. 8\n\n\n\n\n[63] A. Raffin, A. Hill, K. R. Traoré, T. Lesort, N. D. Rodríguez, and\nD. Filliat, “Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics,”\n_CoRR_, vol. abs/1901.08651, 2019. 8\n\n[64] W. Böhmer, J. T. Springenberg, J. Boedecker, M. Riedmiller, and\nK. Obermayer, “Autonomous learning of state representations for\ncontrol: An emerging field aims to autonomously learn state representations for reinforcement learning agents from their real-world sensor\nobservations,” _KI-Künstliche Intelligenz_, vol. 29, no. 4, pp. 353–362,\n2015. 8\n\n[65] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,\nA. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton _et al._, “Mastering the\ngame of go without human knowledge,” _Nature_, vol. 550, no. 7676, p.\n354, 2017. 8\n\n[66] P. Abbeel and A. Y. Ng, “Exploration and apprenticeship learning\nin reinforcement learning,” in _Proceedings of the 22nd international_\n_conference on Machine learning_ . ACM, 2005, pp. 1–8. 9\n\n[67] B. Kang, Z. Jie, and J. Feng, “Policy optimization with demonstrations,” in _International Conference on Machine Learning_, 2018, pp.\n2474–2483. 9\n\n[68] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot,\nD. Horgan, J. Quan, A. Sendonaris, I. Osband _et al._, “Deep q-learning\nfrom demonstrations,” in _Thirty-Second AAAI Conference on Artificial_\n_Intelligence_, 2018. 9\n\n[69] S. Ibrahim and D. Nevin, “End-to-end framework for fast learning\nasynchronous agents,” in _the 32nd Conference on Neural Information_\n_Processing Systems, Imitation Learning and its Challenges in Robotics_\n_workshop_, 2018. 9\n\n[70] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement learning,” in _Proceedings of the twenty-first international_\n_conference on Machine learning_ . ACM, 2004, p. 1. 9\n\n[71] A. Y. Ng, S. J. Russell _et al._, “Algorithms for inverse reinforcement\nlearning.” in _ICML_, 2000. 9\n\n[72] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in\n_Advances in Neural Information Processing Systems_, 2016, pp. 4565–\n4573. 9\n\n[73] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”\nin _Advances in Neural Information Processing Systems 27_, 2014, pp.\n2672–2680. 9\n\n[74] M. Uˇriˇcáˇr, P. Kˇrížek, D. Hurych, I. Sobh, S. Yogamani, and P. Denny,\n“Yes, we gan: Applying adversarial techniques for autonomous driving,” _Electronic Imaging_, vol. 2019, no. 15, pp. 48–1, 2019. 9\n\n[75] E. Leurent, Y. Blanco, D. Efimov, and O.-A. Maillard, “A survey of\nstate-action representations for autonomous driving,” _HAL archives_,\n2018. 9\n\n[76] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-end learning of driving\nmodels from large-scale video datasets,” in _Proceedings of the IEEE_\n_conference on computer vision and pattern recognition_, 2017, pp.\n2174–2182. 9\n\n[77] R. S. Sutton, D. Precup, and S. Singh, “Between mdps and semi-mdps:\nA framework for temporal abstraction in reinforcement learning,”\n_Artificial intelligence_, vol. 112, no. 1-2, pp. 181–211, 1999. 9\n\n[78] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,\n“CARLA: An open urban driving simulator,” in _Proceedings of the_\n_1st Annual Conference on Robot Learning_, 2017, pp. 1–16. 9, 12\n\n[79] C. Li and K. Czarnecki, “Urban driving with multi-objective deep\nreinforcement learning,” in _Proceedings of the 18th International Con-_\n_ference on Autonomous Agents and MultiAgent Systems_ . International\nFoundation for Autonomous Agents and Multiagent Systems, 2019, pp.\n359–367. 9, 10\n\n[80] S. Kardell and M. Kuosku, “Autonomous vehicle control via deep\nreinforcement learning,” Master’s thesis, Chalmers University of Technology, 2017. 9\n\n[81] J. Chen, B. Yuan, and M. Tomizuka, “Model-free deep reinforcement\nlearning for urban autonomous driving,” in _2019 IEEE Intelligent_\n_Transportation Systems Conference (ITSC)_ . IEEE, 2019, pp. 2765–\n2771. 9\n\n[82] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “End-to-end\ndeep reinforcement learning for lane keeping assist,” in _MLITS, NIPS_\n_Workshop_, vol. 2, 2016. 11\n\n[83] A.-E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement learning framework for autonomous driving,” _Electronic Imaging_,\nvol. 2017, no. 19, pp. 70–76, 2017. 11\n\n[84] P. Wang, C.-Y. Chan, and A. de La Fortelle, “A reinforcement learning\nbased approach for automated lane change maneuvers,” in _2018 IEEE_\n_Intelligent Vehicles Symposium (IV)_ . IEEE, 2018, pp. 1379–1384. 11\n\n\n\n16\n\n\n[85] P. Wang and C.-Y. Chan, “Formulation of deep reinforcement learning\narchitecture toward autonomous driving for on-ramp merge,” in _Intel-_\n_ligent Transportation Systems (ITSC), 2017 IEEE 20th International_\n_Conference on_ . IEEE, 2017, pp. 1–6. 11\n\n[86] D. C. K. Ngai and N. H. C. Yung, “A multiple-goal reinforcement\nlearning method for complex vehicle overtaking maneuvers,” _IEEE_\n_Transactions on Intelligent Transportation Systems_, vol. 12, no. 2, pp.\n509–522, 2011. 11\n\n[87] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura,\n“Navigating occluded intersections with autonomous vehicles using\ndeep reinforcement learning,” in _2018 IEEE International Conference_\n_on Robotics and Automation (ICRA)_ . IEEE, 2018, pp. 2034–2039.\n10, 11\n\n[88] A. Keselman, S. Ten, A. Ghazali, and M. Jubeh, “Reinforcement learning with a* and a deep heuristic,” _arXiv preprint arXiv:1811.07745_,\n2018. 11\n\n[89] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kümmerle, H. Königshof, C. Stiller, A. de La Fortelle, and M. Tomizuka,\n“INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic\nMaps,” _arXiv:1910.03088 [cs, eess]_, 2019. 10\n\n[90] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D.\nLam, A. Bewley, and A. Shah, “Learning to drive in a day,” in _2019_\n_International Conference on Robotics and Automation (ICRA)_ . IEEE,\n2019, pp. 8248–8254. 10\n\n[91] M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller, “Embed\nto control: A locally linear latent dynamics model for control from raw\nimages,” in _Advances in neural information processing systems_, 2015.\n10\n\n[92] N. Wahlström, T. B. Schön, and M. P. Deisenroth, “Learning deep\ndynamical models from image pixels,” _IFAC-PapersOnLine_, vol. 48,\nno. 28, pp. 1059–1064, 2015. 10\n\n[93] S. Chiappa, S. Racanière, D. Wierstra, and S. Mohamed, “Recurrent\nenvironment simulators,” in _5th International Conference on Learn-_\n_ing Representations, ICLR 2017, Toulon, France, April 24-26, 2017,_\n_Conference Track Proceedings_ . OpenReview.net, 2017. 10\n\n[94] B. Recht, “A tour of reinforcement learning: The view from continuous control,” _Annual Review of Control, Robotics, and Autonomous_\n_Systems_, 2008. 10\n\n[95] H. Mania, A. Guy, and B. Recht, “Simple random search of static\nlinear policies is competitive for reinforcement learning,” in _Advances_\n_in Neural Information Processing Systems 31_, S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds.,\n2018, pp. 1800–1809. 10\n\n[96] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and\nA. Sumner, “Torcs, the open racing car simulator,” _Software available_\n_at http://torcs. sourceforge. net_, vol. 4, 2000. 12\n\n[97] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity\nvisual and physical simulation for autonomous vehicles,” in _Field and_\n_Service Robotics_ . Springer, 2018, pp. 621–635. 12\n\n[98] N. Koenig and A. Howard, “Design and use paradigms for gazebo, an\nopen-source multi-robot simulator,” in _2004 International Conference_\n_on Intelligent Robots and Systems (IROS)_, vol. 3. IEEE, 2004, pp.\n2149–2154. 12\n\n[99] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Flötteröd,\nR. Hilbrich, L. Lücken, J. Rummel, P. Wagner, and E. Wießner, “Microscopic traffic simulation using sumo,” in _The 21st IEEE International_\n_Conference on Intelligent Transportation Systems_ . IEEE, 2018. 12\n\n[100] C. Quiter and M. Ernst, “deepdrive/deepdrive: 2.0,” Mar. 2018.\n\n[[Online]. Available: https://doi.org/10.5281/zenodo.1248998 12](https://doi.org/10.5281/zenodo.1248998)\n\n[[101] Nvidia, “Drive Constellation now available,” https://blogs.nvidia.com/](https://blogs.nvidia.com/blog/2019/03/18/drive-constellation-now-available/)\n[blog/2019/03/18/drive-constellation-now-available/,](https://blogs.nvidia.com/blog/2019/03/18/drive-constellation-now-available/) 2019, [accessed\n14-April-2019]. 12\n\n[102] A. S. et al., “Multi-Agent Autonomous Driving Simulator built on\n[top of TORCS,” https://github.com/madras-simulator/MADRaS, 2019,](https://github.com/madras-simulator/MADRaS)\n\n[Online; accessed 14-April-2019]. 12\n\n[103] C. Wu, A. Kreidieh, K. Parvate, E. Vinitsky, and A. M. Bayen, “Flow:\nArchitecture and benchmarking for reinforcement learning in traffic\ncontrol,” _CoRR_, vol. abs/1710.05465, 2017. 12\n\n[104] E. Leurent, “A collection of environments for autonomous driv[ing and tactical decision-making tasks,” https://github.com/eleurent/](https://github.com/eleurent/highway-env)\n[highway-env, 2019, [Online; accessed 14-April-2019]. 12](https://github.com/eleurent/highway-env)\n\n[105] F. Rosique, P. J. Navarro, C. Fernández, and A. Padilla, “A systematic\nreview of perception system and simulators for autonomous vehicles\nresearch,” _Sensors_, vol. 19, no. 3, p. 648, 2019. 10\n\n[106] M. Cutler, T. J. Walsh, and J. P. How, “Reinforcement learning with\nmulti-fidelity simulators,” in _2014 IEEE International Conference on_\n\n\n\n\n_Robotics and Automation (ICRA)_ . IEEE, 2014, pp. 3888–3895. 10,\n12\n\n[107] F. C. German Ros, Vladlen Koltun and A. M. Lopez, “Carla au[tonomous driving challenge,” https://carlachallenge.org/, 2019, [Online;](https://carlachallenge.org/)\naccessed 14-April-2019]. 10\n\n[108] W. G. Najm, J. D. Smith, M. Yanagisawa _et al._, “Pre-crash scenario typology for crash avoidance research,” United States. National Highway\nTraffic Safety Administration, Tech. Rep., 2007. 10\n\n[109] D. A. Pomerleau, “Alvinn: An autonomous land vehicle in a neural\nnetwork,” in _Advances in neural information processing systems_, 1989.\n10\n\n[110] D. Pomerleau, “Efficient training of artificial neural networks for\nautonomous navigation,” _Neural Computation_, vol. 3, no. 1, 1991. 10\n\n[111] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,\nP. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang _et al._, “End\nto end learning for self-driving cars,” in _NIPS 2016 Deep Learning_\n_Symposium_, 2016. 10\n\n[112] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner,\nL. Jackel, and U. Muller, “Explaining how a deep neural network trained with end-to-end learning steers a car,” _arXiv preprint_\n_arXiv:1704.07911_, 2017. 10\n\n[113] M. Kuderer, S. Gulati, and W. Burgard, “Learning driving styles for\nautonomous vehicles from demonstration,” in _Robotics and Automation_\n_(ICRA), 2015 IEEE International Conference on_ . IEEE, 2015, pp.\n2641–2646. 10\n\n[114] S. Sharifzadeh, I. Chiotellis, R. Triebel, and D. Cremers, “Learning\nto drive using inverse reinforcement learning and deep q-networks,” in\n_NIPS Workshops_, December 2016. 10\n\n[115] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and\nD. Meger, “Deep reinforcement learning that matters,” in _Thirty-Second_\n_AAAI Conference on Artificial Intelligence_, 2018. 10\n\n[116] Y. Abeysirigoonawardena, F. Shkurti, and G. Dudek, “Generating\nadversarial driving scenarios in high-fidelity simulators,” in _2019 IEEE_\n_International Conference on Robotics and Automation (ICRA)_ . ICRA,\n2019. 10\n\n[117] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige _et al._, “Using simulation\nand domain adaptation to improve efficiency of deep robotic grasping,”\nin _2018 IEEE International Conference on Robotics and Automation_\n_(ICRA)_ . IEEE, 2018, pp. 4243–4250. 11\n\n[118] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-toreal transfer of robotic control with dynamics randomization,” in _2018_\n_IEEE international conference on robotics and automation (ICRA)_ .\nIEEE, 2018, pp. 1–8. 11\n\n[119] Z. W. Xinlei Pan, Yurong You and C. Lu, “Virtual to real reinforcement learning for autonomous driving,” in _Proceedings of the_\n_British Machine Vision Conference (BMVC)_, G. B. Tae-Kyun Kim,\nStefanos Zafeiriou and K. Mikolajczyk, Eds. BMVA Press, September\n2017. 11\n\n[120] A. Bewley, J. Rigley, Y. Liu, J. Hawke, R. Shen, V.-D. Lam, and\nA. Kendall, “Learning to drive from simulation without real world\nlabels,” in _2019 International Conference on Robotics and Automation_\n_(ICRA)_ . IEEE, 2019, pp. 4818–4824. 11\n\n[121] J. Zhang, L. Tai, P. Yun, Y. Xiong, M. Liu, J. Boedecker, and\nW. Burgard, “Vr-goggles for robots: Real-to-sim domain adaptation for\nvisual control,” _IEEE Robotics and Automation Letters_, vol. 4, no. 2,\npp. 1148–1155, 2019. 11\n\n[122] H. Chae, C. M. Kang, B. Kim, J. Kim, C. C. Chung, and J. W.\nChoi, “Autonomous braking system via deep reinforcement learning,”\n_2017 IEEE 20th International Conference on Intelligent Transportation_\n_Systems (ITSC)_, pp. 1–6, 2017. 12\n\n[123] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and\nN. de Freitas, “Sample efficient actor-critic with experience replay,” in\n_5th International Conference on Learning Representations, ICLR 2017,_\n_Toulon, France, April 24-26, 2017, Conference Track Proceedings_ .\nOpenReview.net, 2017. 12\n\n[124] R. Liaw, S. Krishnan, A. Garg, D. Crankshaw, J. E. Gonzalez,\nand K. Goldberg, “Composing meta-policies for autonomous driving using hierarchical deep reinforcement learning,” _arXiv preprint_\n_arXiv:1711.01503_, 2017. 12\n\n[125] M. E. Taylor and P. Stone, “Transfer learning for reinforcement learning\ndomains: A survey,” _Journal of Machine Learning Research_, vol. 10,\nno. Jul, pp. 1633–1685, 2009. 12\n\n[126] D. Isele and A. Cosgun, “Transferring autonomous driving knowledge\non simulated and real intersections,” in _Lifelong Learning: A Reinforce-_\n_ment Learning Approach,ICML WORKSHOP 2017_, 2017. 12\n\n\n\n17\n\n\n[127] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo,\nR. Munos, C. Blundell, D. Kumaran, and M. Botvinick, “Learning\nto reinforcement learn,” _Complete CogSci 2017 Proceedings_, 2016. 12\n\n[128] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and\nP. Abbeel, “Fast reinforcement learning via slow reinforcement learning,” _arXiv preprint arXiv:1611.02779_, 2016. 12\n\n[129] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning\nfor fast adaptation of deep networks,” in _Proceedings of the 34th_\n_International Conference on Machine Learning - Volume 70_, ser.\nICML’17. JMLR.org, 2017, p. 1126–1135. 12\n\n[130] A. Nichol, J. Achiam, and J. Schulman, “On first-order meta-learning\nalgorithms,” _CoRR, abs/1803.02999_, 2018. 12\n\n[131] M. Al-Shedivat, T. Bansal, Y. Burda, I. Sutskever, I. Mordatch, and\nP. Abbeel, “Continuous adaptation via meta-learning in nonstationary\nand competitive environments,” in _6th International Conference on_\n_Learning Representations, ICLR 2018, Vancouver, BC, Canada, April_\n_30 - May 3, 2018, Conference Track Proceedings_ . OpenReview.net,\n2018. 12\n\n[132] D. Ha and J. Schmidhuber, “Recurrent world models facilitate policy\nevolution,” in _Advances in Neural Information Processing Systems_,\n2018. 12\n\n[133] S. Ross and D. Bagnell, “Efficient reductions for imitation learning,”\nin _Proceedings of the thirteenth international conference on artificial_\n_intelligence and statistics_, 2010, pp. 661–668. 12\n\n[134] M. Bansal, A. Krizhevsky, and A. Ogale, “Chauffeurnet: Learning to\ndrive by imitating the best and synthesizing the worst,” in _Robotics:_\n_Science and Systems XV_, 2018. 12\n\n[135] T. Buhet, E. Wirbel, and X. Perrotton, “Conditional vehicle trajectories\nprediction in carla urban environment,” in _Proceedings of the IEEE_\n_International Conference on Computer Vision Workshops_, 2019. 13\n\n[136] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward\ntransformations: Theory and application to reward shaping,” in _ICML_,\nvol. 99, 1999, pp. 278–287. 13\n\n[137] P. Abbeel and A. Y. Ng, “Apprenticeship learning via inverse reinforcement learning,” in _Proceedings of the Twenty-first International_\n_Conference on Machine Learning_, ser. ICML ’04. ACM, 2004. 13\n\n[138] N. Chentanez, A. G. Barto, and S. P. Singh, “Intrinsically motivated\nreinforcement learning,” in _Advances in neural information processing_\n_systems_, 2005, pp. 1281–1288. 13\n\n[139] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven\nexploration by self-supervised prediction,” in _International Conference_\n_on Machine Learning (ICML)_, vol. 2017, 2017. 13\n\n[140] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A.\nEfros, “Large-scale study of curiosity-driven learning,” _arXiv preprint_\n_arXiv:1808.04355_, 2018. 13\n\n[141] J. Zhang and K. Cho, “Query-efficient imitation learning for end-to-end\nsimulated driving,” in _Proceedings of the Thirty-First AAAI Conference_\n_on Artificial Intelligence, San Francisco, California, USA._, 2017, pp.\n2891–2897. 13\n\n[142] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multiagent, reinforcement learning for autonomous driving,” _arXiv preprint_\n_arXiv:1610.03295_, 2016. 13\n\n[143] X. Xiong, J. Wang, F. Zhang, and K. Li, “Combining deep reinforcement learning and safety based control for autonomous driving,” _arXiv_\n_preprint arXiv:1612.00147_, 2016. 13\n\n[144] C. Ye, H. Ma, X. Zhang, K. Zhang, and S. You, “Survival-oriented reinforcement learning model: An effcient and robust deep reinforcement\nlearning algorithm for autonomous driving problem,” in _International_\n_Conference on Image and Graphics_ . Springer, 2017, pp. 417–429. 13\n\n[145] J. Garcıa and F. Fernández, “A comprehensive survey on safe reinforcement learning,” _Journal of Machine Learning Research_, vol. 16,\nno. 1, pp. 1437–1480, 2015. 13\n\n[146] P. Palanisamy, “Multi-agent connected autonomous driving using deep\nreinforcement learning,” in _2020 International Joint Conference on_\n_Neural Networks (IJCNN)_ . IEEE, 2020, pp. 1–7. 13\n\n[147] S. Bhalla, S. Ganapathi Subramanian, and M. Crowley, “Deep multi\nagent reinforcement learning for autonomous driving,” in _Advances in_\n_Artificial Intelligence_, C. Goutte and X. Zhu, Eds. Cham: Springer\nInternational Publishing, 2020, pp. 67–78. 13\n\n[148] A. Wachi, “Failure-scenario maker for rule-based agent using multiagent adversarial reinforcement learning and its application to autonomous driving,” _arXiv preprint arXiv:1903.10654_, 2019. 13\n\n[149] C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, and\nG. Tan, “Distributed multiagent coordinated learning for autonomous\ndriving in highways based on dynamic coordination graphs,” _IEEE_\n_Transactions on Intelligent Transportation Systems_, vol. 21, no. 2, pp.\n735–748, 2020. 13\n\n\n\n\n[150] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford,\nJ. Schulman, S. Sidor, Y. Wu, and P. Zhokhov, “Openai baselines,”\n[https://github.com/openai/baselines, 2017. 14](https://github.com/openai/baselines)\n\n[151] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, and\nD. Lange, “Unity: A general platform for intelligent agents,” _arXiv_\n_preprint arXiv:1809.02627_, 2018. 14\n\n[152] I. Caspi, G. Leibovich, G. Novik, and S. Endrawis, “Reinforcement\n[learning coach,” Dec. 2017. [Online]. Available: https://doi.org/10.](https://doi.org/10.5281/zenodo.1134899)\n[5281/zenodo.1134899 14](https://doi.org/10.5281/zenodo.1134899)\n\n[153] Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo\nCastro, Ethan Holly, Sam Fishman, Ke Wang, Ekaterina Gonina,\nNeal Wu, Chris Harris, Vincent Vanhoucke, Eugene Brevdo,\n“TF-Agents: A library for reinforcement learning in tensorflow,”\n[https://github.com/tensorflow/agents, 2018, [Online; accessed 25-June-](https://github.com/tensorflow/agents)\n[2019]. [Online]. Available: https://github.com/tensorflow/agents 14](https://github.com/tensorflow/agents)\n\n[154] A. Stooke and P. Abbeel, “rlpyt: A research code base for deep\nreinforcement learning in pytorch,” _arXiv preprint arXiv:1909.01500_,\n2019. 14\n\n[155] I. Osband, Y. Doron, M. Hessel, J. Aslanides, E. Sezener, A. Saraiva,\nK. McKinney, T. Lattimore, C. Szepezvari, S. Singh _et al._, “Behaviour\nsuite for reinforcement learning,” _arXiv preprint arXiv:1908.03568_,\n2019. 14\n\n\n**B Ravi Kiran** is the technical lead of machine\nlearning team at Navya, designing and deploying\nrealtime deep learning architectures for perception\ntasks on autonomous shuttles. During his 6 years\nin academic research, he has worked on DNNs for\nvideo anomaly detection, online time series anomaly\ndetection, hyperspectral image processing for tumor\ndetection. He finished his PhD at Paris-Est in 2014\nentitled Energetic lattice based optimization which\nwas awarded the MSTIC prize. He has worked in\nacademic research for over 4 years in embedded\nprogramming and in autonomous driving. During his career he has published\nover 40 articles and journals.\n\n\n**Ibrahim Sobh** Ibrahim has more than 20 years of\nexperience in the area of Machine Learning and\nSoftware Development. Dr. Sobh received his PhD\nin Deep Reinforcement Learning for fast learning\nagents acting in 3D environments. He received his\nB.Sc. and M.Sc. degrees in Computer Engineering\nfrom Cairo University Faculty of Engineering. His\nM.Sc. Thesis is in the field of Machine Learning applied on automatic documents summarization.\nIbrahim has participated in several related national\nand international mega projects, conferences and\nsummits. He delivers training and lectures for academic and industrial entities.\nIbrahim’s publications including international journals and conference papers\nare mainly in the machine and deep learning fields. His area of research\nis mainly in Computer vision, Natural language processing and Speech\nprocessing. Currently, Dr. Sobh is a Senior Expert of AI, Valeo.\n\n\n**Victor Talpaert** is a PhD student at U2IS, ENSTA Paris, Institut Polytechnique de Paris, 91120\nPalaiseau, France. His PhD is directed by Bruno\nMonsuez since 2017, the lab speciality is in robotics\nand complex systems. His PhD is co-sponsored by\nAKKA Technologies in Gyuancourt, France, through\nthe guidance of AKKA’s Autonomous Systems\nTeam. This team has a large focus on Autonomous\nDriving (AD) and the automotive industry in general.\nHis PhD subject is learning decision making for AD,\nwith assumptions such as a modular AD pipeline,\nlearned features compatible with classic robotic approaches and ontology\nbased hierarchical abstractions.\n\n\n\n18\n\n\n**Patrick Mannion** is a permanent member of academic staff at National University of Ireland Galway, where he lectures in Computer Science. He is\nalso Deputy Editor of The Knowledge Engineering\nReview journal. He received a BEng in Civil Engineering, a HDip in Software Development and a\nPhD in Machine Learning from National University\nof Ireland Galway, a PgCert in Teaching & Learning\nfrom Galway-Mayo IT and a PgCert in Sensors\nfor Autonomous Vehicles from IT Sligo. He is a\nformer Irish Research Council Scholar and a former\nFulbright Scholar. His main research interests include machine learning, multiagent systems, multi-objective optimisation, game theory and metaheuristic\nalgorithms, with applications to domains such as transportation, autonomous\nvehicles, energy systems and smart grids.\n\n\n**Ahmad El Sallab** Ahmad El Sallab is the Senior\nChief Engineer of Deep Learning at Valeo Egypt,\nand Senior Expert at Valeo Group. Ahmad has 15\nyears of experience in Machine Learning and Deep\nLearning, where he acquired his M.Sc. and Ph.D.\non 2009 and 2013 in the field. He has worked for\nreputable multi-national organizations in the industry\nsince 2005 like Intel and Valeo. He has over 35\npublications and book chapters in Deep Learning\nin top IEEE and ACM journals and conferences, in\naddition to 30 patents, with applications in Speech,\nNLP, Computer Vision and Robotics.\n\n\n**Senthil Yogamani** is an Artificial Intelligence architect and technical leader at Valeo Ireland. He\nleads the research and design of AI algorithms for\nvarious modules of autonomous driving systems.\nHe has over 14 years of experience in computer\nvision and machine learning including 12 years of\nexperience in industrial automotive systems. He is\nan author of over 90 publications and 60 patents\nwith 1300+ citations. He serves in the editorial board\nof various leading IEEE automotive conferences\nincluding ITSC, IV and ICVES and advisory board\nof various industry consortia including Khronos, Cognitive Vehicles and IS\nAuto. He is a recipient of best associate editor award at ITSC 2015 and best\npaper award at ITST 2012.\n\n\n**Patrick Pérez** is Scientific Director of Valeo.ai,\na Valeo research lab on artificial intelligence for\nautomotive applications. He is currently on the Editorial Board of the International Journal of Computer\nVision. Before joining Valeo, Patrick Pérez has been\nDistinguished Scientist at Technicolor (2009-2918),\nresearcher at Inria (1993-2000, 2004-2009) and at\nMicrosoft Research Cambridge (2000-2004). His\nresearch interests include multimodal scene understanding and computational imaging.\n\n\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-17-0.png)\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-17-1.png)\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-17-2.png)\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-17-3.png)\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-17-4.png)\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-17-5.png)\n\n![](output/images/129983331ca874142a3e8eb2d93d820bdf1f9aca.pdf-17-6.png)\n",
    "ranking": {
      "relevance_score": 0.7061783188648858,
      "citation_score": 0.9,
      "recency_score": 0.3635034766739728,
      "final_score": 0.7106751708728174
    },
    "citation_key": "Kiran2020DeepRL",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "12d1d070a53d4084d88a77b8b143bad51c40c38f",
    "title": "Reinforcement Learning: A Survey",
    "published": "1996-04-30",
    "authors": [
      "L. Kaelbling",
      "M. Littman",
      "A. Moore"
    ],
    "summary": "This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word \"reinforcement.\" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.",
    "pdf_url": "https://www.jair.org/index.php/jair/article/download/10166/24110",
    "doi": "10.1613/jair.301",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "Journal of Artificial Intelligence Research",
    "citation_count": 9530,
    "bibtex": "@Article{Kaelbling1996ReinforcementLA,\n author = {L. Kaelbling and M. Littman and A. Moore},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {J. Artif. Intell. Res.},\n pages = {237-285},\n title = {Reinforcement Learning: A Survey},\n volume = {4},\n year = {1996}\n}\n",
    "markdown_text": "Journal of Arti�cial Intelligence Research \u0004 (\u0001 \u0006) \u0002\u0003\u0007-\u0002\b\u0005 Submitted / \u0005; published \u0005/ \u0006\n\n\nReinforcement Learning: A Survey\n\n\nLeslie Pack Kaelbling lpk@cs.brown.edu\n\nMichael L. Littman mlittman@cs.brown.edu\n\n\nComputer Science Department, Box \u0001 \u00010, Brown University\n\nProvidence, RI 0\u0002 \u0001\u0002-\u0001 \u00010 USA\n\n\nAndrew W. Mo ore awm@cs.cmu.edu\n\n\nSmith Hal l \u0002\u0002\u0001, Carnegie Mel lon University, \u0005000 Forbes Avenue\n\n\nPittsburgh, PA \u0001\u0005\u0002\u0001\u0003 USA\n\n\nAbstract\n\n\nThis pap er surveys the �eld of reinforcement learning from a computer-science p er\nsp ective. It is written to b e accessible to researchers familiar with machine learning. Both\n\nthe historical basis of the �eld and a broad selection of current work are summarized.\n\n\nReinforcement learning is the problem faced by an agent that learns b ehavior through\n\ntrial-and-error interactions with a dynamic environment. The work describ ed here has a\n\nresemblance to work in psychology, but di�ers considerably in the details and in the use\n\nof the word \\reinforcement.\" The pap er discusses central issues of reinforcement learning,\n\nincluding trading o� exploration and exploitation, establishing the foundations of the �eld\n\nvia Markov decision theory, learning from delayed reinforcement, constructing empirical\n\nmo dels to accelerate learning, making use of generalization and hierarchy, and coping with\n\nhidden state. It concludes with a survey of some implemented systems and an assessment\n\nof the practical utility of current metho ds for reinforcement learning.\n\n\n\u0001. Intro duction\n\n\nReinforcement learning dates back to the early days of cyb ernetics and work in statistics,\n\npsychology, neuroscience, and computer science. In the last �ve to ten years, it has attracted\n\nrapidly increasing interest in the machine learning and arti�cial intelligence communities.\n\nIts promise is b eguiling|a way of programming agents by reward and punishment without\n\nneeding to sp ecify how the task is to b e achieved. But there are formidable computational\n\nobstacles to ful�lling the promise.\n\n\nThis pap er surveys the historical basis of reinforcement learning and some of the current\n\nwork from a computer science p ersp ective. We give a high-level overview of the �eld and a\n\ntaste of some sp eci�c approaches. It is, of course, imp ossible to mention all of the imp ortant\n\nwork in the �eld; this should not b e taken to b e an exhaustive account.\n\n\nReinforcement learning is the problem faced by an agent that must learn b ehavior\n\nthrough trial-and-error interactions with a dynamic environment. The work describ ed here\n\nhas a strong family resemblance to ep onymous work in psychology, but di�ers considerably\n\nin the details and in the use of the word \\reinforcement.\" It is appropriately thought of as\n\na class of problems, rather than as a set of techniques.\n\n\nThere are two main strategies for solving reinforcement-learning problems. The �rst is to\n\nsearch in the space of b ehaviors in order to �nd one that p erforms well in the environment.\n\nThis approach has b een taken by work in genetic algorithms and genetic programming,\n\n\n�c \u0001 \u0006 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\n\n\n\n\nKaelbling, Littman, & Moore\n\n\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-1-0.png)\n\n_s_\n\n\n\n_a_\n\n\n\nFigure \u0001: The standard reinforcement-learning mo del.\n\n\nas well as some more novel search techniques (Schmidhub er, \u0001 \u0006). The second is to use\n\nstatistical techniques and dynamic programming metho ds to estimate the utility of taking\n\nactions in states of the world. This pap er is devoted almost entirely to the second set of\n\ntechniques b ecause they take advantage of the sp ecial structure of reinforcement-learning\n\nproblems that is not available in optimization problems in general. It is not yet clear which\n\nset of approaches is b est in which circumstances.\n\n\nThe rest of this section is devoted to establishing notation and describing the basic\n\nreinforcement-learning mo del. Section \u0002 explains the trade-o� b etween exploration and\n\nexploitation and presents some solutions to the most basic case of reinforcement-learning\n\nproblems, in which we want to maximize the immediate reward. Section \u0003 considers the more\n\ngeneral problem in which rewards can b e delayed in time from the actions that were crucial\n\nto gaining them. Section \u0004 considers some classic mo del-free algorithms for reinforcement\n\nlearning from delayed reward: adaptive heuristic critic, T D (�) and Q-learning. Section \u0005\n\ndemonstrates a continuum of algorithms that are sensitive to the amount of computation an\n\nagent can p erform b etween actual steps of action in the environment. Generalization|the\n\ncornerstone of mainstream machine learning research|has the p otential of considerably\n\naiding reinforcement learning, as describ ed in Section \u0006. Section \u0007 considers the problems\n\nthat arise when the agent do es not have complete p erceptual access to the state of the\n\nenvironment. Section \b catalogs some of reinforcement learning's successful applications.\n\nFinally, Section concludes with some sp eculations ab out imp ortant op en problems and\n\nthe future of reinforcement learning.\n\n\n\u0001.\u0001 Reinforcement-Learning Mo del\n\n\nIn the standard reinforcement-learning mo del, an agent is connected to its environment\n\nvia p erception and action, as depicted in Figure \u0001. On each step of interaction the agent\n\nreceives as input, i, some indication of the current state, s, of the environment; the agent\n\nthen cho oses an action, a, to generate as output. The action changes the state of the\n\nenvironment, and the value of this state transition is communicated to the agent through\n\na scalar reinforcement signal, r . The agent's b ehavior, B, should cho ose actions that tend\n\nto increase the long-run sum of values of the reinforcement signal. It can learn to do this\n\nover time by systematic trial and error, guided by a wide variety of algorithms that are the\n\nsub ject of later sections of this pap er.\n\n\n\u0002\u0003\b\n\n\n\n\nReinforcement Learning: A Survey\n\n\nFormally, the mo del consists of\n\n\n� a discrete set of environment states, S ;\n\n\n� a discrete set of agent actions, A; and\n\n\n� a set of scalar reinforcement signals; typically f0; \u0001g, or the real numb ers.\n\n\nThe �gure also includes an input function I, which determines how the agent views the\n\nenvironment state; we will assume that it is the identity function (that is, the agent p erceives\n\nthe exact state of the environment) until we consider partial observability in Section \u0007.\n\nAn intuitive way to understand the relation b etween the agent and its environment is\n\nwith the following example dialogue.\n\n\nEnvironment: You are in state \u0006\u0005. You have \u0004 p ossible actions.\n\n\nAgent: I'll take action \u0002.\n\n\nEnvironment: You received a reinforcement of \u0007 units. You are now in state\n\n\n\u0001\u0005. You have \u0002 p ossible actions.\n\n\nAgent: I'll take action \u0001.\n\n\nEnvironment: You received a reinforcement of -\u0004 units. You are now in state\n\n\n\u0006\u0005. You have \u0004 p ossible actions.\n\n\nAgent: I'll take action \u0002.\n\n\nEnvironment: You received a reinforcement of \u0005 units. You are now in state\n\n\n\u0004\u0004. You have \u0005 p ossible actions.\n\n\n\n.\n\n\n.\n\n\n.\n\n\n\n.\n\n\n.\n\n\n.\n\n\n\nThe agent's job is to �nd a p olicy �, mapping states to actions, that maximizes some\n\nlong-run measure of reinforcement. We exp ect, in general, that the environment will b e\n\nnon-deterministic; that is, that taking the same action in the same state on two di�erent\n\no ccasions may result in di�erent next states and/or di�erent reinforcement values. This\n\nhapp ens in our example ab ove: from state \u0006\u0005, applying action \u0002 pro duces di�ering rein\nforcements and di�ering states on two o ccasions. However, we assume the environment is\n\nstationary; that is, that the probabilities of making state transitions or receiving sp eci�c\n\n\n\nreinforcement signals do not change over time.\n\n\n\n\u0001\n\n\n\nReinforcement learning di�ers from the more widely studied problem of sup ervised learn\ning in several ways. The most imp ortant di�erence is that there is no presentation of in\nput/output pairs. Instead, after cho osing an action the agent is told the immediate reward\n\nand the subsequent state, but is not told which action would have b een in its b est long-term\n\ninterests. It is necessary for the agent to gather useful exp erience ab out the p ossible system\n\nstates, actions, transitions and rewards actively to act optimally. Another di�erence from\n\nsup ervised learning is that on-line p erformance is imp ortant: the evaluation of the system\n\nis often concurrent with learning.\n\n\n\u0001. This assumption may b e disapp ointi ng; after all, op eration in non-stationary environments is one of the\n\nmotivations for buildin g learning systems. In fact, many of the algorithms describ ed in later sections\n\nare e�ective in slowly-varying non-stationary environments, but there is very little theoretical analysis\n\nin this area.\n\n\n\u0002\u0003\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nSome asp ects of reinforcement learning are closely related to search and planning issues\n\nin arti�cial intelligence. AI search algorithms generate a satisfactory tra jectory through a\n\ngraph of states. Planning op erates in a similar manner, but typically within a construct\n\nwith more complexity than a graph, in which states are represented by comp ositions of\n\nlogical expressions instead of atomic symb ols. These AI algorithms are less general than the\n\nreinforcement-learning metho ds, in that they require a prede�ned mo del of state transitions,\n\nand with a few exceptions assume determinism. On the other hand, reinforcement learning,\n\nat least in the kind of discrete cases for which theory has b een develop ed, assumes that\n\nthe entire state space can b e enumerated and stored in memory|an assumption to which\n\nconventional search algorithms are not tied.\n\n\n\u0001.\u0002 Mo dels of Optimal Behavior\n\n\nBefore we can start thinking ab out algorithms for learning to b ehave optimally, we have\n\nto decide what our mo del of optimality will b e. In particular, we have to sp ecify how the\n\nagent should take the future into account in the decisions it makes ab out how to b ehave\n\nnow. There are three mo dels that have b een the sub ject of the ma jority of work in this\n\n\narea.\n\n\nThe �nite-horizon mo del is the easiest to think ab out; at a given moment in time, the\n\nagent should optimize its exp ected reward for the next h steps:\n\n\nh\n\n\n\nE (X\n\n\nt=0\n\n\n\n) ;\n\n\n\nrt\n\n\n\nit need not worry ab out what will happ en after that. In this and subsequent expressions,\n\n\n\nr\n\n\n\nt\n\n\n\nrepresents the scalar reward received t steps into the future. This mo del can b e used in\n\n\n\ntwo ways. In the �rst, the agent will have a non-stationary p olicy; that is, one that changes\n\nover time. On its �rst step it will take what is termed a h-step optimal action. This is\n\nde�ned to b e the b est action available given that it has h steps remaining in which to act\n\nand gain reinforcement. On the next step it will take a (h � \u0001)-step optimal action, and so\n\non, until it �nally takes a \u0001-step optimal action and terminates. In the second, the agent\n\ndo es receding-horizon control, in which it always takes the h-step optimal action. The agent\n\nalways acts according to the same p olicy, but the value of h limits how far ahead it lo oks\n\nin cho osing its actions. The �nite-horizon mo del is not always appropriate. In many cases\n\nwe may not know the precise length of the agent's life in advance.\n\nThe in�nite-horizon discounted mo del takes the long-run reward of the agent into ac\ncount, but rewards that are received in the future are geometrically discounted according\n\nto discount factor �, (where 0 � � < \u0001):\n\n\n\u0001\n\n\n\nE (X\n\n\nt=0\n\n\n\nt\n\n�\n\n\n\nrt\n\n\n\n) :\n\n\n\nWe can interpret � in several ways. It can b e seen as an interest rate, a probability of living\n\nanother step, or as a mathematical trick to b ound the in�nite sum. The mo del is conceptu\nally similar to receding-horizon control, but the discounted mo del is more mathematically\n\ntractable than the �nite-horizon mo del. This is a dominant reason for the wide attention\n\n\nthis mo del has received.\n\n\n\u0002\u00040\n\n\n\n\nReinforcement Learning: A Survey\n\n\nAnother optimality criterion is the average-reward model, in which the agent is supp osed\n\nto take actions that optimize its long-run average reward:\n\n\n\nh\n\nX\n\n\nt=0\n\n\n\n\u0001\n\n\nh\n\n\n\nlim\n\nh!\u0001\n\n\n\nE (\n\n\n\nrt\n\n\n\n) :\n\n\n\nSuch a p olicy is referred to as a gain optimal p olicy; it can b e seen as the limiting case of\n\nthe in�nite-horizon discounted mo del as the discount factor approaches \u0001 (Bertsekas, \u0001 \u0005).\n\nOne problem with this criterion is that there is no way to distinguish b etween two p olicies,\n\none of which gains a large amount of reward in the initial phases and the other of which\n\ndo es not. Reward gained on any initial pre�x of the agent's life is overshadowed by the\n\nlong-run average p erformance. It is p ossible to generalize this mo del so that it takes into\n\naccount b oth the long run average and the amount of initial reward than can b e gained.\n\nIn the generalized, bias optimal mo del, a p olicy is preferred if it maximizes the long-run\n\naverage and ties are broken by the initial extra reward.\n\nFigure \u0002 contrasts these mo dels of optimality by providing an environment in which\n\nchanging the mo del of optimality changes the optimal p olicy. In this example, circles\n\nrepresent the states of the environment and arrows are state transitions. There is only\n\na single action choice from every state except the start state, which is in the upp er left\n\nand marked with an incoming arrow. All rewards are zero except where marked. Under a\n\n�nite-horizon mo del with h = \u0005, the three actions yield rewards of +\u0006:0, +0:0, and +0:0, so\n\nthe �rst action should b e chosen; under an in�nite-horizon discounted mo del with � = 0:,\n\nthe three choices yield +\u0001\u0006:\u0002, +\u0005 :0, and +\u0005\b:\u0005 so the second action should b e chosen;\n\nand under the average reward mo del, the third action should b e chosen since it leads to\n\nan average reward of +\u0001\u0001. If we change h to \u0001000 and � to 0.\u0002, then the second action is\n\noptimal for the �nite-horizon mo del and the �rst for the in�nite-horizon discounted mo del;\n\nhowever, the average reward mo del will always prefer the b est long-term average. Since the\n\nchoice of optimality mo del and parameters matters so much, it is imp ortant to cho ose it\n\ncarefully in any application.\n\nThe �nite-horizon mo del is appropriate when the agent's lifetime is known; one im\np ortant asp ect of this mo del is that as the length of the remaining lifetime decreases, the\n\nagent's p olicy may change. A system with a hard deadline would b e appropriately mo deled\n\nthis way. The relative usefulness of in�nite-horizon discounted and bias-optimal mo dels is\n\nstill under debate. Bias-optimality has the advantage of not requiring a discount parameter;\n\nhowever, algorithms for �nding bias-optimal p olicies are not yet as well-understo o d as those\n\nfor �nding optimal in�nite-horizon discounted p olicies.\n\n\n\u0001.\u0003 Measuring Learning Performance\n\n\nThe criteria given in the previous section can b e used to assess the p olicies learned by a\n\ngiven algorithm. We would also like to b e able to evaluate the quality of learning itself.\n\nThere are several incompatible measures in use.\n\n\n� Eventual convergence to optimal. Many algorithms come with a provable guar\nantee of asymptotic convergence to optimal b ehavior (Watkins & Dayan, \u0001 \u0002). This\n\nis reassuring, but useless in practical terms. An agent that quickly reaches a plateau\n\n\n\u0002\u0004\u0001\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nAverage reward\n\n\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-5-0.png)\n\n+11\n\n\n\nFigure \u0002: Comparing mo dels of optimality. All unlab eled arrows pro duce a reward of zero.\n\n\nat % of optimality may, in many applications, b e preferable to an agent that has a\n\nguarantee of eventual optimality but a sluggish early learning rate.\n\n\n� Sp eed of convergence to optimality. Optimality is usually an asymptotic result,\n\nand so convergence sp eed is an ill-de�ned measure. More practical is the speed of\n\nconvergence to near-optimality. This measure b egs the de�nition of how near to\n\noptimality is su�cient. A related measure is level of performance after a given time,\n\nwhich similarly requires that someone de�ne the given time.\n\n\nIt should b e noted that here we have another di�erence b etween reinforcement learning\n\nand conventional sup ervised learning. In the latter, exp ected future predictive accu\nracy or statistical e�ciency are the prime concerns. For example, in the well-known\n\nPAC framework (Valiant, \u0001 \b\u0004), there is a learning p erio d during which mistakes do\n\nnot count, then a p erformance p erio d during which they do. The framework provides\n\nb ounds on the necessary length of the learning p erio d in order to have a probabilistic\n\nguarantee on the subsequent p erformance. That is usually an inappropriate view for\n\nan agent with a long existence in a complex environment.\n\n\nIn spite of the mismatch b etween emb edded reinforcement learning and the train/test\n\np ersp ective, Fiechter (\u0001 \u0004) provides a PAC analysis for Q-learning (describ ed in\n\nSection \u0004.\u0002) that sheds some light on the connection b etween the two views.\n\n\nMeasures related to sp eed of learning have an additional weakness. An algorithm\n\nthat merely tries to achieve optimality as fast as p ossible may incur unnecessarily\n\nlarge p enalties during the learning p erio d. A less aggressive strategy taking longer to\n\nachieve optimality, but gaining greater total reinforcement during its learning might\n\nb e preferable.\n\n\n� Regret. A more appropriate measure, then, is the exp ected decrease in reward gained\n\ndue to executing the learning algorithm instead of b ehaving optimally from the very\n\nb eginning. This measure is known as regret (Berry & Fristedt, \u0001 \b\u0005). It p enalizes\n\nmistakes wherever they o ccur during the run. Unfortunately, results concerning the\n\nregret of algorithms are quite hard to obtain.\n\n\n\u0002\u0004\u0002\n\n\n\n\nReinforcement Learning: A Survey\n\n\n\u0001.\u0004 Reinforcement Learning and Adaptive Control\n\n\nAdaptive control (Burghes & Graham, \u0001 \b0; Stengel, \u0001 \b\u0006) is also concerned with algo\nrithms for improving a sequence of decisions from exp erience. Adaptive control is a much\n\nmore mature discipline that concerns itself with dynamic systems in which states and ac\ntions are vectors and system dynamics are smo oth: linear or lo cally linearizable around a\n\ndesired tra jectory. A very common formulation of cost functions in adaptive control are\n\nquadratic p enalties on deviation from desired state and action vectors. Most imp ortantly,\n\nalthough the dynamic mo del of the system is not known in advance, and must b e esti\nmated from data, the structure of the dynamic mo del is �xed, leaving mo del estimation\n\nas a parameter estimation problem. These assumptions p ermit deep, elegant and p owerful\n\nmathematical analysis, which in turn lead to robust, practical, and widely deployed adaptive\n\ncontrol algorithms.\n\n\n\u0002. Exploitation versus Exploration: The Single-State Case\n\n\nOne ma jor di�erence b etween reinforcement learning and sup ervised learning is that a\n\nreinforcement-learner must explicitly explore its environment. In order to highlight the\n\nproblems of exploration, we treat a very simple case in this section. The fundamental issues\n\nand approaches describ ed here will, in many cases, transfer to the more complex instances\n\nof reinforcement learning discussed later in the pap er.\n\n\nThe simplest p ossible reinforcement-learning problem is known as the k -armed bandit\n\nproblem, which has b een the sub ject of a great deal of study in the statistics and applied\n\nmathematics literature (Berry & Fristedt, \u0001 \b\u0005). The agent is in a ro om with a collection of\n\nk gambling machines (each called a \\one-armed bandit\" in collo quial English). The agent is\n\np ermitted a �xed numb er of pulls, h. Any arm may b e pulled on each turn. The machines\n\ndo not require a dep osit to play; the only cost is in wasting a pull playing a sub optimal\n\nmachine. When arm i is pulled, machine i pays o� \u0001 or 0, according to some underlying\n\n\n\nprobability parameter pi\n\n\n\n, where payo�s are indep endent events and the pi\n\n\n\ns are unknown.\n\n\n\nWhat should the agent's strategy b e?\n\n\nThis problem illustrates the fundamental tradeo� b etween exploitation and exploration.\n\nThe agent might b elieve that a particular arm has a fairly high payo� probability; should\n\nit cho ose that arm all the time, or should it cho ose another one that it has less information\n\nab out, but seems to b e worse? Answers to these questions dep end on how long the agent\n\nis exp ected to play the game; the longer the game lasts, the worse the consequences of\n\nprematurely converging on a sub-optimal arm, and the more the agent should explore.\n\n\nThere is a wide variety of solutions to this problem. We will consider a representative\n\nselection of them, but for a deep er discussion and a numb er of imp ortant theoretical results,\n\nsee the b o ok by Berry and Fristedt (\u0001 \b\u0005). We use the term \\action\" to indicate the\n\nagent's choice of arm to pull. This eases the transition into delayed reinforcement mo dels\n\nin Section \u0003. It is very imp ortant to note that bandit problems �t our de�nition of a\n\nreinforcement-learning environment with a single state with only self transitions.\n\n\nSection \u0002.\u0001 discusses three solutions to the basic one-state bandit problem that have\n\nformal correctness results. Although they can b e extended to problems with real-valued\n\nrewards, they do not apply directly to the general multi-state delayed-reinforcement case.\n\n\n\u0002\u0004\u0003\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nSection \u0002.\u0002 presents three techniques that are not formally justi�ed, but that have had wide\n\nuse in practice, and can b e applied (with similar lack of guarantee) to the general case.\n\n\n\u0002.\u0001 Formally Justi�ed Techniques\n\n\nThere is a fairly well-develop ed formal theory of exploration for very simple problems.\n\nAlthough it is instructive, the metho ds it provides do not scale well to more complex\n\nproblems.\n\n\n\u0002.\u0001.\u0001 Dynamic-Programming Approach\n\n\nIf the agent is going to b e acting for a total of h steps, it can use basic Bayesian reasoning\n\nto solve for an optimal strategy (Berry & Fristedt, \u0001 \b\u0005). This requires an assumed prior\n\n\n\njoint distribution for the parameters fpi g, the most natural of which is that each pi\n\n\n\nis\n\n\n\nindep endently uniformly distributed b etween 0 and \u0001. We compute a mapping from belief\n\nstates (summaries of the agent's exp eriences during this run) to actions. Here, a b elief state\n\n\n\ncan b e represented as a tabulation of action choices and payo�s: fn\u0001\n\n\n\n; w\u0001\n\n\n\n; w\n\n\n\n; n\u0002\n\n\n\n; n\n\n\n\n; w\u0002\n\n\n\n; : : : ; nk\n\n\n\n; wk\n\n\n\n; w\n\n\n\ng\n\n\n\ndenotes a state of play in which each arm i has b een pulled ni\n\n\n\ntimes with wi\n\n\n\npayo�s. We\n\n\n\nwrite V\n\n\n\n�\n\n\n\n(n\u0001\n\n\n\n; w\n\n\n\n\u0001\n\n\n\n) as the exp ected payo� remaining, given that a total of h pulls\n\n\n\n; wk\n\n\n\n; : : : ; n\n\n\n\nk\n\n\n\nare available, and we use the remaining pulls optimally.\n\n\n\nIf\n\n\n\nPi\n\n\n\nP\n\n\n\nn\n\n\n\ni\n\n\n\n(n\u0001\n\n\n\n; w\u0001\n\n\n\n�\n\n\n\n; : : : ; nk\n\n\n\n; : : : ; n\n\n\n\n; wk\n\n\n\n) = 0. This is\n\n\n\n= h, then there are no remaining pulls, and V\n\n\n\nthe basis of a recursive de�nition. If we know the V\n\n\n\n�\n\n\n\nvalue for all b elief states with t pulls\n\n\n\nremaining, we can compute the V\n\n\n\n�\n\n\n\nvalue of any b elief state with t + \u0001 pulls remaining:\n\n\n\n#\n\n)+\n\n\n\n(\u0001 � �i\n\n\n\n�\n\n\n\n; w\n\n\n\n\u0001; : : : ; n\n\n\n\nV\n\n\n\n�\n\n\n\n(n\u0001\n\n\n\nk\n\n\n\n\"\n\n\n\nFuture payo� if agent takes action i,\n\nthen acts optimally for remaining pulls\n\n\n\n; : : : ; n\n\n\n\n; w\n\n\n\nk\n\n\n\n) = maxi\n\n\n= maxi\n\n\n\nE\n\n\n\n(n\u0001\n\n\n\n; wi\n\n\n\n; : : : ; ni\n\n\n\n; : : : ; n\n\n\n\n�i V\n\n\n\n�\n\n\n\n+ \u0001; : : : ; nk\n\n\n\n+ \u0001; : : : ; n\n\n\n\n; wk\n\n\n\n; w\n\n\n\n+ \u0001; wi\n\n\n\n)V\n\n\n\n!\n\n\n\n(n\u0001\n\n\n\n; wi\n\n\n\n; : : : ; ni\n\n\n\n+ \u0001; wi\n\n\n\n; : : : ; nk\n\n\n\n; : : : ; n\n\n\n\n; wk\n\n\n\n)\n\n\n\nwhere �i\n\n\n\nis the p osterior sub jective probability of action i paying o� given ni, wi\n\n\n\nand\n\n\n\nour prior probability. For the uniform priors, which result in a b eta distribution, �i\n\n\n\n=\n\n\n\n(wi\n\n\n\n+ \u0001)=(ni\n\n\n\n+ \u0002).\n\n\n\nThe exp ense of �lling in the table of V\n\n\n\nvalues in this way for all attainable b elief states\n\n\n\n�\n\n\n\nis linear in the numb er of b elief states times actions, and thus exp onential in the horizon.\n\n\n\u0002.\u0001.\u0002 Gittins Allocation Indices\n\n\nGittins gives an \\allo cation index\" metho d for �nding the optimal choice of action at each\n\nstep in k -armed bandit problems (Gittins, \u0001 \b ). The technique only applies under the\n\ndiscounted exp ected reward criterion. For each action, consider the numb er of times it has\n\nb een chosen, n, versus the numb er of times it has paid o�, w . For certain discount factors,\n\nthere are published tables of \\index values,\" I (n; w ) for each pair of n and w . Lo ok up\n\n\n\nthe index value for each action i, I (ni\n\n\n\n; wi\n\n\n\n; w\n\n\n\n). It represents a comparative measure of the\n\n\n\ncombined value of the exp ected payo� of action i (given its history of payo�s) and the value\n\nof the information that we would get by cho osing it. Gittins has shown that cho osing the\n\naction with the largest index value guarantees the optimal balance b etween exploration and\n\nexploitation.\n\n\n\u0002\u0004\u0004\n\n\n\n\nReinforcement Learning: A Survey\n\n\n_a = 0_ _a = 1_\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-8-0.png)\n\n\n1 2 3 N-1 N 2N 2N-1 N+3 N+2 N+1\n\n\n_r = 0_\n\n\nFigure \u0003: A Tsetlin automaton with \u0002N states. The top row shows the state transitions\n\nthat are made when the previous action resulted in a reward of \u0001; the b ottom\n\nrow shows transitions after a reward of 0. In states in the left half of the �gure,\n\naction 0 is taken; in those on the right, action \u0001 is taken.\n\n\nBecause of the guarantee of optimal exploration and the simplicity of the technique\n\n(given the table of index values), this approach holds a great deal of promise for use in more\n\ncomplex applications. This metho d proved useful in an application to rob otic manipulation\n\nwith immediate reward (Salganico� & Ungar, \u0001 \u0005). Unfortunately, no one has yet b een\n\nable to �nd an analog of index values for delayed reinforcement problems.\n\n\n\u0002.\u0001.\u0003 Learning Automata\n\n\nA branch of the theory of adaptive control is devoted to learning automata, surveyed by\n\nNarendra and Thathachar (\u0001 \b ), which were originally describ ed explicitly as �nite state\n\nautomata. The Tsetlin automaton shown in Figure \u0003 provides an example that solves a\n\n\u0002-armed bandit arbitrarily near optimally as N approaches in�nity.\n\nIt is inconvenient to describ e algorithms as �nite-state automata, so a move was made\n\nto describ e the internal state of the agent as a probability distribution according to which\n\nactions would b e chosen. The probabilities of taking di�erent actions would b e adjusted\n\naccording to their previous successes and failures.\n\nAn example, which stands among a set of algorithms indep endently develop ed in the\n\nmathematical psychology literature (Hilgard & Bower, \u0001 \u0007\u0005), is the linear reward-inaction\n\n\n\u0006\n\n\n\nalgorithm. Let pi\n\n\n\u0006\n\n\n\nb e the agent's probability of taking action i.\n\n\n\u0006\n\n\n\n� When action ai\n\n\n\u0006\n\n\n� When action ai\n\n\n\nsucceeds,\n\n\n\u0006\n\n\n\n\u0006\n\n\nfails, pj\n\n\n\n)\n\n\n\u0006\n\n\n\npi\n\n\npj \u0006\n\n\n\n:= pi\n\n\n:= pj \u0006\n\n\n\n+ �(\u0001 � pi\n\n\n\u0006\n\n\n\n� �p \u0006\n\n\n\nj \u0006\n\n\n\nfor j =\u0006 i\n\n\n\n\u0006\n\n\nremains unchanged (for all j ).\n\n\n\n\u0006\n\n\nThis algorithm converges with probability \u0001 to a vector containing a single \u0001 and the\n\nrest 0's (cho osing a particular action with probability \u0001). Unfortunately, it do es not always\n\nconverge to the correct action; but the probability that it converges to the wrong one can\n\nb e made arbitrarily small by making � small (Narendra & Thathachar, \u0001 \u0007\u0004). There is no\n\nliterature on the regret of this algorithm.\n\n\n\u0002\u0004\u0005\n\n\n\n\nKaelbling, Littman, & Moore\n\n\n\u0002.\u0002 Ad-Ho c Techniques\n\n\nIn reinforcement-learning practice, some simple, ad hoc strategies have b een p opular. They\n\nare rarely, if ever, the b est choice for the mo dels of optimality we have used, but they may\n\nb e viewed as reasonable, computationally tractable, heuristics. Thrun (\u0001 \u0002) has surveyed\n\na variety of these techniques.\n\n\n\u0002.\u0002.\u0001 Greedy Strategies\n\n\nThe �rst strategy that comes to mind is to always cho ose the action with the highest esti\nmated payo�. The �aw is that early unlucky sampling might indicate that the b est action's\n\nreward is less than the reward obtained from a sub optimal action. The sub optimal action\n\nwill always b e picked, leaving the true optimal action starved of data and its sup eriority\n\nnever discovered. An agent must explore to ameliorate this outcome.\n\nA useful heuristic is optimism in the face of uncertainty in which actions are selected\n\ngreedily, but strongly optimistic prior b eliefs are put on their payo�s so that strong negative\n\nevidence is needed to eliminate an action from consideration. This still has a measurable\n\n\ndanger of starving an optimal but unlucky action, but the risk of this can b e made arbitrar\nily small. Techniques like this have b een used in several reinforcement learning algorithms\n\nincluding the interval exploration metho d (Kaelbling, \u0001 \u0003b) (describ ed shortly), the ex\nploration bonus in Dyna (Sutton, \u0001 0), curiosity-driven exploration (Schmidhub er, \u0001 \u0001a),\n\nand the exploration mechanism in prioritized sweeping (Mo ore & Atkeson, \u0001 \u0003).\n\n\n\u0002.\u0002.\u0002 Randomized Strategies\n\n\nAnother simple exploration strategy is to take the action with the b est estimated exp ected\n\nreward by default, but with probability p, cho ose an action at random. Some versions of\n\nthis strategy start with a large value of p to encourage initial exploration, which is slowly\n\ndecreased.\n\n\nAn ob jection to the simple strategy is that when it exp eriments with a non-greedy action\n\nit is no more likely to try a promising alternative than a clearly hop eless alternative. A\n\nslightly more sophisticated strategy is Boltzmann exploration. In this case, the exp ected\n\nreward for taking action a, E R(a) is used to cho ose an action probabilisticall y according to\n\nthe distribution\n\n\n\nP (a) =\n\n\n\nP\n\n\n\neE R(a)=T\n\n\n\n:\n\n\n\n0 \u0002A\n\n\n\n0\n\n\n\na0\n\n\n\n0\n\n\n\neE R(a0\n\n\n\n0 )=T\n\n\n\nThe temperature parameter T can b e decreased over time to decrease exploration. This\n\nmetho d works well if the b est action is well separated from the others, but su�ers somewhat\n\nwhen the values of the actions are close. It may also converge unnecessarily slowly unless\n\nthe temp erature schedule is manually tuned with great care.\n\n\n\u0002.\u0002.\u0003 Interval-based Techniques\n\n\nExploration is often more e�cient when it is based on second-order information ab out the\n\ncertainty or variance of the estimated values of actions. Kaelbling's interval estimation\n\n\n\nalgorithm (\u0001 \u0003b) stores statistics for each action ai : wi\n\n\n\nis the numb er of successes and ni\n\n\n\nthe numb er of trials. An action is chosen by computing the upp er b ound of a \u000100 � (\u0001 � �)%\n\n\n\u0002\u0004\u0006\n\n\n\n\nReinforcement Learning: A Survey\n\n\ncon�dence interval on the success probability of each action and cho osing the action with\n\nthe highest upp er b ound. Smaller values of the � parameter encourage greater exploration.\n\nWhen payo�s are b o olean, the normal approximation to the binomial distribution can b e\n\nused to construct the con�dence interval (though the binomial should b e used for small\n\nn). Other payo� distributions can b e handled using their asso ciated statistics or with\n\nnonparametric metho ds. The metho d works very well in empirical trials. It is also related\n\nto a certain class of statistical techniques known as experiment design metho ds (Box &\n\nDrap er, \u0001 \b\u0007), which are used for comparing multiple treatments (for example, fertilizers\n\nor drugs) to determine which treatment (if any) is b est in as small a set of exp eriments as\n\np ossible.\n\n\n\u0002.\u0003 More General Problems\n\n\nWhen there are multiple states, but reinforcement is still immediate, then any of the ab ove\n\nsolutions can b e replicated, once for each state. However, when generalization is required,\n\nthese solutions must b e integrated with generalization metho ds (see section \u0006); this is\n\nstraightforward for the simple ad-ho c metho ds, but it is not understo o d how to maintain\n\ntheoretical guarantees.\n\nMany of these techniques fo cus on converging to some regime in which exploratory\n\nactions are taken rarely or never; this is appropriate when the environment is stationary.\n\nHowever, when the environment is non-stationary, exploration must continue to take place,\n\nin order to notice changes in the world. Again, the more ad-ho c techniques can b e mo di�ed\n\nto deal with this in a plausible manner (keep temp erature parameters from going to 0; decay\n\nthe statistics in interval estimation), but none of the theoretically guaranteed metho ds can\n\nb e applied.\n\n\n\u0003. Delayed Reward\n\n\nIn the general case of the reinforcement learning problem, the agent's actions determine\n\nnot only its immediate reward, but also (at least probabilistically) the next state of the\n\nenvironment. Such environments can b e thought of as networks of bandit problems, but\n\nthe agent must take into account the next state as well as the immediate reward when it\n\ndecides which action to take. The mo del of long-run optimality the agent is using determines\n\nexactly how it should take the value of the future into account. The agent will have to b e\n\nable to learn from delayed reinforcement: it may take a long sequence of actions, receiving\n\ninsigni�cant reinforcement, then �nally arrive at a state with high reinforcement. The agent\n\nmust b e able to learn which of its actions are desirable based on reward that can take place\n\narbitrarily far in the future.\n\n\n\u0003.\u0001 Markov Decision Pro cesses\n\n\nProblems with delayed reinforcement are well mo deled as Markov decision processes (MDPs).\n\nAn MDP consists of\n\n\n� a set of states S,\n\n\n� a set of actions A,\n\n\n\u0002\u0004\u0007\n\n\n\n\nKaelbling, Littman, & Moore\n\n\n� a reward function R : S � A ! <, and\n\n\n� a state transition function T : S � A ! �(S ), where a memb er of �(S ) is a probability\n\ndistribution over the set S (i.e. it maps states to probabilities). We write T (s; a; s0)\n\n\n\nfor the probability of making a transition from state s to state s0\n\n\n\nusing action a.\n\n\n\nThe state transition function probabilistically sp eci�es the next state of the environment as\n\na function of its current state and the agent's action. The reward function sp eci�es exp ected\n\ninstantaneous reward as a function of the current state and action. The mo del is Markov if\n\n\nthe state transitions are indep endent of any previous environment states or agent actions.\n\nThere are many go o d references to MDP mo dels (Bellman, \u0001 \u0005\u0007; Bertsekas, \u0001 \b\u0007; Howard,\n\n\u0001 \u00060; Puterman, \u0001 \u0004).\n\nAlthough general MDPs may have in�nite (even uncountable) state and action spaces,\n\nwe will only discuss metho ds for solving �nite-state and �nite-action problems. In section \u0006,\n\nwe discuss metho ds for solving problems with continuous input and output spaces.\n\n\n\u0003.\u0002 Finding a Policy Given a Mo del\n\n\nBefore we consider algorithms for learning to b ehave in MDP environments, we will ex\nplore techniques for determining the optimal p olicy given a correct mo del. These dynamic\n\nprogramming techniques will serve as the foundation and inspiration for the learning al\ngorithms to follow. We restrict our attention mainly to �nding optimal p olicies for the\n\nin�nite-horizon discounted mo del, but most of these algorithms have analogs for the �nite\nhorizon and average-case mo dels as well. We rely on the result that, for the in�nite-horizon\n\ndiscounted mo del, there exists an optimal deterministic stationary p olicy (Bellman, \u0001 \u0005\u0007).\n\nWe will sp eak of the optimal value of a state|it is the exp ected in�nite discounted sum\n\nof reward that the agent will gain if it starts in that state and executes the optimal p olicy.\n\nUsing � as a complete decision p olicy, it is written\n\n\n\nr\n\n\n\n(s) = max�\n\n\n\nE\n\n\n\nV\n\n\n\n�\n\n\n\n\u0001\n\nX �\n\n\nt=0\n\n\n\nt\n\n\n\nt\n\n\n\n!\n\n\n\n:\n\n\n\nThis optimal value function is unique and can b e de�ned as the solution to the simultaneous\n\nequations\n\n\n\n; \bs \u0002 S ; (\u0001)\n\n\n\n)\n\n\n\n\u0001\n\nA\n\n\n\n\u0001\n\n\n\n0R(s; a) + �\n\n@\n\n\n\nX\n\n\n\nT (s; a; s0\n\n\n\n0\n\n\n\n0 )V\n\n\n\n�\n\n\n\n0\n\n(s\n\n\n\n0\n\n\n\nV\n\n\n\n�\n\n\n\n(s) = max\n\na\n\n\n\n(s) = max\n\n\n\ns0\n\n\n\n\u0002S\n\n\n\nwhich assert that the value of a state s is the exp ected instantaneous reward plus the\n\nexp ected discounted value of the next state, using the b est available action. Given the\n\noptimal value function, we can sp ecify the optimal p olicy as\n\n\n\n\u0001\n\n)\n\nA\n\n\n\n�\n\n\n\n(s) = arg max\n\na\n\n\n\n0R(s; a) + �\n\n@\n\n\n\nX\n\n\n\nT (s; a; s0\n\n\n\n0\n\n\n\n0)V\n\n\n\n�\n\n\n\n0\n\n(s\n\n\n\n�\n\n\n\n:\n\n\n\ns0\n\n\n\n\u0002S\n\n\n\n\u0003.\u0002.\u0001 Value Iteration\n\n\nOne way, then, to �nd an optimal p olicy is to �nd the optimal value function. It can\n\nb e determined by a simple iterative algorithm called value iteration that can b e shown to\n\n\n\nvalues (Bellman, \u0001 \u0005\u0007; Bertsekas, \u0001 \b\u0007).\n\n\n\u0002\u0004\b\n\n\n\nconverge to the correct V\n\n\n\n�\n\n\n\n\nReinforcement Learning: A Survey\n\n\ninitialize V (s) arbitrarily\n\nloop until policy good enough\n\nloop for s \u0002 S\n\nloop for a \u0002 A\n\n\n\nQ(s; a) := R(s; a) + �\n\n\n\nPs0\n\n\n\n0 )\n\n\n\n0\n\n\n\nP\n\n\n\n0 \u0002S\n\n\n\n0\n\n\n\n0\n\n\n\nT (s; a; s0\n\n\n\n0 0\n\n)V (s\n\n\n\nV (s) := maxa\n\nend loop\n\nend loop\n\n\n\nQ(s; a)\n\n\n\nIt is not obvious when to stop the value iteration algorithm. One imp ortant result\n\nb ounds the p erformance of the current greedy p olicy as a function of the Bel lman residual of\n\nthe current value function (Williams & Baird, \u0001 \u0003b). It says that if the maximum di�erence\n\nb etween two successive value functions is less than �, then the value of the greedy p olicy,\n\n(the p olicy obtained by cho osing, in every state, the action that maximizes the estimated\n\ndiscounted reward, using the current estimate of the value function) di�ers from the value\n\nfunction of the optimal p olicy by no more than \u0002�� =(\u0001 � � ) at any state. This provides an\n\ne�ective stopping criterion for the algorithm. Puterman (\u0001 \u0004) discusses another stopping\n\ncriterion, based on the span semi-norm, which may result in earlier termination. Another\n\nimp ortant result is that the greedy p olicy is guaranteed to b e optimal in some �nite numb er\n\nof steps even though the value function may not have converged (Bertsekas, \u0001 \b\u0007). And in\n\npractice, the greedy p olicy is often optimal long b efore the value function has converged.\n\n\nValue iteration is very �exible. The assignments to V need not b e done in strict order\n\nas shown ab ove, but instead can o ccur asynchronously in parallel provided that the value\n\nof every state gets up dated in�nitely often on an in�nite run. These issues are treated\n\nextensively by Bertsekas (\u0001 \b ), who also proves convergence results.\n\n\nUp dates based on Equation \u0001 are known as ful l backups since they make use of infor\nmation from all p ossible successor states. It can b e shown that up dates of the form\n\n\n\n0\n\n; a\n\n\n\n) � Q(s; a))\n\n\n\nQ(s; a) := Q(s; a) + �(r + � max\n\n\n\n0\n\nQ(s\n\n\n\n0\n\n\n\na\n\n\n\n0\n\ncan also b e used as long as each pairing of a and s is up dated in�nitely often, s\n\n\n\nis sampled\n\n\n\n0\n\n\n\nfrom the distribution T (s; a; s0\n\n\n\n0\n\n), r is sampled with mean R(s; a) and b ounded variance, and\n\n\n\nthe learning rate � is decreased slowly. This typ e of sample backup (Singh, \u0001 \u0003) is critical\n\nto the op eration of the mo del-free metho ds discussed in the next section.\n\n\nThe computational complexity of the value-iteration algorithm with full backups, p er\n\niteration, is quadratic in the numb er of states and linear in the numb er of actions. Com\n\n\n0\n\n\n\nmonly, the transition probabilities T (s; a; s0\n\n\n\n0\n\n) are sparse. If there are on average a constant\n\n\n\nnumb er of next states with non-zero probability then the cost p er iteration is linear in the\n\nnumb er of states and linear in the numb er of actions. The numb er of iterations required to\n\nreach the optimal value function is p olynomial in the numb er of states and the magnitude\n\nof the largest reward if the discount factor is held constant. However, in the worst case\n\nthe numb er of iterations grows p olynomially in \u0001=(\u0001 � � ), so the convergence rate slows\n\nconsiderably as the discount factor approaches \u0001 (Littman, Dean, & Kaelbling, \u0001 \u0005b).\n\n\n\u0002\u0004\n\n\n\n\nKaelbling, Littman, & Moore\n\n\n\u0003.\u0002.\u0002 Policy Iteration\n\n\nThe policy iteration algorithm manipulates the p olicy directly, rather than �nding it indi\nrectly via the optimal value function. It op erates as follows:\n\n\n\nchoose an arbitrary policy � 0\n\nloop\n\n\n\n0\n\n� := �\n\n\ncompute the value function of policy � :\n\nsolve the linear equations\n\n\n\n\u0002S\n\n\n\n0\n\n\n\nV�\n\n\n\n(s) = R(s; � (s)) + �\n\n\n\nPs0\n\n\n\n�\n\n\n\nT (s; � (s); s0\n\n\n\n0)V\n\n\n\n0\n\n(s )\n\n\n\nimprove the policy at each state:\n\n\n\n0\n\n(s\n\n\n\n))\n\n\n\n\u0002S\n\n\n\n0\n\n\n\n�\n\n\n\n0\n\n(s) := arg max\n\n\n\na\n\n\n\nT (s; a; s0\n\n\n\n0 )V�\n\n\n\n(R(s; a) + �\n\n\n\nPs0\n\n\n\nuntil � = � 0\n\n\nThe value function of a p olicy is just the exp ected in�nite discounted reward that will\n\nb e gained, at each state, by executing that p olicy. It can b e determined by solving a set\n\nof linear equations. Once we know the value of each state under the current p olicy, we\n\nconsider whether the value could b e improved by changing the �rst action taken. If it can,\n\nwe change the p olicy to take the new action whenever it is in that situation. This step is\n\nguaranteed to strictly improve the p erformance of the p olicy. When no improvements are\n\np ossible, then the p olicy is guaranteed to b e optimal.\n\n\n\ndistinct p olicies, and the sequence of p olicies improves at\n\n\n\nSince there are at most jAj\n\n\n\njS j\n\n\n\neach step, this algorithm terminates in at most an exp onential numb er of iterations (Puter\nman, \u0001 \u0004). However, it is an imp ortant op en question how many iterations p olicy iteration\n\ntakes in the worst case. It is known that the running time is pseudop olynomial and that for\n\nany �xed discount factor, there is a p olynomial b ound in the total size of the MDP (Littman\n\net al., \u0001 \u0005b).\n\n\n\u0003.\u0002.\u0003 Enhancement to Value Iteration and Policy Iteration\n\n\nIn practice, value iteration is much faster p er iteration, but p olicy iteration takes fewer\n\niterations. Arguments have b een put forth to the e�ect that each approach is b etter for\n\nlarge problems. Puterman's modi�ed policy iteration algorithm (Puterman & Shin, \u0001 \u0007\b)\n\nprovides a metho d for trading iteration time for iteration improvement in a smo other way.\n\nThe basic idea is that the exp ensive part of p olicy iteration is solving for the exact value\n\n\n\nof V� . Instead of �nding an exact value for V�\n\n\n\n, we can p erform a few steps of a mo di�ed\n\n\n\nvalue-iteration step where the p olicy is held �xed over successive iterations. This can b e\n\n\n\nshown to pro duce an approximation to V�\n\nresult in substantial sp eedups.\n\n\n\nthat converges linearly in � . In practice, this can\n\n\n\nSeveral standard numerical-analysis techniques that sp eed the convergence of dynamic\n\nprogramming can b e used to accelerate value and p olicy iteration. Multigrid methods can\n\nb e used to quickly seed a go o d initial approximation to a high resolution value function\n\nby initially p erforming value iteration at a coarser resolution (Rude,� \u0001 \u0003). State aggre\ngation works by collapsing groups of states to a single meta-state solving the abstracted\n\nproblem (Bertsekas & Castanon,~ \u0001 \b ).\n\n\n\u0002\u00050\n\n\n\n\nReinforcement Learning: A Survey\n\n\n\u0003.\u0002.\u0004 Computational Complexity\n\n\nValue iteration works by pro ducing successive approximations of the optimal value function.\n\n\n\nEach iteration can b e p erformed in O (jAjjS j\n\n\n\n\u0002\n\n) steps, or faster if there is sparsity in the\n\n\n\ntransition function. However, the numb er of iterations required can grow exp onentially in\n\nthe discount factor (Condon, \u0001 \u0002); as the discount factor approaches \u0001, the decisions must\n\nb e based on results that happ en farther and farther into the future. In practice, p olicy\n\niteration converges in fewer iterations than value iteration, although the p er-iteration costs\n\n\n\nof O (jAjjS j\u0002\n\n\n\n+ jS j\u0003\n\n\n\n) can b e prohibitive. There is no known tight worst-case b ound available\n\n\n\nfor p olicy iteration (Littman et al., \u0001 \u0005b). Mo di�ed p olicy iteration (Puterman & Shin,\n\n\u0001 \u0007\b) seeks a trade-o� b etween cheap and e�ective iterations and is preferred by some\n\npractictioners (Rust, \u0001 \u0006).\n\nLinear programming (Schrijver, \u0001 \b\u0006) is an extremely general problem, and MDPs can\n\nb e solved by general-purp ose linear-programming packages (Derman, \u0001 \u00070; D'Ep enoux,\n\n\u0001 \u0006\u0003; Ho�man & Karp, \u0001 \u0006\u0006). An advantage of this approach is that commercial-quality\n\nlinear-programming packages are available, although the time and space requirements can\n\nstill b e quite high. From a theoretic p ersp ective, linear programming is the only known\n\nalgorithm that can solve MDPs in p olynomial time, although the theoretically e�cient\n\nalgorithms have not b een shown to b e e�cient in practice.\n\n\n\u0004. Learning an Optimal Policy: Mo del-free Metho ds\n\n\nIn the previous section we reviewed metho ds for obtaining an optimal p olicy for an MDP\n\nassuming that we already had a mo del. The mo del consists of knowledge of the state tran\n\n\nsition probability function T (s; a; s\n\n\n\n0) and the reinforcement function R(s; a). Reinforcement\n\n\n\nlearning is primarily concerned with how to obtain the optimal p olicy when such a mo del\n\nis not known in advance. The agent must interact with its environment directly to obtain\n\ninformation which, by means of an appropriate algorithm, can b e pro cessed to pro duce an\n\noptimal p olicy.\n\nAt this p oint, there are two ways to pro ceed.\n\n\n� Mo del-free: Learn a controller without learning a mo del.\n\n\n� Mo del-based: Learn a mo del, and use it to derive a controller.\n\n\nWhich approach is b etter? This is a matter of some debate in the reinforcement-learning\n\ncommunity. A numb er of algorithms have b een prop osed on b oth sides. This question also\n\napp ears in other �elds, such as adaptive control, where the dichotomy is b etween direct and\n\nindirect adaptive control.\n\nThis section examines mo del-free learning, and Section \u0005 examines mo del-based meth\no ds.\n\n\nThe biggest problem facing a reinforcement-learning agent is temporal credit assignment.\n\nHow do we know whether the action just taken is a go o d one, when it might have far\nreaching e�ects? One strategy is to wait until the \\end\" and reward the actions taken if\n\nthe result was go o d and punish them if the result was bad. In ongoing tasks, it is di�cult\n\nto know what the \\end\" is, and this might require a great deal of memory. Instead, we\n\nwill use insights from value iteration to adjust the estimated value of a state based on\n\n\n\u0002\u0005\u0001\n\n\n\n\nKaelbling, Littman, & Moore\n\n### _r_\n\n\nFigure \u0004: Architecture for the adaptive heuristic critic.\n\n\nthe immediate reward and the estimated value of the next state. This class of algorithms\n\nis known as temporal di�erence methods (Sutton, \u0001 \b\b). We will consider two di�erent\n\ntemp oral-di�erence learning strategies for the discounted in�nite-horizon mo del.\n\n\n\u0004.\u0001 Adaptive Heuristic Critic and TD(�)\n\n\nThe adaptive heuristic critic algorithm is an adaptive version of p olicy iteration (Barto,\n\nSutton, & Anderson, \u0001 \b\u0003) in which the value-function computation is no longer imple\nmented by solving a set of linear equations, but is instead computed by an algorithm called\n\nT D (0). A blo ck diagram for this approach is given in Figure \u0004. It consists of two comp o\nnents: a critic (lab eled AHC), and a reinforcement-learning comp onent (lab eled RL). The\n\nreinforcement-learning comp onent can b e an instance of any of the k -armed bandit algo\nrithms, mo di�ed to deal with multiple states and non-stationary rewards. But instead of\n\nacting to maximize instantaneous reward, it will b e acting to maximize the heuristic value,\n\nv, that is computed by the critic. The critic uses the real external reinforcement signal to\n\nlearn to map states to their exp ected discounted values given that the p olicy b eing executed\n\nis the one currently instantiated in the RL comp onent.\n\nWe can see the analogy with mo di�ed p olicy iteration if we imagine these comp onents\n\nworking in alternation. The p olicy � implemented by RL is �xed and the critic learns the\n\n\n\nvalue function V�\n\n\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-15-0.png)\n\nfor that p olicy. Now we �x the critic and let the RL comp onent learn a\n\n\n\n0\n\nnew p olicy �\n\n\n\nthat maximizes the new value function, and so on. In most implementations,\n\n\n\nhowever, b oth comp onents op erate simultaneously. Only the alternating implementation\n\ncan b e guaranteed to converge to the optimal p olicy, under appropriate conditions. Williams\n\nand Baird explored the convergence prop erties of a class of AHC-related algorithms they\n\ncall \\incremental variants of p olicy iteration\" (Williams & Baird, \u0001 \u0003a).\n\n\n\n0\n\nIt remains to explain how the critic can learn the value of a p olicy. We de�ne hs; a; r; s i\n\n\n\nto b e an experience tuple summarizing a single transition in the environment. Here s is the\n\nagent's state b efore the transition, a is its choice of action, r the instantaneous reward it\n\n\n\nreceives, and s0\n\n\n\nits resulting state. The value of a p olicy is learned using Sutton's T D (0)\n\n\n\nalgorithm (Sutton, \u0001 \b\b) which uses the up date rule\n\n\n\n0\n\nV (s) := V (s) + �(r + � V (s\n\n\n\n) � V (s)) :\n\n\n\n0\n\nWhenever a state s is visited, its estimated value is up dated to b e closer to r + � V (s ),\n\n\n\nsince r is the instantaneous reward received and V (s0\n\n\n\n) is the estimated value of the actually\n\n\n\no ccurring next state. This is analogous to the sample-backup rule from value iteration|the\n\nonly di�erence is that the sample is drawn from the real world rather than by simulating\n\n\n\n0\n\na known mo del. The key idea is that r + � V (s\n\n\n\u0002\u0005\u0002\n\n\n\n) is a sample of the value of V (s), and it is\n\n\n\n\nReinforcement Learning: A Survey\n\n\nmore likely to b e correct b ecause it incorp orates the real r . If the learning rate � is adjusted\n\nprop erly (it must b e slowly decreased) and the p olicy is held �xed, T D (0) is guaranteed to\n\nconverge to the optimal value function.\n\nThe T D (0) rule as presented ab ove is really an instance of a more general class of\n\nalgorithms called T D (�), with � = 0. T D (0) lo oks only one step ahead when adjusting\n\nvalue estimates; although it will eventually arrive at the correct answer, it can take quite a\n\nwhile to do so. The general T D (�) rule is similar to the T D (0) rule given ab ove,\n\n\n\n0\n\nV (u) := V (u) + �(r + � V (s\n\n\n\n) � V (s))e(u) ;\n\n\n\nbut it is applied to every state according to its eligibili ty e(u), rather than just to the\n\nimmediately previous state, s. One version of the eligibil ity trace is de�ned to b e\n\n\n\n�\n\ns;sk\n\n\n\ne(s) =\n\n\n\nt\n\nX (�� )\n\nk =\u0001\n\n\n\nt�k\n\n\n\n, where �\n\n\n\ns;sk\n\n\n\n(\n\n\n\n\u0001 if s = sk\n\n\n0 otherwise\n\n\n\n.\n\n\n\n=\n\n\n\nThe eligibili ty of a state s is the degree to which it has b een visited in the recent past;\n\nwhen a reinforcement is received, it is used to up date all the states that have b een recently\n\nvisited, according to their eligibili ty. When � = 0 this is equivalent to T D (0). When � = \u0001,\n\nit is roughly equivalent to up dating all the states according to the numb er of times they\n\nwere visited by the end of a run. Note that we can up date the eligibili ty online as follows:\n\n\n\ne(s) :=\n\n\n\n(\n\n\n\n� �e(s) + \u0001 if s = current state\n\n� �e(s) otherwise\n\n\n\n.\n\n\n\nIt is computationally more exp ensive to execute the general T D (�), though it often\n\nconverges considerably faster for large � (Dayan, \u0001 \u0002; Dayan & Sejnowski, \u0001 \u0004). There\n\nhas b een some recent work on making the up dates more e�cient (Cichosz & Mulawka, \u0001 \u0005)\n\nand on changing the de�nition to make T D (�) more consistent with the certainty-equivalent\n\nmetho d (Singh & Sutton, \u0001 \u0006), which is discussed in Section \u0005.\u0001.\n\n\n\u0004.\u0002 Q-learning\n\n\nThe work of the two comp onents of AHC can b e accomplished in a uni�ed manner by\n\nWatkins' Q-learning algorithm (Watkins, \u0001 \b ; Watkins & Dayan, \u0001 \u0002). Q-learning is\n\ntypically easier to implement. In order to understand Q-learning, we have to develop some\n\n\n\nadditional notation. Let Q� (s; a) b e the exp ected discounted reinforcement of taking action\n\n\n\na in state s, then continuing by cho osing actions optimally. Note that V\n\n\n\n� (s) is the value\n\n\n\n(s; a) can\n\n\n\nof s assuming the b est action is taken initially, and so V\n\nhence b e written recursively as\n\n\n\n�\n\n\n\n(s) = maxa\n\n\n\nQ�\n\n\n\n(s; a). Q�\n\n\n\n0\n\n\n\n0\n\n; a ) :\n\n\n\n(s; a) = R(s; a) + �\n\n\n\nX\n\n\n\nT (s; a; s0\n\n\n\n) max\n\n\n\n0\n\n(s\n\n\n\nQ\n\n\n\n�\n\n\n\nQ�\n\n\n\ns0\n\n\n\n\u0002S\n\n\n\na\n\n\n\nQ� (s; a), we have �\n\n\n\n(s) = arg max\n\n\n\nNote also that, since V\n\noptimal p olicy.\n\n\n\n(s; a) as an\n\n\n\n�\n\n\n\n(s) = maxa\n\n\n\n�\n\n\n\na\n\n\n\nQ�\n\n\n\nBecause the Q function makes the action explicit, we can estimate the Q values on\nline using a metho d essentially the same as T D (0), but also use them to de�ne the p olicy,\n\n\n\u0002\u0005\u0003\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nb ecause an action can b e chosen just by taking the one with the maximum Q value for the\n\n\ncurrent state.\n\n\nThe Q-learning rule is\n\n\n\n0\n\n; a\n\n\n\n0\n\n\n\n0 ) � Q(s; a)) ;\n\n\n\nQ(s; a) := Q(s; a) + �(r + � max\n\n\n\n0\n\nQ(s\n\n\n\n0\n\n\n\na\n\n\n\n0i is an exp erience tuple as describ ed earlier. If each action is executed in\n\n\n\n0\n\n\n\nwhere hs; a; r; s0\n\n\n\neach state an in�nite numb er of times on an in�nite run and � is decayed appropriately, the\n\n\n\nQ values will converge with probability \u0001 to Q�\n\n\n\n(Watkins, \u0001 \b ; Tsitsiklis, \u0001 \u0004; Jaakkola,\n\n\n\nJordan, & Singh, \u0001 \u0004). Q-learning can also b e extended to up date states that o ccurred\n\nmore than one step previously, as in T D (�) (Peng & Williams, \u0001 \u0004).\n\nWhen the Q values are nearly converged to their optimal values, it is appropriate for\n\nthe agent to act greedily, taking, in each situation, the action with the highest Q value.\n\nDuring learning, however, there is a di�cult exploitation versus exploration trade-o� to b e\n\nmade. There are no go o d, formally justi�ed approaches to this problem in the general case;\n\nstandard practice is to adopt one of the ad hoc metho ds discussed in section \u0002.\u0002.\n\nAHC architectures seem to b e more di�cult to work with than Q-learning on a practical\n\nlevel. It can b e hard to get the relative learning rates right in AHC so that the two\n\ncomp onents converge together. In addition, Q-learning is exploration insensitive: that\n\nis, that the Q values will converge to the optimal values, indep endent of how the agent\n\nb ehaves while the data is b eing collected (as long as all state-action pairs are tried often\n\nenough). This means that, although the exploration-exploitation issue must b e addressed\n\nin Q-learning, the details of the exploration strategy will not a�ect the convergence of the\n\nlearning algorithm. For these reasons, Q-learning is the most p opular and seems to b e the\n\nmost e�ective mo del-free algorithm for learning from delayed reinforcement. It do es not,\n\nhowever, address any of the issues involved in generalizing over large state and/or action\n\nspaces. In addition, it may converge quite slowly to a go o d p olicy.\n\n\n\u0004.\u0003 Mo del-free Learning With Average Reward\n\n\nAs describ ed, Q-learning can b e applied to discounted in�nite-horizon MDPs. It can also\n\nb e applied to undiscounted problems as long as the optimal p olicy is guaranteed to reach a\n\nreward-free absorbing state and the state is p erio dicall y reset.\n\nSchwartz (\u0001 \u0003) examined the problem of adapting Q-learning to an average-reward\n\nframework. Although his R-learning algorithm seems to exhibit convergence problems for\n\nsome MDPs, several researchers have found the average-reward criterion closer to the true\n\nproblem they wish to solve than a discounted criterion and therefore prefer R-learning to\n\nQ-learning (Mahadevan, \u0001 \u0004).\n\nWith that in mind, researchers have studied the problem of learning optimal average\nreward p olicies. Mahadevan (\u0001 \u0006) surveyed mo del-based average-reward algorithms from\n\na reinforcement-learning p ersp ective and found several di�culties with existing algorithms.\n\nIn particular, he showed that existing reinforcement-learning algorithms for average reward\n\n(and some dynamic programming algorithms) do not always pro duce bias-optimal p oli\ncies. Jaakkola, Jordan and Singh (\u0001 \u0005) describ ed an average-reward learning algorithm\n\nwith guaranteed convergence prop erties. It uses a Monte-Carlo comp onent to estimate the\n\nexp ected future reward for each state as the agent moves through the environment. In\n\n\n\u0002\u0005\u0004\n\n\n\n\nReinforcement Learning: A Survey\n\n\naddition, Bertsekas presents a Q-learning-like algorithm for average-case reward in his new\n\ntextb o ok (\u0001 \u0005). Although this recent work provides a much needed theoretical foundation\n\nto this area of reinforcement learning, many imp ortant problems remain unsolved.\n\n\n\u0005. Computing Optimal Policies by Learning Mo dels\n\n\nThe previous section showed how it is p ossible to learn an optimal p olicy without knowing\n\n\n\n0\n\n\n\nthe mo dels T (s; a; s0\n\n\n\n0\n\n) or R(s; a) and without even learning those mo dels en route. Although\n\n\n\nmany of these metho ds are guaranteed to �nd optimal p olicies eventually and use very\n\nlittle computation time p er exp erience, they make extremely ine�cient use of the data they\n\ngather and therefore often require a great deal of exp erience to achieve go o d p erformance.\n\nIn this section we still b egin by assuming that we don't know the mo dels in advance, but\n\nwe examine algorithms that do op erate by learning these mo dels. These algorithms are\n\nesp ecially imp ortant in applications in which computation is considered to b e cheap and\n\nreal-world exp erience costly.\n\n\n\u0005.\u0001 Certainty Equivalent Metho ds\n\n\nWe b egin with the most conceptually straightforward metho d: �rst, learn the T and R\n\nfunctions by exploring the environment and keeping statistics ab out the results of each\n\naction; next, compute an optimal p olicy using one of the metho ds of Section \u0003. This\n\nmetho d is known as certainty equivlance (Kumar & Varaiya, \u0001 \b\u0006).\n\nThere are some serious ob jections to this metho d:\n\n\n� It makes an arbitrary division b etween the learning phase and the acting phase.\n\n\n� How should it gather data ab out the environment initially? Random exploration\n\nmight b e dangerous, and in some environments is an immensely ine�cient metho d of\n\ngathering data, requiring exp onentially more data (Whitehead, \u0001 \u0001) than a system\n\nthat interleaves exp erience gathering with p olicy-buil din g more tightly (Ko enig &\n\nSimmons, \u0001 \u0003). See Figure \u0005 for an example.\n\n\n� The p ossibility of changes in the environment is also problematic. Breaking up an\n\nagent's life into a pure learning and a pure acting phase has a considerable risk that\n\nthe optimal controller based on early life b ecomes, without detection, a sub optimal\n\ncontroller if the environment changes.\n\n\nA variation on this idea is certainty equivalence, in which the mo del is learned continually\n\nthrough the agent's lifetime and, at each step, the current mo del is used to compute an\n\noptimal p olicy and value function. This metho d makes very e�ective use of available data,\n\nbut still ignores the question of exploration and is extremely computationally demanding,\n\neven for fairly small state spaces. Fortunately, there are a numb er of other mo del-based\n\nalgorithms that are more practical.\n\n\n\u0005.\u0002 Dyna\n\n\nSutton's Dyna architecture (\u0001 0, \u0001 \u0001) exploits a middle ground, yielding strategies that\n\nare b oth more e�ective than mo del-free learning and more computationally e�cient than\n\n\n\u0002\u0005\u0005\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nFigure \u0005: In this environment, due to Whitehead (\u0001 \u0001), random exploration would take\n\n\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-19-0.png)\n\ntake O (\u0002n\n\n\n\n) steps to reach the goal even once, whereas a more intelligent explo\n\n\nration strategy (e.g. \\assume any untried action leads directly to goal\") would\n\n\n\n\u0002\n\nrequire only O (n ) steps.\n\n\n\nthe certain^ ty-equivalence approach. It simultaneously uses exp erience to build a mo del (\n\n\n\nT^\n\n\n\nand\n\n\n\nR), uses exp erience to adjust the p olicy, and uses the mo del to adjust the p olicy.\n\n\n\nDyna op erates in a lo op of interaction with the environment. Given an exp erience tuple\n\n\n\n0\n\nhs; a; s ; r i, it b ehaves as follows:\n\n\n0\n\n� Up date the mo del, incrementing statistics for the transition from s to s\n\n\n\non action a\n\n\n^\n\n\n\nand for^ receiving reward r for taking action a in state s. The up dated mo dels are\n\n\n\nT\n\n\n\nand\n\n\n\nR.\n\n\n\n� Up date the p olicy at state s based on the newly up dated mo del using the rule\n\n\n\nX\n\n\n\n^\n\n\n\na\n\n\n\n0\n\n; a ) ;\n\n\n\n0\n\n\n\nQ(s; a) :=\n\n\n\n^\n\nR(s; a) + �\n\n\n\nT (s; a; s0\n\n\n\n0\n\n\n\n0\n\n) max\n\n\n\n0\n\nQ(s\n\n\n\ns0\n\n\n\nwhich is a version of the value-iteration up date for Q values.\n\n\n� Perform k additional up dates: cho ose k state-action pairs at random and up date them\n\naccording to the same rule as b efore:\n\n\n\n) :\n\n\n\n0\n\n\n\nQ(sk\n\n\n\nX T^ (sk\n\ns0\n\n\n\n0\n\nQ(s\n\n\n\n0\n\n; a\n\n\n\n; ak\n\n\n\n^\n\n):=R(sk\n\n\n\n; ak\n\n\n\n) + �\n\n\n\n; ak\n\n\n\n; a\n\n\n\n0\n\n; s\n\n\n\n0\n\n\n\n0\n\n) max\n\n\n\na\n\n\n\n0\n\n� Cho ose an action a\n\n\n\nto p erform in state s0, based on the Q values but p erhaps mo di�ed\n\n\n\nby an exploration strategy.\n\n\nThe Dyna algorithm requires ab out k times the computation of Q-learning p er instance,\n\nbut this is typically vastly less than for the naive mo del-based metho d. A reasonable value\n\nof k can b e determined based on the relative sp eeds of computation and of taking action.\n\nFigure \u0006 shows a grid world in which in each cell the agent has four actions (N, S, E,\n\nW) and transitions are made deterministically to an adjacent cell, unless there is a blo ck,\n\nin which case no movement o ccurs. As we will see in Table \u0001, Dyna requires an order of\n\nmagnitude fewer steps of exp erience than do es Q-learning to arrive at an optimal p olicy.\n\nDyna requires ab out six times more computational e�ort, however.\n\n\n\u0002\u0005\u0006\n\n\n\n\nReinforcement Learning: A Survey\n\n\nFigure \u0006: A \u0003\u0002\u0007\u0007-state grid world. This was formulated as a shortest-path reinforcement\nlearning problem, which yields the same result as if a reward of \u0001 is given at the\n\ngoal, a reward of zero elsewhere and a discount factor is used.\n\n\nSteps b efore Backups b efore\n\n\nconvergence convergence\n\nQ-learning \u0005\u0003\u0001,000 \u0005\u0003\u0001,000\n\nDyna \u0006\u0002,000 \u0003,0\u0005\u0005,000\n\nprioritized sweeping \u0002\b,000 \u0001,0\u00010,000\n\n\nTable \u0001: The p erformance of three algorithms describ ed in the text. All metho ds used\n\nthe exploration heuristic of \\optimism in the face of uncertainty\": any state not\n\npreviously visited was assumed by default to b e a goal state. Q-learning used\n\nits optimal learning rate parameter for a deterministic maze: � = \u0001. Dyna and\n\nprioritized sweeping were p ermitted to take k = \u000200 backups p er transition. For\n\nprioritized sweeping, the priority queue often emptied b efore all backups were\n\nused.\n\n\n\u0002\u0005\u0007\n\n\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-20-0.png)\n\n\nKaelbling, Littman, & Moore\n\n\n\u0005.\u0003 Prioritized Sweeping / Queue-Dyna\n\n\nAlthough Dyna is a great improvement on previous metho ds, it su�ers from b eing relatively\n\nundirected. It is particularly unhelpful when the goal has just b een reached or when the\n\nagent is stuck in a dead end; it continues to up date random state-action pairs, rather than\n\nconcentrating on the \\interesting\" parts of the state space. These problems are addressed\n\nby prioritized sweeping (Mo ore & Atkeson, \u0001 \u0003) and Queue-Dyna (Peng & Williams,\n\n\u0001 \u0003), which are two indep endently-devel op ed but very similar techniques. We will describ e\n\nprioritized sweeping in some detail.\n\nThe algorithm is similar to Dyna, except that up dates are no longer chosen at random\n\nand values are now asso ciated with states (as in value iteration) instead of state-action pairs\n\n(as in Q-learning). To make appropriate choices, we must store additional information in\n\nthe mo del. Each state rememb ers its predecessors: the states that have a non-zero transition\n\nprobability to it under some action. In addition, each state has a priority, initially set to\n\n\nzero.\n\n\nInstead of up dating k random state-action pairs, prioritized sweeping up dates k states\n\nwith the highest priority. For each high-priority state s, it works as follows:\n\n\n\u0006\n\n\n\n� Rememb er the current value of the state: Vold\n\n\n� Up date the state's value\n\n\n\u0006\n\n\n\n= V (s).\n\n\n\u0006\n\n\n\n0\n\n\n\u0006\n\n\n\n0 0\n\n)V (s\n\n\n\u0006\n\n\n\nX\n\n\n\u0006\n\n\n\n^\n\n\n\u0006\n\n\n\n0\n\n\n\u0006\n\n\n\n0 )\n\n\n\u0006\n\n\n\n!\n\n\n\u0006\n\n\n\nV (s) := max\n\na\n\n\n\u0006\n\n\n\n^\n\nR(s; a) + �\n\n\n\u0006\n\n\n\nT (s; a; s0\n\n\n\u0006\n\n\n\n:\n\n\n\u0006\n\n\n\n� Set the state's priority back to 0.\n\n\n� Compute the value change � = jVold\n\n\n\u0006\n\n\n\ns0\n\n\n� V (s)j.\n\n\n\u0006\n\n\n\n� Use � to mo dify the priorities of the predecessors of s.\n\n\n\u0006\n\n\n\nIf we have up dated the V value for state s0\n\n\n\u0006\n\n\n\nand it has changed by amount �, then the\n\n\n\u0006\n\n\n\nimmediate predecessors^of s0 \u0006\n\n\n\nare \u0006 informed of this event. Any state s for^ which there exists\n\n\n\n\u0006 T (s; a; s0\n\n\n\n\u0006 0\n\n\n\n\u0006 0), unless its\n\n\n\n0) =\u0006 0 has its priority promoted to � �\n\n\n\nan action a such that \u0006\n\n\n\nT (s; a; s \u0006\n\n\n\n\u0006\n\npriority already exceeded that value.\n\n\nThe global b ehavior of this algorithm is that when a real-world transition is \\surprising\"\n\n(the agent happ ens up on a goal state, for instance), then lots of computation is directed\n\nto propagate this new information back to relevant predecessor states. When the real\nworld transition is \\b oring\" (the actual result is very similar to the predicted result), then\n\ncomputation continues in the most deserving part of the space.\n\nRunning prioritized sweeping on the problem in Figure \u0006, we see a large improvement\n\nover Dyna. The optimal p olicy is reached in ab out half the numb er of steps of exp erience\n\nand one-third the computation as Dyna required (and therefore ab out \u00020 times fewer steps\n\nand twice the computational e�ort of Q-learning).\n\n\n\u0002\u0005\b\n\n\n\n\nReinforcement Learning: A Survey\n\n\n\u0005.\u0004 Other Mo del-Based Metho ds\n\n\nMetho ds prop osed for solving MDPs given a mo del can b e used in the context of mo del\nbased metho ds as well.\n\n\nRTDP (real-time dynamic programming) (Barto, Bradtke, & Singh, \u0001 \u0005) is another\n\nmo del-based metho d that uses Q-learning to concentrate computational e�ort on the areas\n\nof the state-space that the agent is most likely to o ccupy. It is sp eci�c to problems in which\n\nthe agent is trying to achieve a particular goal state and the reward everywhere else is 0.\n\nBy taking into account the start state, it can �nd a short path from the start to the goal,\n\nwithout necessarily visiting the rest of the state space.\n\nThe Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, \u0001 \u0003; Kirman,\n\n\u0001 \u0004) exploits a similar intuition. It starts by making an approximate version of the MDP\n\nwhich is much smaller than the original one. The approximate MDP contains a set of states,\n\ncalled the envelope, that includes the agent's current state and the goal state, if there is one.\n\nStates that are not in the envelop e are summarized by a single \\out\" state. The planning\n\npro cess is an alternation b etween �nding an optimal p olicy on the approximate MDP and\n\nadding useful states to the envelop e. Action may take place in parallel with planning, in\n\nwhich case irrelevant states are also pruned out of the envelop e.\n\n\n\u0006. Generalization\n\n\nAll of the previous discussion has tacitly assumed that it is p ossible to enumerate the state\n\nand action spaces and store tables of values over them. Except in very small environments,\n\nthis means impractical memory requirements. It also makes ine�cient use of exp erience. In\n\na large, smo oth state space we generally exp ect similar states to have similar values and sim\nilar optimal actions. Surely, therefore, there should b e some more compact representation\n\nthan a table. Most problems will have continuous or large discrete state spaces; some will\n\nhave large or continuous action spaces. The problem of learning in large spaces is addressed\n\nthrough generalization techniques, which allow compact storage of learned information and\n\ntransfer of knowledge b etween \\similar\" states and actions.\n\n\nThe large literature of generalization techniques from inductive concept learning can b e\n\napplied to reinforcement learning. However, techniques often need to b e tailored to sp eci�c\n\ndetails of the problem. In the following sections, we explore the application of standard\n\nfunction-approximation techniques, adaptive resolution mo dels, and hierarchical metho ds\n\nto the problem of reinforcement learning.\n\n\nThe reinforcement-learning architectures and algorithms discussed ab ove have included\n\nthe storage of a variety of mappings, including S ! A (p olicies), S ! < (value functions),\n\nS � A ! < (Q functions and rewards), S � A ! S (deterministic transitions), and S �\n\nA � S ! [0; \u0001] (transition probabilities). Some of these mappings, such as transitions and\n\nimmediate rewards, can b e learned using straightforward sup ervised learning, and can b e\n\nhandled using any of the wide variety of function-approximation techniques for sup ervised\n\nlearning that supp ort noisy training examples. Popular techniques include various neural\nnetwork metho ds (Rumelhart & McClelland, \u0001 \b\u0006), fuzzy logic (Berenji, \u0001 \u0001; Lee, \u0001 \u0001).\n\nCMAC (Albus, \u0001 \b\u0001), and lo cal memory-based metho ds (Mo ore, Atkeson, & Schaal, \u0001 \u0005),\n\nsuch as generalizations of nearest neighb or metho ds. Other mappings, esp ecially the p olicy\n\n\n\u0002\u0005\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nmapping, typically need sp ecialized algorithms b ecause training sets of input-output pairs\n\nare not available.\n\n\n\u0006.\u0001 Generalization over Input\n\n\nA reinforcement-learning agent's current state plays a central role in its selection of reward\nmaximizing actions. Viewing the agent as a state-free black b ox, a description of the\n\ncurrent state is its input. Dep ending on the agent architecture, its output is either an\n\naction selection, or an evaluation of the current state that can b e used to select an action.\n\nThe problem of deciding how the di�erent asp ects of an input a�ect the value of the output\n\nis sometimes called the \\structural credit-assignment\" problem. This section examines\n\napproaches to generating actions or evaluations as a function of a description of the agent's\n\n\ncurrent state.\n\n\nThe �rst group of techniques covered here is sp ecialized to the case when reward is not\n\ndelayed; the second group is more generally applicable.\n\n\n\u0006.\u0001.\u0001 Immediate Reward\n\n\nWhen the agent's actions do not in�uence state transitions, the resulting problem b ecomes\n\none of cho osing actions to maximize immediate reward as a function of the agent's current\n\nstate. These problems b ear a resemblance to the bandit problems discussed in Section \u0002\n\nexcept that the agent should condition its action selection on the current state. For this\n\nreason, this class of problems has b een describ ed as associative reinforcement learning.\n\nThe algorithms in this section address the problem of learning from immediate b o olean\n\nreinforcement where the state is vector valued and the action is a b o olean vector. Such\n\n\nalgorithms can and have b een used in the context of a delayed reinforcement, for instance,\n\nas the RL comp onent in the AHC architecture describ ed in Section \u0004.\u0001. They can also b e\n\ngeneralized to real-valued reward through reward comparison metho ds (Sutton, \u0001 \b\u0004).\n\n\nCRBP The complementary reinforcement backpropagation algorithm (Ackley & Littman,\n\n\u0001 0) (crbp) consists of a feed-forward network mapping an enco ding of the state to an\n\nenco ding of the action. The action is determined probabilistically from the activation of\n\n\n\nthe output units: if output unit i has activation yi\n\n\n\n, then bit i of the action vector has value\n\n\n\n\u0001 with probability yi\n\n\n\n, and 0 otherwise. Any neural-network sup ervised training pro cedure\n\n\n\ncan b e used to adapt the network as follows. If the result of generating action a is r = \u0001,\n\nthen the network is trained with input-output pair hs; ai. If the result is r = 0, then the\n\n\n\n� �\nnetwork is trained with input-output pair hs; ai, where a = (\u0001 � a\u0001\n\n\n\n; : : : ; \u0001 � an\n\n\n\n).\n\n\n\nThe idea b ehind this training rule is that whenever an action fails to generate reward,\n\ncrbp will try to generate an action that is di�erent from the current choice. Although it\n\nseems like the algorithm might oscillate b etween an action and its complement, that do es\n\nnot happ en. One step of training a network will only change the action slightly and since\n\nthe output probabilities will tend to move toward 0.\u0005, this makes action selection more\n\nrandom and increases search. The hop e is that the random distribution will generate an\n\naction that works b etter, and then that action will b e reinforced.\n\n\nARC The asso ciative reinforcement comparison (arc) algorithm (Sutton, \u0001 \b\u0004) is an\n\ninstance of the ahc architecture for the case of b o olean actions, consisting of two feed\n\n\u0002\u00060\n\n\n\n\nReinforcement Learning: A Survey\n\n\nforward networks. One learns the value of situations, the other learns a p olicy. These can\n\nb e simple linear networks or can have hidden units.\n\nIn the simplest case, the entire system learns only to optimize immediate reward. First,\n\nlet us consider the b ehavior of the network that learns the p olicy, a mapping from a vector\n\n\n\ndescribing s to a 0 or \u0001. If the output unit has activation yi, then a, the action generated,\n\nwill b e \u0001 if y + � - 0, where � is normal noise, and 0 otherwise.\n\nThe adjustment for the output unit is, in the simplest case,\n\n\ne = r (a � \u0001=\u0002) ;\n\n\nwhere the �rst factor is the reward received for taking the most recent action and the second\n\nenco des which action was taken. The actions are enco ded as 0 and \u0001, so a � \u0001=\u0002 always has\n\nthe same magnitude; if the reward and the action have the same sign, then action \u0001 will b e\n\nmade more likely, otherwise action 0 will b e.\n\nAs describ ed, the network will tend to seek actions that given p ositive reward. To extend\n\nthis approach to maximize reward, we can compare the reward to some baseline, b. This\n\nchanges the adjustment to\n\ne = (r � b)(a � \u0001=\u0002) ;\n\n\nwhere b is the output of the second network. The second network is trained in a standard\n\nsup ervised mo de to estimate r as a function of the input state s.\n\nVariations of this approach have b een used in a variety of applications (Anderson, \u0001 \b\u0006;\n\nBarto et al., \u0001 \b\u0003; Lin, \u0001 \u0003b; Sutton, \u0001 \b\u0004).\n\n\nREINFORCE Algorithms Williams (\u0001 \b\u0007, \u0001 \u0002) studied the problem of cho osing ac\ntions to maximize immedate reward. He identi�ed a broad class of up date rules that p er\nform gradient descent on the exp ected reward and showed how to integrate these rules with\n\nbackpropagation. This class, called reinforce algorithms, includes linear reward-inaction\n\n(Section \u0002.\u0001.\u0003) as a sp ecial case.\n\n\n\nThe generic reinforce up date for a parameter wij\n\n\n\ncan b e written\n\n\n\nij\n\n\n\n@\n\n\n@ wij\n\n\n\nln (gj\n\n\n\n)\n\n\n\n�w\n\n\n\nij\n\n\n\n(r � b\n\n\n\n= �ij\n\n\n\n)\n\n\n\nwhere �\n\n\n\nij\n\n\n\na reinforcement baseline,\n\n\n\nis a non-negative factor, r the current reinforcement, bij\n\n\n\nand gi\n\n\n\nis the probability density function used to randomly generate actions based on unit\n\n\n\nactivations. Both �ij\n\n\n\nand b\n\nij\n\n\n\ncan take on di�erent values for each wij\n\n\n\n, however, when �ij\n\n\n\n, however, when �\n\n\n\nis constant throughout the system, the exp ected up date is exactly in the direction of the\n\nexp ected reward gradient. Otherwise, the up date is in the same half space as the gradient\n\nbut not necessarily in the direction of steep est increase.\n\n\n\nWilliams p oints out that the choice of baseline, bij, can have a profound e�ect on the\n\nconvergence sp eed of the algorithm.\n\n\nLogic-Based Metho ds Another strategy for generalization in reinforcement learning is\n\nto reduce the learning problem to an asso ciative problem of learning b o olean functions.\n\nA b o olean function has a vector of b o olean inputs and a single b o olean output. Taking\n\ninspiration from mainstream machine learning work, Kaelbling develop ed two algorithms\n\nfor learning b o olean functions from reinforcement: one uses the bias of k -DNF to drive\n\n\n\u0002\u0006\u0001\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nthe generalization pro cess (Kaelbling, \u0001 \u0004b); the other searches the space of syntactic\n\ndescriptions of functions using a simple generate-and-test metho d (Kaelbling, \u0001 \u0004a).\n\nThe restriction to a single b o olean output makes these techniques di�cult to apply. In\n\nvery b enign learning situations, it is p ossible to extend this approach to use a collection\n\nof learners to indep endently learn the individual bits that make up a complex output. In\n\ngeneral, however, that approach su�ers from the problem of very unreliable reinforcement:\n\nif a single learner generates an inappropriate output bit, all of the learners receive a low\n\nreinforcement value. The cascade metho d (Kaelbling, \u0001 \u0003b) allows a collection of learners\n\nto b e trained collectively to generate appropriate joint outputs; it is considerably more\n\nreliable, but can require additional computational e�ort.\n\n\n\u0006.\u0001.\u0002 Delayed Reward\n\n\nAnother metho d to allow reinforcement-learning techniques to b e applied in large state\n\nspaces is mo deled on value iteration and Q-learning. Here, a function approximator is used\n\nto represent the value function by mapping a state description to a value.\n\nMany reseachers have exp erimented with this approach: Boyan and Mo ore (\u0001 \u0005) used\n\nlo cal memory-based metho ds in conjunction with value iteration; Lin (\u0001 \u0001) used backprop\nagation networks for Q-learning; Watkins (\u0001 \b ) used CMAC for Q-learning; Tesauro (\u0001 \u0002,\n\n\u0001 \u0005) used backpropagation for learning the value function in backgammon (describ ed in\n\nSection \b.\u0001); Zhang and Dietterich (\u0001 \u0005) used backpropagation and T D (�) to learn go o d\n\nstrategies for job-shop scheduling.\n\nAlthough there have b een some p ositive examples, in general there are unfortunate in\nteractions b etween function approximation and the learning rules. In discrete environments\n\nthere is a guarantee that any op eration that up dates the value function (according to the\n\nBellman equations) can only reduce the error b etween the current value function and the\n\noptimal value function. This guarantee no longer holds when generalization is used. These\n\nissues are discussed by Boyan and Mo ore (\u0001 \u0005), who give some simple examples of value\n\nfunction errors growing arbitrarily large when generalization is used with value iteration.\n\nTheir solution to this, applicable only to certain classes of problems, discourages such diver\ngence by only p ermitting up dates whose estimated values can b e shown to b e near-optimal\n\nvia a battery of Monte-Carlo exp eriments.\n\nThrun and Schwartz (\u0001 \u0003) theorize that function approximation of value functions\n\nis also dangerous b ecause the errors in value functions due to generalization can b ecome\n\ncomp ounded by the \\max\" op erator in the de�nition of the value function.\n\nSeveral recent results (Gordon, \u0001 \u0005; Tsitsiklis & Van Roy, \u0001 \u0006) show how the appro\npriate choice of function approximator can guarantee convergence, though not necessarily to\n\nthe optimal values. Baird's residual gradient technique (Baird, \u0001 \u0005) provides guaranteed\n\nconvergence to lo cally optimal solutions.\n\nPerhaps the glo ominess of these counter-examples is misplaced. Boyan and Mo ore (\u0001 \u0005)\n\nrep ort that their counter-examples can b e made to work with problem-sp eci�c hand-tuning\n\ndespite the unreliability of untuned algorithms that provably converge in discrete domains.\n\nSutton (\u0001 \u0006) shows how mo di�ed versions of Boyan and Mo ore's examples can converge\n\nsuccessfully. An op en question is whether general principles, ideally supp orted by theory,\n\ncan help us understand when value function approximation will succeed. In Sutton's com\n\n\u0002\u0006\u0002\n\n\n\n\nReinforcement Learning: A Survey\n\n\nparative exp eriments with Boyan and Mo ore's counter-examples, he changes four asp ects\n\nof the exp eriments:\n\n\n\u0001. Small changes to the task sp eci�cations.\n\n\n\u0002. A very di�erent kind of function approximator (CMAC (Albus, \u0001 \u0007\u0005)) that has weak\n\ngeneralization.\n\n\n\u0003. A di�erent learning algorithm: SARSA (Rummery & Niranjan, \u0001 \u0004) instead of value\n\niteration.\n\n\n\u0004. A di�erent training regime. Boyan and Mo ore sampled states uniformly in state space,\n\nwhereas Sutton's metho d sampled along empirical tra jectories.\n\n\nThere are intuitive reasons to b elieve that the fourth factor is particularly imp ortant, but\n\nmore careful research is needed.\n\n\nAdaptive Resolution Mo dels In many cases, what we would like to do is partition\n\nthe environment into regions of states that can b e considered the same for the purp oses of\n\nlearning and generating actions. Without detailed prior knowledge of the environment, it\n\nis very di�cult to know what granularity or placement of partitions is appropriate. This\n\nproblem is overcome in metho ds that use adaptive resolution; during the course of learning,\n\na partition is constructed that is appropriate to the environment.\n\n\nDecision Trees In environments that are characterized by a set of b o olean or discrete\nvalued variables, it is p ossible to learn compact decision trees for representing Q values. The\n\nG-learning algorithm (Chapman & Kaelbling, \u0001 \u0001), works as follows. It starts by assuming\n\nthat no partitioning is necessary and tries to learn Q values for the entire environment as\n\nif it were one state. In parallel with this pro cess, it gathers statistics based on individual\n\ninput bits; it asks the question whether there is some bit b in the state description such\n\nthat the Q values for states in which b = \u0001 are signi�cantly di�erent from Q values for\n\nstates in which b = 0. If such a bit is found, it is used to split the decision tree. Then,\n\nthe pro cess is rep eated in each of the leaves. This metho d was able to learn very small\n\nrepresentations of the Q function in the presence of an overwhelming numb er of irrelevant,\n\nnoisy state attributes. It outp erformed Q-learning with backpropagation in a simple video\ngame environment and was used by McCallum (\u0001 \u0005) (in conjunction with other techniques\n\nfor dealing with partial observability) to learn b ehaviors in a complex driving-simulator. It\n\ncannot, however, acquire partitions in which attributes are only signi�cant in combination\n\n(such as those needed to solve parity problems).\n\n\nVariable Resolution Dynamic Programming The VRDP algorithm (Mo ore, \u0001 \u0001)\n\nenables conventional dynamic programming to b e p erformed in real-valued multivariate\n\nstate-spaces where straightforward discretization would fall prey to the curse of dimension\nality. A k d-tree (similar to a decision tree) is used to partition state space into coarse\n\nregions. The coarse regions are re�ned into detailed regions, but only in parts of the state\n\nspace which are predicted to b e imp ortant. This notion of imp ortance is obtained by run\nning \\mental tra jectories\" through state space. This algorithm proved e�ective on a numb er\n\nof problems for which full high-resolution arrays would have b een impractical. It has the\n\ndisadvantage of requiring a guess at an initially valid tra jectory through state-space.\n\n\n\u0002\u0006\u0003\n\n\n\n\nKaelbling, Littman, & Moore\n\n\n```\n(a)\n\n```\n\n\n\n\n\n|(b)|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||||`G`|\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n\n\n|(c)|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||||`G`|\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n|||||||||||||||||\n\n\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-27-0.png)\n\nFigure \u0007: (a) A two-dimensional maze problem. The p oint rob ot must �nd a path from\n\nstart to goal without crossing any of the barrier lines. (b) The path taken by\n\nPartiGame during the entire �rst trial. It b egins with intense exploration to �nd a\n\nroute out of the almost entirely enclosed start region. Having eventually reached\n\na su�ciently high resolution, it discovers the gap and pro ceeds greedily towards\n\nthe goal, only to b e temp orarily blo cked by the goal's barrier region. (c) The\n\nsecond trial.\n\n\nPartiGame Algorithm Mo ore's PartiGame algorithm (Mo ore, \u0001 \u0004) is another solution\n\nto the problem of learning to achieve goal con�gurations in deterministic high-dimensional\n\ncontinuous spaces by learning an adaptive-resolution mo del. It also divides the environment\n\ninto cells; but in each cell, the actions available consist of aiming at the neighb oring cells\n\n(this aiming is accomplished by a lo cal controller, which must b e provided as part of the\n\nproblem statement). The graph of cell transitions is solved for shortest paths in an online\n\nincremental manner, but a minimax criterion is used to detect when a group of cells is\n\nto o coarse to prevent movement b etween obstacles or to avoid limit cycles. The o�ending\n\ncells are split to higher resolution. Eventually, the environment is divided up just enough to\n\ncho ose appropriate actions for achieving the goal, but no unnecessary distinctions are made.\n\nAn imp ortant feature is that, as well as reducing memory and computational requirements,\n\nit also structures exploration of state space in a multi-resolution manner. Given a failure,\n\nthe agent will initially try something very di�erent to rectify the failure, and only resort to\n\nsmall lo cal changes when all the qualitatively di�erent strategies have b een exhausted.\n\n\nFigure \u0007a shows a two-dimensional continuous maze. Figure \u0007b shows the p erformance\n\nof a rob ot using the PartiGame algorithm during the very �rst trial. Figure \u0007c shows the\n\nsecond trial, started from a slightly di�erent p osition.\n\n\nThis is a very fast algorithm, learning p olicies in spaces of up to nine dimensions in less\n\nthan a minute. The restriction of the current implementation to deterministic environments\n\nlimits its applicabili ty, however. McCallum (\u0001 \u0005) suggests some related tree-structured\n\nmetho ds.\n\n\n\u0002\u0006\u0004\n\n\n\n\nReinforcement Learning: A Survey\n\n\n\u0006.\u0002 Generalization over Actions\n\n\nThe networks describ ed in Section \u0006.\u0001.\u0001 generalize over state descriptions presented as\n\ninputs. They also pro duce outputs in a discrete, factored representation and thus could b e\n\nseen as generalizing over actions as well.\n\nIn cases such as this when actions are describ ed combinatorially, it is imp ortant to\n\ngeneralize over actions to avoid keeping separate statistics for the huge numb er of actions\n\nthat can b e chosen. In continuous action spaces, the need for generalization is even more\n\npronounced.\n\nWhen estimating Q values using a neural network, it is p ossible to use either a distinct\n\nnetwork for each action, or a network with a distinct output for each action. When the\n\naction space is continuous, neither approach is p ossible. An alternative strategy is to use a\n\nsingle network with b oth the state and action as input and Q value as the output. Training\n\nsuch a network is not conceptually di�cult, but using the network to �nd the optimal action\n\ncan b e a challenge. One metho d is to do a lo cal gradient-ascent search on the action in\n\norder to �nd one with high value (Baird & Klopf, \u0001 \u0003).\n\nGullapalli (\u0001 0, \u0001 \u0002) has develop ed a \\neural\" reinforcement-learning unit for use in\n\ncontinuous action spaces. The unit generates actions with a normal distribution; it adjusts\n\nthe mean and variance based on previous exp erience. When the chosen actions are not\n\np erforming well, the variance is high, resulting in exploration of the range of choices. When\n\nan action p erforms well, the mean is moved in that direction and the variance decreased,\n\nresulting in a tendency to generate more action values near the successful one. This metho d\n\nwas successfully employed to learn to control a rob ot arm with many continuous degrees of\n\nfreedom.\n\n\n\u0006.\u0003 Hierarchical Metho ds\n\n\nAnother strategy for dealing with large state spaces is to treat them as a hierarchy of\n\nlearning problems. In many cases, hierarchical solutions intro duce slight sub-optimality in\n\np erformance, but p otentially gain a go o d deal of e�ciency in execution time, learning time,\n\nand space.\n\nHierarchical learners are commonly structured as gated behaviors, as shown in Figure \b.\n\nThere is a collection of behaviors that map environment states into low-level actions and\n\na gating function that decides, based on the state of the environment, which b ehavior's\n\nactions should b e switched through and actually executed. Maes and Bro oks (\u0001 0) used\n\na version of this architecture in which the individual b ehaviors were �xed a priori and the\n\ngating function was learned from reinforcement. Mahadevan and Connell (\u0001 \u0001b) used the\n\ndual approach: they �xed the gating function, and supplied reinforcement functions for the\n\nindividual b ehaviors, which were learned. Lin (\u0001 \u0003a) and Dorigo and Colomb etti (\u0001 \u0005,\n\n\u0001 \u0004) b oth used this approach, �rst training the b ehaviors and then training the gating\n\nfunction. Many of the other hierarchical learning metho ds can b e cast in this framework.\n\n\n\u0006.\u0003.\u0001 Feudal Q-learning\n\n\nFeudal Q-learning (Dayan & Hinton, \u0001 \u0003; Watkins, \u0001 \b ) involves a hierarchy of learning\n\nmo dules. In the simplest case, there is a high-level master and a low-level slave. The master\n\nreceives reinforcement from the external environment. Its actions consist of commands that\n\n\n\u0002\u0006\u0005\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nFigure \b: A structure of gated b ehaviors.\n\n\nit can give to the low-level learner. When the master generates a particular command to\n\nthe slave, it must reward the slave for taking actions that satisfy the command, even if they\n\ndo not result in external reinforcement. The master, then, learns a mapping from states to\n\ncommands. The slave learns a mapping from commands and states to external actions. The\n\nset of \\commands\" and their asso ciated reinforcement functions are established in advance\n\n\nof the learning.\n\nThis is really an instance of the general \\gated b ehaviors\" approach, in which the slave\n\ncan execute any of the b ehaviors dep ending on its command. The reinforcement functions\n\nfor the individual b ehaviors (commands) are given, but learning takes place simultaneously\n\nat b oth the high and low levels.\n\n\n\u0006.\u0003.\u0002 Compositional Q-learning\n\n\nSingh's comp ositional Q-learning (\u0001 \u0002b, \u0001 \u0002a) (C-QL) consists of a hierarchy based on\n\nthe temp oral sequencing of subgoals. The elemental tasks are b ehaviors that achieve some\n\nrecognizable condition. The high-level goal of the system is to achieve some set of condi\ntions in sequential order. The achievement of the conditions provides reinforcement for the\n\nelemental tasks, which are trained �rst to achieve individual subgoals. Then, the gating\n\nfunction learns to switch the elemental tasks in order to achieve the appropriate high-level\n\nsequential goal. This metho d was used by Tham and Prager (\u0001 \u0004) to learn to control a\n\nsimulated multi-link rob ot arm.\n\n\n\u0006.\u0003.\u0003 Hierarchical Distance to Goal\n\n\nEsp ecially if we consider reinforcement learning mo dules to b e part of larger agent archi\ntectures, it is imp ortant to consider problems in which goals are dynamically input to the\n\nlearner. Kaelbling's HDG algorithm (\u0001 \u0003a) uses a hierarchical approach to solving prob\nlems when goals of achievement (the agent should get to a particular state as quickly as\n\np ossible) are given to an agent dynamically.\n\nThe HDG algorithm works by analogy with navigation in a harb or. The environment\n\nis partitioned (a priori, but more recent work (Ashar, \u0001 \u0004) addresses the case of learning\n\nthe partition) into a set of regions whose centers are known as \\landmarks.\" If the agent is\n\n\n\u0002\u0006\u0006\n\n\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-29-0.png)\n\n\nReinforcement Learning: A Survey\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-30-0.png)\n\n## office printer\n\n\nFigure : An example of a partially observable environment.\n\n\ncurrently in the same region as the goal, then it uses low-level actions to move to the goal.\n\nIf not, then high-level information is used to determine the next landmark on the shortest\n\npath from the agent's closest landmark to the goal's closest landmark. Then, the agent uses\n\nlow-level information to aim toward that next landmark. If errors in action cause deviations\n\n\nin the path, there is no problem; the b est aiming p oint is recomputed on every step.\n\n\n\u0007. Partially Observable Environments\n\n\nIn many real-world environments, it will not b e p ossible for the agent to have p erfect and\n\ncomplete p erception of the state of the environment. Unfortunately, complete observability\n\nis necessary for learning metho ds based on MDPs. In this section, we consider the case in\n\nwhich the agent makes observations of the state of the environment, but these observations\n\nmay b e noisy and provide incomplete information. In the case of a rob ot, for instance,\n\nit might observe whether it is in a corridor, an op en ro om, a T-junction, etc., and those\n\nobservations might b e error-prone. This problem is also referred to as the problem of\n\n\\incomplete p erception,\" \\p erceptual aliasing,\" or \\hidden state.\"\n\nIn this section, we will consider extensions to the basic MDP framework for solving\n\npartially observable problems. The resulting formal mo del is called a partial ly observable\n\nMarkov decision process or POMDP.\n\n\n\u0007.\u0001 State-Free Deterministic Policies\n\n\nThe most naive strategy for dealing with partial observability is to ignore it. That is, to\n\ntreat the observations as if they were the states of the environment and try to learn to\n\nb ehave. Figure shows a simple environment in which the agent is attempting to get to\n\nthe printer from an o�ce. If it moves from the o�ce, there is a go o d chance that the agent\n\nwill end up in one of two places that lo ok like \\hall\", but that require di�erent actions for\n\ngetting to the printer. If we consider these states to b e the same, then the agent cannot\n\np ossibly b ehave optimally. But how well can it do?\n\nThe resulting problem is not Markovian, and Q-learning cannot b e guaranteed to con\nverge. Small breaches of the Markov requirement are well handled by Q-learning, but it is\n\np ossible to construct simple environments that cause Q-learning to oscillate (Chrisman &\n\n\n\u0002\u0006\u0007\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nLittman, \u0001 \u0003). It is p ossible to use a mo del-based approach, however; act according to\n\nsome p olicy and gather statistics ab out the transitions b etween observations, then solve for\n\nthe optimal p olicy based on those observations. Unfortunately, when the environment is not\n\nMarkovian, the transition probabilities dep end on the p olicy b eing executed, so this new\n\np olicy will induce a new set of transition probabilities. This approach may yield plausible\n\nresults in some cases, but again, there are no guarantees.\n\nIt is reasonable, though, to ask what the optimal p olicy (mapping from observations to\n\nactions, in this case) is. It is NP-hard (Littman, \u0001 \u0004b) to �nd this mapping, and even the\n\nb est mapping can have very p o or p erformance. In the case of our agent trying to get to the\n\nprinter, for instance, any deterministic state-free p olicy takes an in�nite numb er of steps to\n\nreach the goal on average.\n\n\n\u0007.\u0002 State-Free Sto chastic Policies\n\n\nSome improvement can b e gained by considering sto chastic p olicies; these are mappings\n\nfrom observations to probability distributions over actions. If there is randomness in the\n\nagent's actions, it will not get stuck in the hall forever. Jaakkola, Singh, and Jordan (\u0001 \u0005)\n\nhave develop ed an algorithm for �nding lo cally-optimal sto chastic p olicies, but �nding a\n\nglobally optimal p olicy is still NP hard.\n\nIn our example, it turns out that the optimal sto chastic p olicy is for the agent, when\n\n\n\nin a state that lo oks like a hall, to go east with probability \u0002 �\n\n\n\np\n\n\n\n\u0002 � 0:\u0006 and west with\n\n\n\n\u0002 � \u0001 � 0:\u0004. This p olicy can b e found by solving a simple (in this case)\n\n\n\nprobability\n\n\n\np\n\n\n\nquadratic program. The fact that such a simple example can pro duce irrational numb ers\n\ngives some indication that it is a di�cult problem to solve exactly.\n\n\n\u0007.\u0003 Policies with Internal State\n\n\nThe only way to b ehave truly e�ectively in a wide-range of environments is to use memory\n\nof previous actions and observations to disambiguate the current state. There are a variety\n\nof approaches to learning p olicies with internal state.\n\n\nRecurrent Q-learning One intuitively simple approach is to use a recurrent neural net\nwork to learn Q values. The network can b e trained using backpropagation through time (or\n\nsome other suitable technique) and learns to retain \\history features\" to predict value. This\n\napproach has b een used by a numb er of researchers (Meeden, McGraw, & Blank, \u0001 \u0003; Lin\n\n& Mitchell, \u0001 \u0002; Schmidhub er, \u0001 \u0001b). It seems to work e�ectively on simple problems,\n\nbut can su�er from convergence to lo cal optima on more complex problems.\n\n\nClassi�er Systems Classi�er systems (Holland, \u0001 \u0007\u0005; Goldb erg, \u0001 \b ) were explicitly\n\ndevelop ed to solve problems with delayed reward, including those requiring short-term\n\nmemory. The internal mechanism typically used to pass reward back through chains of\n\ndecisions, called the bucket brigade algorithm, b ears a close resemblance to Q-learning. In\n\nspite of some early successes, the original design do es not app ear to handle partially ob\nserved environments robustly.\n\nRecently, this approach has b een reexamined using insights from the reinforcement\nlearning literature, with some success. Dorigo did a comparative study of Q-learning and\n\nclassi�er systems (Dorigo & Bersini, \u0001 \u0004). Cli� and Ross (\u0001 \u0004) start with Wilson's zeroth\n\n\u0002\u0006\b\n\n\n\n\nReinforcement Learning: A Survey\n\n\nFigure \u00010: Structure of a POMDP agent.\n\n\nlevel classi�er system (Wilson, \u0001 \u0005) and add one and two-bit memory registers. They �nd\n\nthat, although their system can learn to use short-term memory registers e�ectively, the\n\napproach is unlikely to scale to more complex environments.\n\nDorigo and Colomb etti applied classi�er systems to a mo derately complex problem of\n\nlearning rob ot b ehavior from immediate reinforcement (Dorigo, \u0001 \u0005; Dorigo & Colomb etti,\n\n\u0001 \u0004).\n\n\nFinite-history-window Approach One way to restore the Markov prop erty is to allow\n\ndecisions to b e based on the history of recent observations and p erhaps actions. Lin and\n\nMitchell (\u0001 \u0002) used a �xed-width �nite history window to learn a p ole balancing task.\n\nMcCallum (\u0001 \u0005) describ es the \\utile su�x memory\" which learns a variable-width window\n\nthat serves simultaneously as a mo del of the environment and a �nite-memory p olicy. This\n\nsystem has had excellent results in a very complex driving-simulation domain (McCallum,\n\n\u0001 \u0005). Ring (\u0001 \u0004) has a neural-network approach that uses a variable history window,\n\nadding history when necessary to disambiguate situations.\n\n\nPOMDP Approach Another strategy consists of using hidden Markov mo del (HMM)\n\ntechniques to learn a mo del of the environment, including the hidden state, then to use that\n\nmo del to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, \u0001 \u0004;\n\nLovejoy, \u0001 \u0001; Monahan, \u0001 \b\u0002).\n\nChrisman (\u0001 \u0002) showed how the forward-backward algorithm for learning HMMs could\n\nb e adapted to learning POMDPs. He, and later McCallum (\u0001 \u0003), also gave heuristic state\nsplitting rules to attempt to learn the smallest p ossible mo del for a given environment. The\n\nresulting mo del can then b e used to integrate information from the agent's observations in\n\norder to make decisions.\n\n\nFigure \u00010 illustrates the basic structure for a p erfect-memory controller. The comp onent\n\non the left is the state estimator, which computes the agent's belief state, b as a function of\n\nthe old b elief state, the last action a, and the current observation i. In this context, a b elief\n\nstate is a probability distribution over states of the environment, indicating the likeliho o d,\n\ngiven the agent's past exp erience, that the environment is actually in each of those states.\n\nThe state estimator can b e constructed straightforwardly using the estimated world mo del\n\nand Bayes' rule.\n\nNow we are left with the problem of �nding a p olicy mapping b elief states into action.\n\nThis problem can b e formulated as an MDP, but it is di�cult to solve using the techniques\n\ndescrib ed earlier, b ecause the input space is continuous. Chrisman's approach (\u0001 \u0002) do es\n\nnot take into account future uncertainty, but yields a p olicy after a small amount of com\nputation. A standard approach from the op erations-research literature is to solve for the\n\n\n\u0002\u0006\n\n\n\n![](output/images/12d1d070a53d4084d88a77b8b143bad51c40c38f.pdf-32-0.png)\n\n\nKaelbling, Littman, & Moore\n\n\noptimal p olicy (or a close approximation thereof ) based on its representation as a piecewise\nlinear and convex function over the b elief space. This metho d is computationally intractable,\n\nbut may serve as inspiration for metho ds that make further approximations (Cassandra\n\net al., \u0001 \u0004; Littman, Cassandra, & Kaelbling, \u0001 \u0005a).\n\n\n\b. Reinforcement Learning Applications\n\n\nOne reason that reinforcement learning is p opular is that is serves as a theoretical to ol for\n\nstudying the principles of agents learning to act. But it is unsurprising that it has also\n\nb een used by a numb er of researchers as a practical computational to ol for constructing\n\nautonomous systems that improve themselves with exp erience. These applications have\n\nranged from rob otics, to industrial manufacturing, to combinatorial search problems such\n\nas computer game playing.\n\nPractical applications provide a test of the e�cacy and usefulness of learning algorithms.\n\nThey are also an inspiration for deciding which comp onents of the reinforcement learning\n\nframework are of practical imp ortance. For example, a researcher with a real rob otic task\n\ncan provide a data p oint to questions such as:\n\n\n� How imp ortant is optimal exploration? Can we break the learning p erio d into explo\nration phases and exploitation phases?\n\n\n� What is the most useful mo del of long-term reward: Finite horizon? Discounted?\n\nIn�nite horizon?\n\n\n� How much computation is available b etween agent decisions and how should it b e\n\nused?\n\n\n� What prior knowledge can we build into the system, and which algorithms are capable\n\nof using that knowledge?\n\n\nLet us examine a set of practical applications of reinforcement learning, while b earing these\n\nquestions in mind.\n\n\n\b.\u0001 Game Playing\n\n\nGame playing has dominated the Arti�cial Intelligence world as a problem domain ever since\n\nthe �eld was b orn. Two-player games do not �t into the established reinforcement-learning\n\nframework since the optimality criterion for games is not one of maximizing reward in the\n\nface of a �xed environment, but one of maximizing reward against an optimal adversary\n\n(minimax). Nonetheless, reinforcement-learning algorithms can b e adapted to work for a\n\nvery general class of games (Littman, \u0001 \u0004a) and many researchers have used reinforcement\n\nlearning in these environments. One application, sp ectacularly far ahead of its time, was\n\nSamuel's checkers playing system (Samuel, \u0001 \u0005 ). This learned a value function represented\n\nby a linear function approximator, and employed a training scheme similar to the up dates\n\nused in value iteration, temp oral di�erences and Q-learning.\n\nMore recently, Tesauro (\u0001 \u0002, \u0001 \u0004, \u0001 \u0005) applied the temp oral di�erence algorithm\n\n\n\nto backgammon. Backgammon has approximately \u00010\u00020\n\n\n\nstates, making table-based rein\n\n\nforcement learning imp ossible. Instead, Tesauro used a backpropagation-based three-layer\n\n\n\u0002\u00070\n\n\n\n\nReinforcement Learning: A Survey\n\n\n\nTraining\n\n\nGames\n\n\n\nHidden\n\n\nUnits\n\n\n\nResults\n\n\n\nBasic Po or\n\n\nTD \u0001.0 \u000300,000 \b0 Lost by \u0001\u0003 p oints in \u0005\u0001\n\n\ngames\n\nTD \u0002.0 \b00,000 \u00040 Lost by \u0007 p oints in \u0003\b\n\n\ngames\n\nTD \u0002.\u0001 \u0001,\u000500,000 \b0 Lost by \u0001 p oint in \u00040\n\n\ngames\n\n\nTable \u0002: TD-Gammon's p erformance in games against the top human professional players.\n\nA backgammon tournament involves playing a series of games for p oints until one\n\nplayer reaches a set target. TD-Gammon won none of these tournaments but came\n\nsu�ciently close that it is now considered one of the b est few players in the world.\n\n\nneural network as a function approximator for the value function\n\n\nBoard Position ! Probability of victory for current player:\n\n\nTwo versions of the learning algorithm were used. The �rst, which we will call Basic TD\nGammon, used very little prede�ned knowledge of the game, and the representation of a\n\nb oard p osition was virtually a raw enco ding, su�ciently p owerful only to p ermit the neural\n\nnetwork to distinguish b etween conceptually di�erent p ositions. The second, TD-Gammon,\n\nwas provided with the same raw state information supplemented by a numb er of hand\ncrafted features of backgammon b oard p ositions. Providing hand-crafted features in this\n\nmanner is a go o d example of how inductive biases from human knowledge of the task can\n\nb e supplied to a learning algorithm.\n\nThe training of b oth learning algorithms required several months of computer time, and\n\nwas achieved by constant self-play. No exploration strategy was used|the system always\n\ngreedily chose the move with the largest exp ected probability of victory. This naive explo\nration strategy proved entirely adequate for this environment, which is p erhaps surprising\n\ngiven the considerable work in the reinforcement-learning literature which has pro duced\n\nnumerous counter-examples to show that greedy exploration can lead to p o or learning p er\nformance. Backgammon, however, has two imp ortant prop erties. Firstly, whatever p olicy\n\nis followed, every game is guaranteed to end in �nite time, meaning that useful reward\n\ninformation is obtained fairly frequently. Secondly, the state transitions are su�ciently\n\nsto chastic that indep endent of the p olicy, all states will o ccasionally b e visited|a wrong\n\ninitial value function has little danger of starving us from visiting a critical part of state\n\nspace from which imp ortant information could b e obtained.\n\nThe results (Table \u0002) of TD-Gammon are impressive. It has comp eted at the very top\n\nlevel of international human play. Basic TD-Gammon played resp ectably, but not at a\n\nprofessional standard.\n\n\n\u0002\u0007\u0001\n\n\n\n\nFigure \u0001\u0001: Schaal and Atkeson's devil-sticking rob ot. The tap ered stick is hit alternately\n\nby each of the two hand sticks. The task is to keep the devil stick from falling\n\nfor as many hits as p ossible. The rob ot has three motors indicated by torque\n\n\n\nvectors �\u0001\n\n\n\n; �\n\n\n\n\u0002\n\n\n\n; �\u0003\n\n\n\n.\n\n\n\nAlthough exp eriments with other games have in some cases pro duced interesting learning\n\nb ehavior, no success close to that of TD-Gammon has b een rep eated. Other games that\n\nhave b een studied include Go (Schraudolph, Dayan, & Sejnowski, \u0001 \u0004) and Chess (Thrun,\n\n\u0001 \u0005). It is still an op en question as to if and how the success of TD-Gammon can b e\n\nrep eated in other domains.\n\n\n\b.\u0002 Rob otics and Control\n\n\nIn recent years there have b een many rob otics and control applications that have used\n\nreinforcement learning. Here we will concentrate on the following four examples, although\n\nmany other interesting ongoing rob otics investigations are underway.\n\n\n\u0001. Schaal and Atkeson (\u0001 \u0004) constructed a two-armed rob ot, shown in Figure \u0001\u0001, that\n\nlearns to juggle a device known as a devil-stick. This is a complex non-linear control\n\ntask involving a six-dimensional state space and less than \u000200 msecs p er control deci\nsion. After ab out \u00040 initial attempts the rob ot learns to keep juggling for hundreds of\n\nhits. A typical human learning the task requires an order of magnitude more practice\n\nto achieve pro�ciency at mere tens of hits.\n\n\nThe juggling rob ot learned a world mo del from exp erience, which was generalized\n\nto unvisited states by a function approximation scheme known as lo cally weighted\n\nregression (Cleveland & Delvin, \u0001 \b\b; Mo ore & Atkeson, \u0001 \u0002). Between each trial,\n\na form of dynamic programming sp eci�c to linear control p olicies and lo cally linear\n\ntransitions was used to improve the p olicy. The form of dynamic programming is\n\nknown as linear-quadratic-regulator design (Sage & White, \u0001 \u0007\u0007).\n\n\n\u0002\u0007\u0002\n\n\n\n\nReinforcement Learning: A Survey\n\n\n\u0002. Mahadevan and Connell (\u0001 \u0001a) discuss a task in which a mobile rob ot pushes large\n\nb oxes for extended p erio ds of time. Box-pushing is a well-known di�cult rob otics\n\nproblem, characterized by immense uncertainty in the results of actions. Q-learning\n\nwas used in conjunction with some novel clustering techniques designed to enable a\n\nhigher-dimensional input than a tabular approach would have p ermitted. The rob ot\n\nlearned to p erform comp etitively with the p erformance of a human-programmed so\nlution. Another asp ect of this work, mentioned in Section \u0006.\u0003, was a pre-programmed\n\nbreakdown of the monolithic task description into a set of lower level tasks to b e\n\nlearned.\n\n\n\u0003. Mataric (\u0001 \u0004) describ es a rob otics exp eriment with, from the viewp oint of theoret\nical reinforcement learning, an unthinkably high dimensional state space, containing\n\nmany dozens of degrees of freedom. Four mobile rob ots traveled within an enclo\nsure collecting small disks and transp orting them to a destination region. There were\n\nthree enhancements to the basic Q-learning algorithm. Firstly, pre-programmed sig\nnals called progress estimators were used to break the monolithic task into subtasks.\n\nThis was achieved in a robust manner in which the rob ots were not forced to use\n\n\nthe estimators, but had the freedom to pro�t from the inductive bias they provided.\n\nSecondly, control was decentralized. Each rob ot learned its own p olicy indep endently\n\nwithout explicit communication with the others. Thirdly, state space was brutally\n\nquantized into a small numb er of discrete states according to values of a small num\nb er of pre-programmed b o olean features of the underlying sensors. The p erformance\n\nof the Q-learned p olicies were almost as go o d as a simple hand-crafted controller for\n\nthe job.\n\n\n\u0004. Q-learning has b een used in an elevator dispatching task (Crites & Barto, \u0001 \u0006). The\n\nproblem, which has b een implemented in simulation only at this stage, involved four\n\nelevators servicing ten �o ors. The ob jective was to minimize the average squared\n\nwait time for passengers, discounted into future time. The problem can b e p osed as a\n\n\n\nstates even in the most simpli�ed version of\n\n\n\ndiscrete Markov system, but there are \u00010\n\n\n\n\u0002\u0002\n\n\n\nthe problem. Crites and Barto used neural networks for function approximation and\n\nprovided an excellent comparison study of their Q-learning approach against the most\n\np opular and the most sophisticated elevator dispatching algorithms. The squared wait\n\ntime of their controller was approximately \u0007% less than the b est alternative algorithm\n\n(\\Empty the System\" heuristic with a receding horizon controller) and less than half\n\nthe squared wait time of the controller most frequently used in real elevator systems.\n\n\n\u0005. The �nal example concerns an application of reinforcement learning by one of the\n\nauthors of this survey to a packaging task from a fo o d pro cessing industry. The\n\nproblem involves �lling containers with variable numb ers of non-identical pro ducts.\n\nThe pro duct characteristics also vary with time, but can b e sensed. Dep ending on\n\nthe task, various constraints are placed on the container-�lling pro cedure. Here are\n\nthree examples:\n\n\n� The mean weight of all containers pro duced by a shift must not b e b elow the\n\nmanufacturer's declared weight W .\n\n\n\u0002\u0007\u0003\n\n\n\n\nKaelbling, Littman, & Moore\n\n\n� The numb er of containers b elow the declared weight must b e less than P %.\n\n\n\n� No containers may b e pro duced b elow weight W\n\n\n\n0\n\n\n.\n\n\n\nSuch tasks are controlled by machinery which op erates according to various setpoints.\n\nConventional practice is that setp oints are chosen by human op erators, but this choice\n\nis not easy as it is dep endent on the current pro duct characteristics and the current\n\ntask constraints. The dep endency is often di�cult to mo del and highly non-linear.\n\nThe task was p osed as a �nite-horizon Markov decision task in which the state of the\n\nsystem is a function of the pro duct characteristics, the amount of time remaining in\n\nthe pro duction shift and the mean wastage and p ercent b elow declared in the shift\n\nso far. The system was discretized into \u000200,000 discrete states and lo cal weighted\n\nregression was used to learn and generalize a transition mo del. Prioritized sweep\ning was used to maintain an optimal value function as each new piece of transition\n\ninformation was obtained. In simulated exp eriments the savings were considerable,\n\ntypically with wastage reduced by a factor of ten. Since then the system has b een\n\ndeployed successfully in several factories within the United States.\n\n\nSome interesting asp ects of practical reinforcement learning come to light from these\n\nexamples. The most striking is that in all cases, to make a real system work it proved\n\nnecessary to supplement the fundamental algorithm with extra pre-programmed knowledge.\n\nSupplying extra knowledge comes at a price: more human e�ort and insight is required and\n\nthe system is subsequently less autonomous. But it is also clear that for tasks such as\n\nthese, a knowledge-free approach would not have achieved worthwhile p erformance within\n\nthe �nite lifetime of the rob ots.\n\n\nWhat forms did this pre-programmed knowledge take? It included an assumption of\n\nlinearity for the juggling rob ot's p olicy, a manual breaking up of the task into subtasks for\n\nthe two mobile-rob ot examples, while the b ox-pusher also used a clustering technique for\n\nthe Q values which assumed lo cally consistent Q values. The four disk-collecting rob ots\n\nadditionally used a manually discretized state space. The packaging example had far fewer\n\ndimensions and so required corresp ondingly weaker assumptions, but there, to o, the as\nsumption of lo cal piecewise continuity in the transition mo del enabled massive reductions\n\nin the amount of learning data required.\n\nThe exploration strategies are interesting to o. The juggler used careful statistical anal\nysis to judge where to pro�tably exp eriment. However, b oth mobile rob ot applications\n\nwere able to learn well with greedy exploration|always exploiting without delib erate ex\nploration. The packaging task used optimism in the face of uncertainty. None of these\n\nstrategies mirrors theoretically optimal (but computationally intractable) exploration, and\n\nyet all proved adequate.\n\nFinally, it is also worth considering the computational regimes of these exp eriments.\n\nThey were all very di�erent, which indicates that the di�ering computational demands of\n\nvarious reinforcement learning algorithms do indeed have an array of di�ering applications.\n\nThe juggler needed to make very fast decisions with low latency b etween each hit, but\n\nhad long p erio ds (\u00030 seconds and more) b etween each trial to consolidate the exp eriences\n\ncollected on the previous trial and to p erform the more aggressive computation necessary\n\nto pro duce a new reactive controller on the next trial. The b ox-pushing rob ot was meant to\n\n\n\u0002\u0007\u0004\n\n\n\n\nReinforcement Learning: A Survey\n\n\nop erate autonomously for hours and so had to make decisions with a uniform length control\n\ncycle. The cycle was su�ciently long for quite substantial computations b eyond simple Q\nlearning backups. The four disk-collecting rob ots were particularly interesting. Each rob ot\n\nhad a short life of less than \u00020 minutes (due to battery constraints) meaning that substantial\n\nnumb er crunching was impractical, and any signi�cant combinatorial search would have\n\nused a signi�cant fraction of the rob ot's learning lifetime. The packaging task had easy\n\nconstraints. One decision was needed every few minutes. This provided opp ortunities for\n\nfully computing the optimal value function for the \u000200,000-state system b etween every\n\ncontrol cycle, in addition to p erforming massive cross-validation-based optimization of the\n\ntransition mo del b eing learned.\n\nA great deal of further work is currently in progress on practical implementations of\n\nreinforcement learning. The insights and task constraints that they pro duce will have an\n\nimp ortant e�ect on shaping the kind of algorithms that are develop ed in future.\n\n\n. Conclusions\n\n\nThere are a variety of reinforcement-learning techniques that work e�ectively on a variety\n\nof small problems. But very few of these techniques scale well to larger problems. This is\n\nnot b ecause researchers have done a bad job of inventing learning techniques, but b ecause\n\nit is very di�cult to solve arbitrary problems in the general case. In order to solve highly\n\ncomplex problems, we must give up tabula rasa learning techniques and b egin to incorp orate\n\nbias that will give leverage to the learning pro cess.\n\nThe necessary bias can come in a variety of forms, including the following:\n\n\nshaping: The technique of shaping is used in training animals (Hilgard & Bower, \u0001 \u0007\u0005); a\n\nteacher presents very simple problems to solve �rst, then gradually exp oses the learner\n\nto more complex problems. Shaping has b een used in sup ervised-learning systems,\n\nand can b e used to train hierarchical reinforcement-learning systems from the b ottom\n\nup (Lin, \u0001 \u0001), and to alleviate problems of delayed reinforcement by decreasing the\n\ndelay until the problem is well understo o d (Dorigo & Colomb etti, \u0001 \u0004; Dorigo, \u0001 \u0005).\n\n\nlo cal reinforcement signals: Whenever p ossible, agents should b e given reinforcement\n\nsignals that are lo cal. In applications in which it is p ossible to compute a gradient,\n\nrewarding the agent for taking steps up the gradient, rather than just for achieving\n\nthe �nal goal, can sp eed learning signi�cantly (Mataric, \u0001 \u0004).\n\n\nimitation: An agent can learn by \\watching\" another agent p erform the task (Lin, \u0001 \u0001).\n\nFor real rob ots, this requires p erceptual abilities that are not yet available. But\n\nanother strategy is to have a human supply appropriate motor commands to a rob ot\n\nthrough a joystick or steering wheel (Pomerleau, \u0001 \u0003).\n\n\nproblem decomp osition: Decomp osing a huge learning problem into a collection of smaller\n\nones, and providing useful reinforcement signals for the subproblems is a very p ower\nful technique for biasing learning. Most interesting examples of rob otic reinforcement\n\nlearning employ this technique to some extent (Connell & Mahadevan, \u0001 \u0003).\n\n\nre�exes: One thing that keeps agents that know nothing from learning anything is that\n\nthey have a hard time even �nding the interesting parts of the space; they wander\n\n\n\u0002\u0007\u0005\n\n\n\n\nKaelbling, Littman, & Moore\n\n\naround at random never getting near the goal, or they are always \\killed\" immediately.\n\nThese problems can b e ameliorated by programming a set of \\re�exes\" that cause the\n\nagent to act initially in some way that is reasonable (Mataric, \u0001 \u0004; Singh, Barto,\n\nGrup en, & Connolly, \u0001 \u0004). These re�exes can eventually b e overridden by more\n\ndetailed and accurate learned knowledge, but they at least keep the agent alive and\n\np ointed in the right direction while it is trying to learn. Recent work by Millan (\u0001 \u0006)\n\nexplores the use of re�exes to make rob ot learning safer and more e�cient.\n\n\nWith appropriate biases, supplied by human programmers or teachers, complex reinforcement\nlearning problems will eventually b e solvable. There is still much work to b e done and many\n\ninteresting questions remaining for learning techniques and esp ecially regarding metho ds for\n\napproximating, decomp osing, and incorp orating bias into problems.\n\n\nAcknowledgements\n\n\nThanks to Marco Dorigo and three anonymous reviewers for comments that have help ed\n\nto improve this pap er. Also thanks to our many colleagues in the reinforcement-learning\n\ncommunity who have done this work and explained it to us.\n\nLeslie Pack Kaelbling was supp orted in part by NSF grants IRI- \u0004\u0005\u0003\u0003\b\u0003 and IRI\n\u0003\u0001\u0002\u0003 \u0005. Michael Littman was supp orted in part by Bellcore. Andrew Mo ore was supp orted\n\nin part by an NSF Research Initiation Award and by \u0003M Corp oration.\n\n\nReferences\n\n\nAckley, D. H., & Littman, M. L. (\u0001 0). Generalization and scaling in reinforcement learn\ning. In Touretzky, D. S. (Ed.), Advances in Neural Information Processing Systems\n\n\u0002, pp. \u0005\u00050{\u0005\u0005\u0007 San Mateo, CA. Morgan Kaufmann.\n\n\nAlbus, J. S. (\u0001 \u0007\u0005). A new approach to manipulator control: Cereb ellar mo del articulation\n\ncontroller (cmac). Journal of Dynamic Systems, Measurement and Control, \u0007, \u0002\u00020{\n\n\u0002\u0002\u0007.\n\n\nAlbus, J. S. (\u0001 \b\u0001). Brains, Behavior, and Robotics. BYTE Bo oks, Subsidiary of McGraw\nHill, Peterb orough, New Hampshire.\n\n\nAnderson, C. W. (\u0001 \b\u0006). Learning and Problem Solving with Multilayer Connectionist\n\nSystems. Ph.D. thesis, University of Massachusetts, Amherst, MA.\n\n\nAshar, R. R. (\u0001 \u0004). Hierarchical learning in sto chastic domains. Master's thesis, Brown\n\nUniversity, Providence, Rho de Island.\n\n\nBaird, L. (\u0001 \u0005). Residual algorithms: Reinforcement learning with function approxima\ntion. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International\n\nConference on Machine Learning, pp. \u00030{\u0003\u0007 San Francisco, CA. Morgan Kaufmann.\n\n\nBaird, L. C., & Klopf, A. H. (\u0001 \u0003). Reinforcement learning with high-dimensional, con\ntinuous actions. Tech. rep. WL-TR- \u0003-\u0001\u0001\u0004\u0007, Wright-Patterson Air Force Base Ohio:\n\nWright Lab oratory.\n\n\n\u0002\u0007\u0006\n\n\n\n\nReinforcement Learning: A Survey\n\n\nBarto, A. G., Bradtke, S. J., & Singh, S. P. (\u0001 \u0005). Learning to act using real-time dynamic\n\nprogramming. Arti�cial Intel ligence, \u0007\u0002 (\u0001), \b\u0001{\u0001\u0003\b.\n\n\nBarto, A. G., Sutton, R. S., & Anderson, C. W. (\u0001 \b\u0003). Neuronlike adaptive elements that\n\ncan solve di�cult learning control problems. IEEE Transactions on Systems, Man,\n\nand Cybernetics, SMC-\u0001\u0003 (\u0005), \b\u0003\u0004{\b\u0004\u0006.\n\n\nBellman, R. (\u0001 \u0005\u0007). Dynamic Programming. Princeton University Press, Princeton, NJ.\n\n\nBerenji, H. R. (\u0001 \u0001). Arti�cial neural networks and approximate reasoning for intelligent\n\ncontrol in space. In American Control Conference, pp. \u00010\u0007\u0005{\u00010\b0.\n\n\nBerry, D. A., & Fristedt, B. (\u0001 \b\u0005). Bandit Problems: Sequential Al location of Experiments.\n\nChapman and Hall, London, UK.\n\n\nBertsekas, D. P. (\u0001 \b\u0007). Dynamic Programming: Deterministic and Stochastic Models.\n\nPrentice-Hall, Englewo o d Cli�s, NJ.\n\n\nBertsekas, D. P. (\u0001 \u0005). Dynamic Programming and Optimal Control. Athena Scienti�c,\n\nBelmont, Massachusetts. Volumes \u0001 and \u0002.\n\n\nBertsekas, D. P., & Castanon,~ D. A. (\u0001 \b ). Adaptive aggregation for in�nite horizon\n\ndynamic programming. IEEE Transactions on Automatic Control, \u0003\u0004 (\u0006), \u0005\b {\u0005 \b.\n\n\nBertsekas, D. P., & Tsitsiklis, J. N. (\u0001 \b ). Paral lel and Distributed Computation: Numer\nical Methods. Prentice-Hall, Englewo o d Cli�s, NJ.\n\n\nBox, G. E. P., & Drap er, N. R. (\u0001 \b\u0007). Empirical Model-Building and Response Surfaces.\n\nWiley.\n\n\nBoyan, J. A., & Mo ore, A. W. (\u0001 \u0005). Generalization in reinforcement learning: Safely\n\napproximating the value function. In Tesauro, G., Touretzky, D. S., & Leen, T. K.\n\n(Eds.), Advances in Neural Information Processing Systems \u0007 Cambridge, MA. The\n\nMIT Press.\n\n\nBurghes, D., & Graham, A. (\u0001 \b0). Introduction to Control Theory including Optimal\n\nControl. Ellis Horwo o d.\n\n\nCassandra, A. R., Kaelbling, L. P., & Littman, M. L. (\u0001 \u0004). Acting optimally in partially\n\nobservable sto chastic domains. In Proceedings of the Twelfth National Conference on\n\nArti�cial Intel ligence Seattle, WA.\n\n\nChapman, D., & Kaelbling, L. P. (\u0001 \u0001). Input generalization in delayed reinforcement\n\nlearning: An algorithm and p erformance comparisons. In Proceedings of the Interna\ntional Joint Conference on Arti�cial Intel ligence Sydney, Australia.\n\n\nChrisman, L. (\u0001 \u0002). Reinforcement learning with p erceptual aliasing: The p erceptual\n\ndistinctions approach. In Proceedings of the Tenth National Conference on Arti�cial\n\nIntel ligence, pp. \u0001\b\u0003{\u0001\b\b San Jose, CA. AAAI Press.\n\n\n\u0002\u0007\u0007\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nChrisman, L., & Littman, M. (\u0001 \u0003). Hidden state and short-term memory.. Presentation\n\nat Reinforcement Learning Workshop, Machine Learning Conference.\n\n\nCichosz, P., & Mulawka, J. J. (\u0001 \u0005). Fast and e�cient reinforcement learning with trun\ncated temp oral di�erences. In Prieditis, A., & Russell, S. (Eds.), Proceedings of the\n\nTwelfth International Conference on Machine Learning, pp. {\u00010\u0007 San Francisco,\n\nCA. Morgan Kaufmann.\n\n\nCleveland, W. S., & Delvin, S. J. (\u0001 \b\b). Lo cally weighted regression: An approach to\n\nregression analysis by lo cal �tting. Journal of the American Statistical Association,\n\n\b\u0003 (\u00040\u0003), \u0005 \u0006{\u0006\u00010.\n\n\nCli�, D., & Ross, S. (\u0001 \u0004). Adding temp orary memory to ZCS. Adaptive Behavior, \u0003 (\u0002),\n\n\u00010\u0001{\u0001\u00050.\n\n\nCondon, A. (\u0001 \u0002). The complexity of sto chastic games. Information and Computation,\n\n\u0006 (\u0002), \u00020\u0003{\u0002\u0002\u0004.\n\n\nConnell, J., & Mahadevan, S. (\u0001 \u0003). Rapid task learning for real rob ots. In Robot Learning.\n\nKluwer Academic Publishers.\n\n\nCrites, R. H., & Barto, A. G. (\u0001 \u0006). Improving elevator p erformance using reinforcement\n\nlearning. In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural Information\n\nProcessing Systems \b.\n\n\nDayan, P. (\u0001 \u0002). The convergence of TD(�) for general �. Machine Learning, \b (\u0003), \u0003\u0004\u0001{\n\n\u0003\u0006\u0002.\n\n\nDayan, P., & Hinton, G. E. (\u0001 \u0003). Feudal reinforcement learning. In Hanson, S. J., Cowan,\n\nJ. D., & Giles, C. L. (Eds.), Advances in Neural Information Processing Systems \u0005\n\nSan Mateo, CA. Morgan Kaufmann.\n\n\nDayan, P., & Sejnowski, T. J. (\u0001 \u0004). TD(�) converges with probability \u0001. Machine Learn\ning, \u0001\u0004 (\u0003).\n\n\nDean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (\u0001 \u0003). Planning with deadlines in\n\nsto chastic domains. In Proceedings of the Eleventh National Conference on Arti�cial\n\nIntel ligence Washington, DC.\n\n\nD'Ep enoux, F. (\u0001 \u0006\u0003). A probabilistic pro duction and inventory problem. Management\n\nScience, \u00010, \b{\u00010\b.\n\n\nDerman, C. (\u0001 \u00070). Finite State Markovian Decision Processes. Academic Press, New York.\n\n\nDorigo, M., & Bersini, H. (\u0001 \u0004). A comparison of q-learning and classi�er systems. In\n\nFrom Animals to Animats: Proceedings of the Third International Conference on the\n\nSimulation of Adaptive Behavior Brighton, UK.\n\n\nDorigo, M., & Colomb etti, M. (\u0001 \u0004). Rob ot shaping: Developing autonomous agents\n\nthrough learning. Arti�cial Intel ligence, \u0007\u0001 (\u0002), \u0003\u0002\u0001{\u0003\u00070.\n\n\n\u0002\u0007\b\n\n\n\n\nReinforcement Learning: A Survey\n\n\nDorigo, M. (\u0001 \u0005). Alecsys and the AutonoMouse: Learning to control a real rob ot by\n\ndistributed classi�er systems. Machine Learning, \u0001 .\n\n\nFiechter, C.-N. (\u0001 \u0004). E�cient reinforcement learning. In Proceedings of the Seventh\n\nAnnual ACM Conference on Computational Learning Theory, pp. \b\b{ \u0007. Asso ciation\n\nof Computing Machinery.\n\n\nGittins, J. C. (\u0001 \b ). Multi-armed Bandit Al location Indices. Wiley-Interscience series in\n\nsystems and optimization. Wiley, Chichester, NY.\n\n\nGoldb erg, D. (\u0001 \b ). Genetic algorithms in search, optimization, and machine learning.\n\nAddison-Wesley, MA.\n\n\nGordon, G. J. (\u0001 \u0005). Stable function approximation in dynamic programming. In Priedi\ntis, A., & Russell, S. (Eds.), Proceedings of the Twelfth International Conference on\n\nMachine Learning, pp. \u0002\u0006\u0001{\u0002\u0006\b San Francisco, CA. Morgan Kaufmann.\n\n\nGullapalli, V. (\u0001 0). A sto chastic reinforcement learning algorithm for learning real-valued\n\nfunctions. Neural Networks, \u0003, \u0006\u0007\u0001{\u0006 \u0002.\n\n\nGullapalli, V. (\u0001 \u0002). Reinforcement learning and its application to control. Ph.D. thesis,\n\nUniversity of Massachusetts, Amherst, MA.\n\n\nHilgard, E. R., & Bower, G. H. (\u0001 \u0007\u0005). Theories of Learning (fourth edition). Prentice-Hall,\n\nEnglewo o d Cli�s, NJ.\n\n\nHo�man, A. J., & Karp, R. M. (\u0001 \u0006\u0006). On nonterminating sto chastic games. Management\n\nScience, \u0001\u0002, \u0003\u0005 {\u0003\u00070.\n\n\nHolland, J. H. (\u0001 \u0007\u0005). Adaptation in Natural and Arti�cial Systems. University of Michigan\n\nPress, Ann Arb or, MI.\n\n\nHoward, R. A. (\u0001 \u00060). Dynamic Programming and Markov Processes. The MIT Press,\n\nCambridge, MA.\n\n\nJaakkola, T., Jordan, M. I., & Singh, S. P. (\u0001 \u0004). On the convergence of sto chastic iterative\n\ndynamic programming algorithms. Neural Computation, \u0006 (\u0006).\n\n\nJaakkola, T., Singh, S. P., & Jordan, M. I. (\u0001 \u0005). Monte-carlo reinforcement learning in\n\nnon-Markovian decision problems. In Tesauro, G., Touretzky, D. S., & Leen, T. K.\n\n(Eds.), Advances in Neural Information Processing Systems \u0007 Cambridge, MA. The\n\nMIT Press.\n\n\nKaelbling, L. P. (\u0001 \u0003a). Hierarchical learning in sto chastic domains: Preliminary results.\n\nIn Proceedings of the Tenth International Conference on Machine Learning Amherst,\n\nMA. Morgan Kaufmann.\n\n\nKaelbling, L. P. (\u0001 \u0003b). Learning in Embedded Systems. The MIT Press, Cambridge, MA.\n\n\nKaelbling, L. P. (\u0001 \u0004a). Asso ciative reinforcement learning: A generate and test algorithm.\n\nMachine Learning, \u0001\u0005 (\u0003).\n\n\n\u0002\u0007\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nKaelbling, L. P. (\u0001 \u0004b). Asso ciative reinforcement learning: Functions in k -DNF. Machine\n\nLearning, \u0001\u0005 (\u0003).\n\n\nKirman, J. (\u0001 \u0004). Predicting Real-Time Planner Performance by Domain Characterization.\n\nPh.D. thesis, Department of Computer Science, Brown University.\n\n\nKo enig, S., & Simmons, R. G. (\u0001 \u0003). Complexity analysis of real-time reinforcement\n\nlearning. In Proceedings of the Eleventh National Conference on Arti�cial Intel ligence,\n\npp. {\u00010\u0005 Menlo Park, California. AAAI Press/MIT Press.\n\n\nKumar, P. R., & Varaiya, P. P. (\u0001 \b\u0006). Stochastic Systems: Estimation, Identi�cation, and\n\nAdaptive Control. Prentice Hall, Englewo o d Cli�s, New Jersey.\n\n\nLee, C. C. (\u0001 \u0001). A self learning rule-based controller employing approximate reasoning\n\nand neural net concepts. International Journal of Intel ligent Systems, \u0006 (\u0001), \u0007\u0001{ \u0003.\n\n\nLin, L.-J. (\u0001 \u0001). Programming rob ots using reinforcement learning and teaching. In\n\nProceedings of the Ninth National Conference on Arti�cial Intel ligence.\n\n\nLin, L.-J. (\u0001 \u0003a). Hierachical learning of rob ot skills by reinforcement. In Proceedings of\n\nthe International Conference on Neural Networks.\n\n\nLin, L.-J. (\u0001 \u0003b). Reinforcement Learning for Robots Using Neural Networks. Ph.D. thesis,\n\nCarnegie Mellon University, Pittsburgh, PA.\n\n\nLin, L.-J., & Mitchell, T. M. (\u0001 \u0002). Memory approaches to reinforcement learning in non\nMarkovian domains. Tech. rep. CMU-CS- \u0002-\u0001\u0003\b, Carnegie Mellon University, Scho ol\n\nof Computer Science.\n\n\nLittman, M. L. (\u0001 \u0004a). Markov games as a framework for multi-agent reinforcement learn\ning. In Proceedings of the Eleventh International Conference on Machine Learning,\n\npp. \u0001\u0005\u0007{\u0001\u0006\u0003 San Francisco, CA. Morgan Kaufmann.\n\n\nLittman, M. L. (\u0001 \u0004b). Memoryless p olicies: Theoretical limitations and practical results.\n\nIn Cli�, D., Husbands, P., Meyer, J.-A., & Wilson, S. W. (Eds.), From Animals\n\nto Animats \u0003: Proceedings of the Third International Conference on Simulation of\n\nAdaptive Behavior Cambridge, MA. The MIT Press.\n\n\nLittman, M. L., Cassandra, A., & Kaelbling, L. P. (\u0001 \u0005a). Learning p olicies for partially\n\nobservable environments: Scaling up. In Prieditis, A., & Russell, S. (Eds.), Proceed\nings of the Twelfth International Conference on Machine Learning, pp. \u0003\u0006\u0002{\u0003\u00070 San\n\nFrancisco, CA. Morgan Kaufmann.\n\n\nLittman, M. L., Dean, T. L., & Kaelbling, L. P. (\u0001 \u0005b). On the complexity of solving\n\nMarkov decision problems. In Proceedings of the Eleventh Annual Conference on\n\nUncertainty in Arti�cial Intel ligence (UAI{ \u0005) Montreal, Qu�eb ec, Canada.\n\n\nLovejoy, W. S. (\u0001 \u0001). A survey of algorithmic metho ds for partially observable Markov\n\ndecision pro cesses. Annals of Operations Research, \u0002\b, \u0004\u0007{\u0006\u0006.\n\n\n\u0002\b0\n\n\n\n\nReinforcement Learning: A Survey\n\n\nMaes, P., & Bro oks, R. A. (\u0001 0). Learning to co ordinate b ehaviors. In Proceedings Eighth\n\nNational Conference on Arti�cial Intel ligence, pp. \u0007 \u0006{\b0\u0002. Morgan Kaufmann.\n\n\nMahadevan, S. (\u0001 \u0004). To discount or not to discount in reinforcement learning: A case\n\nstudy comparing R learning and Q learning. In Proceedings of the Eleventh Inter\nnational Conference on Machine Learning, pp. \u0001\u0006\u0004{\u0001\u0007\u0002 San Francisco, CA. Morgan\n\nKaufmann.\n\n\nMahadevan, S. (\u0001 \u0006). Average reward reinforcement learning: Foundations, algorithms,\n\nand empirical results. Machine Learning, \u0002\u0002 (\u0001).\n\n\nMahadevan, S., & Connell, J. (\u0001 \u0001a). Automatic programming of b ehavior-based rob ots\n\nusing reinforcement learning. In Proceedings of the Ninth National Conference on\n\nArti�cial Intel ligence Anaheim, CA.\n\n\nMahadevan, S., & Connell, J. (\u0001 \u0001b). Scaling reinforcement learning to rob otics by ex\nploiting the subsumption architecture. In Proceedings of the Eighth International\n\nWorkshop on Machine Learning, pp. \u0003\u0002\b{\u0003\u0003\u0002.\n\n\nMataric, M. J. (\u0001 \u0004). Reward functions for accelerated learning. In Cohen, W. W., &\n\nHirsh, H. (Eds.), Proceedings of the Eleventh International Conference on Machine\n\nLearning. Morgan Kaufmann.\n\n\nMcCallum, A. K. (\u0001 \u0005). Reinforcement Learning with Selective Perception and Hidden\n\nState. Ph.D. thesis, Department of Computer Science, University of Ro chester.\n\n\nMcCallum, R. A. (\u0001 \u0003). Overcoming incomplete p erception with utile distinction memory.\n\nIn Proceedings of the Tenth International Conference on Machine Learning, pp. \u0001 0{\n\n\u0001 \u0006 Amherst, Massachusetts. Morgan Kaufmann.\n\n\nMcCallum, R. A. (\u0001 \u0005). Instance-based utile distinctions for reinforcement learning with\n\nhidden state. In Proceedings of the Twelfth International Conference Machine Learn\ning, pp. \u0003\b\u0007{\u0003 \u0005 San Francisco, CA. Morgan Kaufmann.\n\n\nMeeden, L., McGraw, G., & Blank, D. (\u0001 \u0003). Emergent control and planning in an au\ntonomous vehicle. In Touretsky, D. (Ed.), Proceedings of the Fifteenth Annual Meeting\n\nof the Cognitive Science Society, pp. \u0007\u0003\u0005{\u0007\u00040. Lawerence Erlbaum Asso ciates, Hills\ndale, NJ.\n\n\nMillan, J. d. R. (\u0001 \u0006). Rapid, safe, and incremental learning of navigation strategies. IEEE\n\nTransactions on Systems, Man, and Cybernetics, \u0002\u0006 (\u0003).\n\n\nMonahan, G. E. (\u0001 \b\u0002). A survey of partially observable Markov decision pro cesses: Theory,\n\nmo dels, and algorithms. Management Science, \u0002\b, \u0001{\u0001\u0006.\n\n\nMo ore, A. W. (\u0001 \u0001). Variable resolution dynamic programming: E�ciently learning ac\ntion maps in multivariate real-valued spaces. In Proc. Eighth International Machine\n\nLearning Workshop.\n\n\n\u0002\b\u0001\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nMo ore, A. W. (\u0001 \u0004). The parti-game algorithm for variable resolution reinforcement learn\ning in multidimensional state-spaces. In Cowan, J. D., Tesauro, G., & Alsp ector, J.\n\n(Eds.), Advances in Neural Information Processing Systems \u0006, pp. \u0007\u0001\u0001{\u0007\u0001\b San Mateo,\n\nCA. Morgan Kaufmann.\n\n\nMo ore, A. W., & Atkeson, C. G. (\u0001 \u0002). An investigation of memory-based function ap\nproximators for learning control. Tech. rep., MIT Arti�cal Intelligence Lab oratory,\n\nCambridge, MA.\n\n\nMo ore, A. W., & Atkeson, C. G. (\u0001 \u0003). Prioritized sweeping: Reinforcement learning with\n\nless data and less real time. Machine Learning, \u0001\u0003.\n\n\nMo ore, A. W., Atkeson, C. G., & Schaal, S. (\u0001 \u0005). Memory-based learning for control.\n\nTech. rep. CMU-RI-TR- \u0005-\u0001\b, CMU Rob otics Institute.\n\n\nNarendra, K., & Thathachar, M. A. L. (\u0001 \b ). Learning Automata: An Introduction.\n\nPrentice-Hall, Englewo o d Cli�s, NJ.\n\n\nNarendra, K. S., & Thathachar, M. A. L. (\u0001 \u0007\u0004). Learning automata|a survey. IEEE\n\nTransactions on Systems, Man, and Cybernetics, \u0004 (\u0004), \u0003\u0002\u0003{\u0003\u0003\u0004.\n\n\nPeng, J., & Williams, R. J. (\u0001 \u0003). E�cient learning and planning within the Dyna frame\nwork. Adaptive Behavior, \u0001 (\u0004), \u0004\u0003\u0007{\u0004\u0005\u0004.\n\n\nPeng, J., & Williams, R. J. (\u0001 \u0004). Incremental multi-step Q-learning. In Proceedings of the\n\nEleventh International Conference on Machine Learning, pp. \u0002\u0002\u0006{\u0002\u0003\u0002 San Francisco,\n\nCA. Morgan Kaufmann.\n\n\nPomerleau, D. A. (\u0001 \u0003). Neural network perception for mobile robot guidance. Kluwer\n\nAcademic Publishing.\n\n\nPuterman, M. L. (\u0001 \u0004). Markov Decision Processes|Discrete Stochastic Dynamic Pro\ngramming. John Wiley & Sons, Inc., New York, NY.\n\n\nPuterman, M. L., & Shin, M. C. (\u0001 \u0007\b). Mo di�ed p olicy iteration algorithms for discounted\n\nMarkov decision pro cesses. Management Science, \u0002\u0004, \u0001\u0001\u0002\u0007{\u0001\u0001\u0003\u0007.\n\n\nRing, M. B. (\u0001 \u0004). Continual Learning in Reinforcement Environments. Ph.D. thesis,\n\nUniversity of Texas at Austin, Austin, Texas.\n\n\nRude,� U. (\u0001 \u0003). Mathematical and computational techniques for multilevel adaptive meth\nods. So ciety for Industrial and Applied Mathematics, Philadelphi a, Pennsylvania.\n\n\nRumelhart, D. E., & McClelland, J. L. (Eds.). (\u0001 \b\u0006). Paral lel Distributed Processing:\n\nExplorations in the microstructures of cognition. Volume \u0001: Foundations. The MIT\n\nPress, Cambridge, MA.\n\n\nRummery, G. A., & Niranjan, M. (\u0001 \u0004). On-line Q-learning using connectionist systems.\n\nTech. rep. CUED/F-INFENG/TR\u0001\u0006\u0006, Cambridge University.\n\n\n\u0002\b\u0002\n\n\n\n\nReinforcement Learning: A Survey\n\n\nRust, J. (\u0001 \u0006). Numerical dynamic programming in economics. In Handbook of Computa\ntional Economics. Elsevier, North Holland.\n\n\nSage, A. P., & White, C. C. (\u0001 \u0007\u0007). Optimum Systems Control. Prentice Hall.\n\n\nSalganico�, M., & Ungar, L. H. (\u0001 \u0005). Active exploration and learning in real-valued\n\nspaces using multi-armed bandit allo cation indices. In Prieditis, A., & Russell, S.\n\n(Eds.), Proceedings of the Twelfth International Conference on Machine Learning,\n\npp. \u0004\b0{\u0004\b\u0007 San Francisco, CA. Morgan Kaufmann.\n\n\nSamuel, A. L. (\u0001 \u0005 ). Some studies in machine learning using the game of checkers. IBM\n\nJournal of Research and Development, \u0003, \u0002\u0001\u0001{\u0002\u0002 . Reprinted in E. A. Feigenbaum\n\nand J. Feldman, editors, Computers and Thought, McGraw-Hill, New York \u0001 \u0006\u0003.\n\n\nSchaal, S., & Atkeson, C. (\u0001 \u0004). Rob ot juggling: An implementation of memory-based\n\nlearning. Control Systems Magazine, \u0001\u0004.\n\n\nSchmidhub er, J. (\u0001 \u0006). A general metho d for multi-agent learning and incremental self\nimprovement in unrestricted environments. In Yao, X. (Ed.), Evolutionary Computa\ntion: Theory and Applications. Scienti�c Publ. Co., Singap ore.\n\n\nSchmidhub er, J. H. (\u0001 \u0001a). Curious mo del-buildi ng control systems. In Proc. International\n\nJoint Conference on Neural Networks, Singapore, Vol. \u0002, pp. \u0001\u0004\u0005\b{\u0001\u0004\u0006\u0003. IEEE.\n\n\nSchmidhub er, J. H. (\u0001 \u0001b). Reinforcement learning in Markovian and non-Markovian\n\nenvironments. In Lippman, D. S., Mo o dy, J. E., & Touretzky, D. S. (Eds.), Advances\n\nin Neural Information Processing Systems \u0003, pp. \u000500{\u00050\u0006 San Mateo, CA. Morgan\n\nKaufmann.\n\n\nSchraudolph, N. N., Dayan, P., & Sejnowski, T. J. (\u0001 \u0004). Temp oral di�erence learning of\n\np osition evaluation in the game of Go. In Cowan, J. D., Tesauro, G., & Alsp ector,\n\nJ. (Eds.), Advances in Neural Information Processing Systems \u0006, pp. \b\u0001\u0007{\b\u0002\u0004 San\n\nMateo, CA. Morgan Kaufmann.\n\n\nSchrijver, A. (\u0001 \b\u0006). Theory of Linear and Integer Programming. Wiley-Interscience, New\n\nYork, NY.\n\n\nSchwartz, A. (\u0001 \u0003). A reinforcement learning metho d for maximizing undiscounted re\nwards. In Proceedings of the Tenth International Conference on Machine Learning,\n\npp. \u0002 \b{\u00030\u0005 Amherst, Massachusetts. Morgan Kaufmann.\n\n\nSingh, S. P., Barto, A. G., Grup en, R., & Connolly, C. (\u0001 \u0004). Robust reinforcement\n\nlearning in motion planning. In Cowan, J. D., Tesauro, G., & Alsp ector, J. (Eds.),\n\nAdvances in Neural Information Processing Systems \u0006, pp. \u0006\u0005\u0005{\u0006\u0006\u0002 San Mateo, CA.\n\nMorgan Kaufmann.\n\n\nSingh, S. P., & Sutton, R. S. (\u0001 \u0006). Reinforcement learning with replacing eligibili ty traces.\n\nMachine Learning, \u0002\u0002 (\u0001).\n\n\n\u0002\b\u0003\n\n\n\n\nKaelbling, Littman, & Moore\n\n\nSingh, S. P. (\u0001 \u0002a). Reinforcement learning with a hierarchy of abstract mo dels. In\n\nProceedings of the Tenth National Conference on Arti�cial Intel ligence, pp. \u00020\u0002{\u00020\u0007\n\nSan Jose, CA. AAAI Press.\n\n\nSingh, S. P. (\u0001 \u0002b). Transfer of learning by comp osing solutions of elemental sequential\n\ntasks. Machine Learning, \b (\u0003), \u0003\u0002\u0003{\u0003\u00040.\n\n\nSingh, S. P. (\u0001 \u0003). Learning to Solve Markovian Decision Processes. Ph.D. thesis, Depart\nment of Computer Science, University of Massachusetts. Also, CMPSCI Technical\n\nRep ort \u0003-\u0007\u0007.\n\n\nStengel, R. F. (\u0001 \b\u0006). Stochastic Optimal Control. John Wiley and Sons.\n\n\nSutton, R. S. (\u0001 \u0006). Generalization in Reinforcement Learning: Successful Examples Using\n\nSparse Coarse Co ding. In Touretzky, D., Mozer, M., & Hasselmo, M. (Eds.), Neural\n\nInformation Processing Systems \b.\n\n\nSutton, R. S. (\u0001 \b\u0004). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis,\n\nUniversity of Massachusetts, Amherst, MA.\n\n\nSutton, R. S. (\u0001 \b\b). Learning to predict by the metho d of temp oral di�erences. Machine\n\nLearning, \u0003 (\u0001), {\u0004\u0004.\n\n\nSutton, R. S. (\u0001 0). Integrated architectures for learning, planning, and reacting based\n\non approximating dynamic programming. In Proceedings of the Seventh International\n\nConference on Machine Learning Austin, TX. Morgan Kaufmann.\n\n\nSutton, R. S. (\u0001 \u0001). Planning by incremental dynamic programming. In Proceedings\n\nof the Eighth International Workshop on Machine Learning, pp. \u0003\u0005\u0003{\u0003\u0005\u0007. Morgan\n\nKaufmann.\n\n\nTesauro, G. (\u0001 \u0002). Practical issues in temp oral di�erence learning. Machine Learning, \b,\n\n\u0002\u0005\u0007{\u0002\u0007\u0007.\n\n\nTesauro, G. (\u0001 \u0004). TD-Gammon, a self-teaching backgammon program, achieves master\nlevel play. Neural Computation, \u0006 (\u0002), \u0002\u0001\u0005{\u0002\u0001 .\n\n\nTesauro, G. (\u0001 \u0005). Temp oral di�erence learning and TD-Gammon. Communications of\n\nthe ACM, \u0003\b (\u0003), \u0005\b{\u0006\u0007.\n\n\nTham, C.-K., & Prager, R. W. (\u0001 \u0004). A mo dular q-learning architecture for manipula\ntor task decomp osition. In Proceedings of the Eleventh International Conference on\n\nMachine Learning San Francisco, CA. Morgan Kaufmann.\n\n\nThrun, S. (\u0001 \u0005). Learning to play the game of chess. In Tesauro, G., Touretzky, D. S., &\n\nLeen, T. K. (Eds.), Advances in Neural Information Processing Systems \u0007 Cambridge,\n\nMA. The MIT Press.\n\n\n\u0002\b\u0004\n\n\n\n\nReinforcement Learning: A Survey\n\n\nThrun, S., & Schwartz, A. (\u0001 \u0003). Issues in using function approximation for reinforcement\n\nlearning. In Mozer, M., Smolensky, P., Touretzky, D., Elman, J., & Weigend, A.\n\n(Eds.), Proceedings of the \u0001 \u0003 Connectionist Models Summer School Hillsdale, NJ.\n\nLawrence Erlbaum.\n\n\nThrun, S. B. (\u0001 \u0002). The role of exploration in learning control. In White, D. A., &\n\nSofge, D. A. (Eds.), Handbook of Intel ligent Control: Neural, Fuzzy, and Adaptive\n\nApproaches. Van Nostrand Reinhold, New York, NY.\n\n\nTsitsiklis, J. N. (\u0001 \u0004). Asynchronous sto chastic approximation and Q-learning. Machine\n\nLearning, \u0001\u0006 (\u0003).\n\n\nTsitsiklis, J. N., & Van Roy, B. (\u0001 \u0006). Feature-based metho ds for large scale dynamic\n\nprogramming. Machine Learning, \u0002\u0002 (\u0001).\n\n\nValiant, L. G. (\u0001 \b\u0004). A theory of the learnable. Communications of the ACM, \u0002\u0007 (\u0001\u0001),\n\n\u0001\u0001\u0003\u0004{\u0001\u0001\u0004\u0002.\n\n\nWatkins, C. J. C. H. (\u0001 \b ). Learning from Delayed Rewards. Ph.D. thesis, King's College,\n\nCambridge, UK.\n\n\nWatkins, C. J. C. H., & Dayan, P. (\u0001 \u0002). Q-learning. Machine Learning, \b (\u0003), \u0002\u0007 {\u0002 \u0002.\n\n\nWhitehead, S. D. (\u0001 \u0001). Complexity and co op eration in Q-learning. In Proceedings of the\n\nEighth International Workshop on Machine Learning Evanston, IL. Morgan Kauf\n\nmann.\n\n\nWilliams, R. J. (\u0001 \b\u0007). A class of gradient-estimating algorithms for reinforcement learning\n\nin neural networks. In Proceedings of the IEEE First International Conference on\n\nNeural Networks San Diego, CA.\n\n\nWilliams, R. J. (\u0001 \u0002). Simple statistical gradient-following algorithms for connectionist\n\nreinforcement learning. Machine Learning, \b (\u0003), \u0002\u0002 {\u0002\u0005\u0006.\n\n\nWilliams, R. J., & Baird, I I I, L. C. (\u0001 \u0003a). Analysis of some incremental variants of p olicy\n\niteration: First steps toward understanding actor-critic learning systems. Tech. rep.\n\nNU-CCS- \u0003-\u0001\u0001, Northeastern University, College of Computer Science, Boston, MA.\n\n\nWilliams, R. J., & Baird, I I I, L. C. (\u0001 \u0003b). Tight p erformance b ounds on greedy p olicies\n\nbased on imp erfect value functions. Tech. rep. NU-CCS- \u0003-\u0001\u0004, Northeastern Univer\nsity, College of Computer Science, Boston, MA.\n\n\nWilson, S. (\u0001 \u0005). Classi�er �tness based on accuracy. Evolutionary Computation, \u0003 (\u0002),\n\n\u0001\u0004\u0007{\u0001\u0007\u0003.\n\n\nZhang, W., & Dietterich, T. G. (\u0001 \u0005). A reinforcement learning approach to job-shop\n\nscheduling. In Proceedings of the International Joint Conference on Arti�cial Intel\nlience.\n\n\n\u0002\b\u0005\n\n\n",
    "ranking": {
      "relevance_score": 0.7449218792907973,
      "citation_score": 0.9,
      "recency_score": 0.005921891548532124,
      "final_score": 0.7020375046584114
    },
    "citation_key": "Kaelbling1996ReinforcementLA",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "e9b3b2c2cae611b89fa52ed2221d01fe189452da",
    "title": "Guide to Control: Offline Hierarchical Reinforcement Learning Using Subgoal Generation for Long-Horizon and Sparse-Reward Tasks",
    "published": "2023-08-01",
    "authors": [
      "Wonchul Shin",
      "Yusung Kim"
    ],
    "summary": "Reinforcement learning (RL) has achieved considerable success in many fields, but applying it to real-world problems can be costly and risky because it requires a lot of online interaction. Recently, offline RL has shown the possibility of extracting a solution through existing logged data without online interaction. In this work, we propose an offline hierarchical RL method, Guider (Guide to Control), that can efficiently solve long-horizon and sparse-reward tasks from offline data. The high-level policy sequentially generates a subgoal that can guide the agent to arrive at the final goal, and the lower-level policy learns how to reach each given guided subgoal. In the process of learning from offline data, the key is to make the low-level policy reachable to the generated subgoals. We show that high-quality subgoal generation is possible through pre-training a latent subgoal prior model. The well-regulated subgoal generation improves performance while avoiding distributional shifts in offline RL by breaking down long, complex tasks into shorter, easier ones. For evaluations, Guider outperforms prior offline RL methods in long-horizon robot navigation and complex manipulation benchmarks. Our code is available at https://github.com/gckor/Guider.",
    "pdf_url": "https://www.ijcai.org/proceedings/2023/0469.pdf",
    "doi": "10.24963/ijcai.2023/469",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "International Joint Conference on Artificial Intelligence",
    "citation_count": 11,
    "bibtex": "@Article{Shin2023GuideTC,\n author = {Wonchul Shin and Yusung Kim},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {4217-4225},\n title = {Guide to Control: Offline Hierarchical Reinforcement Learning Using Subgoal Generation for Long-Horizon and Sparse-Reward Tasks},\n year = {2023}\n}\n",
    "markdown_text": "Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n# **Guide to Control: Offline Hierarchical Reinforcement Learning** **Using Subgoal Generation for Long-Horizon and Sparse-Reward Tasks**\n\n\n**Wonchul Shin** and **Yusung Kim** _[∗]_\n\n\nSungkyunkwan University\n_{_ swc0406, yskim525 _}_ @skku.edu\n\n\n\n**Abstract**\n\n\nReinforcement learning (RL) has achieved considerable success in many fields, but applying it to\nreal-world problems can be costly and risky because it requires a lot of online interaction. Recently, offline RL has shown the possibility of extracting a solution through existing logged data\nwithout online interaction. In this work, we propose an offline hierarchical RL method, Guider\n(Guide to Control), that can efficiently solve longhorizon and sparse-reward tasks from offline data.\nThe high-level policy sequentially generates a subgoal that can guide the agent to arrive at the final\ngoal, and the lower-level policy learns how to reach\neach given guided subgoal. In the process of learning from offline data, the key is to make the lowlevel policy reachable to the generated subgoals.\nWe show that high-quality subgoal generation is\npossible through pre-training a latent subgoal prior\nmodel. The well-regulated subgoal generation improves performance while avoiding distributional\nshifts in offline RL by breaking down long, complex tasks into shorter, easier ones. For evaluations, Guider outperforms prior offline RL methods in long-horizon robot navigation and complex\nmanipulation benchmarks. Our code is available at\nhttps://github.com/gckor/Guider.\n\n\n**1** **Introduction**\n\n\nDeep reinforcement learning (RL) has achieved remarkable\nsuccess in a range of domains, such as robotics [Kalashnikov _et al._, 2018], games [Silver _et al._, 2016], and autonomous driving [Balaji _et al._, 2019]. However, these works\nrequire a large amount of online interaction with the environment, and in real-world applications, online interaction\nmay be limited due to high risk and cost. To address this\nproblem, offline RL methods have emerged, which use previously logged data without online interaction. Unlike online\nRL, the value overestimation problem on out-of-distribution\nactions can be fatal because correcting the overestimation is\nimpossible in an offline setting[Fujimoto _et al._, 2019]. Recent\n\n\n_∗_ Corresponding author\n\n\n4217\n\n\n\nstudies have produced promising results via various regularization techniques which conservatively mitigate the distributional shift problems [Wu _et al._, 2019; Kumar _et al._, 2020;\nKostrikov _et al._, 2021].\n\nIn the real world, however, we face difficulties of complex\nand long-horizon tasks with sparse rewards which are still\nchallenging to solve with conventional offline RL algorithms.\nPrevious studies have shown the effectiveness of breaking\ndown these long and complex problems into simpler subtasks\nwith a hierarchical structure for online RL [Nachum _et al._,\n2018; Zhang _et al._, 2020; Bagaria _et al._, 2021]. Among the\nvarious design of hierarchy, goal-conditioned hierarchical RL\nallows a high-level policy to sequentially generate subgoals at\nregular intervals and a low-level policy to learn to reach the\ngenerated subgoals. A key element of a subgoal-based hierarchical RL is stable and compatible training between the\nhigh-level and low-level policies [Wang _et al._, 2022]. Specifically, a high-level policy should provide reasonable subgoals\nthat a low-level policy can easily reach. In a fully offline setting, it is difficult to determine whether a low-level policy\nactually reaches a subgoal generated by a high-level policy\nduring the learning process due to the absence of direct validation in the environment. In addition, offline RL learns from\nfixed datasets which may include a mix of task-agnostic or\nsub-optimal trajectories [Fu _et al._, 2020], while online RL can\nupdate a policy or a value function with sufficient exploration\nfor a specific task.\nIn this work, we propose a novel offline hierarchical RL\nalgorithm, Guider, to address the above challenges. Our primary contribution is pre-training a latent variable model to\nextract the latent distribution of reachable subgoals from an\noffline dataset. With the pre-trained prior distribution model,\nwe can effectively apply additional regularization while training the subgoal generation policy. Within the constraints of\nthe reachable subgoal distribution in latent space, the highlevel policy learns to generate subgoals that ensure a gradual approach to the final goal while maximizing the cumulative rewards. The low-level policy learns to reach a subgoal\ngenerated by the higher level through relabeled rewards of\nhindsight goal relabeling. We benchmark our method on a\nvariety of simulated continuous control tasks including robot\nlocomotion, navigation, and manipulation. Extensive experiments show that our subgoal generation method can successfully break down a complex and long-horizon task into eas\n\n\n\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-1-7.png)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-1-8.png)\n\n(a) Offline dataset\n\n\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-1-4.png)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-1-5.png)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-1-6.png)\n\n(b) Illustrative example of subgoal generation\n\n\n\nFigure 1: (a) Task-agnostic (random starts and goals) trajectories in the Antmaze-large offline dataset. (b) An example of the subgoal\ngeneration scenario. Guider learns hierarchical policies from the offline data. For a new longer trajectory test (unseen in the offline dataset),\na high-level policy can sequentially generate a subgoal (the star mark) toward the new final goal (the flag mark) and a low-level policy can\nfocus on reaching each generated subgoal.\n\n\n\nier short-horizon goal-reaching problems. In most evaluation\ntasks, Guider outperforms prior offline RL baselines.\n\n\n**2** **Related Works**\n\n**2.1** **Offline Reinforcement Learning**\n\nOffline RL methods mainly tackle the distributional shift\nissue between a learned policy and a behavior policy that\nis utilized to collect an offline dataset. Prior works constrain the learned policy to stay close to the offline data\ndistribution via explicit regularization [Wu _et al._, 2019;\nFujimoto _et al._, 2019], conservative value learning [Kumar _et_\n_al._, 2020], and importance sampling [Nachum _et al._, 2019].\nSeveral studies employ advantage-weighted supervised learning approach, which implicitly imposes constraints to the offline dataset. Compared with other types of regularization\nmethods, these implicit constraints avoid excessive conservative updates and allow effective online fine-tuning [Nair\n_et al._, 2020; Kostrikov _et al._, 2021]. Recently, [Yang _et al._,\n2022] has shown promising results on multi-goal manipulation tasks by adapting the advantage-weighted method coupled with detailed weighting scheme and hindsight relabeling. Although previous methods effectively address the distributional shift problem, a substantial challenge of accomplishing long-horizon and complex tasks, particularly with sparse\nreward, still remains. We propose a hierarchical approach\nin offline RL using subgoal generation which efficiently improves performance on such challenging problems. From the\nperspective of a high-level policy, we impose additional regularization toward prior distribution that has been pre-trained\nvia unsupervised learning, in order to avoid generating infeasible subgoals. At the same time, a low-level policy learns to\nreach relatively short-horizon subgoals with an implicit regularization.\n\n\n**2.2** **Hierarchical Reinforcement Learning**\n\nHierarchical reinforcement learning (HRL) aims to solve\ncomplex long-horizon tasks by breaking them into more\ntractable subtasks with multi-level policies. In particular,\ngoal-conditioned HRL in which a high-level policy presents\nsubgoals has shown great potential in a variety of sparse reward problems. The effectiveness of goal-conditioned HRL\n\n\n4218\n\n\n\nrelies on the reasonable subgoal generation, _i.e._, the highlevel policy should provide subgoals that the low-level policy\ncan reach. Recent studies have improved the learning efficiency of goal-conditioned HRL by employing hindsight correction [Nachum _et al._, 2018; Levy _et al._, 2019], adversarial\nlearning [Wang _et al._, 2022], and representation learning of\nsubgoal with contrastive objective [Li _et al._, 2021] under interaction with the environment. However, these works need\nextensive online interaction with the environment, which\ncould be expensive and risky in real-world applications. In\ncontrast, our method focuses on training with only offline data\nincluding trajectories that may be irrelevant to the evaluation\ntasks or collected by sub-optimal policies, without additional\naccess to the environment.\nSome recent studies proposed offline HRL methods using temporally extended skills [Lynch _et al._, 2020; Ajay _et_\n_al._, 2021]. These hierarchical skill learning methods train\nlow-level action policies as reconstruction-based decoders\nof state-action sequences. These works are similar to our\nmethod of utilizing unsupervised pre-training and high-level\ntraining in latent space. However, we separately train lowlevel policy via value-based reinforcement learning, which\nprovides a considerable advantage when using sub-optimal\nmixed data.\n\n\n**3** **Preliminaries**\n\nWe consider a continuous control problem formulated as a\ngoal-conditioned Markov decision process (MDP), denoted\nby a tuple ( _S, G, A, p, r, γ_ ), where _S_ is a state space, _G_ is a\ngoal space, _A_ is an action space, _p_ ( _s_ _[′]_ _|s, a, g_ ) is a transition\nprobability, _r_ ( _s, a, g_ ) is a reward function, and _γ ∈_ (0 _,_ 1]\nis a discount factor. The objective is to obtain an optimal policy _π_ that maximizes the expected discounted return\nE _π_ [ [�] _[∞]_ _t_ =0 _[γ][t][r][t]_ []][.] The goal-conditioned task generally provides sparse reward and the reward function is defined as:\n\n_r_ ( _st, a, st_ +1 _, g_ ) = 1 if _∥s_ _[G]_ _t_ +1 _[−]_ _[g][∥]_ [2] _[ < ϵ]_ (1)\n0 otherwise\n�\n\n\nwhere _s_ _[G]_ _∈G_ is a state that is mapped to goal space, and _ϵ_\nis a given threshold from the environment. The goal space is\ndefined according to the environment and can be a subset of\n\n\n\n\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-2-0.png)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-2-1.png)\n\n\nො𝒔𝓖𝒕+𝒄 KL-regularization\n\n\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-2-2.png)\n\n\n\n(1) Latent variable subgoal model\n\npre-training policy training\n\n\n\n(2) Subgoal generation (high-level), reaching (low-level)\n\n\n\n(3) Online policy evaluation\n\n\n\npre-training\n\n\n\nFigure 2: Overall framework of Guider. A latent variable model is pre-trained to embed reachable subgoals into a latent space. A highlevel policy is trained to generate a reachable subgoal while regularized with a pre-trained prior model. A low-level policy learns to reach a\ngenerated subgoal. In online evaluation, the high-level policy generates a subgoal every _c_ steps to guide the low-level policy.\n\n\n\nthe state space. For instance, the goal space of a robot navigation task is given as a location of the robot torso, while\nthe state space is composed of tens of proprioceptive features\n(e.g. coordinates, angles, velocities). For offline RL settings,\nwe assume that the agent uses only previously collected data\nwithout any interaction with the environment during the training process.\nWe consider the goal-conditioned HRL framework consisting of a high-level policy _πh_ ( _z|s, g_ final) and a low-level policy\n_πl_ ( _a|s, g_ sub _, g_ final), where _z_ is a subgoal in a latent space, _g_ sub\nis a subgoal decoded into a goal space, and _g_ final is the final\ngoal of the downstream task. The high-level policy generates\na subgoal every c-steps, and the low-level policy executes an\naction in each of the steps to reach the generated subgoal. In\nthe following section, we show how to train the latent variable\nmodel which captures the reachable subgoals within c-steps\nfrom the current state, and utilizes the pre-trained model to\nregularize the subgoal generation policy.\n\n\n**4** **Method**\n\n\nIn this section, we describe our method Guider, which trains\nsubgoal generation policy within the constraints of reachable\nsubgoal distribution in latent space. Our objective is to ensure the low-level policy easily reaches a subgoal generated\nby the high-level policy, while both policies are trained to\navoid distributional shifts from the offline dataset without any\nadditional data collection or online interaction. We first introduce how the overall framework is structured, and elaborate\non each training part.\n\n\n**4.1** **Model Overview**\n\nOur model consists of three parts of training: (1) pretraining the latent variable model which embeds reachable\nsubgoals in latent space via unsupervised learning (2) training the high-level policy to generate a subgoal that progressively approaches a given final goal (3) training the lowlevel policy that reaches the subgoal generated at the highlevel. We train the latent variable model comprised of en\n\n4219\n\n\n\ncoder _qψ_ ( _z|s_ _[G]_ _t_ _[, s][G]_ _t_ + _c_ [)][, decoder] _[ p][θ]_ [(] _[s]_ _t_ _[G]_ + _c_ _[|][s]_ _t_ _[G][, z]_ [)][, and trainable]\nprior model _ρω_ ( _z|s_ _[G]_ _t_ [)][. We relabel the offline dataset with]\nthe pre-trained encoder _qψ_ ( _z|s_ _[G]_ _t_ _[, s][G]_ _t_ + _c_ [)][ for the high-level pol-]\nicy training, and the high-level policy _πh_ ( _z|s_ _[G]_ _t_ _[, g]_ [final][)][ is regu-]\nlarized with the pre-trained prior distribution _ρω_ ( _z|s_ _[G]_ _t_ [)][. The]\nlow-level policy _πl_ ( _a|st, g_ sub _, g_ final) is trained by relabeled rewards for reaching the subgoals. At test time, the high-level\npolicy generates a subgoal every c-steps, and the low-level\npolicy selects an action for each step toward the generated\nsubgoal. The architecture of our model is illustrated in Figure 2.\n\n\n**4.2** **Latent Variable Model for Reachable Subgoals**\n\nWe train the latent variable model in order to learn the subgoal\ngeneration policy in the latent space of subgoals that can be\nreached within c-steps from the current state, and not in the\nentire goal space. We sample pairs of random states and cstep next states from the offline dataset and train the encoder\n_qψ_ ( _z|s_ _[G]_ _t_ _[, s][G]_ _t_ + _c_ [)][ and the decoder] _[ p][θ]_ [(] _[s]_ _t_ _[G]_ + _c_ _[|][s]_ _t_ _[G][, z]_ [)][ to reconstruct]\nthe next state in a goal space after c-steps, while conditioned\non the current states. Additionally, we train subgoal prior\nmodel _ρω_ ( _z|s_ _[G]_ _t_ [)][ to obtain the latent distribution when a cur-]\nrent state is given. We jointly train the components of the\nlatent variable model by maximizing the following objective\nfunction based on evidence lower bound (ELBO):\n\n\nE _z∼qψ_ [log _pθ_ ( _s_ _[G]_ _t_ + _c_ _[|][s]_ _t_ _[G][, z]_ [)] _[ −]_ _[βD]_ [KL][(] _[q][ψ]_ [(] _[z][|][s][G]_ _t_ _[, s][G]_ _t_ + _c_ [)] _[∥][ρ][ω]_ [(] _[z][|][s]_ _t_ _[G]_ [))]] _[.]_\n(2)\nThe subgoal prior model is trained to cover the latent distributions of reachable subgoals from the current state, and the\nposterior distribution from the encoder is regularized toward\nthe prior by the KL loss term. _β_ is a weighting parameter\nfor the regularization. We implement a balanced training of\nboth parameterized prior and posterior networks with alternately stopped gradient flow at different learning rates following [Hafner _et al._, 2020]:\n\n\n_D_ KL( _qψ∥ρω_ ) = _τD_ KL(sg( _qψ_ ) _∥ρω_ ) + (1 _−_ _τ_ ) _D_ KL( _qψ∥_ sg( _ρω_ ))\n(3)\n\n\n\n\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n\n\nwhere sg( _·_ ) indicates a stopped gradient flow and _τ_ is the\nweighting parameter.\n\n\n**4.3** **High-level Subgoal Generation Policy**\n\n\nWe apply the pre-trained latent variable model to train the\nhigh-level subgoal generation policy _πh_ ( _z|s_ _[G]_ _, g_ final). In the\nbeginning, we relabel the offline dataset using the trained encoder. An action _at_ is replaced by an embedded latent subgoal _z ∼_ _qψ_ ( _·|s_ _[G]_ _t_ _[, s][G]_ _t_ + _c_ [)][. The next state of the high-level]\ntransition is relabeled as the state mapped to the goal space\nafter c-step _s_ _[G]_ _t_ + _c_ [. In addition, we relabel the original goal]\nwith a random future state after the c-step, employing hindsight relabeling which improves the learning efficiency of a\ngoal-conditioned policy [Andrychowicz _et al._, 2017]. The relabeled reward ˜ _r_ ( _s_ _[G]_ _t_ _[, z, s][G]_ _t_ + _c_ _[,]_ [ ˜] _[g]_ [final][)][ for relabeled goal][ ˜] _[g]_ [final] [is]\ncomputed by equation 1. Consequently, an original transition\ntuple ( _st, at, rt, st_ +1 _, g_ final) in the offline dataset is relabeled\nto ( _s_ _[G]_ _t_ _[, z][t][,]_ [ ˜] _[r][t][, s][G]_ _t_ + _c_ _[,]_ [ ˜] _[g]_ [final][)][.]\nWe optimize the subgoal generation policy _πh_ ( _z|s_ _[G]_ _, g_ final)\nto maximize value function _Q_ ( _s_ _[G]_ _, z, g_ final) while regularized\ntoward the pre-trained subgoal prior _ρω_ ( _z|s_ _[G]_ ) following the\nproposed objective:\n\n\nE _z∼qψ_ [ _Q_ ( _s_ _[G]_ _, z, g_ final) _−_ _D_ KL( _πh_ ( _z|s_ _[G]_ _, g_ final) _∥ρω_ ( _z|s_ _[G]_ ))] _._\n(4)\nMinimization of the reverse KL divergence constrains the\nlearned policy to generate subgoals within a learned subgoal\nspace that is reachable in c-steps from the current state. Our\nregularized policy optimization objective can be applied to an\noff-the-shelf offline RL algorithm. Here, we opt for Conservative Q-Learning (CQL), an extensively used offline actorcritic algorithm [Kumar _et al._, 2020]. A maximum entropy\nregularization term min log _πh_ ( _z|s, g_ final) of the original CQL\npolicy objective which encourages diverse behaviors is replaced with our KL regularization. This regularization plays\nan important role in learning hierarchical policies in offline\nconditions where it is impossible to know whether the lowlevel policy can reach the generated subgoals. We will show\nhow the additional regularization improves the quality of the\ngenerated subgoals and resulting performance in section 5.\n\n\n**4.4** **Low-level Subgoal Reaching Policy**\n\n\nOur subgoal generation method can be used in conjunction with any low-level goal-conditioned policy learning\nalgorithm such as goal-conditioned supervised learning\n(GCSL) [Ghosh _et al._, 2021] or goal-conditioned version of\nexisting offline RL methods. Since our hierarchical framework decomposes complex tasks into relatively easy shorthorizon goal-reaching problems, the lower-level policy can\nbe learned effectively with a simple policy learning algorithm. We introduce a simple advantage-weighted supervised\nlearning method adapted for our goal-conditioned hierarchical framework.\nThe lower-level policy _πl_ ( _a|s, g_ sub _, g_ final) should learn to\nchoose an optimal action to reach a guided subgoal _g_ sub\nwhile avoiding the distributional shift problem of offline RL.\nTherefore, we update the policy by maximizing advantage\n_A_ ( _s, a, g_ sub _, g_ final) with KL divergence regularization toward\n\n\n4220\n\n\n\n**Algorithm 1** Guider\n**Require** : Dataset _D_, subgoal generation period _c_ .\n**Initialize** : Encoder _qψ_, decoder _pθ_, prior _ρω_, high-level\npolicy _πh_, high-level value function _Qh_, low-level policy _πl_,\nlow-level value function _Ql_ .\n\n\n1: **for** _M_ iterations **do**\n2: Sample mini-batch: _{_ ( _s_ _[G]_ _t_ _[, s][G]_ _t_ + _c_ [)] _[} ∼D]_\n3: Update _ψ, θ, ω_ using objective in eq. 2\n4: **end for**\n\n5: **for** _N_ iterations **do**\n6: _# High-level policy training_\n7: Sample mini-batch: _{_ ( _st, st_ + _c, g_ final) _} ∼D_\n8: _z ∼_ _qψ_ ( _s_ _[G]_ _t_ _[, s][G]_ _t_ + _c_ [)]\n9: _g_ ˜final _∼_ _s_ _[G]_ _i_ ( _t_ + _c < i ≤_ _T_ )\n10: Relabel the mini-batch: _{_ ( _s_ _[G]_ _t_ _[, z,]_ [ ˜] _[r][t][, s][G]_ _t_ + _c_ _[,]_ [ ˜] _[g]_ [final][)] _[}]_\n11: Update _Qh_ to minimize TD error\n12: Update _πh_ using objective in eq. 4\n13: _# Low-level policy training_\n14: Sample mini-batch: _{_ ( _st, at, rt, st_ +1 _, g_ final) _} ∼D_\n15: _g_ ˜sub _∼_ _s_ _[G]_ _j_ ( _t < j ≤_ _t_ + _c_ )\n16: Relabel the mini-batch: _{_ ( _st, a,_ ˜ _rt, st_ +1 _,_ ˜ _g_ sub _,_ ˜ _g_ final) _}_\n17: Update _Ql_ to minimize TD error\n18: Update _πl_ using objective in eq. 6\n19: **end for**\n20: **return** Trained policies _πh, πl_ .\n\n\nthe behavior policy _πβ_ ( _s, a, g_ sub _, g_ final), using the following\noptimization formulation:\n\n\nmax E _a∼πl_ [ _A_ ( _s, a, g_ sub _, g_ final)] s.t. _D_ KL( _πl∥πβ_ ) _< ϵ_ (5)\n\n\nwhere _ϵ_ is a threshold value. The above constraint optimization problem is solved by enforcing the Karush-KuhnTucker conditions following prior works [Peng _et al._, 2019;\nNair _et al._, 2020]. The implicitly constrained policy optimization objective is derived as:\n\n\nE _a∼πl_ [exp _A_ ( _s, a, g_ sub _, g_ final) _·_ log _πl_ ( _a|s, g_ sub _, g_ final)] _._ (6)\n\n\nWe train the low-level policy with goal-relabeled data similar to the high-level policy training. However, the relabeling\nstrategy is slightly different considering our subgoal generation framework. We relabel the subgoal _g_ sub to random future\nstates within c-steps as ˜ _g_ sub = _s_ _[G]_ _i_ [for] _[ t < i][ ≤]_ _[t]_ [ +] _[ c]_ [ + 1][, since]\nour high-level policy is trained to generate subgoals reachable\nwithin c-steps. This modified relabeling strategy effectively\naccelerates training the low-level policy. The whole training process of our proposed method is summarized in Algorithm 1.\n\n\n**5** **Experiments**\n\nThrough our experiments, we aim to answer the following\nquestions: (1) Can Guider be effectively applied to a variety of long-horizon and sparse-reward tasks? (2) Can Guider\nlearn stitching knowledge from a fixed dataset including taskagnostic trajectories or mixed sub-optimal behaviors? (3)\nDoes our proposed latent subgoal prior model help generate\n\n\n\n\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n\nOPAL [1] GCSL WGCSL CQL+HER Guider (Ours)\n\n\nAntmaze-umaze        - 49.5 _±_ 5.9 79.5 _±_ 2.3 60.7 _±_ 7.6 **88.5** _±_ **4.4**\n\nAntmaze-medium 81.1 _±_ 3.1 45.3 _±_ 13.7 54.0 _±_ 8.8 28.3 _±_ 5.3 **87.3** _±_ **0.4**\nAntmaze-large 70.3 _±_ 2.9 11.8 _±_ 3.6 17.5 _±_ 6.7 11.3 _±_ 8.2 **80.8** _±_ **4.6**\n\n\nFetchReach-expert 46.0 _±_ 0.1 39.8 _±_ 0.4 46.3 _±_ 0.2 47.3 _±_ 0.2 **48.0** _±_ **0.1**\nFetchPush-expert 18.9 _±_ 0.4 32.8 _±_ 1.6 38.9 _±_ 0.4 38.5 _±_ 1.2 **39.9** _±_ **1.0**\nFetchSlide-expert 0.7 _±_ 0.1 4.4 _±_ 1.4 **10.6** _±_ **0.9** 3.9 _±_ 1.7 7.6 _±_ 0.3\nFetchPick-expert 24.2 _±_ 2.6 27.2 _±_ 0.5 36.1 _±_ 0.5 36.9 _±_ 2.3 **40.9** _±_ **0.6**\n\n\nFetchReach-mixed 45.2 _±_ 0.2 34.5 _±_ 1.0 46.7 _±_ 0.2 46.6 _±_ 0.5 **47.0** _±_ **0.2**\n\nFetchPush-mixed 5.5 _±_ 2.5 6.1 _±_ 2.1 30.8 _±_ 3.2 21.8 _±_ 3.8 **33.4** _±_ **2.3**\n\nFetchSlide-mixed 0.7 _±_ 0.2 1.9 _±_ 0.7 **5.6** _±_ **1.0** 3.7 _±_ 1.2 4.1 _±_ 0.6\n\nFetchPick-mixed 5.9 _±_ 3.0 12.0 _±_ 3.5 22.9 _±_ 5.9 25.8 _±_ 6.9 **33.2** _±_ **2.4**\n\n\nKitchen-complete         - 58.6 _±_ 8.7 57.7 _±_ 4.7 33.3 _±_ 5.7 **68.8** _±_ **7.2**\nKitchen-partial 65.2 _±_ 2.5 55.0 _±_ 14.5 59.4 _±_ 13.3 26.3 _±_ 4.5 **70.4** _±_ **7.8**\nKitchen-mixed 64.6 _±_ 1.8 56.2 _±_ 5.4 49.6 _±_ 2.9 23.5 _±_ 1.1 **67.1** _±_ **5.8**\n\n\nTable 1: Performance of Guider and baselines on all tasks. The scores are averaged over 4 random seeds at the end of training. For every\nseed, we run an evaluation of 100 episodes.\n\n\n\na reasonable subgoal and improve the performance of offline\nhierarchical RL? To answer the first and second questions, we\nevaluate the Guider against a variety of benchmarks dealing\nwith stitching and narrow distribution challenges of offline\nRL. We present ablation studies to answer the third question.\n\n\n**5.1** **Environments and Datasets**\n\nWe evaluate our method on diverse continuous control tasks\nin three simulated robotic domains: maze navigation, arm\nmanipulation, and kitchen manipulation. All these environments provide sparse reward feedback.\n\n\n**Antmaze**\n\nThe antmaze domain requires an agent to control a quadruped\nrobot to navigate to a designated goal position. We use _di-_\n_verse_ dataset from the D4RL benchmark [Fu _et al._, 2020].\nThe dataset is composed of trajectories from random start locations to random goal positions, which are irrelevant to the\ntarget task. Therefore, the agent is required to stitch the parts\nof suboptimal trajectories to find the optimal path to the evaluation goal. The domain contains three maze layouts ( _umaze_,\n_medium_, _large_ ). In particular, the long-horizon _large_ task is\nsubstantially challenging to solve with a conventional offline\nRL approach.\n\n\n**Fetch**\n\nThe fetch domain contains four robotic arm manipulation tasks: _FetchPickAndPlace_, _FetchPush_, _FetchSlide_,\n_FetchReach_ . The tasks are to place defined objects at randomly given target goal positions from random initial states.\nWe use _expert_ and _mixed_ dataset for each task. The _mixed_\ndataset contains a mixture of trajectories collected by 90%\nrandom policy and 10% expert policy provided from [Yang\n_et al._, 2022].\n\n\n**Kitchen**\n\nThe tasks of kitchen domain are to control a robotic arm interacting with several household items to reach a desired goal\nconfiguration. D4RL benchmark [Fu _et al._, 2020] introduces\n\n\n4221\n\n\n\nthree types of datasets collected by human demonstrations.\nThe _complete_ dataset consists of a relatively small number\nof trajectories performing all subtasks in order. The _partial_\ndataset contains a mix of successful trajectories and partially\nperformed trajectories. The partially performed trajectories\ncontain a subset of the desired configuration as well as subtasks irrelevant to the target task. The _mixed_ dataset consists\nof only partially performed trajectories.\n\n\n**5.2** **Baselines**\n\n\nWe compare Guider against prior approaches in goalconditioned supervised learning, offline goal-conditioned RL,\nand offline hierarchical RL.\n\n\n  - **GCSL** [Ghosh _et al._, 2021] is a goal-conditioned supervised learning method using hindsight goal relabeling.\n\n\n  - **WGCSL** [Yang _et al._, 2022] is a state-of-the-art offline\ngoal-conditioned RL algorithm based on advantageweighted supervised learning, using hindsight goal relabeling.\n\n\n  - **CQL+HER** [Kumar _et al._, 2020] is a modified version of offline RL algorithm CQL which learns the outof-distribution value function conservatively, with hindsight goal relabeling added.\n\n\n  - **OPAL** [Ajay _et al._, 2021] is a state-of-the-art offline\nhierarchical RL method based on skill discovery. The\nhigh-level policy directs which skill to use, and the lowlevel policy executes a sequence of actions for the skill.\n\n\nImplementation details and hyperparameters for our methods\nand baselines are provided in supplementary material A.\n\n\n1The results for OPAL on antmaze and kitchen tasks are taken\nfrom [Ajay _et al._, 2021]. Since the official source code of\nOPAL is not available, the results for fetch tasks are from our reimplementation based on the paper.\n\n\n\n\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n\nAntmaze-large\n100\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-5-5.png)\n\n\n**1** **3** **4** 75\n\n\n50\n\n\n25\n\n\n\n0\n\n\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-5-6.png)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-5-7.png)\n\nGuider BC GCSL CQL WGCSL\n\n\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-5-2.png)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-5-3.png)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-5-4.png)\n\nFigure 3: Visualization of sequentially generated subgoals on evaluation for each domain. In antmaze (left), the subgoal suggests a twodimensional coordinate a quadruped robot should reach. In fetch\n(middle), the subgoal guides the intermediate positions where the\nbox should be placed. In kitchen (right), the subgoal presents a subtask to be performed.\n\n\n**5.3** **Overall Results**\n\n\nThe overall results of our experiments are reported in Table 1,\nand we present the learning curves in Figure 6 in the supplementary material. Our method Guider achieves the best performance on 12 out of 14 tasks. Specifically, the results show\nthat Guider outperforms prior methods by a large margin on\nchallenging offline RL problems such as long-horizon tasks\nor using mixed datasets.\nThe antmaze and kitchen are considerably challenging\ntasks as they require learning policies for reaching longhorizon goals from sparse rewards. Besides, all antmaze\ndatasets and _mixed_ dataset of kitchen are composed of taskagnostic datasets, which means that the tasks can not be accomplished by imitating certain trajectories in datasets. The\nagent must learn to stitch meaningful sub-trajectories from\ndatasets to solve new longer trajectories at testing. Our proposed method effectively addresses these challenges by generating adequate subgoals which are easily reachable. Figure 3 shows the generated subgoals from our high-level policy on evaluations of each task. It is remarkable that Guider\nshows a success rate of over 80% in _antmaze-large_, where\nprior methods without the hierarchical approach succeed at\nless than 20%. We observe that our subgoal-based hierarchical method also outperforms OPAL, the skill-based hierarchical approach. We will discuss the difference between the two\napproaches through an ablation study in the next subsection.\nAnother important challenge in offline RL is learning an\noptimal policy from mixed data including optimal and suboptimal trajectories. The agent should learn near-optimal\npolicies from the datasets consisting of only a small portion\nof expert trajectories and the rest of the sub-optimal or random trajectories. In our experiments, the _mixed_ datasets of\nfetch and the _partial_ dataset of kitchen tasks deal with the\nabove challenge. Although the fetch- _mixed_ dataset contains\n10% of expert trajectories and kitchen-partial contains only\n3.2% of optimal trajectories, Guider significantly surpasses\nthe average return of the dataset.\n\n\n**5.4** **Ablation Studies**\n\n\n**Different Low-level Policies**\n\nWe conduct an ablation study to investigate how our subgoal\ngeneration method improves performance on a long-horizon\ntask. We train the unsupervised latent variable model and the\n\n\n4222\n\n\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-5-9.png)\n\nFetchPick-mixed\n40\n\n\n30\n\n\n20\n\n\n10\n\n\n0\n\nGuider BC GCSL CQL WGCSL\n\n\nw/o Guider Subgoal with Guider Subgoal\n\n\nFigure 4: Performance of Guider with various low-level policies\ncompared to individual methods without Guider. All results are averaged over 4 random seeds at the end of training\n\n|Col1|Guider Guider Guider<br>w/oprior w/oCQL|\n|---|---|\n|Antmaze-large<br>FetchPick-mixed<br>Kitchen-partial|80.8_±_4.6<br>57.0_±_11.3<br>78.5_±_11.4<br>33.2_±_2.4<br>20.2_±_5.0<br>28.8_±_0.9<br>70.4_±_7.8<br>38.9_±_16.2<br>59.7_±_10.2|\n\n\n\nTable 2: Ablation study on regularization methods. The performance significantly decreases without prior regularization.\n\n\nhigh-level policy of our proposed Guider framework on the\nantmaze-large task, in conjunction with diverse low-level policy learning methods. Figure 4 shows a significant improvement in performance for all types of low-level policy learning\nwhen combined with Guider’s subgoal generation method.\nThis indicates that Guider takes long-horizon tasks that are\ndifficult to solve with existing methods and turns them into\nsimple tractable problems with guided subgoals.\nInterestingly, we find that Guider combined with behavior\ncloning (BC) leads to performance similar to OPAL. OPAL\nlearns skill-based hierarchical policies, where the low-level\npolicy extracts a temporally-extended sequence of primitive\nactions by behavior cloning loss [Ajay _et al._, 2021]. On the\nother hand, Guider originally learns low-level goal-reaching\npolicy based on trained value function. This reinforcement\nlearning process at the lower level facilitates additional improvement than imitating actions in the dataset. We conjecture that the different approach in low-level policy learning\naccounts for the superior performance of Guider to OPAL,\nalthough both methods use hierarchical architectures.\n\n\n**Regularization Methods**\nWe also examine the importance of our proposed regularization strategy in subgoal generation and performance. We\n\n\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-5-8.png)\n\n\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n\ncompare three different regularization methods while high- 25\nlevel policy training on the antmaze tasks.\n\n\n\n\n  - **Guider** is our proposed method implemented on top\nof CQL with additional regularization toward the prior\n_ρω_ ( _z|s_ _[G]_ _t_ [)][.]\n\n\n  - **Guider w/o prior** learns the high-level policy without\nany additional regularization. It is also implemented on\ntop of CQL.\n\n\n  - **Guider w/o CQL** is implemented on top of Soft ActorCritic (SAC) [Haarnoja _et al._, 2017]. SAC is an online\nactor-critic algorithm without any regularization towards\nthe behavior policy of the offline dataset. We added regularization with the prior _ρω_ ( _z|s_ _[G]_ _t_ [)][.]\n\n\nAs shown in Table 2, the performance of high-level policy\ntraining considerably decreases without prior regularization.\nWe visualize the generated subgoals and the arrived position\non evaluation in Figure 7 in the supplementary material. We\nobserve that the agent reaches the generated subgoal in practice only when prior regularization is imposed. Otherwise,\nthe subgoal is generated too far or at an invalid location in relation to the final goal. These infeasible subgoal generations\nresult in decreased success rate.\n\n\n**Subgoal Generation Period**\nWe conduct experiments with varying subgoal generation period _c_ to investigate the influence of this hyperparameter. As\nshown in Table 3, the subgoal generation period does not critically affect the performance of Guider. However, we understand that too short a subgoal generation period insufficiently\nbenefits from a temporal abstraction of the hierarchical architecture, and too long a period makes it difficult to reach the\nsubgoal with the low-level policy. We chose the appropriate\nrange of the hyperparameter considering the episode length of\ntasks. In the case of antmaze, it shows the best performance\naround _c_ = 50, which is 1/20 of the episode length of 1000.\nAdditional results on the other tasks are provided in the supplementary material. More sophisticated ways to determine\nthe subgoal generation period can be studied for future work.\n\n\n**Latent Subgoal vs. Reconstructed Subgoal**\nWe investigate the effectiveness of using a decoder to reconstruct a latent subgoal generated by a high-level policy. Our proposed model learns a low-level policy conditioned on the subgoal in goal space, not in latent space.\nDuring the evaluation, a generated latent subgoal from the\nhigh-level policy is decoded into a goal space before being provided to the low-level policy. However, we do not\nclaim that using a decoder is mandatory for our method.\nAs shown in Figure 5, the difference between Guider with\nand without a decoder is not significant, although slightly\nbetter performance is achieved with the decoder. Our experiments are conducted on environments possessing relatively low-dimensional spatial goal space, which provides\nclear information. Considering the property of these environments, learning goal-reaching policy in goal space is favorable. On the other hand, one could expect that learning low-level policy conditioned on latent subgoal improves\nperformance on high-dimensional observations such as RGB\n\n\n4223\n\n\n|Col1|25 50 75 100|\n|---|---|\n|Antmaze-large<br>Antmaze-medi|77.5<br>80.8<br>79.3<br>73.8<br>um<br>83.3<br>87.3<br>85.0<br>81.5|\n\n\n\nTable 3: Ablation study on subgoal generation period _c_ . The results\nare averaged over 4 random seeds.\n\n\n\n50\n\n\n40\n\n\n30\n\n\n20\n\n\n10\n\n\n0\n\n\n100\n\n\n80\n\n\n60\n\n\n40\n\n\n20\n\n\n0\n\n\n\nFetchPick-expert\n\n\n0 1 2 3 4 5\nTraining Steps (×1e [5] )\n\n\nAntmaze-large\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-6-3.png)\n\n\n0 1 2 3 4 5\nTraining Steps (×1e [5] )\n\n\n\n50\n\n\n40\n\n\n30\n\n\n20\n\n\n10\n\n\n0\n\n\n100\n\n\n80\n\n\n60\n\n\n40\n\n\n20\n\n\n0\n\n\n\nFetchPick-mixed\n\n\n0 1 2 3 4 5\nTraining Steps (×1e [5] )\n\n\nKitchen-partial\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-6-2.png)\n\n\n0 1 2 3 4 5\nTraining Steps (×1e [5] )\n\n\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-6-0.png)\n\n![](output/images/e9b3b2c2cae611b89fa52ed2221d01fe189452da.pdf-6-1.png)\n\nwith decoder w/o decoder\n\n\nFigure 5: Learning curve of Guider with and without a decoder. Our\nmethod suggests decoding a latent subgoal generated from the highlevel policy before passing it to the low-level policy. However, the\nperformance does not drastically decrease even without a decoder.\nFor high-dimensional observation and goal space, an approach using\nlatent subgoal-conditioned low-level policy without a decoder can\nalso be considered. All results are averaged over 4 random seeds\nand the shaded region represents the standard deviation.\n\n\nimages, as suggested in prior works [Rafailov _et al._, 2020;\nHafner _et al._, 2022]. Through these empirical results, we suggest that the general framework of Guider can be extended to\ndiverse high-dimensional observations.\n\n\n**6** **Conclusion**\n\nIn this work, we propose Guider, an offline hierarchical reinforcement learning method that learns to generate subgoals\nat the high level and reach the generated subgoals at the low\nlevel. Our unsupervised pre-training of the subgoal prior distribution in latent space can effectively regularize the subgoal generation policy. The generated subgoal can be easily\nreached by simple low-level policies. Empirical studies show\nthat our proposed method outperforms prior offline RL methods on long-horizon and sparse-reward tasks. An interesting direction of future work would be generating meaningful\nsubgoals from high-dimensional space such as offline RGB\nimages. We also plan to design a flexible subgoal generation\nmodel where the generation period can vary depending on the\nsituation.\n\n\n\n\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n\n\n**Acknowledgments**\n\n\nThis work was supported partly by the Institute of Information and Communications Technology Planning and Evaluation (IITP) grant funded by the Korea Government (MSIT)\n(No. 2022-0-01045, Self-directed Multi-Modal Intelligence\nfor solving unknown, open domain problems), (No. 20220-00688, AI Platform to Fully Adapt and Refect PrivacyPolicy Changes), and (No. 2019-0-00421, Artifcial Intelligence\nGraduate School Program(Sungkyunkwan University)).\n\n\n**References**\n\n[Ajay _et al._, 2021] Anurag Ajay, Aviral Kumar, Pulkit\nAgrawal, Sergey Levine, and Ofir Nachum. Opal: Offline\nprimitive discovery for accelerating offline reinforcement\nlearning. In _International Conference on Learning Repre-_\n_sentations_, 2021.\n\n[Andrychowicz _et al._, 2017] Marcin Andrychowicz, Dwight\nCrow, Alex Ray, Jonas Schneider, Rachel Fong, Peter\nWelinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and\nWojciech Zaremba. Hindsight experience replay. In _Ad-_\n_vances in Neural Information Processing Systems_, pages\n5048––5058, 2017.\n\n[Bagaria _et al._, 2021] Akhil Bagaria, Jason Senthil, Matthew\nSlivinski, and George Konidaris. Robustly learning composable options in deep reinforcement learning. In _Inter-_\n_national Joint Conference on Artificial Intelligence_, 2021.\n\n[Balaji _et al._, 2019] Bharathan Balaji, Sunil Mallya, Sahika\nGenc, Saurabh Gupta, Leo Dirac, Vineet Khare, Gourav\nRoy, Tao Sun, Yunzhe Tao, Brian Townsend, Eddie Calleja, Sunil Muralidhara, and Dhanasekar Karuppasamy. Deepracer: Educational autonomous racing platform for experimentation with sim2real reinforcement\nlearning. _arXiv preprint arXiv:1911.01562_, 2019.\n\n[Fu _et al._, 2020] Justin Fu, Aviral Kumar, Ofir Nachum,\nGeorge Tucker, and Sergey Levine. D4rl: Datasets for\ndeep data-driven reinforcement learning. _arXiv preprint_\n_arXiv:2004.07219_, 2020.\n\n[Fujimoto _et al._, 2019] Scott Fujimoto, David Meger, and\nDoina Precup. Off-policy deep reinforcement learning\nwithout exploration. In _International Conference on Ma-_\n_chine Learning_, pages 2052–2062. PMLR, 2019.\n\n[Ghosh _et al._, 2021] Dibya Ghosh, Abhishek Gupta, Ashwin\nReddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and\nSergey Levine. Learning to reach goals via iterated supervised learning. In _International Conference on Learning_\n_Representations_, 2021.\n\n[Haarnoja _et al._, 2017] Tuomas Haarnoja, Haoran Tang,\nPieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In _International Con-_\n_ference on Machine Learning_, pages 1352–1361. PMLR,\n2017.\n\n[Hafner _et al._, 2020] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with\ndiscrete world models. _arXiv preprint arXiv:2010.02193_,\n2020.\n\n\n4224\n\n\n\n\n[Hafner _et al._, 2022] Danijar Hafner, Kuang-Huei Lee, Ian\nFischer, and Pieter Abbeel. Deep hierarchical planning\nfrom pixels. _arXiv preprint arXiv:2206.04114_, 2022.\n\n[Kalashnikov _et al._, 2018] Dmitry Kalashnikov, Alex Irpan,\nPeter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang,\nDeirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Scalable deep reinforcement learning for vision-based robotic manipulation.\nIn _Conference on Robot Learning_, 2018.\n\n[Kostrikov _et al._, 2021] Ilya Kostrikov, Ashvin Nair, and\nSergey Levine. Offline reinforcement learning with implicit q-learning. _arXiv preprint arXiv:2110.06169_, 2021.\n\n[Kumar _et al._, 2020] Aviral Kumar, Aurick Zhou, George\nTucker, and Servey Levine. Conservative q-learning\nfor offline reinforcement learning. _arXiv preprint_\n_arXiv:2006.04779_, 2020.\n\n[Levy _et al._, 2019] Andrew Levy, George Konidaris, Robert\nPlatt, and Kate Saenko. Learning multi-level hierarchies\nwith hindsight. In _International Conference on Learning_\n_Representations_, 2019.\n\n[Li _et al._, 2021] Siyuan Li, Lulu Zheng, Jianhao Wang, and\nChongjie Zhang. Learning subgoal representations with\nslow dynamics. In _International Conference on Learning_\n_Representations_, 2021.\n\n[Lynch _et al._, 2020] Corey Lynch, Mohi Khansari, Ted Xiao,\nVikash Kumar, Jonatah Tompson, Sergey Levine, and\nPierre Sermanet. Learning latent plans from play. In _Con-_\n_ference on Robot Learning_, 2020.\n\n[Nachum _et al._, 2018] Ofir Nachum, Shixiang Gu, Honglak\nLee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. In _Advances in Neural Information_\n_Processing Systems_, pages 3307–3317, 2018.\n\n[Nachum _et al._, 2019] Ofir Nachum, Bo Dai, Ilya Kostrikov,\nYinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy gradient from arbitrary experience. _arXiv_\n_preprint arXiv:1912.02074_, 2019.\n\n[Nair _et al._, 2020] Ashvin Nair, Abhishek Gupta, Murtaza\nDalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. _arXiv preprint_\n_arXiv:2006.09359_, 2020.\n\n[Peng _et al._, 2019] Xue B. Peng, Aviral Kumar, Grace\nZhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.\n\n[Rafailov _et al._, 2020] Rafael Rafailov, Tianhe Yu, Aravind\nRajeswaran, and Chelsea Finn. Offline reinforcement\nlearning from images with latent space models. _arXiv_\n_preprint arXiv:2012.11547_, 2020.\n\n[Silver _et al._, 2016] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche1, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik\nGrewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,\nTimothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,\nThore Graepel, and Demis Hassabis. Mastering the game\n\n\n\n\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI-23)\n\n\nof go with deep neural networks and tree search. _Nature_,\n529(7587):484–489, 2016.\n\n[Wang _et al._, 2022] Vivienne H. Wang, Joni Pajarinen,\nTinghuai Wang, and Joni-Kristian Kamarainen. Hierarchical reinforcement learning with adversarially guided subgoals. _arXiv preprint arXiv:2201.09635_, 2022.\n\n[Wu _et al._, 2019] Yifan Wu, George Tucker, and Ofir\nNachum. Behavior regularized offline reinforcement\nlearning. _arXiv preprint arXiv:1911.11361_, 2019.\n\n[Yang _et al._, 2022] Rui Yang, Yiming Lu, Wenzhe Li, Hao\nSun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie\nZhang. Rethinking goal-conditioned supervised learning\nand its connection to offline rl. In _International Confer-_\n_ence on Learning Representations_, 2022.\n\n[Zhang _et al._, 2020] Jesse Zhang, Haonan Yu, and Wei Xu.\nHierarchical reinforcement learning by discovering intrinsic options. In _International Conference on Learning Rep-_\n_resentations_, 2020.\n\n\n4225\n\n\n",
    "ranking": {
      "relevance_score": 0.7441243077326792,
      "citation_score": 0.5482330388398067,
      "recency_score": 0.6659175506541748,
      "final_score": 0.6971253782462542
    },
    "citation_key": "Shin2023GuideTC",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "a94aaf192fc1d46d697e4d7eb3e999021ec88b46",
    "title": "Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning",
    "published": "2022-06-24",
    "authors": [
      "Yunfei Li",
      "Tian Gao",
      "Jiaqi Yang",
      "Huazhe Xu",
      "Yi Wu"
    ],
    "summary": "It has been a recent trend to leverage the power of supervised learning (SL) towards more effective reinforcement learning (RL) methods. We propose a novel phasic approach by alternating online RL and offline SL for tackling sparse-reward goal-conditioned problems. In the online phase, we perform RL training and collect rollout data while in the offline phase, we perform SL on those successful trajectories from the dataset. To further improve sample efficiency, we adopt additional techniques in the online phase including task reduction to generate more feasible trajectories and a value-difference-based intrinsic reward to alleviate the sparse-reward issue. We call this overall algorithm, PhAsic self-Imitative Reduction (PAIR). PAIR substantially outperforms both non-phasic RL and phasic SL baselines on sparse-reward goal-conditioned robotic control problems, including a challenging stacking task. PAIR is the first RL method that learns to stack 6 cubes with only 0/1 success rewards from scratch.",
    "pdf_url": "https://arxiv.org/pdf/2206.12030",
    "doi": "10.48550/arXiv.2206.12030",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "International Conference on Machine Learning",
    "citation_count": 23,
    "bibtex": "@Article{Li2022PhasicSR,\n author = {Yunfei Li and Tian Gao and Jiaqi Yang and Huazhe Xu and Yi Wu},\n booktitle = {International Conference on Machine Learning},\n pages = {12765-12781},\n title = {Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning},\n year = {2022}\n}\n",
    "markdown_text": "### **Phasic Self-Imitative Reduction for** **Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n**Yunfei Li** [* 1] **Tian Gao** [* 1] **Jiaqi Yang** [2] **Huazhe Xu** [3] **Yi Wu** [1 4]\n\n\n\n**Abstract**\n\n\nIt has been a recent trend to leverage the power of\nsupervised learning (SL) towards more effective\nreinforcement learning (RL) methods. We propose a novel phasic approach by alternating online RL and offline SL for tackling sparse-reward\ngoal-conditioned problems. In the online phase,\nwe perform RL training and collect rollout data\nwhile in the offline phase, we perform SL on those\nsuccessful trajectories from the dataset. To further\nimprove sample efficiency, we adopt additional\ntechniques in the online phase including task reduction to generate more feasible trajectories and\na value- difference-based intrinsic reward to allevi\nate the sparse-reward issue. We call this overall algorithm, _PhAsic self-Imitative_ _Reduction_ (PAIR).\nPAIR substantially outperforms both non-phasic\nRL and phasic SL baselines on sparse-reward\ngoal-conditioned robotic control problems, including a challenging stacking task. PAIR is the first\nRL method that learns to stack _6 cubes_ with only\n_0/1 success rewards_ from scratch.\n\n\n**1. Introduction**\n\n\nDespite great advances achieved by deep reinforcement\nlearning (RL) in a wide range of application domains such\nas playing games (Mnih et al., 2015; Schrittwieser et al.,\n2020), controlling robots (Lillicrap et al., 2016; Hwangbo\net al., 2019; Akkaya et al., 2019), and solving scientific\nproblems (Jeon & Kim, 2020), deep RL methods have been\nempirically shown to be brittle and extremely sensitive to\nhyper-parameter tuning (Tucker et al., 2018; Ilyas et al.,\n\n\n*Equal contribution 1Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China [2] Department\nof Electrical Engineering and Computer Sciences, University\nof California, Berkeley, CA, USA [3] Stanford University, CA,\nUSA [4] Shanghai Qi Zhi Institute, Shanghai, China. Correspondence to: Yunfei Li _<_ liyf20@mails.tsinghua.edu.cn _>_, Yi Wu\n_<_ jxwuyi@gmail.com _>_ .\n\n\n_Proceedings of the 39_ _[th]_ _International Conference on Machine_\n_Learning_, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-0-0.png)\n\n_Figure 1._ Overall workflow of PAIR. PAIR iteratively alternates\nbetween online RL and offline SL phases. During online phases,\nthe agent is trained using both environment reward and a valuedifference-based intrinsic reward. Meantime, it collects trajectories\nand augments them with task reduction into successful demonstrations for offline training. In the offline phases, the agent runs supervised learning to improve its policy using the generated dataset.\n\n\n2020; Engstrom et al., 2020; Yu et al., 2021a; Andrychowicz et al., 2021), which largely limits the practice use of\ndeep RL in many real-world scenarios. On the contrary, supervised learning (SL) provides another learning paradigm\nby imitating given demonstrations, which is much simpler\nfor tuning and typically results in a much more steady optimization process (Lynch et al., 2020; Ghosh et al., 2020).\nInspired by the success of training powerful fundamental\nmodels by SL (Brown et al., 2020; Dosovitskiy et al., 2020;\nJumper et al., 2021), it has also been a recent trend in RL\nto leverage the power of SL to develop more powerful and\nstable deep RL algorithms (Levine, 2021).\n\n\nOne representative line of research that incorporates SL\ninto RL is offline RL, which assumes that a large offline\ndataset of transition data is available and solely performs\nlearning on the dataset without interacting with the environment (Lange et al., 2012; Wu et al., 2019; Levine et al., 2020;\nKumar et al., 2020; Fujimoto & Gu, 2021). However, both\nempirical (Yu et al., 2021b) and theoretical (Rashidinejad\net al., 2021) evidence suggests that the success of recent offline RL methods rely on the quality of the dataset. Accord\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\ningly, popular offline RL datasets are typically constructed\nby human experts, which may not be feasible for many realworld problems. Besides, offline RL suffers from a final\nperformance gap compared with online RL algorithms.\n\n\nAnother representative line of research is self-imitation\nlearning (SIL) (Oh et al., 2018), which combines SL and\nRL in a completely online fashion. SIL directly performs\nSL over selected self-generated rollout trajectories by treating the SL objective as an auxiliary loss and optimizing it\njointly with the standard RL objective. SIL does not require\na dataset in advance. However, optimizing a mixed objective consisting of both RL and SL makes the optimization\nprocess even more brittle and requires substantial efforts of\nparameter tuning to achieve best empirical performance.\n\n\nIn this paper, we propose a simple phasic approach, _PhAsic_\n_self-Imitative_ _Reduction_ (PAIR), to effectively balance both\nRL and SL training for goal-conditioned sparse-reward problems. The main principle of PAIR is to alternate between RL\nand SL: in the RL phase, we solely perform standard online\nRL training for optimization stability and collect rollout trajectories as a dataset for the offline SL phase; while in the SL\nphase, we pick out successful trajectories as SL signals and\nrun imitation learning to improve policy. To improve sample\nefficiency, PAIR also includes two additional techniques in\nthe RL phase: 1) a value-difference-based intrinsic reward\nthat alleviates the sparse-reward issue, and 2) _task reduction_,\na data augmentation technique that largely increases the\ntotal number of successful trajectories, especially for hard\ncompositional tasks. Our theoretical analysis suggests that\ntask reduction can converge _exponentially_ faster than the\nvanilla phasic approach that does not use task reduction.\n\n\nWe implement PAIR with Proximal Policy Optimization\n(PPO) for RL and behavior cloning for SL and we conduct experiments on a variety of goal-conditioned control\nproblems, including relatively simple benchmarks such\nas pushing and ant-maze navigation, and a challenging\nsparse-reward cube-stacking task. PAIR substantially outperforms all the baselines including non-phasic online methods, which jointly perform self-imitation and RL training,\nand phasic SL methods, which only perform supervised\nlearning on self-generated trajectories (Ghosh et al., 2020).\nWe highlight that PAIR successfully learns to stack 6 cubes\nwith 0/1 rewards from scratch. To our knowledge, PAIR is\nthe _first_ deep RL method that could solve this challenging\nsparse reward task.\n\n\n**2. Related Work**\n\n\n**Goal-conditioned RL:** We study goal-conditioned reinforcement learning (Kaelbling, 1993) with sparse reward in\nthis work. Goal-conditioned RL enables one agent to solve\na variety of tasks by predicting actions given both observa\n\n\ntions and goals, and is studied in a number of works (Schaul\net al., 2015; Nair et al., 2018b; Pong et al., 2018; Veeriah\net al., 2018; Zhao et al., 2019; Eysenbach et al., 2020).\nAlthough some techniques like relabeling (Andrychowicz\net al., 2017) are proposed to address the sparse reward issue\nwhen learning goal-conditioned policies, there are still challenges in long-horizon problems (Nasiriany et al., 2019).\n\n\n**Offline reinforcement learning:** Offline RL (Lange et al.,\n2012; Levine et al., 2020) is a popular line of research\nthat incorporates SL into RL, which studies extracting a\ngood policy from a fixed transition dataset. A large portion\nof offline RL methods focus on regularized dynamic programming (e.g., Q-learning) (Kumar et al., 2020; Fujimoto\n& Gu, 2021), with the constraint that the resulting policy\ndoes not deviate too much from the behavior policy in the\ndataset (Wu et al., 2019; Peng et al., 2019; Kostrikov et al.,\n2021). Some other works directly treat policy learning as a\nsupervised learning problem, and learn the policy in a conditioned behavior cloning manner (Chen et al., 2021; Janner\net al., 2021; Furuta et al., 2021; Emmons et al., 2021), which\ncan be considered as special cases of upside down RL and\nreward-conditioned policies (Schmidhuber, 2019; Kumar\net al., 2019). There are also methods that learn transition\ndynamics from offline data before extracting policies in a\nmodel-based way (Matsushima et al., 2020; Kidambi et al.,\n2020). Some works also consider a single online fine-tuning\nphase after offline learning (Nair et al., 2020; Lu et al., 2021;\nMao et al., 2022; Uchendu et al., 2022) while we repeatedly\nalternate between offline and online training.\n\n\n**Imitation learning in RL:** Imitation learning is a framework for learning policies from demonstrations, which has\nbeen shown to largely improve the sample complexity of\nRL methods (Hester et al., 2018; Rajeswaran et al., 2018)\nand help overcome exploration issues (Nair et al., 2018a).\nSelf-imitation learning (SIL) (Oh et al., 2018), which imitates good data rolled out by the RL agent itself, does not\nrequire any external dataset and has been shown to help exploration in sparse reward tasks. Our method also performs\nimitation learning over self-generated data. Self-imitation\nobjective is optimized jointly with the RL objective, while\nwe propose to perform SL and RL separately in two phases.\nThe idea of substituting joint optimization with iterative\ntraining for minimal interference between different objectives is similar to phasic policy gradient (Cobbe et al., 2021).\nGoal-conditioned supervised learning (GCSL) (Ghosh et al.,\n2020) is perhaps the most related work to ours. GCSL\nrepeatedly performs imitation learning on self collected relabeled data without any RL updates while we leverage the\npower from both RL and SL and adopt further task reduction\nfor enhanced data augmentation.\n\n\n**Sparse reward problems:** There are orthogonal interests\nin solving long horizon sparse reward with hierarchical\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\nmodeling (Stolle & Precup, 2002; Kulkarni et al., 2016;\nBacon et al., 2017; Nachum et al., 2018) or by encouraging\nexploration (Singh et al., 2005; Burda et al., 2019; Badia\net al., 2020; Ecoffet et al., 2021). Our framework can be\nalso viewed as an effective solution to tackle challenging\nsparse-reward compositional problems.\n\n\n**3. Preliminary**\n\n\nWe consider the setting of goal-conditioned Markov\ndecision process with 0/1 sparse rewards defined by\n( _S, A, P_ ( _s_ _[′]_ _|s, a_ ) _, G, r_ ( _s, a, g_ ) _, ρ_ 0 _, γ_ ). _S_ is the state space,\n_A_ is the action space, _G_ is the goal space and _γ_ is the discounted factor. _P_ ( _s_ _[′]_ _|s, a_ ) denotes transition probability\nfrom state _s_ to _s_ _[′]_ after taking action _a_ . The reward function\n_r_ ( _s, a, g_ ) is 1 only if the goal _g_ is reached at the current\nstate _s_ within some precision threshold and otherwise 0. In\nthe beginning of each episode, the initial state _s_ 0 is sampled\nfrom a distribution _ρ_ 0 and a goal _g_ is sampled from the goal\nspace. An episode terminates when the goal is achieved or\nit reaches a maximum number of steps.\n\n\nThe agent is represented as a goal-conditioned stochastic\npolicy _πθ_ ( _a|s, g_ ) parametrized by _θ_ . The optimal policy _πθ⋆_\nshould maximize the objective _J_ ( _θ_ ) defined by the expected\ndiscounted cumulative reward over all the goals, i.e.,\n\n\n\nSuccess\n\n\n𝑠𝑡+1\n\n\n𝑉𝑡+1 = 0.88\n\n\n𝑠𝑡+1\n\n\n𝑉𝑡+1 = 0.41\n\n\nFailure\n\n\n\n𝑠0\n\n\n\nadvi ≥0\n\n\n𝑠t\n\n\n𝑎0, …, 𝑎𝑡−1\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-2-1.png)\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-2-2.png)\n\n𝑉0 = 0.01 𝑉t = 0.86\n\n\nadvi ≤0\n\n\n## 𝜏1\n\n𝑎𝑡\n\n\n𝑎𝑡\n\n## 𝜏2\n\n\n\n�\n\n\n\n_J_ ( _θ_ ) = _J_ ( _πθ_ ) = E _g∈G,at∼πθ_\n\n\n\n�� _t_\n\n\n\n_γ_ _[t]_ _r_ ( _st, at, g_ )\n\n_t_\n\n\n\n_._ (1)\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-2-0.png)\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-2-3.png)\n\n_Figure 2._ Motivation of value-difference intrinsic reward. Trajectories _τ_ 1 and _τ_ 2 only differ at a single action _at_ . _τ_ 1 succeeds and will\nhave positive advantage. _τ_ 2 fails since _at_ knocks down the cubes,\nthen all the transitions in _τ_ 2 will have negative advantages due to\n0/1 goal-conditioned reward, even if early actions are perfect.\n\n\nods may have other choices. Note that self-imitation learning (SIL) is a paradigm that jointly optimizes the RL objective _J_ ( _θ_ ) and the SL objective _L_ ( _θ_ ) in an online fashion.\n\n\n**4. Method**\n\n\nOur phasic solution PAIR consists of 3 components, including the online RL phase (Sec. 4.1), which also collects\nrollout trajectories, task reduction (Sec. 4.2) as a mean for\ndata augmentation, and the offline phase (Sec. 4.3), which\nperforms SL on self-generated demonstrations. We summarize the overall algorithm in Sec. 4.4.\n\n\n**4.1. RL Phase with Intrinsic Rewards**\n\n\nThe RL phase follows any standard online RL training.\nSpecifically in our work, we adopt PPO (Schulman et al.,\n2017) as our RL algorithm, which trains both policy _πθ_ and\nvalue function _Vψ_ using rollout trajectories. When a trajectory _τ_ = ( _g_ ; _st, at_ ) is successful, i.e., goal _g_ is reached, we\nkeep this trajectory _τ_ as a positive demonstration towards\ngoal _g_ in the dataset _D_ for the offline phase.\n\n\n**Value-difference-based intrinsic rewards:**\n\n\nThe sparse-reward issue is a significant challenge for online\nRL training, which can yield substantially high variance in\ngradients. In particular, let’s assume _γ_ = 1 for simplicity\nand consider a single trajectory _τ_ = ( _g_ ; _st, at_ ). In our goalconditioned setting, the return _Rt_ on _τ_ will be binary and\nthe value function _Vψ_ ( _st, g_ ) will be approximately between\n0 and 1. Hence, the advantage function for state-action pair\n\n\n\n**Policy gradient** optimizes _J_ ( _θ_ ) via the gradient computation\n\n\n\n�\n\n\n\n_∇J_ ( _θ_ ) = E _g,at_\n\n\n\n�� _t_\n\n\n\n( _Rt −_ _Vψ_ ( _st, g_ )) _∇_ log _π_ ( _at|st, g_ )\n\n_t_\n\n\n\n_,_\n\n\n\nwhere _Vψ_ ( _st, g_ ) is the goal-conditioned value function parameterized by _ψ_ and _Rt_ denotes the discounted return\nstarting from time _t_ on the current trajectory. Note that in\nour goal-conditioned setting with 0/1 rewards, _Rt_ will be\neither [1] 0 or 1 and the value function _Vψ_ ( _st, g_ ) can be approximately interpreted as the discounted success rate from\nstate _st_ towards goal _g_ .\n\n\n**Goal-conditioned imitation learning** optimizes a policy\n_πθ_ by running supervised learning over a given demonstration dataset _D_ = _{τ_ : ( _g_ ; _s_ 0 _, a_ 0 _, s_ 1 _, a_ 1 _, . . ._ ) _}_, where _τ_\ndenotes a single trajectory. The supervised learning loss\n_L_ ( _θ_ ) is typically defined by\n\n\n_L_ ( _θ_ ) = _−_ E( _g_ ; _s,a_ ) _∈D_ [ _w_ ( _s, a, g_ ) log _π_ ( _a|s, g_ )] _,_ (2)\n\n\nwhere _w_ ( _s, a, g_ ) is some sample weight. Behavior cloning\n(BC) simply sets _w_ ( _s, a, g_ ) as 1 while more advanced meth\n\n1More precisely, the return _Rt_ will be 0 or close to 1 due to the\ndiscount factor _γ_ .\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\nmuch successful trajectories as possible. In our framework,\nwe consider two data augmentation techniques to boost the\npositive samples in the dataset _D_, i.e., _goal relabeling_ and\n_task reduction_ . We will first describe the simpler one, goal\nrelabeling, before moving to a much more powerful technique, task reduction.\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-3-0.png)\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-3-1.png)\n\n_Figure 3._ Illustration of task reduction. _(Left)_ Given a trajectory\nthat fails to reach _g_ from _s_ 0 (orange arrows), task reduction\nsearches for an intermediate _sB_, then executes the resulting two\nsub-tasks (green and red arrows) to generate a successful demonstration. _(Right)_ Heatmap of composed value in Eq. 4 w.r.t. _sB_ .\nThe green star denotes the position of optimal _sB_ .\n\n\n( _st, at_ ) _∈_ _τ_, i.e., _Rt −_ _V_ ( _st_ ), will be either positive or negative for the entire trajectory _τ_ . Imagine a concrete example\nas illustrated in Fig. 2, where we have two trajectories, a\nsuccessful trajectory _τ_ 1 and a failed trajectory _τ_ 2. These\ntwo trajectories only differ at the final timestep _st_ +1, which\nmay be due to some random exploration noise at action _at_ .\nHowever, only due to a single mistake, all the state-action\npairs from _τ_ 2 will have negative advantages, even though\nmost of the actions in _τ_ 2 are indeed approaching the desired\ntarget. Likewise, in a successful trajectory, it is also possible\nthat some single action is poor but eventually the subsequent\nactions fix this early mistake.\n\n\nBesides the trajectory-based advantage computation, it will\nbe beneficial to have some effective signal of whether a\ntransition ( _st, at, st_ +1) is properly “ _approaching_ ” the desired goal _g_ or not. Our suggestion is to use _Vψ_ ( _s, g_ ) as\nan empirical measure: if _at_ is a good action, it should lead\nto a higher state value, i.e., _Vψ_ ( _st_ +1 _, g_ ) _−_ _Vψ_ ( _st, g_ ) _>_ 0,\nwhich indicates that _at_ moves to a state with a higher success rate; similarly, a poor action will result in a value drop,\ni.e., _Vψ_ ( _st_ +1 _, g_ ) _−_ _Vψ_ ( _st, g_ ) _<_ 0. Accordingly, we propose to adopt value difference as an intrinsic reward _r_ [int] for\nstabilizing goal-conditioned RL training as follows\n\n\n_r_ [int] ( _st, at, g_ ) := _Vψ_ ( _st_ +1 _, g_ ) _−_ _Vψ_ ( _st, g_ ) _._\n\n\nWe remark that _r_ [int] relies on an accurately learned value\nfunction _Vψ_ ( _s, g_ ). Therefore, we suggest to only train\n_Vψ_ ( _s, g_ ) over the sparse goal-conditioned rewards while\nusing another value head to fit the intrinsic rewards for critic\nlearning. Similar techniques have been previously explored\nin (Burda et al., 2019) as well.\n\n\n**4.2. Task Reduction as Data Augmentation**\n\n\nIn order to perform effective supervised learning in the offline phase, it is critical that the online phase can generate as\n\n\n\n**Goal relabeling** was originally proposed by Andrychowicz\net al. (2017). In our goal-conditioned learning setting, for\neach failed trajectory _τ_ = ( _g_ ; _st, at_ ) originally targeted at\ngoal _g_, we can create an artificial goal _g_ _[′]_ by setting _g_ _[′]_ = _sj_\nfor some reached state _sj ∈_ _τ_, which naturally yields a\nsuccessful trajectory _τ_ _[′]_ as follows:\n\n\n_τ_ _[′]_ _←_ ( _g_ _[′]_ = _sj_ ; _st_ =0: _j, at_ =0: _j_ ) where _sj ∈_ _τ._ (3)\n\n\nTherefore, despite its simplicity, goal relabeling can convert\nevery failed trajectory _τ_ to a positive demonstration without\nany further interactions with the environment.\n\n\n**Task reduction** was originally proposed by Li et al. (2020b).\nThe main idea is to decompose a challenging task into a\ncomposition of two simpler sub-tasks so that both sub-tasks\ncan be solved by the current policy. As illustrated in the left\npart of Fig. 3, given a goal _g_ from state _s_ 0, task reduction\nsearches for the best sub-goal _s_ _[⋆]_ _B_ [through a 1-step planning]\nprocess over the universal value function _Vψ_ ( _s, g_ ) as follows\n\n\n_s_ _[⋆]_ _B_ [= arg max] (4)\n_sB_ _[V][ψ]_ [(] _[s]_ [0] _[, s][B]_ [)] _[ ⊕]_ _[V][ψ]_ [(] _[s][B][, g]_ [)] _[,]_\n\n\nwhere _⊕_ is a composition operator typically implemented\nas multiplication. Since the value function is learned, such a\nsearch process can be accomplished without any further environment interactions by gradient descent or cross-entropy\nmethod. A heatmap of the composed value for sub-goal\nsearch (Eq.(4)) is visualized in the right part of Fig. 3. Notably, after _s_ _[⋆]_ _B_ [is obtained, we would still need to execute]\nthe policy in the environment following the sub-goal _s_ _[⋆]_ _B_ [and]\nthen the final goal _g_ in order to obtain a valid demonstration.\nCompared to goal relabeling, task reduction consumes additional samples and may even fail to produce a successful\ntrajectory when either of the two sub-tasks fails. Hence,\ntask reduction can be expensive in the early stage of training\nwhen the policy and value function have not yet been well\ntrained. However, we will show both theoretically (Sec. 5)\nand empirically (Sec. 6.2) that task reduction can lead to an\nexponentially faster convergence compared to using goal\nrelabeling solely in long-horizon problems.\n\n\nBy default, PAIR utilizes both goal relabeling and task reduction for data augmentation unless otherwise stated.\n\n\n**4.3. Offline SL Phase**\n\n\nAfter a dataset _D_ of successful demonstrations, including\naugmented trajectories, is collected, we switch from RL\ntraining to offline SL by performing advantage weighted\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\n**Algorithm 1** Phasic Self-Imitative Reduction\n\n\n1: **Initialize:** goal-conditioned policy _πθ_ ( _·|s, g_ ), value\nfunction _Vψ_ ( _s, g_ ).\n2: **for** _k ←_ 1 _,_ 2 _, · · ·_ **do**\n3: Sample a batch of trajectories _B ←{τ_ : ( _g_ ; _st, at_ ) _}_\nw.r.t. the current policy _πθ_\n4: Update _πθ, Vψ_ by RL on _B_ (Sec. 4.1)\n5: Set dataset _D ←∅_\n\n6: **for** _τ ∈B_ **do**\n\n7: **if not** is ~~s~~ uccess( _τ_ ) **then**\n8: _τ ←_ data ~~a~~ ugment( _τ, πθ, Vψ_ ) (Sec. 4.2)\n9: **end if**\n10: _D ←D ∪{τ_ _}_\n11: **end for**\n\n12: Train _πθ_ with SL over _D_ (Sec. 4.3)\n13: **end for**\n\n\nbehavior cloning (BC). In particular, we set the weight\n_w_ ( _s, a, g_ ) in Eq. (2) to exp ( _β_ [1] [(] _[R][ −]_ _[V][φ]_ [(] _[s, g]_ [)))][ follow-]\n\ning (Peng et al., 2019). We remark that we only train policy\nin the offline phase while keeping the value function _Vψ_\nunchanged for algorithmic simplicity. We also empirically\nfind that training value function during the offline phase\ndoes not improve the overall performance.\n\n\nIt is feasible to adopt more advanced methods in the offline\nphase, such as offline RL methods, which typically assume\na pre-constructed dataset but conceptually compatible with\nour phasic learning process. We conduct experiments by\nsubstituting BC with two popular offline RL methods, decision transformer (Chen et al., 2021) and AWAC (Nair et al.,\n2020), in Sec. 6.1. Empirical results show that these alternatives are much more brittle than BC and perform poorly\nwithout a high-quality warm-start dataset. Thus, we simply\nuse BC as our SL method in this paper.\n\n\n**4.4. PAIR: Phasic Self-Imitative Reduction**\n\n\nBy repeatedly performing the online RL phase with task\nreduction and the offline SL phase, we derive our final\nalgorithm, PAIR. The pseudo-code is summarized in Alg. 1.\nMore implementation details can be found in Appendix B.3.\nMore analysis on the phasic training scheme vs. joint RL\nand SL optimization is in Appendix B.4.\n\n\n**5. Theoretical Analysis**\n\n\nFirst, we establish the correctness of our framework. The\nfollowing theorems follows directly from the GCSL framework in (Ghosh et al., 2020), because both GCSL and PAIR\nare built upon SL.\n\n\n**Theorem 5.1** (Ghosh et al. (2020), Theorem 3.1) **.** _Let J_ ( _π_ )\n_be defined in Eq. (1) and J_ PAIR( _π_ ) = _−L_ ( _θ_ ) _as defined in_\n\n\n\n_Eq. (2). Let_ ˜ _π be the data collection policy induced by data_\n_augmentation. Then_\n\n\n_J_ ( _π_ ) _≥_ _J_ PAIR( _π_ ) _−_ 4 _T_ ( _T −_ 1) _α_ [2] + _C,_ (5)\n\n\n_where α_ = max _s,g D_ TV( _π_ ( _·|s, g_ ) _∥π_ ˜( _·|s, g_ )) _and C is a_\n_constant independent of π._ [2]\n\n\n**Theorem 5.2** (Ghosh et al. (2020), Theorem 3.2) **.** _Assume_\n_deterministic transition and that_ ˜ _π has full support. Define_\n\n\n_ϵ_ := max _s,g_ _[D]_ [TV][(] _[π]_ [(] _[·|][s, g]_ [)] _[∥][π]_ [ˆ] _[⋆]_ [(] _[·|][s, g]_ [))] _[.]_ (6)\n\n\n_Then J_ ( _π_ _[⋆]_ ) _−_ _J_ ( _π_ ) _≤_ _ϵ · T_ _, where_ ˆ _π_ _[⋆]_ _minimizes L_ ( _θ_ )\n_defined in Eq. (2) and π_ _[⋆]_ _is the optimal policy._\n\n\nHere, we present theoretical justification to illustrate why\nour algorithm is efficient on sparse-reward composite combinatorial tasks.\n\n\n**Theorem 5.3.** _Under mild assumptions, the PAIR frame-_\n_work could use exponentially less number of iterations com-_\n_pared to the phasic framework that does not use task reduc-_\n_tion, e.g., GCSL (Ghosh et al., 2020)._\n\n\nWe defer the exact statement of Theorem 5.3 and its proof\nto Appendix A.\n\n\n**6. Experiment**\n\n\nWe aim to answer the following questions in this section:\n\n\n  - _Does phasic training of RL and SL objectives perform_\n_better than non-phasic joint optimization?_\n\n\n  - _Is PAIR compatible with offline RL methods other than_\n_BC?_\n\n\n  - _Does PAIR achieve exponential improvement over base-_\n_lines_ without _task reduction?_\n\n\n  - _Are all the algorithmic components of PAIR necessary_\n_for good performance?_\n\n\n  - _Can PAIR be used to solve challenging long-horizon_\n_sparse-reward problems, e.g., cube stacking?_\n\n\nWe consider 3 goal-conditioned control problems with\nincreasing difficulty: (i) short-horizon robotic pushing\n(adopted from (Nair et al., 2018b)), (ii) ant navigation in a\nU-shaped maze, and (iii) robotic stacking with up to 6 cubes.\nAll the tasks are with only 0-1 sparse reward.\n\n\nWe compare PAIR with non-phasic RL baslines that jointly\nperform RL and SL as well as phasic SL baselines that\n\n\n\n2 _D_ TV( _µ∥ν_ ) := 1\n\n\n\n12 �\n\n\n\n_D_ TV( _µ∥ν_ ) := 2 � _A_ _[|][µ]_ [(] _[a]_ [)] _[ −]_ _[ν]_ [(] _[a]_ [)] _[|]_ [ d] _[a]_ [ is the total variation]\n\nbetween distribution _µ_ and _ν_ over the action set _A_ .\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\n\n\n\n|. PAIR<br>0.8 S SI IR avg.succ.rate<br>L<br>0.6 PPO<br>0.4 GCSL<br>0.2<br>0.0<br>0 1|Col2|Col3|\n|---|---|---|\n|0<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>avg. succ. rate<br>PAIR<br>~~SIR~~<br>SIL<br>PPO<br>GCSL|||\n|0<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>avg. succ. rate<br>PAIR<br>~~SIR~~<br>SIL<br>PPO<br>GCSL|||\n|0<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>avg. succ. rate<br>PAIR<br>~~SIR~~<br>SIL<br>PPO<br>GCSL|2<br>3<br>||\n\n\n|0<br>8<br>6<br>4|P<br>SI|AIR<br>R|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|4<br>6<br>8<br>0|SI<br>|L<br>||||\n|4<br>6<br>8<br>0|P<br>~~G~~|O<br>~~CSL~~||||\n|0<br>1<br>0<br>2|0<br>1<br>0<br>2|2<br>3<br>4|2<br>3<br>4|2<br>3<br>4|2<br>3<br>4|\n\n\n\n_Figure 7._ ( _Left_ ) Average success rate over uniformly sampled tasks,\nwhere ant and goal positions are uniformly sampled. ( _Right_ ) Success rate evaluated on one particularly difficult task configuration\nwith ant and goal initialized at two ends of the maze.\n\n\nnon-phasic RL method, i.e., SIL, which utilizes the successful relabeled demonstrations by jointly optimizing SL\nand RL objectives. As shown in Fig. 5, PAIR (red) gets\nhigher success rate than SIL (blue) using fewer samples\nwhile both methods perform better than naive PPO, which\nis a pure RL baseline. We also compare with phasic SL\nmethod, GCSL (Ghosh et al., 2020), which only performs\niterative SL on relabeled data without RL training. Following the original implementation of GCSL, every trajectory\nis relabeled as a successful one targeting at an achieved\nstate. GCSL learns fast in the beginning, but achieves a\nsubstantially lower final success rate than other methods.\n\n\n**Combining with offline RL methods:** Here we provide\nthe results with initial attempts to combine our framework\nwith representative offline RL methods, AWAC (Nair et al.,\n2020) and decision transformer (DT) (Chen et al., 2021).\n\n\nOriginal AWAC performs a single fine-tuning phase after\noffline pretraining (Nair et al., 2020; Lu et al., 2021). Following the phasic framework of PAIR, we alternate between\noffline and online AWAC updates. We examine both training from scratch without any dataset prepared in advance,\nor starting from the offline phase with a warm-start dataset\nof successful trajectories. The results are shown on the left\nof Fig. 6. Phasic-AWAC from scratch (red) continues making progress as it switches from offline to online phase, but\nfinally converges to a much worse policy compared with the\nvariant initialized with a warm-start dataset (blue).\n\n\nDT predicts actions conditioning on a sequence of desired returns, past states and actions via a transformer\nmodel (Vaswani et al., 2017). Since DT is proposed only\nfor a single offline phase, we similarly adopt PPO for RL\ntraining in the online phase. We use a context length of 5 for\nsequence conditioning and train DT both from scratch and\nfrom prepared warm-start dataset with successful demonstrations. As shown in the right plot of Fig. 6, the performance\nprogressively improves within each online phase and SL\nphase. Phasic-DT with warm-start (blue) outperforms the\nvariant from scratch and gets similar final performance as\nPAIR but the variance is much higher.\n\n\nWe generally observe that offline RL algorithms within our\n\n\n\n\n\n\n|PAIR<br>SIL<br>PPO|Col2|\n|---|---|\n|~~GCSL~~||\n|||\n|||\n||0.6<br>0.8<br>1.0|\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-5-10.png)\n\n_Figure 4._ An initial\nstate and a success\nful state in “ _Push_ ”\n\nenvironment.\n\n\n\n_Figure 5._ Average success rate vs. number\nof samples in “ _Push_ ”. PAIR achieves the\nbest performance compared to non-phasic,\npure RL and pure SL baselines.\n\n\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-5-12.png)\n\n\n|.<br>rate<br>0.8<br>0.6 succ.<br>0.4 from s avg.<br>f Pr Ao Im d<br>0.2 R<br>0.0 0.2 0.4 0.6 0.<br>sam les|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.<br>samles<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>avg. succ. rate<br>~~from s~~<br>from d<br>PAIR||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.<br>samles<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>avg. succ. rate<br>~~from s~~<br>from d<br>PAIR||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.<br>samles<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>avg. succ. rate<br>~~from s~~<br>from d<br>PAIR||||~~from s~~<br>from d|~~ cratch~~<br> ata|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.<br>samles<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>avg. succ. rate<br>~~from s~~<br>from d<br>PAIR||||PAIR||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.<br>samles<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>avg. succ. rate<br>~~from s~~<br>from d<br>PAIR||||PAIR|8<br>1.0<br>1e6|\n\n\n\n_Figure 6._ Combining PAIR with offline algorithms AWAC (left)\nand DT (right) in “ _Push_ ” domain. We train them both from scratch\n(red) and from a demonstration dataset (blue). The performances\nof PAIR with PPO/BC are in dashed purple line.\n\n\nonly perform SL over self-generated data. For non-phasic\nRL baselines, we consider naive PPO, plain self-imitation\nlearning with goal relabeling (SIL) (Oh et al., 2018) and\nself-imitation learning with task reduction (SIR) (Li et al.,\n2020b). For phasic SL baselines, we consider goalconditioned supervised learning (GCSL) (Ghosh et al.,\n2020). We emphasize that for a fair comparison, all the\nRL-based baselines leverage our proposed intrinsic rewards.\nAll the experiments are repeated over 3 random seeds on a\nsingle desktop machine with a GTX3090 GPU. More implementation and experiment details can be found in appendix.\n\n\n**6.1. Sawyer Push**\n\n\nWe first answer whether our phasic framework can outperform non-phasic training algorithms on a simple “ _Push_ ”\ntask. As illustrated in Fig. 4, a Sawyer robot is tasked to\npush the puck to the goal within 50 steps. The initial position of the robot hand, the puck and the goal are randomly\ninitialized on the table. Since it only requires very few steps\nto reach the goal – even the largest distance between puck\nand goal is within 20 steps of actions using a well trained\npolicy, task reduction would not make its best use and may\nhurt sample efficiency due to extra sample consumption. In\nthis particularly simple domain, we only use goal relabeling\nas a single data augmentation technique in PAIR.\n\n\n**Effectiveness of phasic training:** We compare PAIR with\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\n\n\n|10<br>.<br>08<br>.<br>06<br>.<br>04<br>.<br>02<br>.<br>00<br>.<br>0 1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n|0<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>.0|||||\n|0<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>.0||Non|line = 4|0 218|\n|0<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>.0||Non<br>~~N~~|line = 2<br>~~ = 1~~|0 2~~18~~<br>~~ 0 2~~18|\n|0<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>.0||~~on~~<br>Non|~~line~~<br>line = 2|<br>18|\n|0<br>1<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>.0||2<br>3<br>4|2<br>3<br>4|2<br>3<br>4|\n\n\n(c) Sensitivity analysis of _N_ online.\n\n\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-6-8.png)\n\n(a) Ablate PAIR components.\n\n\n\n(b) The effect of goal-relabeling on SIL.\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-6-9.png)\n\n_Figure 8._ Ablation studies on algorithmic components of PAIR in “ _Ant Maze_ ”. (a) Performances after removing different components\nof PAIR. Using reduction data and intrinsic reward help the most for PAIR. (b) Validate the effectiveness of goal relabeling on SIL. (c)\nComparison of different _N_ online, i.e., samples collected in the RL phase. PAIR works well with less frequent phase switches.\n\n\nphasic framework would require a good dataset to initialize\nand do not perform as robustly as the simple PPO and BC\ncombination when starting from scratch. Therefore, we\nconfirm our use of BC/PPO for the remaining experiments\nand leave a more competitive combination with offline RL\nalgorithms as future work.\n\n\n**6.2. Ant Maze**\n\n\n\nWe then consider a harder task “ _Ant Maze_ ”, which requires\na locomotion policy with continuous actions to control the\nant robot and plan over an extended horizon to navigate\ninside a 2-D U-shape maze. The observations include the\ncenter of mass and the joint position and velocities of the\nant robot. The goal is a 2-D vector denoting the _xy_ position\nof that the robot should navigate to. For each episode, the\ninitial position of the ant’s center and the goal position are\nuniformly sampled over the empty space inside the maze.\nWe define the _hard task_ configuration as when the ant and\nthe goal are initialized at two different ends of the maze (see\nFig. 3). In this domain, we leverage both task reduction and\ngoal relabeling as data augmentation techniques in PAIR.\n\n\n**Effectiveness of PAIR and exponential improvement on**\n**hard goals:** As shown in Fig. 7, we evaluate the performances of different algorithms with the average success\nrate over the entire task space (left) and on hard tasks only\n(right). We compare PAIR with: SIR, which runs SL and RL\njointly using both reduction and relabeling data; SIL, a plain\nnon-phasic RL method without task reduction; GCSL and\nvanilla PPO. We observe that all the methods that utilize task\n\nreduction (i.e., PAIR, SIR) can finally converge to a much\nhigher average success rate (left plot) and outperform SIL\nand vanilla PPO. The gap becomes substantially larger on\nthe performance on hard situations (right plot), where only\nmethods with task reduction are able to produce non-zero\nsuccess rate within the given sample budget. This empirical observation is consistent with our theoretical analysis\nin Sec. 5 on the effectiveness of task reduction. We also\n\nremark that PAIR achieves a significantly higher sample\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-6-16.png)\n\n_Figure 9._ Illustration of the stacking environment. The left figure\nshows an initial state, where the red ball denotes the identity and\nthe desired position of a target cube. On the right we show a\nsuccessful state for this task, where the red cube is close to the\ngoal and the robot hand does not touch the tower.\n\n\nefficiency compared with the non-phasic method SIR on the\nhard cases. Notably, GCSL completely fails in this problem.\nWe empirically observe that the ant robot may frequently get\nstuck in the early stage of training (e.g., it may accidentally\nfall over and cannot recover anymore). We hypothesis that\nthe failure of GCSL is largely due to a tremendous amount\nof supervision from such corrupted trajectories. This suggests that online RL training would be necessary compared\nwith running SL only (e.g., SIL performs well on this task).\n\n\n**Ablation studies:** We then study the effectiveness of different components of PAIR. From Figure 8a, we can find\nthat removing value-difference-based intrinsic reward makes\nPAIR converge slower while the policy is still able to achieve\na high success rate after convergence. After removing task\nreduction, the performance becomes significantly worse. By\ncontrast, when removing goal relabeling, the performance\ndrop is negligible. These observations suggest that task reduction is the most critical component in PAIR. In Figure 8b,\nwe additionally examine the effectiveness of goal relabeling within a non-phasic RL framework, i.e., SIL, where we\ncan clearly observe a performance drop of SIL with goal\nrelabeling turned off. This indicates that goal relabeling\ncan be still beneficial for methods that do not utilize task\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\n|Col1|PAIR<br>PAIRw/orint|Col3|\n|---|---|---|\n||<br>SIR||\n||SIR w/o rint<br>||\n||~~SIL~~||\n||||\n||||\n|0.0<br>0.5<br>1.0|0.0<br>0.5<br>1.0|1.5|\n\n\n_Figure 10._ Average success rate in “stack-6-cube” with sparse reward. PAIR solves the task with high success rate most efficiently.\n\n\n\nAlgorithm SR @ 7.5e7 steps SR @ 1.5e8 steps\n\n\nPAIR **0.693** _±_ **0.176** **0.955** _±_ **0.005**\nPAIR w/o _r_ [int] 0.493 _±_ 0.156 0.952 _±_ 0.016\n\nSIR 0.026 _±_ 0.043 0.815 _±_ 0.039\nSIR w/o _r_ [int] 0.003 _±_ 0.003 0.688 _±_ 0.011\n\nSIL 0.000 _±_ 0.000 0.000 _±_ 0.000\n\n\n_Table 1._ Mean and standard deviation of success rates for stacking\n6 boxes with different algorithms over 3 seeds. The policies are\nevaluated at 7.5e7 and 1.5e8 environment samples.\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-7-5.png)\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-7-6.png)\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-7-7.png)\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-7-8.png)\n\n_Figure 11._ Learned policy of PAIR agent for stacking 6 randomly initialized cubes.\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-7-4.png)\n\nreduction. Finally, in Figure 8c, we also perform sensitivity\nanalysis on the frequency of alternating between the offline\nand online phase. We perform one offline SL phase after\ncollecting _N_ online samples in the online RL phase. We can\nobserve that a relatively low alternating frequency is critical\nto the success of PAIR, which suggests that a large dataset\nfor SL training and a long RL fine-tuning period help accelerate learning. We notice that when the phase changes too\nfrequently, PAIR can be unstable or even fail to learn.\n\n\n**6.3. Stacking**\n\n\nFinally, we test whether PAIR can solve an extremely challenging long-horizon sparse-reward robotic manipulation\nproblem. We build a robotic control environment that features stacking multiple cubes given only final success reward. The simulated scene is shown in Fig. 9. A Franka\nPanda arm is mounted on the side of a table. On top of\nthe table, there are a total of _N_ cubes with random initial\npositions. A goal spot specifies a target cube using its color\nand also its desired position. The robot’s task is to manipulate the cubes on the table so that the specified target cube\ncan remain stable within 3cm distance to the goal position,\nwhile its hand is at least 10cm apart from that position at the\nsame time. In order to accomplish this task, the robot must\nbuild a “base” tower using other non-target cubes. During\nthe whole construction process, _the robot cannot receive_\n_any external reward for intermediate manipulations unless_\n_when the target cube is in place_ . We perform curriculum on\nthe number of cubes to stack, and use the similar task reduction process described in (Li et al., 2020b). More details of\ntraining specifications can be found in Appendix B.1.3.\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-7-9.png)\n\n**Performance on stacking with 6 cubes:** We report the\nsuccess rate on the most challenging “stack-6-cube” scenario using PAIR, SIR and SIL algorithms in Fig. 10 and\nTable 1. PAIR achieves an impressive 95.5% success rate in\nthis challenging task. SIR is making considerable progress\nthanks to task reduction, but it learns significantly less sample efficiently than PAIR. SIL baseline without task reduction fails completely, getting 0 success rate throughout the\ntraining process. We also show the effectiveness of intrinsic\nreward in online training: similar to the findings in other\ndomains, _r_ [int] can significantly speed up the training process.\n\n\n**Learned strategies:** The learned policy using PAIR is visualized in Fig. 11. In the initial frame, all 6 cubes are scattered\nrandomly on the table. The robot then picks up non-target\ncubes, transports them to accurate positions aligned with\nthe goal spot one by one. Even when picking up the red\ncube which locates close to the half-built tower, the robot is\ncautious enough to avoid knocking down the tower.\n\n\n**7. Conclusion**\n\n\nWe propose a phasic training method PAIR that efficiently\ncombines offline supervised learning with online reinforcement learning for sparse-reward goal-conditioned problems,\nsuch as robotic stacking of multiple cubes. PAIR repeatedly\nalternates between SL on self-generated datasets and RL\nfine-tuning and leverages value difference as intrinsic rewards and task reduction as data augmentation. We validate\nthe effectiveness of PAIR both theoretically and empirically\non a variety of domains. We remark that PAIR provides a\ngeneral learning paradigm that has the potential to be com\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\nbined with more advanced offline RL methods, even though\nour initial attempts are not satisfactory. We hope PAIR can\nbe a promising step to take advantage of both supervised\nlearning and reinforcement learning and help make RL a\nmore scalable tool for complex real-world challenges.\n\n\n**Acknowledgements**\n\n\nWe thank Jingzhao Zhang for valuable discussion on phasic optimization. Yi Wu is supported by 2030 Innovation\nMegaprojects of China (Programme on New Generation\nArtificial Intelligence) Grant No. 2021AAA0150000.\n\n\n**References**\n\n\nAkkaya, I., Andrychowicz, M., Chociej, M., Litwin, M.,\nMcGrew, B., Petron, A., Paino, A., Plappert, M., Powell,\nG., Ribas, R., et al. Solving rubik’s cube with a robot\nhand. _arXiv preprint arXiv:1910.07113_, 2019.\n\n\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,\nR., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and\nZaremba, W. Hindsight experience replay. In _Proceedings_\n_of the 31st International Conference on Neural Informa-_\n_tion Processing Systems_, pp. 5055–5065, 2017.\n\n\nAndrychowicz, M., Raichuk, A., Stanczyk, P., Orsini, M.,´\nGirgin, S., Marinier, R., Hussenot, L., Geist, M., Pietquin,\nO., Michalski, M., Gelly, S., and Bachem, O. What\nmatters for on-policy deep actor-critic methods? a largescale study. In _International Conference on Learning_\n_Representations_, 2021.\n\n\nAzuma, K. Weighted sums of certain dependent random\nvariables. _Tohoku Mathematical Journal, Second Series_,\n19(3):357–367, 1967.\n\n\nBacon, P.-L., Harb, J., and Precup, D. The option-critic architecture. In _Proceedings of the Thirty-First AAAI Con-_\n_ference on Artificial Intelligence_, pp. 1726–1734, 2017.\n\n\nBadia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot,\nB., Kapturowski, S., Tieleman, O., Arjovsky, M., Pritzel,\nA., Bolt, A., and Blundell, C. Never give up: Learning\ndirected exploration strategies. In _International Confer-_\n_ence on Learning Representations_ [, 2020. URL https:](https://openreview.net/forum?id=Sye57xStvB)\n[//openreview.net/forum?id=Sye57xStvB.](https://openreview.net/forum?id=Sye57xStvB)\n\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,\nWinter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,\nGray, S., Chess, B., Clark, J., Berner, C., McCandlish,\nS., Radford, A., Sutskever, I., and Amodei, D. Language\nmodels are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.),\n\n\n\n_Advances in Neural Information Processing Systems_, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020.\n\n\nBubeck, S. Convex optimization: Algorithms and complexity. _Found. Trends Mach. Learn._, 8(3-4):231–357, 2015.\ndoi: 10.1561/2200000050.\n\n\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O.\nExploration by random network distillation. In _In-_\n_ternational Conference on Learning Representations_,\n[2019. URL https://openreview.net/forum?](https://openreview.net/forum?id=H1lJJnR5Ym)\n\n[id=H1lJJnR5Ym.](https://openreview.net/forum?id=H1lJJnR5Ym)\n\n\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,\nLaskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence\nmodeling. _arXiv preprint arXiv:2106.01345_, 2021.\n\n\nCobbe, K., Hilton, J., Klimov, O., and Schulman, J. Phasic policy gradient. In Meila, M. and Zhang, T. (eds.),\n_Proceedings of the 38th International Conference on Ma-_\n_chine Learning, ICML 2021, 18-24 July 2021, Virtual_\n_Event_, volume 139 of _Proceedings of Machine Learning_\n_Research_, pp. 2020–2027. PMLR, 2021.\n\n\nCoumans, E. and Bai, Y. Pybullet, a python module for\nphysics simulation for games, robotics and machine learning. 2016.\n\n\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. In\n_International Conference on Learning Representations_,\n2020.\n\n\nEcoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and\nClune, J. First return, then explore. _Nature_, 590(7847):\n580–586, 2021.\n\n\nEmmons, S., Eysenbach, B., Kostrikov, I., and Levine, S.\nRvs: What is essential for offline rl via supervised learning? _arXiv preprint arXiv:2112.10751_, 2021.\n\n\nEngstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos,\nF., Rudolph, L., and Madry, A. Implementation matters\nin deep rl: A case study on ppo and trpo. In _International_\n_Conference on Learning Representations_, 2020.\n\n\nEysenbach, B., Salakhutdinov, R., and Levine, S. Clearning: Learning to achieve goals via recursive classification. In _International Conference on Learning Rep-_\n_resentations_, 2020.\n\n\nFujimoto, S. and Gu, S. S. A minimalist approach to offline\nreinforcement learning. _arXiv preprint arXiv:2106.06860_,\n2021.\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\nFuruta, H., Matsuo, Y., and Gu, S. S. Generalized decision\ntransformer for offline hindsight information matching.\nIn _Deep RL Workshop NeurIPS 2021_, 2021.\n\n\nGhosh, D., Gupta, A., Reddy, A., Fu, J., Devin, C. M.,\nEysenbach, B., and Levine, S. Learning to reach goals via\niterated supervised learning. In _International Conference_\n_on Learning Representations_, 2020.\n\n\nHester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul,\nT., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Osband,\nI., et al. Deep q-learning from demonstrations. In _Thirty-_\n_second AAAI conference on artificial intelligence_, 2018.\n\n\nHwangbo, J., Lee, J., Dosovitskiy, A., Bellicoso, D., Tsounis, V., Koltun, V., and Hutter, M. Learning agile and\ndynamic motor skills for legged robots. _Science Robotics_,\n4(26), 2019.\n\n\nIlyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos,\nF., Rudolph, L., and Madry, A. A closer look at deep policy gradients. In _International Conference on Learning_\n_Representations_, 2020.\n\n\nJanner, M., Li, Q., and Levine, S. Offline reinforcement\nlearning as one big sequence modeling problem. _Ad-_\n_vances in Neural Information Processing Systems_, 34,\n2021.\n\n\nJeon, W. and Kim, D. Autonomous molecule generation\nusing reinforcement learning and docking to develop potential novel inhibitors. _Scientific reports_, 10(1):1–11,\n2020.\n\n\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M.,\nRonneberger, O., Tunyasuvunakool, K., Bates, R., Z [ˇ] ´ıdek,\nA., Potapenko, A., et al. Highly accurate protein structure\nprediction with alphafold. _Nature_, 596(7873):583–589,\n2021.\n\n\nKaelbling, L. Learning to achieve goals. In _Proc. of IJCAI-_\n_93_, pp. 1094–1098, 1993.\n\n\nKidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims,\nT. Morel: Model-based offline reinforcement learning. In\n_NeurIPS_, 2020.\n\n\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In _ICLR (Poster)_, 2015.\n\n\nKostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning. _arXiv preprint_\n_arXiv:2110.06169_, 2021.\n\n\nKulkarni, T. D., Narasimhan, K., Saeedi, A., and Tenenbaum, J. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation.\n_Advances in neural information processing systems_, 29:\n3675–3683, 2016.\n\n\n\nKumar, A., Peng, X. B., and Levine, S. Reward-conditioned\npolicies. _arXiv preprint arXiv:1912.13465_, 2019.\n\n\nKumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning. In\nLarochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and\nLin, H. (eds.), _Advances in Neural Information Process-_\n_ing Systems 33: Annual Conference on Neural Informa-_\n_tion Processing Systems 2020, NeurIPS 2020, December_\n_6-12, 2020, virtual_, 2020.\n\n\nLange, S., Gabel, T., and Riedmiller, M. Batch reinforcement learning. In _Reinforcement learning_, pp. 45–73.\nSpringer, 2012.\n\n\nLevine, S. Understanding the world through action. In\nFaust, A., Hsu, D., and Neumann, G. (eds.), _Conference_\n_on Robot Learning, 8-11 November 2021, London, UK_,\nvolume 164 of _Proceedings of Machine Learning Re-_\n_search_, pp. 1752–1757. PMLR, 2021.\n\n\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on\nopen problems. _arXiv preprint arXiv:2005.01643_, 2020.\n\n\nLi, R., Jabri, A., Darrell, T., and Agrawal, P. Towards practical multi-object manipulation using relational reinforcement learning. In _2020 IEEE International Conference on_\n_Robotics and Automation (ICRA)_, pp. 4051–4058. IEEE,\n2020a.\n\n\nLi, Y., Wu, Y., Xu, H., Wang, X., and Wu, Y. Solving\ncompositional reinforcement learning problems via task\nreduction. In _International Conference on Learning Rep-_\n_resentations_, 2020b.\n\n\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T.,\nTassa, Y., Silver, D., and Wierstra, D. Continuous control\nwith deep reinforcement learning. In _ICLR (Poster)_, 2016.\n\n\nLu, Y., Hausman, K., Chebotar, Y., Yan, M., Jang, E., Herzog, A., Xiao, T., Irpan, A., Khansari, M., Kalashnikov,\nD., and Levine, S. AW-opt: Learning robotic skills\nwith imitation andreinforcement at scale. In _5th Annual_\n\n_Conference on Robot Learning_ [, 2021. URL https:](https://openreview.net/forum?id=xwEaXgFa0MR)\n[//openreview.net/forum?id=xwEaXgFa0MR.](https://openreview.net/forum?id=xwEaXgFa0MR)\n\n\nLynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J.,\nLevine, S., and Sermanet, P. Learning latent plans from\nplay. In _Conference on Robot Learning_, pp. 1113–1132.\nPMLR, 2020.\n\n\nMao, Y., Wang, C., Wang, B., and Zhang, C. Moore: Modelbased offline-to-online reinforcement learning, 2022.\n\n\nMatsushima, T., Furuta, H., Matsuo, Y., Nachum, O., and\nGu, S. Deployment-efficient reinforcement learning via\nmodel-based offline optimization. In _International Con-_\n_ference on Learning Representations_, 2020.\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. _nature_, 518(7540):\n529–533, 2015.\n\n\nNachum, O., Gu, S. S., Lee, H., and Levine, S. Data-efficient\nhierarchical reinforcement learning. _Advances in Neural_\n_Information Processing Systems_, 31:3303–3313, 2018.\n\n\nNair, A., McGrew, B., Andrychowicz, M., Zaremba, W.,\nand Abbeel, P. Overcoming exploration in reinforcement\nlearning with demonstrations. In _2018 IEEE Interna-_\n_tional Conference on Robotics and Automation (ICRA)_,\npp. 6292–6299. IEEE, 2018a.\n\n\nNair, A., Dalal, M., Gupta, A., and Levine, S. Awac: Accelerating online reinforcement learning with offline datasets.\n2020.\n\n\nNair, A. V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine,\nS. Visual reinforcement learning with imagined goals.\n_Advances in Neural Information Processing Systems_, 31:\n9191–9200, 2018b.\n\n\nNasiriany, S., Pong, V., Lin, S., and Levine, S. Planning\nwith goal-conditioned policies. _Advances in Neural Infor-_\n_mation Processing Systems_, 32:14843–14854, 2019.\n\n\nOh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learning. In _International Conference on Machine Learning_,\npp. 3878–3887. PMLR, 2018.\n\n\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S.\nAdvantage-weighted regression: Simple and scalable\noff-policy reinforcement learning. _arXiv preprint_\n_arXiv:1910.00177_, 2019.\n\n\nPong, V., Gu, S., Dalal, M., and Levine, S. Temporal difference models: Model-free deep rl for model-based control.\nIn _International Conference on Learning Representations_,\n2018.\n\n\nRajeswaran, A., Kumar, V., Gupta, A., Vezzani, G.,\nSchulman, J., Todorov, E., and Levine, S. Learning\ncomplex dexterous manipulation with deep reinforcement learning and demonstrations. In Kress-Gazit, H.,\nSrinivasa, S. S., Howard, T., and Atanasov, N. (eds.),\n_Robotics:_ _Science and Systems XIV, Carnegie Mel-_\n_lon University, Pittsburgh, Pennsylvania, USA, June_\n_26-30, 2018_, 2018. doi: 10.15607/RSS.2018.XIV.\n[049. URL http://www.roboticsproceedings.](http://www.roboticsproceedings.org/rss14/p49.html)\n[org/rss14/p49.html.](http://www.roboticsproceedings.org/rss14/p49.html)\n\n\nRashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell,\nS. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _arXiv preprint_\n_arXiv:2103.12021_, 2021.\n\n\n\nSchaul, T., Horgan, D., Gregor, K., and Silver, D. Universal\nvalue function approximators. In _International conference_\n_on machine learning_, pp. 1312–1320. PMLR, 2015.\n\n\nSchmidhuber, J. Reinforcement learning upside down:\nDon’t predict rewards–just map them to actions. _arXiv_\n_preprint arXiv:1912.02875_, 2019.\n\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,\nSifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,\nD., Graepel, T., et al. Mastering atari, go, chess and shogi\nby planning with a learned model. _Nature_, 588(7839):\n604–609, 2020.\n\n\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\n_CoRR_ [, abs/1707.06347, 2017. URL http://arxiv.](http://arxiv.org/abs/1707.06347)\n[org/abs/1707.06347.](http://arxiv.org/abs/1707.06347)\n\n\nSingh, S., Barto, A. G., and Chentanez, N. Intrinsically motivated reinforcement learning. Technical report, MASSACHUSETTS UNIV AMHERST DEPT OF COM\nPUTER SCIENCE, 2005.\n\n\nStolle, M. and Precup, D. Learning options in reinforcement\nlearning. In _International Symposium on abstraction,_\n_reformulation, and approximation_, pp. 212–223. Springer,\n2002.\n\n\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics\nengine for model-based control. In _2012 IEEE/RSJ Inter-_\n_national Conference on Intelligent Robots and Systems_,\npp. 5026–5033. IEEE, 2012.\n\n\nTucker, G., Bhupatiraju, S., Gu, S., Turner, R., Ghahramani, Z., and Levine, S. The mirage of action-dependent\nbaselines in reinforcement learning. In _International con-_\n_ference on machine learning_, pp. 5015–5024. PMLR,\n2018.\n\n\nUchendu, I., Xiao, T., Lu, Y., Zhu, B., Yan, M., Simon, J.,\nBennice, M., Fu, C., Ma, C., Jiao, J., et al. Jump-start\nreinforcement learning. _arXiv preprint arXiv:2204.02372_,\n2022.\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In _Advances in neural information_\n_processing systems_, pp. 5998–6008, 2017.\n\n\nVeeriah, V., Oh, J., and Singh, S. Many-goals reinforcement\nlearning. _arXiv preprint arXiv:1806.09605_, 2018.\n\n\nWu, Y., Tucker, G., and Nachum, O. Behavior regularized offline reinforcement learning. _arXiv preprint_\n_arXiv:1911.11361_, 2019.\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\nYu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A., and Wu,\nY. The surprising effectiveness of mappo in cooperative,\nmulti-agent games. _arXiv preprint arXiv:2103.01955_,\n2021a.\n\n\nYu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine,\nS., and Finn, C. Conservative data sharing for multitask offline reinforcement learning. _Advances in Neural_\n_Information Processing Systems_, 34, 2021b.\n\n\nZhao, R., Sun, X., and Tresp, V. Maximum entropyregularized multi-goal reinforcement learning. In _Interna-_\n_tional Conference on Machine Learning_, pp. 7553–7562.\nPMLR, 2019.\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n[The project webpage is at https://sites.google.com/view/pair-gcrl.](https://sites.google.com/view/pair-gcrl)\n\n\n**A. Missing Proofs in Section 5**\n\n\nIn this section, we prove Theorem 5.3.\n\n\nHere, we make the following assumptions to facilitate our analysis. First, we consider a goal-conditioned MDP with\ndeterministic transition. We assume that the goal set is the same as the state space, i.e., _G_ = _S_ . Furthermore, we consider\nsparse-reward environment where the reward function is defined by _r_ ( _s, a, g_ ) = I _{s_ = _g}_ .\n\n\nWe note that our framework draw on-policy trajectories with random state-goal pairs. To further simplify our theoretical\nanalysis, we assume that each state-goal pair is sampled at least once in each iteration. Here, we may meet two practical\nissues. First, the state space may be continuous, which makes it impossible to sample each state-goal pair once. This may fit\ninto our theoretical analysis by discretizing the state space. Second, _N_ online may not be large enough. This fits into our\nanalysis by merging several consecutive iterations.\n\n\nWe use _π_ [(] _[k]_ [)] to denote the policy by the end of the _k_ -th iteration, and _π_ [(0)] denotes the initial policy. We assume that the\ninitial policy _π_ [(0)] can reach any one-step goal. Specifically, if _P_ ( _s_ _[′]_ _|s, a_ ) = 1 then _π_ [(0)] ( _a|s, g_ = _s_ _[′]_ ) = 1.\n\n\nFor a state _s_, a goal _g_, and a (deterministic) policy _π_, we define\n\n\n\n_π_\n_s_ _−→_ _g_ :=\n\n\nand we define\n\n\n\ntrue _,_ Pr[ _∃t ≥_ 0 : _st_ = _g | s_ 0 = _s, ∀i ≥_ 0 _, ai ∼_ _π_ ( _·|si, g_ ) _, si_ +1 _∼_ _P_ ( _·|si, ai_ )] = 1 _,_\n(7)\n�false _,_ otherwise _._\n\n\n\n_d_ ( _s, s_ _[′]_ ) := min _{t_ : _∃_ policy _π_ such that Pr[ _st_ = _s_ _[′]_ _| s_ 0 = _s, ∀i ≥_ 0 _, ai ∼_ _π_ ( _·|si, g_ ) _, si_ +1 _∼_ _P_ ( _·|si, ai_ )] = 1 _},_ (8)\n\n\n_π_ [(] _[k]_ [)]\n_ℓk_ := sup _{∀s, g_ : _d_ ( _s, g_ ) _≤_ _ℓ_ : _s_ _−−→_ _g_ is true _}._ (9)\n_ℓ_\n\n\nFinally, we let\n\n\n_D_ = max (10)\n_s,s_ _[′]_ _∈S_ _[{][d]_ [(] _[s, s][′]_ [) :] _[ d]_ [(] _[s, s][′]_ [)] _[ <]_ [ +] _[∞}]_\n\n\nbe the maximum possible length of trajectory from state _s_ to goal _s_ _[′]_ such that _s_ _[′]_ is reachable from _s_ .\n\n\n**Lemma A.1.** _For the PAIR framework, we have ℓk ≥_ 2 _ℓk−_ 1 _for every iteration k._\n\n\n_Proof._ We prove by induction. For every state-goal pair _s, g_ such that _d_ ( _s, g_ ) _≤_ 2 _ℓk−_ 1, we can find some state _s_ _[′]_ such that\n\n_π_ [(] _[k]_ [)] _π_ [(] _[k]_ [)]\n_d_ ( _s, s_ _[′]_ ) _≤_ _ℓk−_ 1 and _d_ ( _s_ _[′]_ _, g_ ) _≤_ _ℓk−_ 1. By the inductive hypothesis, we have that _s_ _−−→_ _s_ _[′]_ and _s_ _[′]_ _−−→_ _g_ are true.\n\n\nNow we claim that every ( _s, g_ ) _∈S × G_ would have a success trajectory after the _k_ -th iteration. Note that by our\nassumption, the framework would roll out a trajectory from _s_ to _g_ using _π_ ( _·|s, g_ ). If it succeeds, then we get a trajectory,\n\n_π_ [(] _[k][−]_ [1)] _π_ [(] _[k][−]_ [1)]\nelse task reduction (cf. Section 4.2) would find the state _s_ _[′]_ such that _s_ _−−−−→_ _s_ _[′]_ and _s_ _[′]_ _−−−−→_ _g_ are true, because\n_V_ ( _s, s_ _[′]_ ) = _V_ ( _s_ _[′]_ _, g_ ) = 1. Therefore, task reduction would run _π_ [(] _[k][−]_ [1)] ( _·|s, g_ = _s_ _[′]_ ) followed by _π_ [(] _[k][−]_ [1)] ( _·|s_ _[′]_ _, g_ ), and get a\n\n_π_ [(] _[k]_ [)]\nsuccessful trajectory from _s_ to _g_ . Finally, the SL step would learn the policy _π_ [(] _[k]_ [)] such that _s_ _−−→_ _g_ is true.\n\n\n**Lemma A.2.** _The PAIR framework uses at most O_ ( _|S|_ [2] log _D_ ) _samples to learn a policy that could go from any state to_\n_any reachable goal._\n\n\n_Proof._ By the definition of _ℓk_, we know that _π_ [(] _[k]_ [)] could go from any state to any reachable goal if _ℓk ≥_ _D_ . By our\nassumption, we have _ℓ_ 0 = 1. Therefore, by Lemma A.1, we have _ℓk ≥_ _D_ after _k_ = _O_ (log _D_ ), i.e., PAIR uses at most\n_O_ (log _D_ ) iterations.\n\n\nBy our assumption, PAIR uses _O_ ( _|S||G|_ ) = _O_ ( _|S|_ [2] ) samples, because each state-goal pair is sampled for constant number\nof times. Thus we conclude that PAIR uses at most _O_ ( _|S|_ [2] log _D_ ) samples in total.\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n**Lemma A.3.** _Without PAIR, under assumptions described in Appendix A, for hard tasks, with high probability, the algorithm_\n_needs_ Ω( _|S|_ [2] _D_ ) _samples to learn a policy that could go from any state to any reachable goal._\n\n\n_Proof._ We consider the following hard task. Suppose _s_ 0 _, a_ 0 _, · · ·, sD_ is a trajectory of length _D_ that maximizes Eq. (10).\nWe assume that the initial policy _π_ [(0)] is a uniformly random policy and we assume Pr[ _π_ [(0)] ( _·|si, g_ = _sD_ )]] _∝|A|_ _[−]_ [1] for\n\n_π_ [(] _[k]_ [)]\n_i ≤_ _D −_ 2. [3] We observe that for the SL framework, if _si_ _−−→_ _sD_ is true then the dataset must contain a trajectory from _si_\n_π_ [(] _[k]_ [)]\nto _sD_, and thus the policy _π_ [(] _[k]_ [)] learned by SL would satisfy _sj_ _−−→_ _sD_ for every _j ≥_ _i_ . For this, we analyze the sample\ncomplexity by analyzing how many iterations would be enough for the algorithm to learn a policy from state _s_ 0 to goal\n\n_π_ [(] _[k]_ [)]\n_sD_ . To facilitate analysis, we define _i_ [(] _[k]_ [)] to be the smallest number such that _si_ ( _k_ ) _−−→_ _sD_ . Then the algorithm learns the\ndesired policy after _k_ iterations only if _i_ [(] _k_ ) = 0.\n\n\nNext, we prove that E[ _i_ [(] _[k][−]_ [1)] _−_ _i_ [(] _[k]_ [)] _|i_ [(] _[k][−]_ [1)] ] _≤_ _O_ (1). To change _i_ [(] _[k][−]_ [1)], the algorithm must roll out a success trajectory with\ngoal _sD_ from some _sj_ with _j ≤_ _i_ . Note that\n\n\nPr[ _i_ [(] _[k][−]_ [1)] _−_ _i_ [(] _[k]_ [)] = _j|i_ [(] _[k][−]_ [1)] ] _≤_ Pr[algorithm rolls out a success trajectory from _si_ ( _k−_ 1) _−j_ to _sD_ ] (11)\n\n_≤_ Pr[ _π_ [(] _[k][−]_ [1)] could go from _si_ ( _k−_ 1) _−j_ to _si_ ( _k−_ 1)] (12)\n\n_≤_ _O_ ( _|A|_ _[−][j]_ ) _,_ (13)\n\n\nso\n\n\n\nE[ _i_ [(] _[k][−]_ [1)] _−_ _i_ [(] _[k]_ [)] _|i_ [(] _[k][−]_ [1)] ] =\n\n\n\n_i_ [(] _[k][−]_ [1)]\n� _j ·_ Pr[ _i_ [(] _[k][−]_ [1)] _−_ _i_ [(] _[k]_ [)] = _j|i_ [(] _[k][−]_ [1)] ] (14)\n\n_j_ =0\n\n\n\n_∞_\n�\n\n\n\n� _j · |A|_ _[−][j]_ ) (15)\n\n_j_ =0\n\n\n\n_≤_ _O_ (\n\n\n\n_≤_ _O_ ( _|A|_ _[−]_ [1] ) (16)\n\n\n_≤_ _O_ (1) _._ (17)\n\n\nTherefore, by the Azuma-Hoeffding inequality (Azuma, 1967), we conclude that with high probability 1 _−_ _δ_, we have\n_i_ [(] _[k]_ [)] _< D_ for _k_ = Θ( _D −_ � _D_ log _δ_ _[−]_ [1] ) _≥_ Ω( _D_ ), i.e., _k ≥_ Ω( _D_ ) with high probability. Together with that _|S|_ [2] trajectories\n\nare rolled out in each iteration, we conclude that the sample complexity is with high probability at least Ω( _|S|_ [2] _D_ ).\n\n\nFinally, we conclude Theorem 5.3 by collecting Lemmas A.2 and A.3.\n\n\n**B. Experiment Details**\n\n\n**B.1. Environment Description**\n\n\nIn this section, we describe all the environment-specific details regarding MDP definitions.\n\n\nB.1.1. SAWYER PUSH\n\n\n“ _Push_ ” is a robotic pushing environment adopted from (Nair et al., 2018b) simulated with MuJoCo (Todorov et al., 2012)\nengine. Each episode lasts for at most 50 steps.\n\n\n**Observation:** The agent can observe 2-D hand position and 2-D puck position in each step.\n\n\n**Action space:** An action is a 2-D vector denoting the relative movement of the robot hand. The height of the hand and the\nstate of the gripper are fixed. Each dimension ranges from -1 to 1, and is categorized into 3 bins (therefore, there are a total\nof 9 different actions).\n\n\n3Actually, we can construct such an MDP. Assume we have ( _D_ + 2) states _S_ = _{s_ 0 _, · · ·, sD, sD_ +1 _}_ and 2 actions _A_ = _{_ 0 _,_ 1 _}_ . The\ntransition is given by _P_ ( _sD_ +1 _|si, a_ = 0) = 1 and _P_ ( _si_ +1 _|si, a_ = 1) = 1 for 0 _≤_ _i ≤_ _D_ . Such an MDP can similarly be constructed for\n_|A| ≥_ 3.\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\n**Goal and reward:** The puck goal is a 2-D vector denoting the target position on the table. The agent can only get reward\nwhen the _l_ 2 distance between puck and goal is smaller than 5cm.\n\n\n**Initial state:** Each episode starts with the hand initialized in a 0.03m _×_ 0.03m region on the right side of the table, the\npuck initialized in a 0.07m _×_ 0.07m square, and the goal is uniformly sampled in a 0.4m _×_ 0.4m square on the table.\n\n\nB.1.2. ANT MAZE\n\n\nThis environment is adopted from (Nachum et al., 2018). The scene is a 24m _×_ 24m maze simulated with MuJoCo (Todorov\net al., 2012). The maximum number of steps for each episode is 500 steps.\n\n\n**Observation:** Joint positions and joint velocities of the ant robot. The center of mass of the robot is included in the joint\npositions.\n\n\n**Action:** 8-D real-valued vector controlling the motors on the robot. The actions output from the policy network are clipped\nto -30 to 30 before sent to the simulator.\n\n\n**Goal and reward:** The goals are 2-D vectors denoting the desired _xy_ position of the ant. The agent gets reward when the\ndistance between the ant’s center of mass and the goal is smaller than 2m.\n\n\n**Initial state:** Both the ant and the goal are uniformly sampled in the empty space (not collided with walls) in the maze. For\nhard tasks, the ant is initialized at coordinate (0m, 0m), and the goal is at coordinate (0m, 16m).\n\n\nB.1.3. STACKING\n\n\nThe stacking environment is built with PyBullet (Coumans & Bai, 2016).\n\n\n**Observation:** The observation is a concatenation of robot state and objects states. The robot state contains 6-D end effector\npose, 3-D end effector linear velocity, 1-D finger position, and 1-D finger velocity. Each object state consists of its 6-D pose,\n3-D relative position to the robot’s end effector, 3-D linear velocity, 3-D angular velocity and 1-D 0/1 indicator denoting the\nidentity of the target cube.\n\n\n**Action:** A concatenation of 3-D relative movement of the end effector and 1-D finger control signal. Each dimension is\ndiscretized into 20 bins.\n\n\n**Goal and reward:** A goal specifies a 3-D desired position and an identity denoting which cube must be close to the goal\nspot. A non-zero reward is only given when the target cube is within 3cm distance to the goal, and when the end effector is\nat least 10cm away from the goal.\n\n\n**Adaptive curriculum:** Since directly training on stacking tasks with large number of objects pose severe exploration\nchallenges, we adopt a simple curriculum on the number of cubes to stack, similar to the one in (Li et al., 2020a). In\nthe beginning, 70% of the tasks only need to stack one cube, and the other tasks are uniformly sampled to stack 2 to _N_\ncubes. Whenever the average success rate reaches 0.6, the curriculum proceeds from sampling “stack- _n_ -cubes” with 70%\nprobability to “stack-( _n_ + 1)-cubes” tasks. We switch to offline SL training when the curriculum proceeds and the success\nrate for stacking _n_ + 1 cubes is lower than 0.5.\n\n\n**B.2. Combination with Offline RL**\n\n\nB.2.1. OFFLINE RL COMBINATION IN SAWYER PUSH\n\n\nFor the warm-start dataset, we collect rollout trajectories generated by random policy, where we keep successful samples\nand do goal relabelling on failed trajectories. Therefore, the dataset only consists of successful demonstrations. The dataset\nsize for phasic-DT is 55 _K_ steps, and 15 _k_ for phasic-AWAC. Phasic-AWAC switches from online to offline training after\ncollecting 200 _k_ online samples to match the total number of phasic switches in original PAIR. The online training of\nPhasic-DT is based on PPO, therefore we adopt the same number of PPO updates as PAIR for phasic-DT before switching\nto its offline phase.\n\n\nB.2.2. ADDITIONAL RESULTS IN STACK SCENARIO\n\n\nWe also try to run phasic-DT in the stacking domain. We train phasic-DT starting from the offline phase with a warm-up\ndataset. The warm-up dataset consists of successful trajectories in “stack-1-cube” scenario generated by a pretrained PPO\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n||||~~stack 6~~|\n\n\n\n_Figure 12._ Average success rate of phasic-DT in “stack-6-cube” domain.\n\n\n\n\n\n\n\n\n\n![](output/images/a94aaf192fc1d46d697e4d7eb3e999021ec88b46.pdf-15-10.png)\n\n\n|. w/ovtrain<br>08 w/v t rain<br>.<br>06<br>.<br>04<br>.<br>02<br>.<br>00 02 04 06 08 1<br>. . . . .|Col2|w/ov<br>w/v t r|train<br>ain|Col5|\n|---|---|---|---|---|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>w/o v train<br>w/ v train|||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>w/o v train<br>w/ v train|||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>w/o v train<br>w/ v train|||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1<br>0.2<br>0.4<br>0.6<br>0.8<br>.<br>w/o v train<br>w/ v train|||||\n\n\n|/ovtrain<br>/v t rain|Col2|Col3|\n|---|---|---|\n||||\n||||\n||||\n|2<br>3<br>4<br>|2<br>3<br>4<br>|2<br>3<br>4<br>|\n\n\n\n_Figure 13._ Comparison of training value network or not in PAIR offline phase. Left plot is in “Push” domain, right plot is in “Ant Maze”.\n\n\nmodel. We similarly adopt adaptive curriculum to alleviate exploration difficulty as when training PAIR. The results are\nshown in Figure 12. The success rate of phasic-DT for stacking 6 cubes grows slowly, and does not converge to a value as\nhigh as the original PAIR.\n\n\n**B.3. Algorithm Implementations**\n\n\n**Network architecture:** We use separate policy and value networks with the same architecture for PPO-based algorithms.\nThe specific network architectures for different domains are as follows. For “Push”, we use MLPs of hidden size 400 and\n300. For “Ant Maze”, the networks are MLPs of hidden size 256 and 256. For “Stacking”, we use a transformer (Vaswani\net al., 2017)-based architecture, which stacks 2 self-attention blocks with one head and 64 hidden units, and then goes\nthough linear heads to output action distributions or values. Since there are exponential number of possible actions as we\ndiscretize each action dimension into several bins, we assume different action dimensions are independent, and use separate\nlinear heads to predict distributions of different dimensions.\n\n\n**PAIR implementation:** We use PPO (Schulman et al., 2017) for online training and advantage weighted imitation learning\nfor offline training by default. For PPO training, we use _N_ worker parallel workers to collect transitions ( _s, a, r, s_ _[′]_ ) from\nthe environments synchronously. When applying the value-difference-based intrinsic reward, _r_ is replaced by the sum of\nenvironment reward and the intrinsic reward calculated with the current value network. After each worker gets _N_ steps data\npoints, we run _N_ epoch epochs of PPO training by splitting all the collected on-policy data into _N_ batch batches. The successful\ntrajectories from these on-policy batch are directly stored into the offline data _D_, and the failed trajectories are cached into\na failure buffer _B_ fail. We repeat the data collection - PPO training phase until the total amount of on-policy data reaches\n_N_ online. Then we perform data augmentation (task reduction, goal relabeling) over _B_ fail so as to generate more successful\ndata, and insert the resulting demonstrations into _D_ as well. After the dataset _D_ is constructed, we run _M_ epoch epochs of\nadvantage weighted behavior cloning with batch size _m_ . We use GAE advantage calculated with the value network learned\nafter the online phase as ( _R −_ _V_ ) for each data point. We use Adam optimizer (Kingma & Ba, 2015) for PPO and supervised\nlearning. All the hyper-parameters are listed in Table 2.\n\n\nWe do not additionally train the value network using the offline dataset in offline phase since we empirically find that training\n_V_ does not help the overall performance (see Figure 13).\n\n\n**GCSL baseline implementation:** There is a slight difference between our implementation and the original version from\n(Ghosh et al., 2020): we collect and relabel data in a more phasic fashion, i.e., we perform imitation learning on the data\nbatch collected from only the previous online collection phase. We keep the number of online steps before offline imitation\nlearning the same as PAIR. The original GCSL maintains a data buffer throughout the training process similar to the setting\n\n\n\n\n**Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning**\n\n\nDomain Push Ant Maze Stacking\n\n\n_N_ worker 4 64 64\n_N_ online 10 _·_ 2 [14] 20 _·_ 2 [18] adaptive\n_α_ 0.5\n\n_N_ steps 4096\n_N_ epoch 10\n_N_ batch 32\n_β_ 1\n_M_ epoch 10\n_m_ 64\n\nlr 2.5e-4\n\n\n_Table 2._ Hyperparameters of PAIR.\n\n\n\n|8<br>.<br>6<br>.<br>4<br>.|Col2|Col3|\n|---|---|---|\n|.4<br>.6<br>.8|||\n|.4<br>.6<br>.8|||\n|.4<br>.6<br>.8|||\n|0.0<br>0.2<br>0.4<br>.2||~~GCSL~~ ~~phasic~~<br>GCSLoriginal|\n|0.0<br>0.2<br>0.4<br>.2||0.6<br>0.8<br>1.0<br>|\n\n\n_Figure 14._ Comparison between phasic GCSL and the original version.\n\n\n\n|8<br>.<br>6<br>.<br>4<br>.<br>2<br>.|Col2|cs|l=0|03|Col6|\n|---|---|---|---|---|---|\n|.2<br>.4<br>.6<br>.8||~~.~~<br>csl=0.|~~.~~<br>csl=0.|1||\n|.2<br>.4<br>.6<br>.8||c~~sl~~=0.|c~~sl~~=0.|3|3|\n|.2<br>.4<br>.6<br>.8||||||\n|0.0<br>0.5<br>1.0<br>.0||||||\n\n\n_Figure 15._ Success rate of joint RL+SL with different _c_ _[SL]_ in “stack6-cube” task. We adopt _c_ _[SL]_ = 0.1 for SIR in the main paper.\n\n\n\nin off-policy RL. We find our phasic GCSL can get better performance than the original version (see Figure 14), so we\npresent the results of phasic GCSL in the main paper.\n\n\n**DT baseline implementation:** The policy and value networks adopt the same GPT-2 architecture similar to the original\nversion in (Chen et al., 2021). They output actions and values conditioning on desired return, past states, and past actions.\nTo avoid gradient explosion, only the last token of the predicted action or value sequence is used for updating the model. To\nstabilize training, we remove all dropout layers from the transformer model. We use a context length of 5 for sequence\nconditioning. The specific feature extractors for processing single-step observations in different domains are as follows.\nFor “Push”, we use MLPs of hidden size 300 and 300 for representation learning and we train separate policy and value\nnetworks. For “Stacking”, we use a transformer-based architecture similar to PAIR for representation learning except that\nthe size of hidden layer is 128. We find that using shared or separate feature extractors for policy and value networks leads\nto similar performances. Therefore we share parameters for them to save computational resources.\n\n\n**B.4. Analysis on Phasic vs. Joint RL and SL Optimization**\n\n\nPAIR decouples RL and SL objectives in two phases instead of optimizing them jointly ( _L_ _[RL]_ + _c_ _[SL]_ _L_ _[SL]_ ), since the two\nobjectives can largely interfere with each other and the choice of _c_ _[SL]_ is empirically brittle to tune. The RL objective is\npolicy gradient over rollout data (Eqn. 1), which requires (primarily) on-policy samples (both success and failures) to make\npolicy improvement. The SL objective (Eqn. 2) is performed over the successful dataset with both success-only rollout\nsamples and off-policy augmentation trajectories. These two objectives operate on very distinct data distributions. If _c_ _[SL]_ is\ntoo large, the gradient will be pulled away from the policy improvement direction, which makes policy learning unstable or\neven breaks training. If _c_ _[SL]_ is too small, the objective may not sufficiently leverage the augmented successful data learning\nto slow convergence. We report sensitivity analysis in Fig. 15 by trying different _c_ _[SL]_ in the joint objective. With phasic\ntraining, RL and SL are decoupled and the interference is largely reduced. We can also intuitively interpret the benefit of\ndecoupling RL and SL into two phases by echoing one result in stochastic optimization: smaller optimization step size\nshould be taken when the gradient noise is large (Theorem 6.2 in (Bubeck, 2015)). RL objective is with large gradient noise,\nwhile SL offers clear supervision signal. When jointly optimizing RL and SL, only small step size is allowed which leads to\nslow convergence; when optimizing RL and SL separately, different step sizes can be chosen for different objectives, thus\nconverges faster.\n\n\n",
    "ranking": {
      "relevance_score": 0.7392184328390901,
      "citation_score": 0.6047789115646258,
      "recency_score": 0.5500280129167398,
      "final_score": 0.6934114865919622
    },
    "citation_key": "Li2022PhasicSR",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "525c1fc4aebcd6c5ed41b543e949b429a35b9a18",
    "title": "Asymptotic Convergence and Performance of Multi-Agent Q-Learning Dynamics",
    "published": "2023-01-23",
    "authors": [
      "A. Hussain",
      "F. Belardinelli",
      "G. Piliouras"
    ],
    "summary": "Achieving convergence of multiple learning agents in general $N$-player games is imperative for the development of safe and reliable machine learning (ML) algorithms and their application to autonomous systems. Yet it is known that, outside the bounds of simple two-player games, convergence cannot be taken for granted. To make progress in resolving this problem, we study the dynamics of smooth Q-Learning, a popular reinforcement learning algorithm which quantifies the tendency for learning agents to explore their state space or exploit their payoffs. We show a sufficient condition on the rate of exploration such that the Q-Learning dynamics is guaranteed to converge to a unique equilibrium in any game. We connect this result to games for which Q-Learning is known to converge with arbitrary exploration rates, including weighted Potential games and weighted zero sum polymatrix games. Finally, we examine the performance of the Q-Learning dynamic as measured by the Time Averaged Social Welfare, and comparing this with the Social Welfare achieved by the equilibrium. We provide a sufficient condition whereby the Q-Learning dynamic will outperform the equilibrium even if the dynamics do not converge.",
    "pdf_url": "http://arxiv.org/pdf/2301.09619",
    "doi": "10.48550/arXiv.2301.09619",
    "fields_of_study": [
      "Computer Science",
      "Mathematics"
    ],
    "venue": "Adaptive Agents and Multi-Agent Systems",
    "citation_count": 15,
    "bibtex": "@Article{Hussain2023AsymptoticCA,\n author = {A. Hussain and F. Belardinelli and G. Piliouras},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n pages = {1578-1586},\n title = {Asymptotic Convergence and Performance of Multi-Agent Q-Learning Dynamics},\n year = {2023}\n}\n",
    "markdown_text": "# Asymptotic Convergence and Performance of Multi-Agent Q-learning Dynamics\n\n#### Aamal Abbas Hussain aamal.hussain15@imperial.ac.uk Francesco Belardinelli francesco.belardinelli@imperial.ac.uk Georgios Piliouras georgios@sutd.edu.sg\n\n**Abstract**\n\n\nAchieving convergence of multiple learning agents in general _N_ -player games is imperative for\nthe development of safe and reliable machine learning (ML) algorithms and their application to autonomous systems. Yet it is known that, outside the bounds of simple two-player games, convergence\ncannot be taken for granted.\nTo make progress in resolving this problem, we study the dynamics of smooth Q-Learning, a\npopular reinforcement learning algorithm which quantifies the tendency for learning agents to explore\ntheir state space or exploit their payoffs. We show a sufficient condition on the rate of exploration\nsuch that the Q-Learning dynamics is guaranteed to converge to a unique equilibrium in any game.\nWe connect this result to games for which Q-Learning is known to converge with arbitrary exploration\nrates, including weighted Potential games and weighted zero sum polymatrix games.\nFinally, we examine the performance of the Q-Learning dynamic as measured by the Time Averaged Social Welfare, and comparing this with the Social Welfare achieved by the equilibrium. We\nprovide a sufficient condition whereby the Q-Learning dynamic will outperform the equilibrium even\nif the dynamics do not converge.\n\n### **1 Introduction**\n\n\nUnderstanding the behaviour of multi-agent learning systems has been a hallmark problem of game\ntheory and online learning. The requirement is that agents must explore potentially sub-optimal decisions\nwhilst interacting with other agents to ultimately maximise their long-term reward. In contrast to online\nlearning with a single agent, this poses a fundamentally non-stationary problem, in which convergence\nto an equilibrium is not always guaranteed.\nIn fact, recent work has consistently found that, when learning on games, agents may present a wide\narray of behaviours. This includes cycles [1, 2, 3], and even chaos [4, 5, 6, 7, 8]. Furthermore, the\nequilibria of a game need not be unique, so even if convergence is guaranteed, it may be to one of many\n(or even a continuum) of equilibria. Thus, predicting the behaviour of online learning in games with\nmany players becomes a particularly challenging problem.\n\n\n1\n\n\n\n\n1 INTRODUCTION 2\n\n\nYet it remains an important problem to solve. Recent advances in machine learning require training\nmultiple neural networks for applications to generative models [9, 10]. In order for such applications to\nbe realised, it is required that that training provably converges to an equilibrium. Of equal importance is\nthat this equilibrium be unique, so that the outcome remains consistent regardless of initial conditions.\nA unique equilibrium guarantees not only the reproducibility of the system, but also ensures that desired\nbehaviours will persist, even if the system is perturbed from its desired state. Similarly, the most complex\ntasks often require the interaction of multiple autonomous agents [11]. This again requires that agents\nare able to reliably equilibrate their behaviour.\nThere is a strong, and ongoing, effort in the research community to understand these learning behaviours, with positive convergence results being found in an assortment of game structures. For instance, games with two players and two actions are well understood [12, 13, 14]. Beyond this, some of\nthe most widely studied games are potential games, in which agents collaborate to maximise a shared\nglobal function, and zero sum games (and its network variants), in which agents are in competition. Indeed, it has been found that a number of learning algorithms, including Fictitious Play [15], Q-Learning\n\n[16, 17, 4], Replicator Dynamics [18, 19, 20] all converge to equilibria (though not always unique) in\npotential games [21, 22]. In zero sum games, the former two converge to a unique fixed point [23, 24],\nwhereas the latter is known to cycle, always maintaining its distance from the equilibrium [1].\nOutside of this class of games, however, the story becomes much more complicated. A wide array of\nresults show learning algorithms may be chaotic in even the simplest games [5]. Such complex behaviours\nbecome even more pronounced as the number of agents in the game increases [7]. However, they are also\ninfluenced by the structure of the game and the parameters of the learning algorithm [25]. This dichotomy\nbetween the range of possible learning behaviours and the convergence requirement of the applications\nmotivate our central question:\n\n\n_Are there learning dynamics such that convergence to a unique equilibrium is guaranteed in any game?_\n\n\n**Main Contribution.** To answer this, we study the _(smooth) Q-Learning_ dynamics, a popular online\nlearning model which captures the behaviour of agents who must balance their tendency to explore their\nstrategy space against exploiting their payoffs.\nIn the first of our contributions, we answer the above question in a positive manner. Namely, we\nshow that, through sufficient exploration, agents engaged in any game can reach a unique equilibrium.\nWe parameterise the amount of exploration required in terms of the size of the game and the number of\nplayers. We then revisit previously established results of smooth Q-Learning and show that convergence\nof the algorithm in weighted potential and weighted network zero sum games both follow as special cases\nof our main result.\n\nTo qualify our convergence result, we consider the _payoff performance_ of learning dynamics in terms\nof the sum of payoffs received by all agents. We provide a condition whereby learning dynamics that\ndo not converge can outperform the equilibrium. In such a case, it may be beneficial to assume no\nexploration on the parts of the agents.\nThis result is supported by our experiments in which examples of games are considered where payoff\nperformance degrades as agents are pushed towards an equilibrium through exploration.\n\n\n**Related Work.** Our work applies the framework of monotone games, which encompass a number\nof classes of games, including potential [26] and network zero sum games [27]. Indeed recent work has\nconsidered the question of designing games which are monotone [28]. Most prominent in this is the study\nof network games [29]. These works relate the monotonicity of the game with properties of the network.\nFrom a design perspective, this makes the study of monotone games rather attractive, as it becomes\n\n\n\n\n2 PRELIMINARIES 3\n\n\npossible to make any game, be it co-operative or competitive, monotone. This fact is also exploited\nin [30], in which it was shown how any game, regardless of its structure, can be made monotone by\nappropriate parameter tuning.\nIn addition, monotone games have been used to design online learning algorithms that converge to\na Nash Equilibrium [31, 32]. However, many of these require the gradient (or estimates) of their cost\nfunctions at each step. Ideally, an online learning algorithm should require only obtained rewards at each\ntime step. To resolve this, [28] derive a distributed algorithm which converges to a Nash Equilibrium in\nmonotone games. These results are advanced in [33] in which an algorithm is developed which converges,\nwhilst also achieving no-regret; this was the first instance of a no-regret algorithm who could provably\nconverge in a monotone game. In a similar manner, [26] show the convergence of the Best Response\nAlgorithm in a class of network games which satisfy the monotonicity property.\nOur work departs from the above by considering Q-Learning, and by lifting strong technical assumptions on the payoff functions. Specifically, we do not assume a form of the payoffs as in [26], or the\ngrowth of the function as in [33]. In addition, we require no knowledge of the cost gradients as in [34]. Finally, our work considers the generalised class of _weighted_ monotone games, rather than the unweighted\ncase considered by the above. This class of games is also considered in [35, 36], in which variations of\nonline gradient descent are analyzed. However, the former requires weighted strong monotonicity (which\nis much more restrictive even than strict monotonicity) and the latter requires strong assumptions on the\nparameters of the learning algorithm.\nOur work also touches upon the Follow the Regularised Leader (FTRL) dynamic. The strongest\nconvergence result regarding this dynamic in continuous time is a negative one: FTRL does not converge\nto a Mixed Nash Equilibrium [37], or to any equilibrium in zero sum games [1]. The strongest positive\nresult, and the one most similar to our own is [38] in which it is shown that FTRL converges in unweighted\nstrictly monotone games, a more restrictive class that that analysed here. Other positive results regard\nFTRLs local convergence to a strict Nash Equilibrium [39] and its convergence in time average [38].\nOur paper is structured as follows. We begin in Section 2 by outlining the setting and tools through\nwhich we analyse convergence of learning. Section 3 proceeds with our main results, namely that convergence of Q-Learning dynamics can be achieved through sufficient exploration (Theorem 1), and that\nnon-convergent learning dynamics can outperform the equilibrium (Theorem 4). Our experiments in\nSection 4 validate these results by showing examples of games in which convergence occurs through\nincreased exploration, although at the cost of decreasing payoff across all agents.\n\n### **2 Preliminaries**\n\n\nIn this section, we expand on the necessary background for our main results; specifically how the game\nmodel is set up, the learning dynamics of interest, and the techniques used in our analysis.\n\n#### **2.1 Game Model**\n\n\nIn our study, we consider a game Γ = ( _N_ _,_ ( _Sk, uk_ ) _k∈N_ ), where _N_ denotes a finite set of agents indexed\nby _k_ = 1 _, . . ., N_ . Each agent _k ∈N_ is equipped with a finite set of actions denoted by _Sk_ with the\nnumber of actions _nk_ := _|Sk|_ as well as a payoff function _uk_ . We denote a _mixed strategy_ (hereafter just\n_strategy_ ) **x** _k_ of an agent _k_ as a probability vector over its actions. Then, the set of all strategies of agent\n_k_ is ∆ _k_ := _{_ **x** _k ∈_ R _[n][k]_ : [�] _i_ _[x][ki]_ [ = 1] _[, x][ki][ ≥]_ [0] _[}]_ [ on which act the payoff functions] _[ u][k]_ [ : ∆] _[k][ ×]_ [ ∆] _[−][k][ →]_ [R][.]\n\nWe denote by **x** := ( **x** _k_ ) _k∈N ∈_ ∆= _×k_ ∆ _k_ the _joint strategy_ of all agents and, for any _k_, **x** _−k_ :=\n( **x** _l_ ) _l∈N\\{k} ∈_ ∆ _−k_ the joint strategy of all agents other than _k_ .\n\n\n\n\n2 PRELIMINARIES 4\n\n\nFor any **x** _∈_ ∆, we define the reward to agent _k_ for playing action _i ∈_ _Sk_ as _rki_ ( **x** ) := _[∂u]_ _∂x_ _[ki]_ _ki_ [(] **[x]** [)] [. We]\n\nwrite _rk_ ( **x** ) = ( _rki_ ( **x** )) _i∈Sk_ as the concatenation of all rewards to agent _k_ . Using this notation we can\nwrite _uk_ ( **x** ) = _⟨_ **x** _k, rk_ ( **x** ) _⟩_ in which _⟨_ **x** _,_ **y** _⟩_ = **x** _[⊤]_ **y** denotes the standard inner product in R _[n]_ . With this in\nmind, we define the _Equilibrium_ of a game.\n\n\n**Definition 1** (Equilibrium) **.** A joint mixed strategy **¯x** _∈_ ∆ is an _Equilibrium_ if, for all agents _k_ and all\n**x** _k ∈_ ∆ _k_\n_⟨_ **x** _k, rk_ ( **¯x** ) _⟩≤⟨_ **¯x** _k, rk_ ( **¯x** ) _⟩_ (1)\n\n\n**¯x** is a _strict equilibrium_ if the inequality (1) is strict for all **x** _k ̸_ = **¯x** _k_ .\nThe equilibrium concept which we are primarily interested in with regards to Q-Learning is the Quantal Response Equilibrium (QRE) [40], which acts as an equilibrium concept when agents have bounded\nrationality.\n\n\n**Definition 2** (Quantal Response Equilibrium (QRE)) **.** A joint mixed strategy **¯x** _∈_ ∆ is a _Quantal Re-_\n_sponse Equilibrium_ (QRE) if, for all agents _k_ and all actions _i ∈_ _Sk_\n\n\nexp( _rki_ ( **¯x** _−k_ ) _/Tk_ )\n**¯x** _ki_ = ~~�~~ _j∈Sk_ [exp(] _[r][kj]_ [(] **[¯x]** _[−][k]_ [)] _[/T][k]_ [)]\n\n\nwhere _Tk_ denotes the _exploration rate_ of the agent: low values of _Tk_ correspond to a higher tendency\nto exploit the best performing action. This, purely rational behaviour, corresponds to the _Nash Equilib-_\n_rium_ in the limit _Tk →_ 0. Higher values of _Tk_, meanwhile, implies a higher exploration rate. The two\nequilibria concepts are related by the following results\n\n\n**Proposition 1** ([30] Proposition 2) **.** Consider a game Γ = ( _N_ _,_ ( _Sk, uk_ ) _k∈N_ ) and take any _Tk >_ 0. Define\nthe _perturbed game_ Γ _[H]_ = ( _N_ _,_ ( _Sk, u_ _[H]_ _k_ _[k][ ∈N]_ [)][ with the modified payoffs]\n\n\n_u_ _[H]_ _k_ [(] **[x]** _[k][,]_ **[ x]** _[−][k]_ [) =] _[ ⟨]_ **[x]** _[k][, r][k]_ [(] **[x]** _[−][k]_ [)] _[⟩−]_ _[T][k][⟨]_ **[x]** _[k][,]_ [ ln] **[ x]** _[k][⟩]_\n\n\nThen **¯x** _∈_ ∆ is a QRE of Γ if and only if it is an NE of Γ _[H]_ .\n\n\nFinally, in the application of our results (specifically Lemma 1), we will consider the _influence bound_\nof a game, which gives a notion of its _size_ . Formally, we apply the definition from [30]\n\n\n**Definition 3** (Influence Bound) **.** Consider a finite normal form game Γ, the _influence bound δ_ is given by\n\n\n_δ_ = max˜\n_k∈N_ _,i∈Sk,s−k,s−k∈S−k_ _[{|][r][ki]_ [(] _[s][−][k]_ [)] _[ −]_ _[r][ki]_ [(˜] _[s][−][k]_ [)] _[|}]_\n\n\nwhere the pure strategies _s−k,_ ˜ _s−k ∈_ _S−k_ differ only in the strategies of one agent _l ̸_ = _k_ .\n\n\nSince _|rki_ ( _s−k_ ) _−_ _rki_ (˜ _s−k_ ) _|_ measures the change in reward to agent _k_ for playing action _i_ due to a\nchange the other players’ actions, the influence bound _δ_ can be thought of as a measure of the maximum\ninfluence (in terms of reward) that any agent could receive from their opponents. In Section 4, we consider\ntwo games and show how their influence bound can be readily determined.\n\n\n\n\n2 PRELIMINARIES 5\n\n#### **2.2 Learning Model**\n\n\nThe principal multi agent learning model that we analyse in this study is the _smooth Q-Learning_ (QL)\ndynamic [17] which is foundational in economics [41] and artificial intelligence [16]. In [17, 4] a continuous time approximation of the Q-Learning algorithm was found which accurately captures the behaviour\nof learning agents through the following ODE:\n\n\n_x_ ˙ _ki_\n= _rki_ ( **x** ) _−⟨_ **x** _k, rk_ ( **x** ) _⟩−_ _Tk_ (ln _xki −⟨_ **x** _k,_ ln **x** _k⟩_ ) (2)\n_xki_\n\n\nA full review of this dynamic is beyond the scope of this study, however a derivation of (2) can be found\nin [24] as well as the result that fixed points of the Q-Learning dynamic correspond to the QRE of the\ngame Γ.\nIt was shown in [22] that the transformation between the game Γ and the perturbed game Γ _[H]_ as\ndescribed in Sec. 2.1 also relates the Q-Learning Dynamics to the the well studied _replicator dynamics_\n(RD) [18, 19], in the following manner.\n\n\n**Lemma 1** ([22]) **.** Consider the game Γ = ( _N_ _,_ ( _Sk, uk_ ) _k∈N_ ) and some _Tk >_ 0. Then the Q-Learning\ndynamics (2) can be written as\n\n\n_x_ ˙ _ki_\n= _rki_ _[H]_ [(] **[x]** [)] _[ −]_ � **x** _k, rk_ _[H]_ [(] **[x]** [)] � (3)\n_xki_\n\n\nwhere _rki_ _[H]_ [(] **[x]** [) =] _[ r][ki]_ [(] **[x]** [)] _[ −]_ _[T][k]_ [ (ln] _[ x][ki]_ [ + 1)][. In particular, (2) corresponds to RD in the perturbed game]\nΓ _[H]_ = ( _N_ _,_ ( _Sk, u_ _[H]_ _k_ [)] _[k][∈N]_ [)][.]\n\n\n**Follow the Regularised Leader** It is known [39, 37] that RD is derived as a particular instance of\nthe _Follow the Regularised Leader_ (FTRL) dynamic [42]. In essence, FTRL requires that the agents\nmaximise their cumulative payoff up to the current time _t_ . However, it also imposes a regularisation on\nthe agents’ actions which softens the arg max function. More formally we have that for every agent _k_,\n\n\n_t_\n**y** _k_ ( _t_ ) = **y** _k_ (0) + **r** _k_ ( **x** ( _s_ )) _ds_\n�0\n\n**x** _k_ ( _t_ ) = _Qk_ ( **y** _t_ ) := arg max (4)\n**x** _k∈_ ∆ _k_ _[{⟨]_ **[x]** _[k][,]_ **[ y]** _[k][⟩−]_ _[h][k]_ [(] **[x]** _[k]_ [)] _[}]_\n\n\nTo make RD compatible with FTRL, we make the following assumption on the regularisers\n\n\n**Assumption 1.** For every agent _k_, the regulariser _hk_ is:\n\n\nI Continuously differentiable with differential _∇hk_ which itself is Lipschitz on ∆ _k_, with constant\n_Lk_ .\n\n\nII _Steep_ in that _||∇hk_ ( **x** ) _|| →∞_ as **x** _→_ _∂_ ∆ (c.f. [37]).\n\n\nIII Strongly convex on ∆ _k_, i.e. there exists _κ_ such that, for any **x** _k,_ **y** _k ∈_ ∆ _k_\n\n\n_⟨∇hk_ ( **y** _k_ ) _,_ **x** _k −_ **y** _k⟩≤_ _hk_ ( **y** _k_ ) _−_ _hk_ ( **x** _k_ ) _−_ _[κ]_\n\n2 _[||]_ **[x]** _[k][ −]_ **[y]** _[k][||]_ [2]\n\n\nThe choice of _hk_ and _rk_ will, of course, depend on the application scenario. Our interest in this work\nis to consider the case _hk_ ( _xk_ ) = [�] _i_ _[x][ki]_ [ ln] _[ x][ki]_ [, which satisfies Assumption 1, and from which RD is]\n\nderived. As mentioned, smooth Q-Learning describes RD in the perturbed game Γ _[H]_ . This perturbation\nwill be instrumental in proving our results on convergence.\n\n\n\n\n3 LEARNING IN WEIGHTED MONOTONE GAMES 6\n\n#### **2.3 Variational Inequalities and Game Theory**\n\n\nIn this work we will examine game-theoretic concepts through the lens of Variational Inequalities [31].\nThis branch of research modifies the problem of finding a Equilibrium of a game to that of finding a\nsolution to a variational inequality, which is defined as follows.\n\n\n**Definition 4** (Variational Inequalities) **.** Consider a set _X ⊂_ R _[d]_ and a map _F_ : _X →_ R _[d]_ . The Variational\nInequality _V I_ ( _X_ _, F_ ) is given as\n\n\n_⟨_ **x** _−_ **¯x** _, F_ ( **¯x** ) _⟩≥_ 0 _,_ for all **x** _∈X_ _._ (5)\n\n\nWe say that **¯x** _∈X_ belongs to the set of solutions to a variational inequality _V I_ ( _X_ _, F_ ) if it satisfies (5).\n\n\nIn this work, we will be considering the state space _X_ = ∆= _×k_ ∆ _k_ alonside the map _F_ : ∆ _→_ R _[Nn]_\n\ndefined as\n_F_ ( **x** ) = ( _Fk_ ( **x** )) _k∈N_ = ( _−rk_ ( **x** )) _k∈N_\n\nThis map is sometimes referred to as the _pseudo-gradient_ of the game [28]. Its properties allow for\nconditions to be found under which the equilibrium **¯x** is unique. To illuminate these conditions, we have\nthe following definition.\n\n\n**Definition 5** (Weighted Monotone Game) **.** A game Γ with a continuous pseudo-gradient _F_ is _weighted_\n_monotone_ if there exist positive constants _w_ 1 _, . . ., wd_ such that, for all **x** _,_ **y** _∈_ ∆,\n\n\n_⟨_ **x** _−_ **y** _, F_ ( **x** ; **w** ) _−_ _F_ ( **y** ; **w** ) _⟩≥_ 0 (6)\n\n\nwhere _F_ ( **x** _,_ **w** ) = ( _w_ 1 _F_ 1 _, . . ., wdFd_ ) _[⊤]_ .\n\n\nNaturally, the game is _weighted strictly monotone_ if, for all **x** _̸_ = **y** _∈_ ∆, inequality (6) is strict.\nThrough these elements, it is possible to explore the nature of the equilibrium of a game from a\nvariational perspective (see for instance [31, 26, 30]). The seminal results from this analysis, outlined in\ndepth in [31] are:\n\n\n**Lemma 2.** Consider a game Γ with pseudo-gradient _F_ ( **x** ). Then, **¯x** _∈_ ∆ is an Equilibrium of Γ if and\nonly if it satisfies\n_⟨_ **x** _−_ **¯x** _, F_ ( **x** ) _⟩≥_ 0 for all **x** _∈_ ∆ _._\n\n\n**Lemma 3.** If Γ = ( _N_ _,_ ( _Sk, uk_ ) _k∈N_ ) is a strictly monotone game, it has a unique Equilibrium **¯x** _∈_ ∆.\n\n\nIn our work, we will leverage the fact that both of these Lemmas extend readily to the weighted case\n(as all weights are assumed non-negative). By applying these properties, it is our goal to examine whether\nthe equilibrium will be reached by learning agents.\n\n### **3 Learning in Weighted Monotone Games**\n\n\nWe have two main results in this section. The first is that multi-agent Q-Learning can achieve convergence to an equilibrium in _any_ game through sufficient exploration, parameterised by _Tk_ . Following this\nresult, we consider the optimality of convergence to an equilibrium. Namely we show a sufficient condition (convergence in time-average) in which non-fixed point behaviour (e.g. cycles) outperforms the\nequilibrium in terms of payoff.\n\n\n\n\n3 LEARNING IN WEIGHTED MONOTONE GAMES 7\n\n#### **3.1 Convergence through Sufficient Exploration**\n\n\nWithout further delay, we state our first main result\n\n\n**Theorem 1.** Let _δ_ be the influence bound of an arbitrary game Γ with _N_ players. Then, Q-Learning\nconverges to the unique QRE **¯x** if, for all agents _k_,\n\n\n_Tk > δ_ ( _N −_ 1) (7)\n\n\nThis result provides a general condition from which the convergence to the QRE can be guaranteed\nin any game. As one might expect, the amount of exploration (the size of _Tk_ ) required to achieve this\nwill be influenced by the size of the game (parameterised by _δ_ ) and the number of players in the game\n(parameterised by _N_ ). An interesting point to note is that this results supports that of [7] in which it was\nshown that non-fixed point behaviour is more likely with low exploration rates, if the number of players\nis increased.\n\n\n_Proof Sketch._ Theorem 1 relies on two points: first, the perturbed game Γ _[H]_ is weighted strictly monotone\n(which gives uniqueness of **¯x** ) if _Tk_ satisfies (7), and second, the QL dynamics converges to a fixed point\nin such games. The first point is shown through [30] Theorem 1 so we focus on the latter.\nWe achieve this in the following manner. First we show that, along trajectories of FTRL in weighted\nstrictly monotone games, the distance to the equilibrium **¯x** is decreasing. From this, the convergence\nof replicator in strictly monotone games is immediate. The reader will recall that Q-Learning describes\nreplicator dynamics in a perturbed game. We show that, if the original game Γ is weighted monotone,\nthen the corresponding perturbed game Γ _[H]_ is weighted strictly monotone. Putting all this together yields\nthe convergence of QL in weighted monotone games.\n\n\n**Step 1: Convergence of FTRL** In order to show that the distance to the equilibrium **¯x** is being decreased, we must first define what we mean by _distance_ . We do this through the _Bregman Divergence_\n\n[42].\n\n\n**Definition 6.** Consider a set of functions _hk_ : ∆ _k →_ R and a set of positive scalars **w** = ( _w_ 1 _, . . ., wN_ )\nwith _k ∈N_ . The _Weighted Bregman Divergence_ induced by _h_ = ( _hk_ ) _k∈N_ between a set of probability\nvectors **x** _,_ **y** _∈_ ∆ with **x** = ( **x** _k_ ) _k∈N_, **y** = ( **y** _k_ ) _k∈N_ is given by\n\n\n_WB_ ( **x** _||_ **y** ; _h_ ) = � _wkDB_ ( **x** _k||_ **y** _k_ ; _h_ )\n\n\n_k_\n\n= � _wk_ ( _hk_ ( **y** _k_ ) _−_ _hk_ ( **x** _k_ ) _−⟨∇hk_ ( **x** _k_ ) _,_ **y** _k −_ **x** _k⟩_ ) _._\n\n\n_k_\n\n\n_Remark._ In particular, with the choice _hk_ ( _xk_ ) = [�] _i_ _[x][ki]_ [ ln] _[ x][ki]_ [, the Weighted Bregman Divergence corre-]\n\nsponds to the _Weighted Kullback-Leibler (KL) Divergence_ defined by\n\n\n\n_WKL_ ( **x** _||_ **y** ) = �\n\n\n\n� _wkDKL_ ( **x** _k||_ **y** _k_ ) = �\n\n_k∈N_ _k∈N_\n\n\n\n_wk_\n\n� �\n\n_k∈N_ _i∈S_\n\n\n\n_xki_ ln _[x][ki]_\n\n�\n\n_yki_\n\n_i∈Sk_\n\n\n\n_._ (8)\n_yki_\n\n\n\n**Theorem 2.** Consider a weighted strictly monotone game Γ. If each agent follows an FTRL algorithm\nwhose regulariser satisfies Assumption 1 then, for any initial condition **x** (0), **x** ( _t_ ) minimises the weighted\nBregman divergence towards the unique equilibrium **¯x** .\n\n\n\n\n3 LEARNING IN WEIGHTED MONOTONE GAMES 8\n\n\n_Remark._ The question of convergence to the equilibrium **¯x** can also be established through Theorem 4.9\nof [38] and showing that the assumptions required by their theorem are met by Assumption 1, which\nwe do in the Appendix. However, in order to progress towards convergence of Q-Learning in _weighted_\n_monotone games_ (i.e. to lift the strictness requirement), we take the extra step of showing that the\n_Bregman Divergence_ is decreasing along trajectories.\n\n\n**Step 2: Convergence of RD** Using the specialisation _hk_ ( _xk_ ) = [�] _i_ _[x][ki]_ [ ln] _[ x][ki]_ [, and the relation between]\n\nthe Bregman and KL Divergences from, Theorem 2 implies that _WKL_ ( **¯x** _||_ **x** ( _t_ )) is a Lyapunov function\nfor RD in weighted strictly monotone games.\n\n\n**Corollary 1.** Consider a weighted strictly monotone game Γ. Then trajectories **x** ( _t_ ) under the replicator\ndynamics minimise the KL-Divergence to the unique Equilibrium.\n\n\n_Remark._ Corollary 1 mirrors the result found by [43] (Prop. 4.5), which also showed that the KLdivergence decreases. Here, we show that this is a special case of Theorem 2.\n\n\n**Step 3: Convergence of Q-Learning** Finally, we recognise that the same transformation which takes Γ\nto Γ _[H]_ also takes Q-Learning to RD (c.f. [22] Lemma 3.1). Putting this all together, if _Tk_ satisfies (7), the\nperturbed game Γ _[H]_ is strictly monotone, which yields the convergence of RD to the unique equilibrium\n**¯x** . This immediately gives the convergence of QL in the original game Γ.\n\n#### **3.2 Convergence through Arbitrary Exploration**\n\n\nWhilst Theorem 1 gives a condition to achieve convergence through sufficient exploration, it is known\nthat there are games for which convergence to a QRE can be achieved with any non-zero exploration rate\n_Tk_ . These games are: weighted potential games [22] and weighted zero sum polymatrix games [24]. In\nthis section, we point out that, under suitable game structures, the reduced requirement on exploration\nin these games stems from the fact that they are both examples of _monotone_ games. We can, therefore,\napply the following Theorem to yield convergence of QL in such games.\n\n\n**Theorem 3.** If the game Γ is weighted monotone then, for any _T >_ 0, Q-Learning converges to the\nunique QRE.\n\n\nThe proof of this statement, presented in full in the Appendix, lies in recognising that the transformation which takes Γ to Γ _[H]_ is an additive term of the form\n\n\n_−Tk_ (ln _xki_ + 1)\n\n\nwhich is a strictly monotone function in **x** _k_ . As such, if Γ is already weighted monotone then, for any\npositive value of _Tk_, Γ _[H]_ must be weighted strictly monotone.\n\n\n**Weighted Potential Games** Our first application concerns the dynamics of learning in weighted potential games. These concern the cooperative setting in which the game admits a global function over\nall agents’ strategies. Formally, a game is called a weighted potential game if there exists a function\n_U_ : ∆ _→_ R and positive weights _w_ 1 _, . . ., wN >_ 0 such that, for each player _k ∈N_, _U_ ( **x** _k,_ **x** _−k_ ) _−_\n_U_ ( **y** _k,_ **x** _−k_ ) = _wk_ ( _uk_ ( **x** _k,_ **x** _−k_ ) _−_ _uk_ ( **y** _k,_ **x** _−k_ )) for all **x** _k,_ **y** _k ∈_ ∆ _k_ and all **x** _−k ∈_ ∆ _−k_ .\n\n\n**Lemma 4.** Consider a weighted potential game Γ with concave potential _U_ ( **x** ). Then, for any _TK >_ 0,\nthe Q-Learning dynamics converges to the unique QRE **¯x** .\n\n\n\n\n3 LEARNING IN WEIGHTED MONOTONE GAMES 9\n\n\nOf course, it must be noted that, in many games, the potential is not concave and so require a different\napproach towards showing convergence. [22] performs such an analysis and also considers the geometry\nof the multiple QRE which can exist due to the non-concavity of the potential.\n\n\n**Weighted Zero Sum Polymatrix Games** In this application, we consider the competitive setting through\nthe _weighted zero sum network polymatrix game_ . Formally, a network polymatrix game\nΓ = �( _N_ _, E_ ) _,_ ( _Skl, A_ _[kl]_ )( _k,l_ ) _∈E_ � includes a network ( _N_ _, E_ ) in which _E_ consists of pairs of agents _k, l ∈N_\nwho are connected in the network. Each edge is imbued with payoff matrices _A_ _[kl]_ which denotes the\nreward to agent _k_ against agent _l_ . The total payoff received by agent _k_, then, is\n\n\n_uk_ ( **x** _k,_ **x** _−k_ ) = � _⟨_ **x** _k, A_ _[kl]_ **x** _l⟩_ (9)\n\n( _k,l_ ) _∈E_\n\n\nA game is called _weighted zero sum network polymatrix game_ if there exists positive constants _w_ 1 _, . . ., wN_\nsuch that, for all **x** _∈_ ∆\n� _wk⟨_ **x** _k, A_ _[kl]_ **x** _l⟩_ = 0 (10)\n\n_k∈N_\n\n\n**Lemma 5.** Consider a weighted zero sum network polymatrix game Γ. The unique QRE **x** is globally\nasymptotically stable under QL for any _Tk >_ 0.\n\n\nThe convergence of Q-Learning in this class of games is also proven through an alternate proof in\n\n[24], in which the weighted KL-divergence was also found as a Lyapunov function. Lemma 5 proves this\npoint by showing that weighted network polymatrix zero sum games fall under the more general class of\nweighted monotone games. In such a manner, the results of [24], can be extend to all weighted monotone\n\ngames.\n\n#### **3.3 Learning Outperforms the Equilibrium**\n\n\nIn our second result, we consider the optimality of exploration as a means of reaching the equilibrium. We\nknow, for instance, that RD, which corresponds to QL with zero exploration rates _Tk_, will not converge\nin a merely monotone game. However, in this case, we know from [38] that the _time-average_ of the\ntrajectory does reach the equilibrium. In this case we are presented with the following question\n_Should we apply non-zero exploration rates Tk to ensure convergence to an equilibrium, or allow the_\n_trajectory to remain non convergent?_\nWe answer this question by considering the _payoff performance_ of the dynamic, where performance\nis measured by the Time Averaged Social Welfare (TSW)\n\n\n\n1\n_TSW_ = lim\n_t→∞_ _t_\n\n\n\n_t_\n\n_SW_ ( **x** ( _s_ )) _ds._ (11)\n\n�0\n\n\n\nwhere _SW_ ( **x** ) = [�] _k_ _[u][k]_ [(] **[x]** _[k][,]_ **[ x]** _[−][k]_ [)][. Intuitively, the Social Welfare] _[ SW]_ [(] **[x]** [)][ measures the sum of pay-]\n\noffs received by all agents at some state **x** _∈_ ∆. In (11), this quantity is averaged along trajectories of\nthe learning dynamic. From the following theorem we see that, in monotone games, the social welfare\nis higher if agents play according to FTRL (and therefore do not converge), than if they were to play\naccording to the equilibrium.\n\n\n**Theorem 4.** Consider a polymatrix game Γ = (( _N_ _, E_ ) _,_ ( _Skl, A_ _[kl]_ )( _k,l_ ) _∈E_ ). It is the case that\n\n\n_uk_ ( **x** _k,_ **x** _−k_ ) = � _⟨_ **x** _k, A_ _[kl]_ **x** _l⟩._\n\n( _k,l_ ) _∈E_\n\n\n\n\n3 LEARNING IN WEIGHTED MONOTONE GAMES 10\n\n\n\n1 _t_ �0 _t_ _[R]_ [(] _[s]_ [)] _[ ds]_ [ = 0][, where] _[ R]_ [(] _[t]_ [) =][ �]\n\n\n\nSupposing that the learning dynamic is such that lim _t→∞_ 1\n\n\n\n_k_ _[R][k]_ [(] _[t]_ [)][ and]\n\n\n\n_Rk_ ( _t_ ) = max\n**x** _[′]_ _k_ _[∈]_ [∆] _[k]_\n\n\n\n_t_\n\n[ _uk_ ( **x** _[′]_ _k_ _[, x][−][k]_ [(] _[s]_ [))] _[ −]_ _[u][k]_ [(] **[x]** _[k]_ [(] _[s]_ [)] _[,]_ **[ x]** _[−][k]_ [(] _[s]_ [))]] _[ ds,]_\n\n�0\n\n\n\nis the _regret_ of agent _k_ [42] and supposing also that the time averaged strategy _µ_ ( _t_ ) := [1] _t_ �0 _t_ **[x]** [(] _[s]_ [)] _[ ds]_ [ along]\n\nthe learning dynamics converges to the equilibrium **¯x**, then TSW is asymptotically greater than or equal\nto the Social Welfare of the equilibrium **¯x**, i.e. _TSW ≥_ _SW_ ( **¯x** )\n\n\n\nis the _regret_ of agent _k_ [42] and supposing also that the time averaged strategy _µ_ ( _t_ ) := [1] _t_ �0 _t_ **[x]** [(] _[s]_ [)] _[ ds]_ [ along]\n\nthe learning dynamics converges to the equilibrium **¯x**, then TSW is asymptotically greater than or equal\nto the Social Welfare of the equilibrium **¯x**, i.e. _TSW ≥_ _SW_ ( **¯x** )\n\n\n_Remark._ In particular this result holds for FTRL dynamics in monotone games, for which the time average approaches the equilibrium **¯x** asymptotically [38]. In addition, FTRL is a _no-regret_ dynamic, in that\nlim _t→∞_ 1 _t_ �0 _t_ _[R][k]_ [(] _[s]_ [)] _[ ds]_ [ = 0][ for all agents] _[ k]_ [.]\n\n\n\n_Remark._ In particular this result holds for FTRL dynamics in monotone games, for which the time average approaches the equilibrium **¯x** asymptotically [38]. In addition, FTRL is a _no-regret_ dynamic, in that\nlim _t→∞_ 1 � _t_ _[R][k]_ [(] _[s]_ [)] _[ ds]_ [ = 0][ for all agents] _[ k]_ [.]\n\n\n\n_Proof._ Define _µk_ ( _t_ ) _∈_ ∆ _k_ as the time averaged strategy of agent _k_, i.e.\n\n\n\n_µk_ ( _t_ ) = [1]\n\n_t_\n\n\n\n_t_\n\n**x** _k_ ( _s_ ) _ds._ (12)\n\n�0\n\n\n\nThen,\n\n\n\n_uk_ ( **x** _k_ ( _s_ ) _,_ **x** _−k_ ( _s_ )) _ds_\n0\n\n\n\n_Rk_ ( _t_ ) = max\n**x** _[′]_ _k_ _[∈]_ [∆] _[k]_\n\n\n\n�0 _t_\n\n\n\n_t_ _t_\n\n_uk_ ( **x** _[′]_ _k_ _[, x][−][k]_ [(] _[s]_ [))] _[ ds][ −]_\n0 �0\n\n\n\n_t_\n\n_≥_\n�0\n\n\n_t_\n\n=\n�0\n\n\n\n_t_ _t_\n\n_uk_ ( _µk_ ( _t_ ) _, x−k_ ( _s_ )) _ds −_\n0 �0\n\n\n\n_t_\n\n� _A_ _[kl]_ **x** _l_ ( _s_ ) _ds −_\n\n�0\n( _k,l_ ) _∈E_\n\n\n\n_uk_ ( **x** _k_ ( _s_ ) _,_ **x** _−k_ ( _s_ )) _ds_\n0\n\n\n\n� _A_ _[kl]_ **x** _l_ ( _s_ ) _ds_\n\n( _k,l_ ) _∈E_\n\n\n\n_µk_ ( _t_ ) _·_ �\n0\n\n\n\n**x** _k_ ( _s_ ) _·_ �\n0\n\n\n\nThen, dividing by _t_ and applying (12) to **x** _l_, the inequality reads\n\n\n\n� _A_ _[kl]_ **x** _l_ ( _s_ ) _ds_\n\n( _k,l_ ) _∈E_\n\n\n\n1\n\n\n\n� _A_ _[kl]_ _µl_ ( _t_ ) _−_ [1] _t_\n\n( _k,l_ ) _∈E_\n\n\n\n_t_ _[R][k]_ [(] _[t]_ [)] _[ ≥]_ _[µ][k]_ [(] _[t]_ [)] _[ ·]_ �\n\n\n\n_t_\n\n\n\n�0 _t_\n\n\n\n**x** _k_ ( _s_ ) _·_ �\n0\n\n\n\nTaking the sum over all agents _k_ and defining _R_ ( _t_ ) = [�] _k_ _[R][k]_ [(] _[t]_ [)][ we have]\n\n\n\n� _A_ _[kl]_ **x** _l_ ( _s_ ) _ds_ �\n\n( _k,l_ ) _∈E_\n\n\n\n1\n\n_t_ _[R]_ [(] _[t]_ [)] _[ ≥]_ �\n\n_k_\n\n\n\n� _A_ _[kl]_ _µl_ ( _t_ ) _−_ [1] _t_\n\n( _k,l_ ) _∈E_\n\n\n\n� _µk_ ( _t_ ) _·_ �\n\n\n\n_t_\n\n\n\n�0 _t_\n\n\n\n**x** _k_ ( _t_ ) _·_ �\n0\n\n\n\n= _SW_ ( _µ_ ( _t_ )) _−_ [1]\n\n_t_\n\n\n\n_t_\n\n_SW_ ( **x** ( _s_ )) _ds_\n\n�0\n\n\n\n= _⇒_ [1]\n\n_t_\n\n\n\n�0 _t_\n\n\n\n_SW_ ( **x** ( _s_ )) _ds ≥_ _SW_ ( _µ_ ( _t_ )) _−_ [1]\n0 _t_\n\n\n\n_t_ _[R]_ [(] _[t]_ [)]\n\n\n\n_t_\n\n_SW_ ( **x** ( _s_ )) _ds ≥_ lim\n_t→∞_ _[SW]_ [(] _[µ]_ [(] _[t]_ [))]\n\n�0\n\n\n\n�0 _t_\n\n\n\n1\n= _⇒_ lim\n_t→∞_ _t_\n\n\n\n1\n= _⇒_ lim\n_t→∞_ _t_\n\n\n\nwhere, in the final equality, we use the assumption that lim _t→∞_ 1 _t_ _[R]_ [(] _[t]_ [) = 0][. Applying now the assumption]\n\nthat _µ_ ( _t_ ) _→_ **¯x**, we have the final result that\n\n\n\n1\nlim\n_t→∞_ _t_\n\n\n\n_t_\n\n_SW_ ( **x** ( _s_ )) _ds ≥_ _SW_ ( **¯x** ) _._\n\n�0\n\n\n\n\n4 EXPERIMENTS 11\n\n\n_Remark._ We point out here that Theorem 4 defines payoff performance as a _time-average_ and as a sum\nover all agents. As such, it is possible that a particular agent is losing out on payoff consistently throughout learning. Similarly, it is possible that at some time _t_, all agents are performing worse than the equilibrium. Finally, the result concerns asymptotic behaviour, so it could be the case that the equilibrium\ninitially outperforms the learning dynamic. However, Theorem 4 shows that, eventually, the cumulative,\ntime-averaged performance (11) will be better than the equilibrium.\n\n\nSimilar results on the performance of non-fixed point learning dynamics have been found for Fictitious Play [44], Replicator Dynamics [45] and no-regret learning [46]. In the latter most of these, TSW\nwas also applied as a measure of payoff performance and it was found that the payoff in discrete time _on-_\n_line mirror descent_ outperforms the Nash Equilibrium. Theorem 4 adds to the body of literature studying\npayoff performance of learning by showing a sufficient condition (convergence in time-average) under\nwhich any no-regret learning algorithm (including Fictitious Play and Replicator) will outperform the\nequilibrium.\n\n#### **3.4 Discussion of Results**\n\n\nTheorem 1 presents an concrete approach for achieving convergence to a unique equilibrium in any\ngame. This allows us to abstract beyond the typical arena of study: namely weighted potential or zero\nsum network games. In fact, we showed convergence in both of these cases due to the fact that they\nsatisfy the monotonicity assumption and, therefore, satisfy the assumptions of Theorem 3.\nThough general in nature, these results are not without their limitations. They rely on the assumption\nof a discrete action set, so that agent strategies all evolve on ∆. This allows us to assume the existence of\nan Equilibrium, through the compactness of ∆. However, generalising to arbitrary continuous action sets\nwould widen the range of applications which our work encompasses. In addition, Theorem 3 is derived\nfor continuous time QL. This is a reasonable stance to take as it has been shown repeatedly that continuous\ntime approximations of algorithms provide a strong basis for analysing the algorithms themselves [47,\n17]. However, the accuracy of discrete time algorithms is always dependent on parameters, most notably\nstep sizes. Such an analysis of the discrete variants presents a fruitful avenue for further research.\nTo qualify Theorem 1, we point out that it does not give any indication regarding the performance\nof the system, merely its behaviour. Furthermore, its success relies on the increasing exploration rates\nof the agents and, therefore, at the cost of their exploitation. We showed in Theorem 4 that, under\ncertain conditions, agents following a non-convergent dynamic may actually outperform the equilibrium\nin terms of payoff. Our experiments monitor this effect further by showing that, in certain games, as _Tk_\nis increased the performance of the system decreases.\n\n### **4 Experiments**\n\n\nIn the following experiments, we consider the limitation discussed in the previous section. Namely, we\ntest to see whether the optimality of the learning algorithm may decrease as _Tk_ is increased. To do this\nwe consider two examples: the Mismatching Pennies and the Shapley games. The former, proposed in\n\n[45], is composed of a network of three agents, each equipped with two actions, Heads and Tails. The\npayoff given to each agent _k_ is given by\n\n\n_uk_ = _uk_ ( **x** _k,_ **x** _j_ ) = **x** _k_ **Ax** _k−_ 1\n\n\n0 1\n**A** = _, M ≥_ 1 _._\n_M_ 0\n� �\n\n\n\n\n4 EXPERIMENTS 12\n\n\n_Figure 1:_ _Trajectories of Q-Learning, red dots indicate initial conditions whilst black markers indicate final_\n_positions. TSW is averaged over all three initial conditions._\n\n\n_(a)_ _(b)_\n\n\n_Figure 2: Normalised TSW against T for 35 randomly generated games. TSW is normalised to lie between [-1, 1]_\n_in each game. Results are averaged over 10 initial conditions. The red line denotes the mean TSW across all 35_\n_games. a. Five Players, Five Actions. b. Seven Players, Five Actions_\n\n\nFor the latter [48], we examine a network variation on the original two player game towards a three player\nnetwork variant in which payoffs, parameterised by _β ∈_ (0 _,_ 1) are defined as\n\n_uk_ = _uk_ ( **x** _k,_ **x** _−k_ ) = **x** _k_ **Ax** _k−_ 1 + **x** _k_ **B** _[⊤]_ **x** _k_ +1\n\n\n\n![](output/images/525c1fc4aebcd6c5ed41b543e949b429a35b9a18.pdf-11-0.png)\n\n![](output/images/525c1fc4aebcd6c5ed41b543e949b429a35b9a18.pdf-11-1.png)\n\n![](output/images/525c1fc4aebcd6c5ed41b543e949b429a35b9a18.pdf-11-2.png)\n\n![](output/images/525c1fc4aebcd6c5ed41b543e949b429a35b9a18.pdf-11-3.png)\n\n _, B_ =\n\n\n\n\n\n _−_ 0 _β_ _−_ 1 _β_ 10\n\n 1 0 _−β_\n\n\n\n\n\n _,_\n\n\n\n_A_ =\n\n\n\n _β_ 1 10 _β_ 0\n\n 0 _β_ 1\n\n\n\n\n5 CONCLUSION 13\n\n\n_Remark._ In the case of a network polymatrix game, the influence bound of the game is\n\n\n\nmax˜ � _A_ _[k]_ [�]\n_k∈N_ _,i∈Sk,s−k,s−k∈S−k_ _[|]_\n\n\n\n_i,s−k_ _[−]_ � _A_ _[k]_ [�]\n\n\n\n˜\n_i,s−k_ _[|][.]_\n\n\n\nIn other words, it is the maximum difference between any row elements across the payoff matrices for\nall agents. In the Mismatching game, the influence bound is _M_ whilst the Shapley game has influence\nbound 1 + _β_ .\n\n\nThese games were analysed in [45] and [44] respectively, and it was shown that while the given\nlearning algorithm (RD in the former, Fictitious Play in the latter) did not converge to the NE, the agents\nactually received greater payoff through learning than they would have if they had only played the equilibrium strategy. The implication is that the agents were better off (in terms of payoff) by not converging\nto an equilibrium.\nIn Fig. 1 we plot the trajectories of QL for varying choices of _Tk_ in each of the games. For the sake\nof simplicity, we enforce that all agents have the same _Tk_ so we drop the _k_ notation. The trajectories\nare displayed on the space (0 _,_ 1) [3] with each axis corresponding to the probability with which each agent\nplays their first action. Above each figure is displayed the choice of _T_ for which Q-Learning is run, as\nwell as the Time-averaged Social Welfare (TSW) along the trajectory, given by (11).\nTwo points become immediately clear from Fig. 1. The first is that as _T_ is increased, the dynamics\nbreak no longer cycle around the equilibrium but rather converge to a unique equilibrium. While this\noccurs, however, TSW is decreasing. In fact, even in the case of equilibriation, trajectories which take\nlonger to reach the QRE gain a larger TSW. It is clear then, that it is in the agents’ benefit if the dynamics\nremain unstable, at least as far as payoff is concerned.\nIn Fig. 2, we move beyond these indicative examples by evaluating TSW on 35 randomly generated\ngames as _T_ is increased. In order to accurately compare games with differnet payoff functions, we divide\n� _k_ _[u][k]_ [(] **[x]** _[k]_ [(] _[t]_ [)] _[,]_ **[ x]** _[k]_ [(] _[t]_ [))][ by the maximum possible cumulative payoff that an agent could receive in the game.]\n\nThis ensures that TSW remains within [ _−_ 1 _,_ 1] in all games. It is clear once again that, in general, TSW\ndecreases as _T_ increases, i.e. as the game move towards more convergent behaviour. Of course, this does\nnot hold in every game. However, the red line, which denotes the mean TSW across all games, suggests\nthat this trend is the expected behaviour for a randomly selected game.\n\n### **5 Conclusion**\n\n\nOur community has made strong strides in showing that online learning in games does not always reach\nan equilibrium. At the same time, the rising use of multiple interacting agents in machine learning\napplications necessitates placing guarantees on learning. In this paper, we make a step towards resolving\nthis dichotomy by considering how the structure of a game, beyond the correlation between agent payoffs,\naffects online learning.\nSpecifically, we considered the asymptotic convergence to unique fixed points through Q-Learning\n(QL). Our analysis shows that the convergence in this popular learning dynamic can be guaranteed\nthrough sufficient exploration on the part of all agents. We also subsume convergence results in coordination (potential) games and competitive (network zero sum) games for which any positive rate of\nexploration is required. We then consider the impact of convergence through the lens of payoff performance and show that no-regret algorithms will outperform the equilibrium in terms of payoff, so long\nas the time-average trajectory reaches an equilibrium. In our experiments we show that this behaviour\nholds for a large number of games. An interesting point for future work would be to develop an analytical\nunderstanding for how often non-convergent learning dynamics outperform the equilibria of the game.\n\n\n\n\n5 CONCLUSION 14\n\n\nAs our study has shown, convergence of dynamics is inextricably linked to exploration. As such, by\nstudying the optimality of non-convergent dynamics, one may assess quantitatively the trade-off between\nexploration and exploitation.\n\n#### **Acknowledgements**\n\n\nAamal Hussain and Francesco Belardinelli are partly funded by the UKRI Centre for Doctoral Training in\nSafe and Trusted Artificial Intelligence (grant number EP/S023356/1). This research/project is supported\nin part by the National Research Foundation, Singapore and DSO National Laboratories under its AI\nSingapore Program (AISG Award No: AISG2-RP-2020-016), NRF 2018 Fellowship NRF-NRFF201807, NRF2019-NRF-ANR095 ALIAS grant, grant PIESGP-AI-2020-01, AME Programmatic Fund (Grant\nNo.A20H6b0151) from the Agency for Science, Technology and Research (A*STAR) and Provost’s\nChair Professorship grant RGEPPV2101.\n\n### **Appendix**\n\n#### **Results on Learning in Games**\n\n\nA fundamental point to be noted of FTRL is that its dynamics evolve in the payoff space. To be able\nto translate this into a dynamical system on ∆, we must consider the relation between _yk_ and the corresponding state vector _xk_ . We do this through the following Lemma.\n\n\n**Lemma 6** ([37] Lemma B.4) **. x** _k_ = _Qk_ ( **y** _k_ ) if and only if there exist _µk ∈_ R and _νki ∈_ R+ such that, for\nall _i ∈_ _Sk_ the following hold\n\n\nI _yki_ = _∂x_ _[∂][h]_ _ki_ _[k]_ [+] _[ µ][k][ −]_ _[ν][ki]_\n\n\nII _νkixki_ = 0\n\n\nIn particular, if _hk_ is steep, then _νki_ = 0 for all _i_ .\n\n\nThe proof of this Lemma is in [37] and so we omit it here. We make use of this lemma in proving\nTheorem 2 which on the relation of the Bregman Divergence to trajectories generated by FTRL.\nWe begin by considering the _Fenchel coupling_ generated by _hk_ defined by\n\n\n_Fk_ ( **x** _,_ **y** ) = _hk_ ( **x** ) + _h_ _[∗]_ _k_ [(] **[y]** [)] _[ −⟨]_ **[x]** _[,]_ **[ y]** _[⟩]_\n\n\nIn [38], it was shown that, for regularisers who are strongly convex, the Fenchel coupling is a Lyapunov\nfunction for FTRL in strictly monotone games, provided the regulariser also satisfies the _reciprocity_\ncondition: for any **x** and any sequence **y** _n_\n\n\n_Qk_ ( **y** _n_ ) _→_ **x** = _⇒_ _Fk_ ( **x** _,_ **y** _n_ ) _→_ 0\n\n\nThe converse of this statement is already satisfied by the strong convexity of _hk_ . We begin by showing\nthat regularisers which satisfy Assumption 1 satisfy the reciprocity condition.\n\n\n**Lemma 7.** Any regulariser _hk_ which satisfies 1 also satisfies that for any **x** _∈_ ∆ and any sequence\n( **y** _n_ ) _n∈_ N _∈_ R _[n]_,\n_Qk_ ( **y** _n_ ) _→_ **x** _⇐⇒_ _Fk_ ( **x** _,_ **y** _n_ ) _→_ 0 (13)\n\n\n\n\n5 CONCLUSION 15\n\n\n_Proof._ Define, for any _n ∈_ N, **x** _n_ = _Qk_ ( **y** _n_ ) := arg max **x** _∈_ ∆ _k {⟨_ **x** _,_ **y** _n⟩−_ _hk_ ( **x** ) _}_ . For the forward\ndirection:\n\n\n_Fk_ ( **x** _,_ **y** _n_ ) = _hk_ ( **x** ) + _h_ _[∗]_ _k_ [(] **[y]** _[n]_ [)] _[ −⟨]_ **[x]** _[,]_ **[ y]** _[n][⟩]_\n= _hk_ ( **x** ) + ( _⟨_ **x** _n,_ **y** _n⟩−_ _hk_ ( **x** _n_ )) _−⟨_ **x** _,_ **y** _n⟩_\n= _hk_ ( **x** ) _−_ _hk_ ( **x** _n_ ) + _⟨_ **x** _n,_ **y** _n⟩−⟨_ **x** _,_ **y** _n⟩_\n= _hk_ ( **x** ) _−_ _hk_ ( **x** _n_ ) + _⟨_ **y** _n,_ **x** _n −_ **x** _⟩_\n\n\nBy the convexity of _hk_\n_⟨∇hk_ ( **x** ) _,_ **x** _n −_ **x** _⟩≤_ _hk_ ( **x** _n_ ) _−_ _hk_ ( **x** )\n\n\nSo that\n\n\n_Fk_ ( **x** _,_ **y** _n_ ) _≤−⟨∇hk_ ( **x** ) _,_ **x** _n −_ **x** _⟩_ + _⟨_ **y** _n,_ **x** _n −_ **x** _⟩_\n= _⟨_ **y** _n −∇hk_ ( **x** ) _,_ **x** _n −_ **x** _⟩_\n\n\nWe apply Lemma 6 and Assumption 1 to say that, since **x** _n_ = _Qn_ ( **y** _n_ ), **y** _n_ = _∇hk_ ( **x** _n_ ) _−_ _µk_ ⊮ _[⊤]_, so we\nare left with\n\n\n_Fk_ ( **x** _,_ **y** _n_ ) _≤⟨∇hk_ ( **x** _n_ ) _−∇hk_ ( **x** ) _,_ **x** _n −_ **x** _⟩−_ _µk⟨_ ⊮ _,_ **x** _n −_ **x** _⟩_\n_≤||∇hk_ ( **x** _n_ ) _−∇hk_ ( **x** ) _|| · ||_ **x** _n −_ **x** _||_\n_≤_ _Lk||_ **x** _n −_ **x** _||_ [2]\n\n\nwhere the second inequality follows from the Cauchy-Schwartz Inequality and the final result follows\nfrom Assumption 1.1. Then, taking the limit as _n →∞_, _||_ **x** _n −_ **x** _||_ [2] _→_ 0 which, alongside the fact that\n_Fk_ ( **x** _,_ **y** ) _≥_ 0 for all **x** _,_ **y** gives the required result. This last fact follows directly from the Fenchel-Young\nInequality.\nFor the reverse direction, we apply Lemma 4.8 of [38] and Assumption 1.2.\n\n\nThe advantage of making Assumption 1.1 is that it allows for the Bregman Divergence to be related\nto the Fenchel Coupling. This is formalised through the following extension of [49] Lemma C.1 to an\narbitrary number of agents.\n\n\n**Lemma 8.** For any agent _k_ following FTRL with regulariser _hk_ who satisfies Assumption 1,\n\n\n_DB_ ( **x** _k||_ **x** _k_ ( _t_ ); _hk_ ) = _Fk_ ( **x** _k,_ **y** _k_ ( _t_ )) = _h_ ( **x** _k_ ) + _h_ _[∗]_ _k_ [(] **[y]** _[k]_ [(] _[t]_ [))] _[ −⟨]_ **[x]** _[k][,]_ **[ y]** _[k][⟩]_\n\n\nfor any **x** _k ∈_ ∆ _k_\n\n\n_Proof._ Here, we extend Lemma C.1 of [49] to the general _N_ -player game. Here, we recall again that\n**x** ( _t_ ) = _Qk_ ( **y** ( _t_ )) = arg max **x** _∈_ ∆ _k {⟨_ **x** _,_ **y** ( _t_ ) _⟩−_ _hk_ ( **x** ) _}_\n\n\n_DB_ ( **x** _||_ **x** ( _t_ ); _hk_ ) _−_ _Fk_ ( **x** _,_ **y** ( _t_ )) = _hk_ ( **x** ) _−_ _hk_ ( **x** ( _t_ )) _−⟨∇hk_ ( **x** ( _t_ )) _,_ **x** _−_ **x** ( _t_ ) _⟩−_ ( _hk_ ( **x** ) + _h_ _[∗]_ _k_ [(] **[y]** [(] _[t]_ [))] _[ −⟨]_ **[y]** [(] _[t]_ [)] _[,]_ **[ x]** _[⟩]_ [)]\n= _−hk_ ( **x** ( _t_ )) _−⟨∇hk_ ( **x** ( _t_ )) _,_ **x** _−_ **x** ( _t_ ) _⟩−⟨_ **y** ( _t_ ) _,_ **x** ( _t_ ) _⟩_ + _hk_ ( **x** ( _t_ )) + _⟨_ **y** ( _t_ ) _,_ **x** _⟩_\n= _⟨∇hk_ ( **x** ( _t_ )) _−_ **y** ( _t_ ) _,_ **x** _−_ **x** ( _t_ ) _⟩_\n\n= 0\n\n\nwhere the final result follows from the fact that **x** ( _t_ ) = _Qn_ ( **y** ( _t_ )) _⇐⇒_ **y** ( _t_ ) = _∇hk_ ( **x** ( _t_ )) _−_ _µk_ ⊮ _[⊤]_ .\n\n\n\n\n5 CONCLUSION 16\n\n\nWe use this to find, for each agent, the time derivative of the Weighted Bregman Divergence between\na trajectory **x** ( _t_ ) generated by FTRL and any other strategy **y** _∈_ ∆ in terms of the pseudo-gradient of the\ngame _F_ . We formalise this through the following Lemma.\n\n\n**Lemma 9.** In any game Γ with pseudo-gradient map _F_, let **x** ( _t_ ) denote the trajectory generated by FTRL\nwith regularisers ( _hk_ ) _k∈N_ and let **y** _∈_ ∆. Then, for any positive set of weights **w** = ( _wk_ ) _k∈N_ .\n\n\n_d_\n(14)\n_dt_ _[D][B]_ [(] **[y]** _[k][,]_ **[ x]** _[k]_ [(] _[t]_ [);] _[ h][k]_ [) =] _[ −]_ [(] _[⟨]_ **[x]** _[k][ −]_ **[y]** _[k][, F][k]_ [(] **[x]** [)] _[ −]_ _[F][k]_ [(] **[y]** [)] _[⟩]_ [+] _[ ⟨]_ **[x]** _[k][ −]_ **[y]** _[k][, F][k]_ [(] **[y]** [)] _[⟩]_ [)]\n\n\n_Proof._\n\n_dtd_ _[D][B]_ [(] **[¯x]** _[k][||]_ **[x]** _[k]_ [(] _[t]_ [);] _[ h][k]_ [) =] _dt_ _[d]_ [(] _[h][k]_ [(] **[¯x]** _[k]_ [) +] _[ h]_ _k_ _[∗]_ [(] **[y]** _[k]_ [(] _[t]_ [))] _[ −⟨]_ **[y]** _[k]_ [(] _[t]_ [)] _[,]_ **[ ¯x]** _[k]_ [(] _[t]_ [)] _[⟩]_ [)] (15)\n\n= _⟨∇h_ _[∗]_ _k_ [(] **[y]** [(] _[t]_ [))] _[,]_ [ ˙] **[y]** [(] _[t]_ [)] _[⟩−⟨]_ **[y]** [˙] _[k]_ [(] _[t]_ [)] _[,]_ **[ ¯x]** _[k][⟩]_ (16)\n= _⟨∇h_ _[∗]_ _k_ [(] **[y]** [(] _[t]_ [))] _[ −]_ **[¯x]** _[k][,]_ [ ˙] **[y]** _[k]_ [(] _[t]_ [)] _[⟩]_ (17)\n= _⟨_ **x** _k_ ( _t_ ) _−_ **¯x** _k, rk_ ( **x** ( _t_ )) _⟩_ (18)\n= _⟨rk_ ( **x** ( _t_ )) _−_ _rk_ ( **¯x** ) _,_ **x** _k_ ( _t_ ) _−_ **¯x** _k⟩_ + _⟨rk_ ( **¯x** ) _,_ **x** _k_ ( _t_ ) _−_ **¯x** _k⟩_\n= _−_ ( _⟨_ **x** _k_ ( _t_ ) _−_ **¯x** _k, Fk_ ( **x** ( _t_ )) _−_ _Fk_ ( **¯x** ) _⟩_ + _⟨_ **x** _k_ ( _t_ ) _−_ **¯x** _k, Fk_ ( **¯x** ) _⟩_ )\n\n\nwhere _h_ _[∗]_ _k_ [is the convex conjugate of] _[ h][k]_ [ and 18 holds due to [42] (2.13).]\n\n\n_Proof of Theorem 2._ Let _w_ 1 _, . . . wN >_ 0 be constants for which _F_ is weighted strictly monotone.\n\n\n\n_d_\n_dt_ _[W][B]_ [(] **[¯x]** _[||]_ **[x]** [(] _[t]_ [);] _[ h]_ [) =] �\n\n_k_\n\n\n\n_d_\n(19)\n_dt_ _[w][k][D][B]_ [(] **[¯x]** _[||]_ **[x]** [(] _[t]_ [);] _[ h]_ [)]\n\n\n\n= _−_ � ( _⟨_ **x** _k_ ( _t_ ) _−_ **¯x** _k, wkFk_ ( **x** ( _t_ )) _−_ _wkFk_ ( **¯x** ) _⟩_ + _⟨_ **x** _k_ ( _t_ ) _−_ **¯x** _k, wkFk_ ( **¯x** ) _⟩_ ) (20)\n\n\n_k_\n\n\n\n= _−_ ( _⟨_ **x** ( _t_ ) _−_ **¯x** _, F_ ( **x** ( _t_ ); **w** ) _−_ _F_ ( **¯x** ; **w** ) _⟩_ + _⟨_ **x** ( _t_ ) _−_ **¯x** _, F_ ( **¯x** ; **w** ) _⟩_ ) (21)\n\n_≤_ 0 (22)\n\n\nApplying the fact that **¯x** is a Equilibrium, Lemma 2 implies that _⟨_ **x** _−_ **¯x** _, F_ ( **¯x** ; **w** ) _⟩≥_ 0. Since,\nin addition, _F_ is weighted strictly monotone, the final inequality holds. Equality holds if and only if\n**x** ( _t_ ) = **¯x** . Thus, _WB_ ( **¯x** _||_ **x** ( _t_ ); _h_ ) is a strict Lyapunov function for FTRL, converging to **¯x** .\n\n\n_Proof of Theorem 1._ Theorem 1 of [30] implies that the given inequality (7) ensures the strong monotonicity of the perturbed game Γ _[H]_ . In such a case, the equilibrium **¯x** is unique and, from Corollary 1,\nconvergence of RD is guaranteed. Applying the relationship between RD and QL, the convergence of\nQL in Γ to the unique QRE **¯x** follows.\n\n#### **Potential Games and Network Zero Sum Games**\n\n\n**Lemma 10.** If the game Γ has a weighted monotone pseudo gradient _F_ then, for any _T >_ 0, the pseudo\ngradient of the perturbed game Γ _[H]_ is weighted strictly monotone.\n\n\n_Proof._ Let us define _F_ (resp. _F_ _[H]_ ) as the pseudo gradient of Γ (resp. Γ _[H]_ ). We recall that the transformation between rewards in Γ and Γ _[H]_ is given by\n\n\n_rki_ _[H]_ [(] **[x]** [) =] _[ r][ki]_ [(] **[x]** [)] _[ −]_ _[T][k]_ [(ln] _[ x][ki]_ [+ 1)]\n\n\n\n\n5 CONCLUSION 17\n\n\nAs such we have that, for any **x** _,_ **y** _∈_ ∆ and any weighted _w_ 1 _, . . ., wN >_ 0,\n\n\n_⟨_ **x** _−_ **y** _, F_ _[H]_ ( **x** ) _−_ _F_ _[H]_ ( **y** ; **w** ) _⟩_ = � _⟨_ **x** _k −_ **y** _k, −wkrk_ _[H]_ [(] **[x]** [)] _[ −]_ [(] _[−][w][k][r]_ _k_ _[H]_ [(] **[y]** [))] _[⟩]_ (23)\n\n\n_k_\n\n= � _⟨_ **x** _k −_ **y** _k, wk_ ( _−rk_ ( **x** ) + _Tk_ ln **x** _k_ ) _−_ _wk_ ( _−rk_ ( **y** ) + _Tk_ ln **y** _k_ ) _⟩_\n\n\n_k_\n\n(24)\n\n= _⟨_ **x** _−_ **y** _, F_ ( **x** ; **w** ) _−_ _F_ ( **y** ; **w** ) _⟩_ + � _wkTk⟨_ **x** _−_ **y** _,_ ln **x** _−_ ln **y** _⟩_ (25)\n\n\n_k_\n\n\nSince: ln is a strictly monotone operator, _wk >_ 0 and, by assumption, we have that _F_ is weighted\nmonotone, the result holds for any choice of _Tk >_ 0.\n\n\n_Proof of Theorem 3._ If Γ has a weighted monotone pseudo-gradient then, by Lemma 10, the perturbed\ngame Γ _[H]_ has a weighted strictly monotone pseudo-gradient. By Corollary 1, RD converges in Γ _[H]_ which,\nfrom Lemma 1 gives convergence of Q-Learning.\n\n\nThe proof of Lemma 4 relies on the following proposition.\n\n\n**Proposition 2.** Let _g_ : _X →Y_ be an operator with derivative _Dg_ : _X →Y_ . If _g_ is convex, then _Dg_ is\n\nmonotone.\n\n\n_Proof._ Suppose for the sake of contradiction that _Dg_ is not monotone. I.e. that, for some **x** _,_ **x** _[′]_ _∈X_\n\n\n_⟨_ **x** _−_ **x** _[′]_ _, Dg_ ( **x** ) _−_ _Dg_ ( **x** _[′]_ ) _⟩_ _<_ 0 _._ (26)\n\n\nFrom the convexity of _g_, we have that\n\n\n_g_ ( **x** ) _≥_ _g_ ( **x** _[′]_ ) + _⟨_ **x** _−_ **x** _[′]_ _, Dg_ ( **x** _[′]_ ) _⟩_ (27)\n_g_ ( **x** _[′]_ ) _≥_ _g_ ( **x** ) + _⟨_ **x** _[′]_ _−_ **x** _, Dg_ ( **x** ) _⟩_ (28)\n\n\nTaking the sum, we have that\n\n\n_g_ ( **x** ) + _g_ ( **x** _[′]_ ) _≥_ _g_ ( **x** ) + _g_ ( **x** _[′]_ ) _−⟨_ **x** _−_ **x** _[′]_ _, Dg_ ( **x** ) _−_ _Dg_ ( **x** _[′]_ ) _⟩_ (29)\n\n\nwhich is a contradiction.\n\n\n**Lemma 11.** Any weighted potential game Γ with concave potential ( **x** ) has a monotone pseudo-gradient\n_F_ ( **x** ).\n\n\n_Proof._ By the definition of the weighted potential game, there are positive constants _w_ 1 _, . . ., wN >_ 0\nsuch that\n_D_ **x** _kU_ ( **x** ) = _wkD_ **x** _kuk_ ( **x** ) = _−wkFk_ ( **x** ) _._ (30)\n\n\nSince _U_ ( **x** ) is concave and _wk >_ 0, _−D_ **x** _kuk_ ( **x** ) is monotone. Taking the sum over all agents _k_, we\nachieve the required result.\n\n\n_Proof of Lemma 4._ By Lemma 11 we have that, if the potential is concave (resp. strictly concave),\nthen _F_ ( **x** ) is monotone (resp. strictly monotone). In the concave case, we have from Lemma 10 that\nthe perturbed game has a strictly monotone _F_ ( **x** ). Then, from Theorem 3 we have convergence of QLearning.\n\n\n\n\nREFERENCES 18\n\n\n\n**Proposition 3.** For any weighted polymatrix zero sum game Γ,\n\n� _wk⟨_ **x** _[′]_ _k_ _[,]_ � _A_ _[kl]_ **x** _l⟩_ + � _⟨_ **x** _k,_\n\n\n\n_wk⟨_ **x** _[′]_ _k_ _[,]_ �\n\n_k_\n\n\n\n� _A_ _[kl]_ **x** _l⟩_ + �\n\n( _k,l∈E_ ) _k_\n\n\n\n_⟨_ **x** _k,_ �\n_k_ _∈E_\n\n\n\n� _A_ _[kl]_ **x** _[′]_ _l_ _[⟩]_ [= 0] (31)\n\n( _k,l∈E_ )\n\n\n\nfor any **x** _,_ **x** _[′]_ _∈_ ∆\n\n\n_Proof._ This proposition follows directly from [27] (Lemma 1) which considers general payoffs in network zero sum games and is also an adjustment of [24] (Lemma 4.3) which considers **x** _[′]_ to specifically\nbe the QRE **¯x** . For the sake of completeness, however, we reproduce the proof by [27] here\n\n\n\n� _ujl_ ( **x** _j,_ **x** _l_ )\n\n( _j,l_ ) _∈E−k_\n\n\n\n_−wkuk_ ( **y** _k,_ **x** _−k_ ) = � _wj_\n\n_j∈N−k_\n\n\n\n\n\n\n\n_ujk_ ( **x** _j,_ **y** _k_ ) + �\n ( _j,l_ ) _∈E_\n\n\n\n\n\n\n\n\n\n� _ujl_ ( **x** _j,_ **x** _l_ )\n\n( _j,l_ ) _∈E−k_\n\n\n\n_−_\n�\n\n\n\n_wkuk_ ( **y** _k,_ **x** _−k_ ) = �\n\n_k_ _k_\n\n\n\n� _wj_\n\n_j∈N−k_\n\n\n\n\n\n\n\n_ujk_ ( **x** _j,_ **y** _k_ ) + �\n ( _j,l_ ) _∈E_\n\n\n\n\n\n\n\n\n\n_k_\n\n\n\n=\n�\n\n\n_k_\n\n\n\n� _wjujk_ ( **x** _j,_ **y** _k_ ) + �\n\n_j∈N−k_ _k_\n\n\n\n�\n\n\n\n_k_\n\n\n\n� _wjujl_ ( **x** _j,_ **x** _l_ )\n\n( _j,l_ ) _∈E−k_\n\n\n\n�\n\n_j∈N−k_\n\n\n\n= � _wkuk_ ( **x** _k,_ **y** _−k_ ) (32)\n\n\n_k_\n\n\nfrom which the result follows.\n\n\n_Proof of Lemma 5._ For a weighted zero-sum polymatrix game, for any **x** _,_ **x** _[′]_ _∈_ ∆\n\n\n\n_−⟨_ **x** _−_ **x** _[′]_ _, F_ ( **x** ; **w** ) _−_ _F_ ( **x** _[′]_ ; **w** ) _⟩_ = �\n\n\n\n_wk⟨_ **x** _k −_ **x** _[′]_ _k_ _[,]_ �\n_k_ _∈E_\n\n\n\n_A_ _[kl]_ **x** _l −_\n\n� �\n\n( _k,l∈E_ ) ( _k,l∈E_\n\n\n\n_A_ _[kl]_ **x** _[′]_\n\n� _l_ _[⟩]_\n\n( _k,l∈E_ )\n\n\n\n� _A_ _[kl]_ **x** _l⟩_\n\n( _k,l∈E_ )\n\n\n_A_ _[kl]_ **x** _[′]_\n\n� _l_ _[⟩]_\n\n( _k,l∈E_ )\n\n\n\n=\n�\n\n\n\n_wk⟨_ **x** _[′]_ _k_ _[,]_ �\n_k_ ( _k,l∈E_\n\n_wk⟨_ **x** _[′]_ _k_ _[,]_ �\n_k_ _∈E_\n\n\n\n_wk⟨_ **x** _k,_ �\n_k_ ( _k,l∈E_\n\n_wk⟨_ **x** _k,_ �\n_k_ _∈E_\n\n\n\n+\n�\n\n\n\n_A_ _[kl]_ **x** _[′]_\n\n� _l_ _[⟩]_ [+] �\n\n( _k,l∈E_ ) _k_\n\n� _A_ _[kl]_ **x** _l⟩_ + �\n\n( _k,l∈E_ ) _k_\n\n\n\n= 0\n\n\nwhere the first two terms of the final equality are zero due to the weighted zero sum property and the\nfinal two are zero due to Prop. 3. Therefore, the game is weighted monotone. Convergence to a unique\nQRE holds due to Theorem 3.\n\n### **References**\n\n\n[1] P. Mertikopoulos, C. Papadimitriou, and G. Piliouras, “Cycles in adversarial regularized learning,”\n_Proceedings_, pp. 2703–2717, 2018.\n\n\n[2] A. Czechowski and G. Piliouras, “Poincar´e-Bendixson Limit Sets in Multi-Agent Learning;\nPoincar´e-Bendixson Limit Sets in Multi-Agent Learning,” in _International Conference on Au-_\n_tonomous Agents and Multiagent Systems_, 2022.\n\n\n\n\nREFERENCES 19\n\n\n[3] T. Galla, “Cycles of cooperation and defection in imperfect learning,” _Journal of Statistical Me-_\n_chanics: Theory and Experiment_, vol. 2011, 8 2011.\n\n\n[4] Y. Sato and J. P. Crutchfield, “Coupled replicator equations for the dynamics of learning in multiagent systems,” _Physical Review E_, vol. 67, p. 015206, 1 2003.\n\n\n[5] Y. Sato, E. Akiyama, and J. D. Farmer, “Chaos in learning a simple two-person game,” _Proceedings_\n_of the National Academy of Sciences of the United States of America_, vol. 99, pp. 4748–4751, 4\n2002.\n\n\n[6] T. Galla and J. D. Farmer, “Complex dynamics in learning complicated games,” _Proceedings of the_\n_National Academy of Sciences of the United States of America_, vol. 110, no. 4, pp. 1232–1236,\n2013.\n\n\n[7] J. B. T. Sanders, J. D. Farmer, and T. Galla, “The prevalence of chaotic dynamics in games with\nmany players,” _Scientific Reports_, vol. 8, no. 1, p. 4902, 2018.\n\n\n[8] G. P. Andrade, R. Frongillo, M. Belkin, and S. Kpotufe, “Learning in Matrix Games can be Arbitrarily Complex,” 7 2021.\n\n\n[9] T. Che, Y. Li, A. Paul Jacob, Y. Bengio, and W. Li, “Mode Regularized Generative Adversarial\nNetworks,” in _International Conference on Learning Representations_, 7 2017.\n\n\n[10] Q. Hoang, T. D. Nguyen, T. Le, and D. Phung, “MGAN: Training Generative Adversarial Nets with\nMultiple Generators,” in _International Conference on Learning Representations_, 2 2018.\n\n\n[11] H. Hamann, _Swarm Robotics: A Formal Approach_ . Springer International Publishing, 2018.\n\n\n[12] M. Pangallo, J. B. Sanders, T. Galla, and J. D. Farmer, “Towards a taxonomy of learning dynamics\nin 2 × 2 games,” _Games and Economic Behavior_, vol. 132, pp. 1–21, 3 2022.\n\n\n[13] A. I. Metrick and B. Polak, “Fictitious play in 2 • 2 games: a geometric proof of convergence*,”\n_Econ. Theory_, vol. 4, pp. 923–933, 1994.\n\n\n[14] A. Kianercy and A. Galstyan, “Dynamics of Boltzmann Q learning in two-player two-action\ngames,” _Physical Review E - Statistical, Nonlinear, and Soft Matter Physics_, vol. 85, p. 041145,\n4 2012.\n\n\n[15] Y. Shoham and K. Leyton-Brown, _Multiagent Systems: Algorithmic, Game-Theoretic, and Logical_\n_Foundations_ . Cambridge University Press, 2008.\n\n\n[16] R. Sutton and A. Barto, _Reinforcement Learning: An Introduction_ . MIT Press, 2018.\n\n\n[17] K. Tuyls, P. J. T Hoen, and B. Vanschoenwinkel, “An evolutionary dynamical analysis of multiagent learning in iterated games,” 1 2006.\n\n\n[18] J. Maynard Smith, “The theory of games and the evolution of animal conflicts,” _Journal of Theoret-_\n_ical Biology_, vol. 47, pp. 209–221, 9 1974.\n\n\n[19] J. Hofbauer and K. Sigmund, “Evolutionary Game Dynamics,” _BULLETIN (New Series) OF THE_\n_AMERICAN MATHEMATICAL SOCIETY_, vol. 40, no. 4, pp. 479–519, 2003.\n\n\n\n\nREFERENCES 20\n\n\n[20] J. Hofbauer and K. Sigmund, _Evolutionary Games and Population Dynamics_ . Cambridge University Press, 5 1998.\n\n\n[21] C. Harris, “On the Rate of Convergence of Continuous-Time Fictitious Play,” _Games and Economic_\n_Behavior_, vol. 22, pp. 238–259, 2 1998.\n\n\n[22] S. Leonardos and G. Piliouras, “Exploration-exploitation in multi-agent learning: Catastrophe theory meets game theory,” _Artificial Intelligence_, vol. 304, p. 103653, 2022.\n\n\n[23] C. Ewerhart and K. Valkanova, “Fictitious play in networks,” _Games and Economic Behavior_,\nvol. 123, pp. 182–206, 9 2020.\n\n\n[24] S. Leonardos, G. Piliouras, and K. Spendlove, “Exploration-Exploitation in Multi-Agent Competition: Convergence with Bounded Rationality,” _Advances in Neural Information Processing Systems_,\nvol. 34, pp. 26318–26331, 12 2021.\n\n\n[25] M. Pangallo, T. Heinrich, and J. D. Farmer, “Best reply structure and equilibrium convergence in\ngeneric games,” _Science Advances_, vol. 5, 2 2019.\n\n\n[26] F. Parise and A. Ozdaglar, “A variational inequality framework for network games: Existence,\nuniqueness, convergence and sensitivity analysis,” _Games and Economic Behavior_, vol. 114,\npp. 47–82, 3 2019.\n\n\n[27] A. Kadan and H. Fu, “Exponential Convergence of Gradient Methods in Concave Network ZeroSum Games,” _Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial_\n_Intelligence and Lecture Notes in Bioinformatics)_, vol. 12458 LNAI, pp. 19–34, 2021.\n\n\n[28] T. Tatarenko and M. Kamgarpour, “Learning Nash Equilibria in Monotone Games,” _Proceedings of_\n_the IEEE Conference on Decision and Control_, vol. 2019-December, pp. 3104–3109, 12 2019.\n\n\n[29] E. Melo, “A Variational Approach to Network Games,” _SSRN Electronic Journal_, 11 2018.\n\n\n[30] E. Melo, “On the Uniqueness of Quantal Response Equilibria and Its Application to Network\nGames,” _SSRN Electronic Journal_, 6 2021.\n\n\n[31] F. Facchinei and J. S. Pang, “Finite-Dimensional Variational Inequalities and Complementarity\nProblems,” _Finite-Dimensional Variational Inequalities and Complementarity Problems_, 2004.\n\n\n[32] P. Mertikopoulos and Z. Zhou, “Learning in games with continuous action sets and unknown payoff\nfunctions,” _Mathematical Programming_, vol. 173, pp. 465–507, 2019.\n\n\n[33] T. Tatarenko and M. Kamgarpour, “Bandit Learning in Convex Non-Strictly Monotone Games,”\n_arXiv e-prints_, p. arXiv:2009.04258, 9 2020.\n\n\n[34] P. Coucheney, B. Gaujal, and P. Mertikopoulos, “Penalty-Regulated Dynamics and Robust Learning\nProcedures in Games,” _https://doi.org/10.1287/moor.2014.0687_, vol. 40, pp. 611–633, 11 2014.\n\n\n[35] Z. Zhou, P. Mertikopoulos, A. L. Moustakas, N. Bambos, and P. Glynn, “Robust Power Management via Learning and Game Design,” _Operations Research_, vol. 69, no. 1, pp. 331–345, 2021.\n\n\n[36] A. H´eliou, P. Mertikopoulos, and Z. Zhou, “Gradient-Free Online Learning in Games with Delayed\nRewards,” in _Proceedings of the 37th International Conference on Machine Learning_, ICML’20,\nJMLR.org, 2020.\n\n\n\n\nREFERENCES 21\n\n\n[37] E.-V. Vlatakis-Gkaragkounis, L. Flokas, T. Lianeas, P. Mertikopoulos, and G. Piliouras, “No-Regret\nLearning and Mixed Nash Equilibria: They Do Not Mix,” _Advances in Neural Information Process-_\n_ing Systems_, vol. 33, pp. 1380–1391, 2020.\n\n\n[38] S. Hadikhanloo, R. Laraki, P. Mertikopoulos, and S. Sorin, “Learning in nonatomic games part I\nFinite action spaces and population games,” _Journal of Dynamics and Games. 2022_, vol. 0, no. 0,\np. 0, 2022.\n\n\n[39] P. Mertikopoulos and W. H. Sandholm, “Learning in Games via Reinforcement and Regularization,”\n_https://doi.org/10.1287/moor.2016.0778_, vol. 41, pp. 1297–1324, 8 2016.\n\n\n[40] C. F. Camerer, T. H. Ho, and J. K. Chong, “Behavioural game theory: Thinking, learning and teaching,” _Advances in Understanding Strategic Behaviour: Game Theory, Experiments and Bounded_\n_Rationality_, pp. 120–180, 1 2004.\n\n\n[41] C. Camerer and T. H. Ho, “Experience-weighted attraction learning in normal form games,” _Econo-_\n_metrica_, vol. 67, pp. 827–874, 7 1999.\n\n\n[42] S. Shalev-Shwartz, “Online Learning and Online Convex Optimization,” _Foundations and Trends_\n_in Machine Learning_, vol. 4, no. 2, 2011.\n\n\n[43] S. Sorin and C. Wan, “Finite composite games: Equilibria and dynamics,” _Journal of Dynamics and_\n_Games_, vol. 3, no. 1, pp. 101–120, 2016.\n\n\n[44] G. Ostrovski and S. van Strien, “Payoff performance of fictitious play,” _Journal of Dynamics and_\n_Games_, vol. 1, pp. 621–638, 8 2014.\n\n\n[45] R. Kleinberg, K. Ligett, G. Piliouras, and E. Tardos, “Beyond the Nash Equilibrium Barrier,” _Inno-_\n_vations in Computer Science_, 2011.\n\n\n[46] I. Anagnostides, I. Panageas, G. Farina, and T. Sandholm, “On Last-Iterate Convergence Beyond\nZero-Sum Games,” in _Proceedings of the 39th International Conference on Machine Learning_\n(K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of _Pro-_\n_ceedings of Machine Learning Research_, pp. 536–581, PMLR, 8 2022.\n\n\n[47] J. Hofbauer and S. Sorin, “Best response dynamics for continuous zero–sum games,” _Discrete and_\n_Continuous Dynamical Systems - B. 2006, Volume 6, Pages 215-224_, vol. 6, p. 215, 10 2005.\n\n\n[48] L. S. Shapley, “Some Topics in Two-Person Games,” in _Advances in Game Theory. (AM-52)_, pp. 1–\n28, Princeton University Press, 5 2016.\n\n\n[49] K. Abe, M. Sakamoto, and A. Iwasaki, “Mutation-Driven Follow the Regularized Leader for LastIterate Convergence in Zero-Sum Games,” in _Conference on Uncertainty in Artificial Intelligence_,\n2022.\n\n\n",
    "ranking": {
      "relevance_score": 0.726854124158478,
      "citation_score": 0.5637945149406467,
      "recency_score": 0.6085161916840235,
      "final_score": 0.6824084090674662
    },
    "citation_key": "Hussain2023AsymptoticCA",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "76f8e64e2db3f788467e65a19056b1add9a70768",
    "title": "Curriculum Learning for Robot Manipulation Tasks With Sparse Reward Through Environment Shifts",
    "published": "2024",
    "authors": [
      "Erdi Sayar",
      "Giovanni Iacca",
      "Alois C. Knoll"
    ],
    "summary": "Multi-goal reinforcement learning (RL) with sparse rewards poses a significant challenge for RL methods. Hindsight experience replay (HER) addresses this challenge by learning from failures and replacing the desired goals with achieved states. However, HER often becomes inefficient when the desired goals are far away from the initial states. This paper introduces co-adapting hindsight experience replay with environment shifts (in short, COHER). COHER generates progressively more complex tasks as soon as the agent’s success surpasses a predefined threshold. The generated tasks and agent are coupled to optimize the behavior of the agent within each task-agent pair. We evaluate COHER on various sparse reward robotic tasks that require obstacle avoidance capabilities and compare COHER with hindsight goal generation (HGG), curriculum-guided hindsight experience replay (CHER), and vanilla HER. The results show that COHER consistently outperforms the other methods and that the obtained policies can avoid obstacles without having explicit information about their position. Lastly, we deploy such policies to a real Franka robot for Sim2Real analysis. We observe that the robot can achieve the task by avoiding obstacles, whereas policies obtained with other methods cannot. The videos and code are publicly available at: https://erdiphd.github.io/COHER/.",
    "pdf_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10480429.pdf",
    "doi": "10.1109/ACCESS.2024.3382264",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "IEEE Access",
    "citation_count": 3,
    "bibtex": "@Article{Sayar2024CurriculumLF,\n author = {Erdi Sayar and Giovanni Iacca and Alois C. Knoll},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {46626-46635},\n title = {Curriculum Learning for Robot Manipulation Tasks With Sparse Reward Through Environment Shifts},\n volume = {12},\n year = {2024}\n}\n",
    "markdown_text": "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\n\n\n_Digital Object Identifier_\n\n# Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\nERDI SAYAR [1], (Student Member, IEEE), GIOVANNI IACCA [2], (Senior Member, IEEE), and ALOIS\nKNOLL [1], (Fellow, IEEE)\n1Technical University of Munich, Munich, 85748 Germany (e-mail: erdi.sayar@tum.de, knoll@in.tum.de)\n2University of Trento, Trento, 38123 Italy (e-mail: giovanni.iacca@unitn.it)\n\n\nCorresponding author: Erdi Sayar (e-mail: erdi.sayar@tum.de).\n\n\n**ABSTRACT** Multi-goal reinforcement learning (RL) with sparse rewards poses a significant challenge for\nRL methods. Hindsight experience replay (HER) addresses this challenge by learning from failures and\nreplacing the desired goals with achieved states. However, HER often becomes inefficient when the desired\ngoals are far away from the initial states. This paper introduces co-adapting hindsight experience replay with\nenvironment shifts (in short, COHER). COHER generates progressively more complex tasks as soon as the\nagent’s success surpasses a predefined threshold. The generated tasks and agent are coupled to optimize the\nbehavior of the agent within each task-agent pair. We evaluate COHER on various sparse reward robotic tasks\nthat require obstacle avoidance capabilities and compare COHER with hindsight goal generation (HGG),\ncurriculum-guided hindsight experience replay (CHER), and vanilla HER. The results show that COHER\nconsistently outperforms the other methods and that the obtained policies can avoid obstacles without having\nexplicit information about their position. Lastly, we deploy such policies to a real Franka robot for Sim2Real\nanalysis. We observe that the robot can achieve the task by avoiding obstacles, whereas policies obtained\n[with other methods cannot. The videos and code are publicly available at: https://erdiphd.github.io/COHER/.](https://erdiphd.github.io/COHER/)\n\n\n**INDEX TERMS** Curriculum learning-based reinforcement learning, hindsight experience replay, multi-goal\nreinforcement learning, robotic control.\n\n\n\n**I. INTRODUCTION**\n\n\nReinforcement Learning (RL) has shown outstanding\nachievements in solving complex tasks, such as games [1],\n\n[2] and robotics [3], [4]. Multi-goal RL aims to learn a goalconditioned policy that generalizes across different goals.\nLearning a goal-conditioned policy for multiple goals requires a significantly larger amount of data than single-task\nlearning, as the agent needs to collect data from different\ngoals. Off-policy RL algorithms are used to reduce the\namount of data needed for learning [5]. However, most of the\noff-policy RL algorithms owe their success to well-designed\nreward functions [6]. However, designing a proper reward\nfunction for situations in which the admissible behavior is\n\nunknown is not easy. Moreover, designing a reward function\nusually requires expert knowledge in RL and a priori information about the task. Thus, binary rewards [7], indicating\nwhether or not the task is accomplished, can be leveraged\nto overcome the reward design issue. However, most of the\nexisting RL algorithms suffer under binary reward settings,\nbecause of the sparse reward signal due to insufficiency of\n\n\n\nsuccessful experiences. Hindsight experience replay (HER)\n\n[8] addresses the sparse reward issue by replacing the desired\ngoals with the achieved states sampled from failed episodes.\nThe main drawback of this method is that, if the desired goals\nare far away from the achieved states, HER cannot solve these\ntasks effectively, as no reward signal is provided. To overcome\nthis issue, curriculum learning-based RL algorithms [9]–\n\n[12] have been proposed, that start from a simple task and\ngradually increase its difficulty. However, many of these\nmethods rely on some sort of heuristic in order to decompose\nthe complex task into a simpler one. These heuristics might\nnot be optimal with respect to the environment. For instance,\nin a robot manipulation task, if the desired goal is far from\nthe initial position of the end-effector of the robot, we might\ndivide the distance between the initial position and the desired\ngoal into smaller pieces and then guide the robot gradually\nto the desired goal by replacing it with an intermediate goal\nthat gradually approaches the desired goal. In the end, the\nagent learns how to achieve the task. However, what if there\nare obstacles in the environment? In this case, we need to\n\n\n\nVOLUME 11, 2023 1\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n\nknow the position of the obstacles in advance, in order to\ndesign optimal heuristics. This knowledge may not always be\navailable though. Therefore, in this work, we try to address\nthe following question: _Is it possible to build an RL algorithm_\n_that adapts to changing environments without specific prior_\n_knowledge about the task?_\nPrevious works have address this question, e.g., by introducing the so-called minimal criterion co-evolution (MCC)\n\n[13] and, based on it, the Paired Open-Ended Trailblazer\n(POET) algorithm [14]. The concept of MCC was developed\nto demonstrate that a very simple minimum criterion (MC)\ncan lead to an open-ended evolution of two co-evolving\npopulations: a population of agents, and a population of\nenvironments with different levels of complexity. MCC was\ndemonstrated for the very first time in a maze navigation\nproblem [13]. Mazes are, in fact, a paradigmatic example\nof tasks with sparse, delayed reward [15], for which various\napproaches based on quality search [16], [17] or novelty [18]\nhave been proposed. According to the setting proposed in\n\n[13], tasks (mazes) are co-evolved with agents (maze navigators) controlled by neural networks. As a result, mazes\nget more complicated while neural networks become more\nefficient at navigating those mazes, to satisfy the MC. In the\noriginal MCC method, agents are optimized via the NeuroEvolution of Augmenting Topologies (NEAT) algorithm\n\n[19], which is an evolutionary algorithm that evolves both the\nstructure and the weights of the neural network. Moreover,\nenvironments in MCC should be solved by the current population of agents, otherwise they are discarded. Unlike MCC, the\nPOET algorithm optimizes the current agents for a dedicated\namount of time, to then create slightly harder environments\nonce the current environments are solved by the agents in the\ncurrent generation. The optimization algorithm used for the\nagents in POET is based on Evolution Strategies (ES) [20],\n\n[21], a black-box optimization method that has been shown to\nachieve promising results in several RL benchmark problems\n\n[22], [23].\nAn alternative to these approaches is represented by curriculum learning-based RL algorithms [9]–[12], [24]–[26],\nwhich generate intermediate goals to help break down longterm desired goals into more manageable subgoals, serving\nas stepping stones towards achieving the desired goal in a\nconstant environment. However, these methods either lack\na mechanism to consider obstacles, require prior knowledge\nabout environments and obstacles or are limited in their\n\nability to perform different manipulation tasks. For example,\nMHER [25] adds a dynamic model to the original HER\nalgorithm, i.e., it learns environmental dynamics using onestep ahead models and generates virtual achieved goals from\nmodel-based interactions rather than past collected states as\nin the HER. However, this approach cannot learn efficiently\nin complex robot manipulation tasks, such as those involving\ninteractions with objects and collisions. MEGA [26] enhances\nexploration by maximizing the entropy of the achieved goal\ndistribution, focusing on underexplored regions. This strategy effectively steers exploration towards the frontier of the\n\n\n\nachievable goal set, effectively forming a curriculum that\nnarrows the gap between the initial state and the desired\ngoals. However, a limitation of this approach is the absence\nof obstacle handling, necessitating the removal of goals with\nlow Q-values to ensure the proper functioning of the heuristic\nfunction. HGG [9] generates curriculum goals by selecting\nthem from the visited state set, based on an objective that\njointly minimizes the Wasserstein distance and maximizes\nthe value function. Similarly, CHER [10] selects curriculum\ngoals based on the _curiosity_ and _proximity_ criteria. Specifically, curiosity encourages the choice of curriculum goals\nat diverse ranges, while proximity prefers curriculum goals\nthat are closer to the desired goal. However, HGG and CHER\nuse the Euclidean distance to approximate the measure of\nthe Wasserstein distance and design the proximity metric,\nrespectively. Hence, they are not applicable in environments\nwith obstacles. The studies [11], [24] introduce a graph-based\ndistance metric extension to HGG and CHER, to circumvent\nthe obstacle during the curriculum goal generation. However,\nthese methods require the position and dimension of the\nobstacles in order to create a graph-based distance metric.\nFurthermore, they require that obstacles have a convex shape.\nBbox-HGG [12], instead, addresses manipulation tasks involving dynamic obstacles by utilizing image observations.\nObjects from the environment are identified using BboxEncoder, which is trained to recognize the bounding boxes of\nobjects prior to initiating RL training. However, this approach\nis limited to robot manipulation tasks that do not necessitate\ngripper control such as slide and push tasks.\nFollowing up on these works, in this paper we propose a\nnovel framework for curriculum generation through environment shifts in the context of sparse rewards and multi-goal RL\nin environments characterized by the presence of obstacles.\nWe call our proposed method ‘‘co-adapting hindsight experience replay with environment shift’’ (in short, COHER).\nDifferently from POET, which uses ES to optimize the agents,\nin COHER we use the deep deterministic policy gradient\n(DDPG) algorithm [4], because RL algorithms have been\nshown to perform better than ES in dynamic environments\n\n[23]. Another difference with POET is that, while in POET\nenvironments are automatically evolved, in COHER we predefine a population of environments and allow the algorithm\nto shift from one environment to the next one whenever the\n\ntest success rate on the current environment reaches a predefined threshold. While this approach is not open-ended as in\nPOET, it allows us to achieve effective curriculum learning\nwith a limited number of environment shifts, hence resulting\nin a computationally feasible computation. This is especially\nimportant in computationally expensive tasks, such as the 3D\nphysics-based simulations of a robot interacting with an environment with obstacles that we address in this work, where it\n\nis not feasible to test hundreds or thousands of environments\n\nas in the simpler 2D simulations of a bipedal walker [27]\nconsidered in [14]. Finally, in contrast to HGG and CHER,\nin COHER we generate the curriculum by breaking down\nthe most challenging environment into gradually increasing\n\n\n\n2 VOLUME 11, 2023\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n\nlevels of difficulty, starting from the easiest and progressing\nto the hardest one, while remaining entirely agnostic to the\nobstacle properties such as shape, size, and position.\nWe test our proposed approach in a multi-goal RL task with\nsparse rewards, considering a 7-DOF fetch robotic both in\nthe MuJoCo simulation environment [28] and in a real-world\nsetting. To summarize, the main contributions of this paper\nare the following:\n\n_•_ We generate a curriculum through a novel DDPG-based coadapting approach that adapts agent-environment pairs to\nprogressively more challenging environments, specifically\non robot manipulation tasks, without explicitly providing\nthe algorithm neither the obstacles’ positions nor their\nsizes.\n\n\n_•_ We perform Sim2Real transfer by deploying the trained\npolicy on a Franka robot in a real-world setting and demonstrate the ability of the policy to successfully avoid obstacles in increasingly more challenging environments.\nThe rest of the paper is structured as follows. In the next\nSection, briefly review the related works. In Section III, we\nintroduce the background concepts on multi-goal RL and\nDDPG. Then, we describe the proposed method in Section IV.\nThe experimental results are presented in Section V, followed\nby the conclusions provided in Section VI.\n\n\n**II. RELATED WORK**\n\n**Curriculum learning in multi-goal RL:** Universal Value\nFunction Approximator (UVFA) parametrizes the goal using\na function approximator [29], which is then used to allow\nthe agent to learn multiple goals and generalize to unseen\ngoals in a single policy. As discussed earlier, HER [8] instead\nreplaces the desired goals with the achieved states sampled\nfrom failed episodes. However, as we mentioned, although\nHER can handle the sparse reward problem in multi-goal\nRL settings, it fails at solving tasks in which the desired\ngoals are distant from the initial states. The reason is that\nthe achieved goals are sampled from failed episodes which\nare mostly distributed around the initial state. Curriculum\nlearning-based RL algorithms resolve this issue by starting\nfrom simple tasks and gradually increasing their difficulty.\nHER can also be considered a form of implicit curriculum\nlearning because the achieved goals are easier to achieve than\nthe desired goals. The major drawback of HER is that the\nachieved goals are sampled uniformly from the replay buffer.\nHowever, these samples are substantially different from each\nother. Therefore, Fang et al. [10] proposed curriculum-guided\nHER (CHER) to select the achieved goals based on proximity\nand diversity. Hindsight goal generation (HGG) [9] generates\nintermediate goals that maximize a given value function and\nminimize the Wasserstein distance between the target goal\nand the achieved goal distribution. It should be noted that, because both CHER’s proximity metric and HGG’s Wasserstein\ndistance are based on Euclidean distance, these algorithms\nmay yield an infeasible path for the robot, which may be\nblocked by obstacles. As known, in fact, metrics based on\nEuclidean distance measure only the distance over a straight\n\n\n\nIn our proposed method, we train the agents by using\nDDPG [4]. DDPG is an off-policy actor-critic algorithm that\nconsists of a deterministic policy _πθ_ ( _s, g_ ) : _S × G_ _→_\n_A_, parameterized by _θ_, and a state-action value function\n_Qη_ ( _s, a, g_ ) : _S × A × G →_ R, parameterized by _η_ . Gaussian\nnoise with zero mean ( _µ_ = 0) and constant std. dev. ( _σ_ = 0.2)\nis added to the deterministic policy _πθ_ to improve exploration.\nThe behavior policy, _πb_, is then used for collecting the results\non the episodes:\n\n\n_πb_ ( _s, g_ ) = _πθ_ ( _s, g_ ) + _N_ ( _µ, σ_ [2] ) _._ (3)\n\n\n\nline between any two points, regardless of the presence of\nobstacles in the environment.\n\nTo overcome this issue, Bing et al. [11] came up with\nthe idea of using a graph-based distance metric instead of\nan Euclidean distance metric as an extension of HGG. This\n\nalgorithm, however, assumes that the position and size of the\nobstacle are known in advance in order to create a graph.\n**Evolutionary Strategies:** Evolution strategies (ES) [30]\nis a family of black-box optimization techniques inspired by\nnatural evolution. In [14], authors used Natural Evolution\nStrategies (NES) [21], a class of ES that iteratively update a\nsearch distribution by calculating an estimated gradient with\nrespect to the distribution of the search parameters. Salimans\net al. [22] found that NES has appealing features, such as\nbeing invariant to the action frequencies and being capable of\ndealing with delayed rewards. Moreover, the NES algorithm\nis highly parallelizable and as such it can be used as an\neffective alternative to traditional RL methods. Zhang et al.\n\n[23] compared ES with deep RL in continuous control tasks\nand showed that ES can compete with deep RL algorithms,\napart from the cases where environments are dynamic.\n\n\n**III. BACKGROUND**\n\nMulti-goal RL can be represented as a goal-oriented Markov\nDecision Process (MDP) _⟨S, A, G, T, R, p, γ⟩_, where: _S_ is a\ncontinuous state space; _A_ is a continuous action space; _G_ is a\nset of goals; _T_ : _S ×A×S →_ [0 _,_ 1] is the unknown transition\nprobability function from state _s_ to state _s_ _[′]_ when taking action\n_a_, _R_ : _S ×A×G →_ R is a reward function; _p_ ( _s_ 0 _, g_ ) is a joint\nprobability distribution over the initial state _s_ 0 and the desired\ngoal _g_ ; and _γ ∈_ [0 _,_ 1] is a discount factor.\nA commonly used sparse reward function in multi-goal RL\ncan be defined as:\n\n\n\n_r_ ( _s, a, g_ ) =\n\n\n\n0 if _∥ϕ_ ( _s_ ) _−_ _g∥_ 2 [2] _[≤]_ _[ϵ][R][,]_\n(1)\n_−_ 1 otherwise\n�\n\n\n\nwhere _ϵR_ is a fixed threshold value and _ϕ_ : _S →G_ is a\nmapping function from states to achieved goals. The objective\nof multi-goal RL is to learn a policy _π_ _[∗]_ : _S × G →A_\nthat maximizes the expected return. This problem can be\nformalized as follows:\n\n_π_ _[∗]_ = arg max _J_ ( _π_ ) where:\n\n_π_\n\n\n\n_J_ ( _π_ ) = _Es_ 0= _s,at_ _∼π_ ( _·|st_ _,g_ ) _,st_ +1 _∼P_ ( _·|st_ _,at_ )\n\n\n\n_∞_ (2)\n� _γ_ _[t]_ _r_ ( _st_ _, at_ _, g_ ) _._\n\n\n_t_ =0\n\n\n\nVOLUME 11, 2023 3\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n\nThe Q-value function approximator is trained by minimizing\nthe Temporal Difference (TD) error defined as a loss function\nbelow:\n\n\n_Lcritic_ = E( _s,a,r,s′,g_ ) _∼B_ �( _y −_ _Qη_ ( _s, a, g_ )) [2][�] (4)\n\n\nwhere _B_ is the replay buffer and\n\n\n_y_ = _r_ + _γQη_ ( _s_ _[′]_ _, πθ_ ( _s_ _[′]_ _, g_ ) _, g_ ) _._ (5)\n\n\nSubsequently, the policy _π_ is updated using policy gradient\non the following loss function.\n\n\n_Lactor_ = _−_ E( _s,g_ ) _∼B_ [( _Q_ ( _s, πθ_ ( _s, g_ ) _, g_ ))] (6)\n\n\n**IV. PROPOSED METHOD**\n\nIn the following, we assume that we are working on a robot\nmanipulation task in an environment with obstacles. Therefore, in the description of the proposed method we will refer to this specific task. Nevertheless, the method could be\nin principle extended to other kinds of tasks, provided that\nthe environments can be characterized by different levels of\ndifficulty.\nOur proposed method works as follows. We execute a\ncurriculum learning process in which we pre-define a population of environments _X_ (each one characterized by a different number of obstacles, in different positions and with\ndifferent sizes) and an agent _Y_ (i.e., a neural network). The\nenvironments can be generated either by the algorithm itself,\nor manually (as we do in the present study), and added to\nthe environment population in order of increasing difficulty.\nIn the population, the first environment _X_ 0 is always the\nmost simple one, i.e., the one without obstacles. In order to\ndecide when to generate the next environment, we pair the\nfirst environment _X_ 0 from the population _X_ with the agent\n_Y_ and optimize the agent’s behavior in that environment\nuntil it reaches a predefined success rate. After satisfying the\nsuccess rate, the new environment _X_ 1, slightly harder than the\nprevious one _X_ 0 is generated (e.g., by adding obstacles and/or\nchanging their positions or size). In principle, this process\ncould be continued in an open-ended manner, i.e., without\nspecific bounds. As a result, we could continuously create\never more challenging environments, each one originating\nfrom the previous one, and the training could continue indefinitely. However, for practical experimental reasons, we\nset an upper bound ( _E_ ) to limit the maximum number of\nenvironments.\n\nWith this approach, the agent seeks to solve the newly\ngenerated environments by utilizing its existing skills, which\nare acquired from the previous environments. In this way,\nthe agent transfers and adapts its existing behavior to the\nnew environment. Moreover, we ensure that the agents attain\nthe predefined success rate in the current environment before\nsolving the next one [31].\nAlgorithm 1 describes our method in the form of pseudocode. As shown in the pseudo-code, we start with a very simple environment and train it using the HER framework. When\n\n\n\nthe performance becomes greater than or equal to the predefined success rate _δ_, the next (more challenging) environment\nis created and the agent tries to solve the new environment\nwith its current skills. Success is defined as reaching a target\nposition within a distance set by a threshold _ϵR_, as shown\nin equation (1). After each episode, we run a predefined\nnumber of test rollouts ( _ntest−rollouts_ ) with the current policy\nand calculate the success rate _δ_ based on how many rollouts\nout of _ntest−rollouts_ succeeded in the task. Table 2 provides the\nvalues for the parameters defined in the algorithm.\n\n\n**V. EXPERIMENTS**\n\nWe conduct experiments on the MuJoCo simulation environments provided by OpenAI Gym, which is a standard\nbenchmark for multi-goal RL. Two standard manipulation\ntasks, both based on a 7-DOF fetch robotic manipulator [28],\nare chosen, namely PickAndPlace and FetchPush. Because\nthe environments may be generated in an open-ended way but\ntraining is computationally expensive, we limit the maximum\nnumber of environments ( _E_ ) to 4 for both tasks. That allows\nus to train on both tasks multiple times to prove our concept\nand provide statistics.\n**PickAndPlace** : The PickAndPlace task with 4 different\n\nenvironments is shown in Fig. 1. The objective is to grasp the\ncube and bring it to the target position. The cube is shown as a\nblack box, and its initial position is sampled uniformly within\nthe yellow area. The target is the red dot, which is sampled\nuniformly within the blue region. The obstacles are colored\nin magenta. The task’s difficulty is gradually increased by\nadding fixed blocks to the different locations on the table, and\nfour different environments are generated in total. In the first\nenvironment, shown in Fig. 1a, the robot learns how to pick\nup the cube and place it on the target position. In the second\nenvironment, shown in Fig. 1b, an obstacle with 0.2m width,\n0.02m depth, and 0.5m height is placed on the other side of\nthe robot on the table. In the third environment, shown in Fig.\n1c, another obstacle with 0.3m width, 0.02m depth, and 0.3m\nheight is placed. In the last environment, shown in Fig. 1d,\nan obstacle with 0.2m width, 0.02m depth, and 0.9m height\nis placed in front of the target sampled area.\n**FetchPush** : The FetchPush task with 4 different environ\nments is shown in Fig. 2. A cube (the black box) and a target\n(the red dot) are sampled uniformly within the yellow and\nblue areas, respectively. The objective is to push the cube\ninto the target position with a clamped gripper. The task’s\ndifficulty is gradually increased by adding fixed obstacles\n(colored in magenta) at different locations on the table, and\nalso in this case four different environments are generated\nin total. In the first environment, shown in Fig. 2a, the robot\nlearns how to push the cube to the target point. In the second\nenvironment, shown in Fig. 2b, the robot needs to adapt its\nlearned policy from the previous environment to avoid the\nobstacle. In the third environment, shown in Fig. 2c, there is\nonly a 10cm gap between the two obstacles, and the robot\nshould push the cube through this gap. Another obstacle is\nplaced in the middle of the table in the fourth environment\n\n\n\n4 VOLUME 11, 2023\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n**Algorithm 1** Co-adapting hindsight experience replay (COHER)\n\n**Input:** Environment population _X_, maximum number of environment _E_, number of episodes _M_, number of timesteps _T_\n\nSelect an off-policy algorithm A _▷_ In our case A is DDPG\nInitialize replay buffer _B ←∅_\nInitialize _X_ with the first environment _X_ 0\nInitialize environment counter _n ←_ 0\n\n_ϵR ←_ 0 _._ 05, _ntest−rollouts ←_ 99\n**while** _n < E_ **do**\n\n\nSelect environment _Xn_\n**for** episode = 1 _. . . M_ **do**\n\nSample a desired goal _g_ and an initial state _s_ 0\n**for** _t_ = 0 _. . . T_ **do** _▷_ Rollout episode\n_at_ = _π_ ( _st_ _, g_ )\nExecute the action _at_, obtain a next state _st_ +1 and reward _rt_\nStore transition ( _st_ _, at_ _, rt_ _, st_ +1 _, g_ ) in replay buffer _B_\nSample a set of additional goals from achieved states for replay _G_ := _S_ (episode)\n**for** _g_ _[′]_ _∈G_ **do** _▷_ Hindsight goal [8]\nRecompute reward _rt_ _[′]_\nStore transition ( _st_ _, at_ _, rt_ _[′][,][ s][t]_ [+1] _[,][ g][′]_ [)][ in replay buffer] _[ B]_\n**end for**\n\n**end for**\n\nSample a mini batch _b_ from replay buffer _B_\nUpdate value function _Q_ with _b_ to minimize _Lcritic_ in equation (4)\nUpdate policy _π_ with _b_ to minimize _Lactor_ in equation (6)\n_successrate ←_ 0\n\n**for** _t_ = 0 _. . . ntest−rollouts_ **do** _▷_ Test rollouts\n_at_ = _π_ ( _st_ _, g_ )\nExecute the action _at_, obtain a next state _st_ +1 and reward _rt_\n**if** _∥ϕ_ ( _st_ +1) _−_ _g∥_ 2 [2] _[≤]_ _[ϵ][R]_ **[then]**\n_successrate ←_ _successrate_ + 1 _/ntest−rollouts_\n**end if**\n\n**end for**\n\n**if** _successrate ≥_ _δ_ **then**\n\nCreate the next environment _Xn_ +1\n_n ←_ _n_ + 1\n\n**end if**\n\n**end for**\n\n**end while**\n\n\n\nshown in Fig. 2d, and the robot must avoid it in order to reach\nthe target position.\nWe adopt the identical control actions and state configurations as those presented in the paper proposing HER [8].\nIn both tasks, the state is a vector consisting of the position,\norientation, linear velocity, and angular velocity of the robot’s\nend-effector, as well as the position of the cube and target.\nThe action space is a 4-dimensional vector, with the first three\nelements specifying the desired relative gripper position at\nthe next timestep, and the last element specifying the desired\ndistance between the two fingers of the gripper. The control\nis executed with a frequency of 1kHz, through the real-time\nUbuntu kernel and a Python wrapper to the Franka library [1] .\nNote that the robotic agent learns to avoid obstacles through\n\n\n[1https://frankaemika.github.io/docs/libfranka.html](https://frankaemika.github.io/docs/libfranka.html)\n\n\n\ntrial and error as an inherent part of RL, by experiencing collisions. In particular, when the agent collides with obstacles\n(or even with its own body) and becomes trapped, it fails\nto complete the task and receives no reward. Of note, no\ninformation about the obstacles is included in the state vector,\nbut the agent figures out the best actions to avoid collisions\nwith the help of the generated curriculum environments that\ngradually increase in difficulty. It is assumed that the task\nis accomplished if the cube reaches the goal within a given\ndistance threshold, see equation (1), in which case it receives\na non-negative reward 0.\n\n\n_A. COMPARATIVE ANALYSIS_\n\nWe compare the performance of our framework (COHER)\nagainst vanilla HER, HGG, and CHER. During training with\nCOHER, environments co-adapt with the agent. When the\n\n\n\nVOLUME 11, 2023 5\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n\ncurrent environment _Xn_ performance reaches the predefined\nsuccess rate _δ_, the next environment _Xn_ +1 is selected and\nthe agent tries to solve the new environment with its learned\nmodel. On the other hand, HER, HGG, and CHER are trained\ndirectly on the last (i.e., the most difficult) environment in the\npopulation considered in COHER. Our goal is to demonstrate\nhow the co-adapting training method accelerates learning.\n\nPickAndPlace and FetchPush are run with 20 and 40 different seeds [2], respectively. The success rate _δ_ is chosen as 0 _._ 7\nand 0 _._ 9, respectively for PickAndPlace and FetchPush tasks,\nbased the average success rates reported in the paper proposing HGG [9], and remains constant during the training of each\ndifferent environment within a task. As the outcomes of each\n\nepisode can be influenced by multiple random factors in the\nsimulation, the agent completes the task by using a different\nnumber of episodes at each run. Therefore, for illustration\npurposes, in Fig. 3a and 4a (respectively for PickAndPlace\nand FetchPush), we consider the worst-case training for COHER and HER (i.e., the run that took the largest number of\nepisodes to successfully accomplish the task, if any) and the\nbest-case training for HGG and CHER (i.e., the run that took\nthe smallest number of episodes to solve the task, if any).\nIn this way, we can show that, in the worst-case for both\nalgorithms, COHER solves the task faster than HER (but,\nit turns out that also the best-case for COHER needs less\n\nepisodes than the best-case for HER). Furthermore, we can\nshow that in the worst-case COHER needs less episodes than\nHGG and CHER in their best-case training.\nIn the same figures, the environment transition points are\ndepicted as orange, brown, and purple dots. It can be seen\nthat, with COHER, the performance drops as soon as the\nnext challenging environment is generated, but the RL algorithm adapts itself to the new environment until it reaches\nthe success rate. As indicated by the colored dots, COHER\nrequires 54400 and 34550 episodes to complete the task,\nrespectively for PickAndPlace and FetchPush. Concerning\nPickAndPlace, the first environment takes 24850 episodes,\nwhile the second and third environments are generated at\n33800 and 46700 episodes, respectively. In other words, 8950\nand 12900 episodes are required to reach the given success\nrate for them. As for FetchPush, solving the first environment\ntakes 8950 episodes, while the second and third environments\nare generated at 12150 and 16550 episodes, respectively. In\nother words, 3200 and 4400 episodes are required to reach\nthe given success rate for them. Compared to COHER, HER\nrequires 223550 and 68200 episodes to reach the same success rate, respectively for PickAndPlace and FetchPush. On\nthe other hand, HGG and CHER get stuck in most cases\nin the presence of obstacles, because as discussed earlier\ntheir heuristic method for generating the curriculum is based\non Euclidean distance. For PickAndPlace in particular, the\nsuccess rate of HGG is always 0.\nThe total number of episodes required to complete the two\n\n\n2The number of runs is different for the two tasks due to limitations on the\ncomputational resources.\n\n\n\ntasks across the different runs is shown in Fig. 3b and Fig.\n4b, respectively for PickAndPlace and FetchPush. The mean\nand median values are shown as a black dashed line and a\n\nblack solid line, respectively. The corresponding numerical\nvalues are reported in Table 1. The difference on the number of episodes is statistically significant for both tasks, i.e.\nCOHER uses less episodes than HER (Wilcoxon Rank-Sum\ntest, _α_ = 0 _._ 05; PickAndPlace _p_ = 0 _._ 000954; FetchPush\n\n\nEpisodes COHER HER Episodes COHER HER\nMean 22101 50502 Mean 21287 28594\n\nMedian 18800 31850 Median 20450 27050\n\nStd. dev. 9301 54767 Std. dev. 4196 10032\n\n|p = 0.0144|41).|Col3|\n|---|---|---|\n|Episodes|COHER|HER|\n|Mean|22101|50502|\n|Median|18800|31850|\n|Std. dev.|9301|54767|\n\n\n|Episodes|COHER|HER|\n|---|---|---|\n|Mean|21287|28594|\n|Median|20450|27050|\n|Std. dev.|4196|10032|\n\n\n\nTABLE 1: Descriptive statistics for the number of episodes\nrequired to complete the PickAndPlace (left) and the FetchPush (right) tasks.\n\n\nFig. 3c and Fig. 4c show the number of episodes required\nfor each environment in order to reach the predefined success rate. On average, the environments require 6411.29,\n2575.81, 7948.39, and 5216.13 episodes for PickAndPlace\nand 5810.25, 2332.05, 3453.85, and 9741.03 episodes for\nFetchPush. Moreover, the figures shed light on the difficulty\nlevel of each environment. Since the robot starts in the first environment without knowing anything about the task, it takes\non average a little bit longer than the second environment. In\nthe second environment, the obstacle is located on the other\nside of the robot arm, and the location of the obstacle does\nnot intersect with the sampled area of the initial position (i.e.,\nthe yellow area) of the cube. As a result, the robot can easily\napply the skills it learned in the first environment. After the\nrobot succeeds in the second environment, it learns to avoid\nthe obstacle either by pushing the cube around it or by moving\nthe cube above it, depending on the task. When the third\nenvironment is introduced, the robot arm is blocked more\n\noften than in the second environment. The reason is that the\n\nthird environment has a much smaller gap than the second\none, and also that the robot has just learned to go through the\nsafe way, reaching the goal on the other side of the obstacle in\nthe second environment, but now another obstacle is located\non its safely learned path.\nAs for the last environment, the obstacle is located in the\n\nmiddle of the table for FetchPush and on the left side of the\n\ntable for PickAndPlace. The last environment for FetchPush\n\ntakes the longest to be solved because a newly located obstacle intersects with the sampled area of the target. Furthermore,\nthe robot needs to push the cube around it and bring it to the\ntarget point.\n\n\n_B. SIM2REAL_\n\nEach individual training with a different seed in COHER\nconverges ultimately to the predefined success rate at certain\nepisodes. Once this criterion is met, the policy is chosen\nas the final policy. We tested the final policy found on the\nPickAndPlace task on a real 7-DOF Franka robot. Specifically, we designed the fourth environment as a real-world\n\n\n\n6 VOLUME 11, 2023\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n(a) The 1st environment. (b) The 2nd environment. (c) The 3rd environment. (d) The 4th environment.\n\n\nFIGURE 1: Environment shifts in the PickAndPlace task.\n\n\n(a) The 1st environment. (b) The 2nd environment. (c) The 3rd environment. (d) The 4th environment.\n\n\nFIGURE 2: Environment shifts in the FetchPush task.\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-3.png)\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-4.png)\n\n1.0\n\n\n0.8\n\n\n0.6\n\n\n0.4\n\n\n0.2\n\n\n0.0\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-0.png)\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-5.png)\n\n|COHER (ours) HGG<br>HER CHER|COHER (ours) HGG<br>HER CHER|\n|---|---|\n|||\n\n\nEpisode ×10 [5]\n\n\n(a)\n\n\n\n5\n× 10\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-1.png)\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-6.png)\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-8.png)\n\nCOHER HER\n\nMethods\n\n\n(b)\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-2.png)\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-7.png)\n\n4\n× 10\n\n\nenv1 env2 env3 env4\n\nEnvironments\n\n\n(c)\n\n\n\n2.00\n\n\n1.50\n\n\n1.00\n\n\n0.50\n\n\n\n2.50\n\n\n2.00\n\n\n1.50\n\n\n1.00\n\n\n0.50\n\n\n0.00\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-6-9.png)\n\nFIGURE 3: Results for the PickAndPlace task. **(a)** Success rate of the worst-case training of COHER and HER, and the best-case\ntraining of HGG and CHER. Environment transitions during training with COHER are indicated by orange, brown, and purple\ndots. **(b)** Number of episodes required to reach the success rate using the COHER and HER methods in 20 different runs. **(c)**\nNumber of episodes required to reach the success rate for each environment individually with COHER in 20 different runs.\n\n\n\nreplica of the simulation environment shown in Fig. 1d. The\nresulting environment is shown in Fig. 5, which also shows\nthe measurement of the obstacles’ height. The output of the\npolicy is the linear motion of the end-effector in Cartesian\nspace relative to its current position, as well as the state of the\ngripper gap. The output values from the linear motion are directly given to the Franka robot. On the other hand, the gripper\nstate of the robot receives in simulation one actuation value\n\nat every timestep. If we fed these values directly to the real\nFranka robot, this would slow down the robot’s movement\nbecause at each timestep the robot would have to wait for the\ngripper to finish its movement before executing the next one.\nMoreover, the gripper would get clamped and in the long run\n\n\n\nthis would make the gripper unusable, due to hardware issues.\nTherefore, we used a threshold to close and open the gripper.\n\n\nAs the cube’s initial position is stationary, its position in\nrelation to the robot’s reference frame could be found either\n\nby using a camera with a red filter or by measuring it w.r.t. the\nrobot’s origin. However, when the cube is grasped, it might\nbe occluded by the gripper, making it infeasible to obtain its\nposition either by using a camera or by measuring it at each\ntimestep. Therefore, in our experiments, the gripper position\nwas assigned to the cube position as soon as the gripper was\nclamped. The gripper position, along with the relevant information on the state of the robot, could be obtained using the\n\n\n\nVOLUME 11, 2023 7\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n\n1.2\n\n\n1.0\n\n\n0.8\n\n\n0.6\n\n\n0.4\n\n\n0.2\n\n\n0.0\n\n\n\n4\n\n\n\n1.75\n\n\n1.50\n\n\n1.25\n\n\n1.00\n\n\n0.75\n\n\n0.50\n\n\n0.25\n\n\n\n4\n× 10\n\n\nenv1 env2 env3 env4\n\nEnvironments\n\n\n(c)\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-7-0.png)\n\nEpisode ×10 [4]\n\n\n(a)\n\n\n\nCOHER HER\n\nMethods\n\n\n(b)\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-7-1.png)\n\nFIGURE 4: Results for the FetchPush task. **(a)** Success rate of the worst-case training of COHER and HER, and the best-case\ntraining of HGG and CHER. Environment transitions during training with COHER are indicated by orange, brown, and purple\ndots. **(b)** Number of episodes required to reach the success rate using the COHER and HER methods in 40 different runs. **(c)**\nNumber of episodes required to reach the success rate for each environment individually with COHER in 40 different runs.\n\n\nenv1 0.9 FIGURE 5: PickAndPlace Sim2Real scenario. **(a)** The realenv2env3 0.90.9 world environment, replicating the one of Fig. 1d. **(b)** The\n\n|Parameter|Value|\n|---|---|\n|Number of environments (_E_<br>Episodes (_M_)<br>Timesteps (_T_)<br>_γ_<br>Replay buffer (_B_) size<br>Mini batch (_b_) size<br>Polyak-averaging coeffcien<br>Probability of HER experien<br>Replay Prioritization<br>Success rates_ δ_ for PickAnd<br>env1<br>env2<br>env3<br>env4<br>Success rates_ δ_ for Push<br>env1<br>env2<br>env3<br>env4|)<br>4<br>50<br>100<br>0_._98<br>104<br>256<br> t<br>0_._95<br>  ce replay<br>0_._8<br>Energy-Based Prioritization [32]<br> Place<br>0.7<br>0.7<br>0.7<br>0.7<br>0.9<br>0.9<br>0.9<br>0.9|\n\n\n\nTABLE 2: Hyperparameter settings for COHER.\n\n\n\nFrankx library [3] . The final policy achieved by COHER could\navoid obstacles and complete the task successfully. On the\nother hand, the policies found in the first environment could\nnot complete the task without colliding with obstacles. We\nimplemented Sim2Real for four different target locations and\ncaptured the video of the robot from different perspectives.\nSim2real videos and code are available on our project website\n[at the following link: https://erdiphd.github.io/COHER/.](https://erdiphd.github.io/COHER/)\n\n\n**VI. CONCLUSIONS**\n\nWe presented a novel framework for co-adapting curriculum\nlearning with sparse rewards and multi-goal RL, dubbed COHER, and tested in simulation on two different robot manipulation tasks: PickAndPlace and FetchPush. Furthermore,\nthe PickAndPlace task was chosen for Sim2Real implementation using a Franka robot. We proved that the proposed\nco-adapting method is more sample-efficient than the vanilla\n\n\n[3https://github.com/pantor/frankx](https://github.com/pantor/frankx)\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-7-2.png)\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-7-3.png)\n\nHER method. Furthermore, we were able to solve both tasks\nwith COHER without explicitly giving the algorithm obstacle\npositions, whereas the vanilla HER requires more samples\nwhile HGG, as well as CHER, get stuck in obstacles.\n\n**Limitations and future works** The present study involves\na manual design of the environments, with the underlying\nprinciple of making the task increasingly more difficult as it\nis accomplished. Since such manual design could be timeconsuming, using an intelligent algorithm to design a (potentially large) number of environments would be an interesting\ndirection for future research. However, one potential issue\nassociated with this approach would be that the computational\ntime required would significantly increase. On the other hand,\nthe difference in difficulty between any two subsequent environments would decrease as the number of environments\n\nincreases, making it easier for the agent to accomplish the\noverall task. Beyond a certain number of environments, the\ndifficulty of two consecutive environments may not differ\nsignificantly anymore. This could potentially decrease our\nsample efficiency, as the agent might require many training\n\n\n\n8 VOLUME 11, 2023\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n\nsteps to master an environment that is almost the same as the\nprevious one. Our general intuition is that there exists a tradeoff between number of environments and sample efficiency.\nHowever, finding this trade-off automatically is hard, and\nmore investigation is needed in this direction. Furthermore,\nwhile in this work we assumed that the agent should reach the\nsame pre-defined success rate for each environment before\nchanging to the next slightly harder environment, in some\nscenarios it might be possible that different success rates\nshould be set for different environments, such that the optimal\nsuccess rate, resulting in the smallest number of total training\nepisodes, should be determined for each environment.\nLastly, it should be noted that the proposed method can be\ngeneralized to tasks beyond robot manipulation, such as maze\nnavigation, robot locomotion, puzzle solving, urban planning,\nand assembly tasks. In those cases, the level of difficulty will\nobviously have to be defined differently from what we did\nin this study (i.e., based on the presence and configuration\nof obstacles), e.g. one may need to take into account the\nsteepness of roughness of terrain for locomotion, the number\nand inter-dependency of assembly tasks, etc. Future research\nshould be aimed at applying our method to those tasks, also\naddressing any possible scalability issues.\n\n\n**REFERENCES**\n\n\n[1] Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and\nGraves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin, ‘‘Playing Atari with Deep Reinforcement Learning,’’ 2013,\narXiv:1312.5602.\n\n[2] Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu,\nAndrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex\nand Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg\nand Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou,\nIoannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and\nLegg, Shane and Hassabis, Demis, ‘‘Human-level control through deep\nreinforcement learning,’’ _Nature_, vol. 518, no. 7540, pp. 529–533, 2015.\n\n[3] Kober, Jens and Bagnell, J. Andrew and Peters, Jan, ‘‘Reinforcement\nlearning in robotics: A survey,’’ _The International Journal of Robotics_\n_Research_, vol. 32, no. 11, pp. 1238–1274, 2013.\n\n[4] Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and\nHeess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and\nWierstra, Daan, ‘‘Continuous control with deep reinforcement learning,’’\narXiv:1509.02971.\n\n[5] Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine,\nSergey, ‘‘Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,’’ arXiv:1801.01290.\n\n[6] Ng, Andrew Y. and Coates, Adam and Diel, Mark and Ganapathi, Varun\nand Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric, ‘‘Autonomous Inverted Helicopter Flight via Reinforcement Learning,’’ in\n_Experimental Robotics IX_, Ang, Marcelo H. and Khatib, Oussama, Ed.\nBerlin Heidelberg: Springer, 2006, vol. 21, pp. 363–372, Series Title:\nSpringer Tracts in Advanced Robotics.\n\n[7] Seo, Minah and Vecchietti, Luiz Felipe and Lee, Sangkeum and Har, Dongsoo, ‘‘Rewards Prediction-Based Credit Assignment for Reinforcement\nLearning With Sparse Binary Rewards,’’ _IEEE Access_, vol. 7, pp. 118 776–\n118 791, 2019.\n\n[8] Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider,\nJonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin,\nJosh and Abbeel, Pieter and Zaremba, Wojciech, ‘‘Hindsight Experience\nReplay,’’ arXiv:1707.01495.\n\n[9] Ren, Zhizhou and Dong, Kefan and Zhou, Yuan and Liu, Qiang and Peng,\nJian, ‘‘Exploration via Hindsight Goal Generation,’’ arXiv:1906.04279.\n\n[10] Fang, Meng and Zhou, Tianyi and Du, Yali and Han, Lei and Zhang,\nZhengyou, ‘‘Curriculum-guided Hindsight Experience Replay,’’ in _Ad-_\n_vances in Neural Information Processing Systems_, Wallach, H. and\n\n\n\nLarochelle, H. and Beygelzimer, A. and Alché-Buc, F. d’ and Fox, E. and\nGarnett, R., Ed., vol. 32. Curran Associates, Inc., 2019.\n\n[11] Bing, Zhenshan and Brucker, Matthias and Morin, Fabrice O. and Li, Rui\nand Su, Xiaojie and Huang, Kai and Knoll, Alois, ‘‘Complex Robotic\nManipulation via Graph-Based Hindsight Goal Generation,’’ _IEEE Trans-_\n_actions on Neural Networks and Learning Systems_, pp. 1–14, 2021.\n\n[12] Z. Bing, E. Álvarez, L. Cheng, F. O. Morin, R. Li, X. Su, K. Huang, and\nA. Knoll, ‘‘Robotic manipulation in dynamic scenarios via bounding-boxbased hindsight goal generation,’’ _IEEE Transactions on Neural Networks_\n_and Learning Systems_, vol. 34, no. 8, pp. 5037–5050, 2023.\n\n[13] Brant, Jonathan C. and Stanley, Kenneth O., ‘‘Minimal criterion coevolution: a new approach to open-ended search,’’ in _Proceedings of the Genetic_\n_and Evolutionary Computation Conference_ . ACM, 2017-07, pp. 67–74.\n\n[14] Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O.,\n‘‘Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions,’’\narXiv:1901.01753.\n\n[15] Yaman, Anil and Iacca, Giovanni and Mocanu, Decebal Constantin and\nFletcher, George and Pechenizkiy, Mykola, ‘‘Learning with Delayed\nSynaptic Plasticity,’’ in _Genetic and Evolutionary Computation Confer-_\n_ence_ . ACM, 2019, pp. 152–160.\n\n[16] Auerbach, Joshua E and Iacca, Giovanni and Floreano, Dario, ‘‘Gaining\ninsight into quality diversity,’’ in _Genetic and Evolutionary Computation_\n_Conference Companion_ . ACM, 2016, pp. 1061–1064.\n\n[17] Bizzotto, Edoardo and Yaman, Anil and Iacca, Giovanni, ‘‘Promoting\nBehavioral Diversity via Multi-Objective/Quality-Diversity Novelty Producing Synaptic Plasticity,’’ in _2021 IEEE Symposium Series on Computa-_\n_tional Intelligence (SSCI)_ . IEEE, 2021, pp. 01–08.\n\n[18] Yaman, Anil and Iacca, Giovanni and Mocanu, Decebal Constantin and\nFletcher, George and Pechenizkiy, Mykola, ‘‘Novelty Producing Synaptic\nPlasticity,’’ in _Genetic and Evolutionary Computation Conference Com-_\n_panion_ . ACM, 2020, pp. 93–94.\n\n[19] Stanley, Kenneth O. and Miikkulainen, Risto, ‘‘Evolving Neural Networks\nthrough Augmenting Topologies,’’ _Evolutionary Computation_, vol. 10,\nno. 2, pp. 99–127, 2002.\n\n[20] Hansen, Nikolaus and Arnold, Dirk V. and Auger, Anne, ‘‘Evolution Strategies,’’ in _Springer Handbook of Computational Intelligence_, Kacprzyk,\nJanusz and Pedrycz, Witold, Ed. Springer Berlin Heidelberg, 2015, pp.\n871–898.\n\n[21] Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun,\nYi and Schmidhuber, Jürgen, ‘‘Natural Evolution Strategies,’’ 2011,\narXiv:1106.4487.\n\n[22] Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and\nSutskever, Ilya, ‘‘Evolution Strategies as a Scalable Alternative to Reinforcement Learning,’’ arXiv:1703.03864.\n\n[23] Zhang, Shangtong and Zaiane, Osmar R., ‘‘Comparing Deep Reinforcement Learning and Evolutionary Methods in Continuous Control,’’\narXiv:1712.00006.\n\n[24] Bing, Zhenshan and Zhou, Hongkuan and Li, Rui and Su, Xiaojie and\nMorin, Fabrice Oliver and Huang, Kai and Knoll, Alois, ‘‘Solving Robotic\nManipulation with Sparse Reward Reinforcement Learning via GraphBased Diversity and Proximity,’’ _IEEE Transactions on Industrial Elec-_\n_tronics_, pp. 1–1, 2022.\n\n[25] R. Yang, M. Fang, L. Han, Y. Du, F. Luo, and X. Li, ‘‘Mher: Model-based\nhindsight experience replay,’’ _arXiv preprint arXiv:2107.00306_, 2021.\n\n[26] S. Pitis, H. Chan, S. Zhao, B. Stadie, and J. Ba, ‘‘Maximum entropy\ngain exploration for long horizon multi-goal reinforcement learning,’’ in\n_International Conference on Machine Learning_ . PMLR, 2020, pp. 7750–\n7761.\n\n[27] Ha, David, ‘‘Reinforcement Learning for Improving Agent Design,’’ _Arti-_\n_ficial Life_, vol. 25, no. 4, pp. 352–365, 2019.\n\n[28] Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew,\nBob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin,\nJosh and Chociej, Maciek and Welinder, Peter and Kumar, Vikash and\nZaremba, Wojciech, ‘‘Multi-Goal Reinforcement Learning: Challenging\nRobotics Environments and Request for Research,’’ arXiv:1802.09464.\n\n[29] Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David,\n‘‘Universal value function approximators,’’ in _International conference on_\n_machine learning_ . PMLR, 2015, pp. 1312–1320.\n\n[30] Rechenberg, I., ‘‘Evolutionsstrategien,’’ in _Simulationsmethoden in der_\n_Medizin und Biologie_, Schneider, Berthold and Ranft, Ulrich, Ed. Springer\nBerlin Heidelberg, 1978, vol. 8, pp. 83–114, Series Title: Medizinische\nInformatik und Statistik.\n\n\n\nVOLUME 11, 2023 9\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3382264\n\n\nSayar _et al._ : Curriculum learning for robot manipulation tasks with sparse reward through environment shifts\n\n\n\n\n[31] Fujimoto, Scott and Hoof, Herke and Meger, David, ‘‘Addressing Function\nApproximation Error in Actor-Critic Methods,’’ in _Proceedings of the 35th_\n_International Conference on Machine Learning_ . PMLR, 2018, pp. 1587–\n1596, ISSN: 2640-3498.\n\n[32] Zhao, Rui and Tresp, Volker, ‘‘Energy-based hindsight experience prioritization,’’ in _Conference on Robot Learning_ . PMLR, 2018, pp. 113–122.\n\n\nERDI SAYAR (Student, IEEE) received B.Sc. and\nB.Eng. degrees from Kocaeli University, Turkey,\nand Bochum Applied Science, Germany, respectively. In 2020, he obtained his M.Sc. degree from\nRWTH Aachen, Germany. Currently, he is a Ph.D.\nstudent at the Informatics 6 Department, Technical University of Munich. His research interests\nprimarily focus on robotics controlled by artificial\nneural networks and their related applications.\n\n\nGIOVANNI IACCA (SM) is an Associate Professor in Information Engineering at the Department\nof Information Engineering and Computer Science of the University of Trento, Italy, where he\nfounded the Distributed Intelligence and Optimization Lab (DIOL). Previously, he worked as a postdoctoral researcher in Germany (RWTH Aachen,\n2017-2018), Switzerland (University of Lausanne\nand EPFL, 2013-2016), and The Netherlands (INCAS3, 2012-2016), as well as in industry in the\nareas of software engineering and industrial automation. He is co-PI of\nthe PATHFINDER-CHALLENGE project \"SUSTAIN\" (2022-2026). Previously, he was co-PI of the FET-Open project \"PHOENIX\" (2015-2019).\nHe has received two best paper awards (EvoApps 2017 and UKCI 2012).\nHis research focuses on computational intelligence, distributed systems, and\nexplainable AI applied e.g. to medicine. In these fields, he co-authored more\nthan 140 peer-reviewed publications. He is actively involved in organizing\ntracks and workshops at some of the top conferences in computational\nintelligence, and he regularly serves as a reviewer for several journals and\nconference committees. He is an Editorial Board Member for Applied Soft\nComputing and Associate Editor for Frontiers in Robotics and AI.\n\n\n\nALOIS KNOLL (Fellow, IEEE) received the M.Sc.\ndegree in electrical/communications engineering\nfrom the University of Stuttgart, Stuttgart, Germany, in 1985, and the Ph.D. degree (summa cum\nlaude) in computer science from the Technical University of Berlin (TU Berlin), Berlin, Germany, in\n1988. He served on the faculty of the Computer\nScience Department, TU Berlin, until 1993. He\njoined the University of Bielefeld, Bielefeld, Germany, as a Full Professor, where he served as the\nDirector of the Technical Informatics Research Group until 2001. Since 2001,\nhe has been a Professor at the Department of Informatics, Technical University of Munich (TUM), Munich, Germany. His research interests include\ncognitive, medical, and sensor-based robotics; multi-agent systems; data\nfusion; adaptive systems; multimedia information retrieval; model-driven\ndevelopment of embedded systems with applications to automotive software\nand electric transportation; and simulation systems for robotics and traffic.\nDr. Knoll was a member of the EU’s highest advisory board on information\ntechnology, the Information Society Technology Advisory Group (ISTAG),\nfrom 2007 to 2009, and its subgroup on Future and Emerging Technologies\n(FETs). In this capacity, he was actively involved in developing the concept\nof the European Union (EU) FET flagship projects.\n\n\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-9-0.png)\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-9-1.png)\n\n![](output/images/76f8e64e2db3f788467e65a19056b1add9a70768.pdf-9-2.png)\n\n10 VOLUME 11, 2023\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4\n\n\n",
    "ranking": {
      "relevance_score": 0.7355452521567241,
      "citation_score": 0.42293680297397773,
      "recency_score": 0.7747265403948539,
      "final_score": 0.6769416911439878
    },
    "citation_key": "Sayar2024CurriculumLF",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "5feeabb408da8b4eb0c8bc101064016867ba20d2",
    "title": "Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning",
    "published": "2023-10-02",
    "authors": [
      "Qiwei Di",
      "Heyang Zhao",
      "Jiafan He",
      "Quanquan Gu"
    ],
    "summary": "Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation. Our work extends the previous instance-dependent results within simpler function classes, such as linear and differentiable function to a more general framework.",
    "pdf_url": "https://arxiv.org/pdf/2310.01380",
    "doi": "10.48550/arXiv.2310.01380",
    "fields_of_study": [
      "Computer Science",
      "Mathematics"
    ],
    "venue": "International Conference on Learning Representations",
    "citation_count": 6,
    "bibtex": "@Article{Di2023PessimisticNL,\n author = {Qiwei Di and Heyang Zhao and Jiafan He and Quanquan Gu},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning},\n volume = {abs/2310.01380},\n year = {2023}\n}\n",
    "markdown_text": "Published as a conference paper at ICLR 2024\n\n## - PESSIMISTIC NONLINEAR LEAST-SQUARES VALUE IT\n### ERATION FOR OFFLINE REINFORCEMENT LEARNING\n\n\nQiwei Di [1], Heyang Zhao [1], Jiafan he [1], Quanquan Gu [1]\n\n1Department of Computer Science, University of California, Los Angeles\n{qiwei2000,hyzhao,jiafanhe19,qgu}@cs.ucla.edu,\n\n\nABSTRACT\n\n\nOffline reinforcement learning (RL), where the agent aims to learn the optimal\npolicy based on the data collected by a behavior policy, has attracted increasing\nattention in recent years. While offline RL with linear function approximation has\nbeen extensively studied with optimal results achieved under certain assumptions,\nmany works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an\noracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our\nalgorithmic design comprises three innovative components: (1) a variance-based\nweighted regression scheme that can be applied to a wide range of function classes,\n(2) a subroutine for variance estimation, and (3) a planning phase that utilizes a\npessimistic value iteration approach. Our algorithm enjoys a regret bound that has\na tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation.\nOur work extends the previous instance-dependent results within simpler function\nclasses, such as linear and differentiable function to a more general framework.\n\n\n1 INTRODUCTION\n\nOffline reinforcement learning (RL), also known as batch RL, is a learning paradigm where an\nagent learns to make decisions based on a set of pre-collected data, instead of interacting with the\nenvironment in real-time like online RL. The goal of offline RL is to learn a policy that performs well\nin a given task, based on historical data that was collected from an unknown environment. Recent\nyears have witnessed significant progress in developing offline RL algorithms that can leverage large\namounts of data to learn effective policies. These algorithms often incorporate powerful function\napproximation techniques, such as deep neural networks, to generalize across large state-action\nspaces. They have achieved excellent performances in a wide range of domains, including the\ngames of Go and chess (Silver et al., 2017; Schrittwieser et al., 2020), robotics (Gu et al., 2017;\nLevine et al., 2018), and control systems (Degrave et al., 2022).\nSeveral works have studied the theoretical guarantees of offline tabular RL and proved near-optimal\nsample complexities in this setting (Xie et al., 2021b; Shi et al., 2022; Li et al., 2022). However,\nthese algorithms cannot handle real-world applications with large state and action spaces. Consequently, a significant body of research has devoted to offline RL with function approximation. For\nexample, (Jin et al., 2021b) proposed the first efficient algorithm for offline RL with linear MDPs,\nemploying the principle of pessimism. Subsequently, numerous works have presented a range\nof algorithms for offline RL with linear function approximation, as seen in Zanette et al. (2021);\nMin et al. (2021); Yin et al. (2022a); Xiong et al. (2023); Nguyen-Tang et al. (2023). Among them,\nsome works have instance-dependent (a.k.a., problem-dependent) upper bound (Jin et al., 2021b;\nYin et al., 2022a; Xiong et al., 2023; Nguyen-Tang et al., 2023), which matches the worst-case result when dealing with the “hard instance” and performs better in easy cases.\nTo address the complexities of working with more complex function classes, recent research has\nshifted the focus towards offline reinforcement learning (RL) with general function approximation\n(Chen & Jiang, 2019; Xie et al., 2021a). Utilizing the principle of pessimism first used in Jin et al.\n(2021b), Xie et al. (2021a) enforced pessimism at the initial state over the set of functions consistent\nwith the Bellman equations. Their algorithm requires solving an optimization problem over all the\npotential policies and corresponding version space, which includes all functions with lower Bellmanerror. To overcome this limitation, Xie et al. (2021a) proposed a practical algorithm which has a poor\n\n\n1\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\ndependency on the function class complexity. Later, Cheng et al. (2022) proposed an adversarially\ntrained actor critic method based upon the concept of relative pessimism. Their result is not statistically optimal and was later improved by Zhu et al. (2023). Both algorithms’ implementation\nrelies on a no-regret policy optimization oracle. Another line of works, such as (Zhan et al., 2022;\nOzdaglar et al., 2023; Rashidinejad et al., 2021), sought to solve the offline RL problem through a\nlinear programming formulation, which requires some additional convexity assumptions on the policy class. However, most of these works only have worst-case regret guarantee. The only exception\nis Yin et al. (2022b), which studies the general differentiable function class and proposed an LSVItype algorithm. For more general function classes, how to get instance-dependent characterizations\nis still an open problem. Therefore, a natural question arises:\nCan we design a computationally tractable algorithm that is statistically efficient with respect to the\ncomplexity of nonlinear function class and has an instance-dependent regret bound?\nWe give an affirmative answer to the above question in this work. Our contributions are listed as\nfollows:\n\n\n- We propose a pessimism-based algorithm Pessimistic Nonlinear Least-Square Value Iteration\n(PNLSVI) designed for nonlinear function approximation, which strictly generalizes the existing pessimism-based algorithms for both linear and differentiable function approximation\n(Xiong et al., 2023; Yin et al., 2022b). Our algorithm is oracle-efficient, i.e., it is computationally efficient when there exists an efficient regression oracle and bonus oracle for the function\nclass (e.g., generalized linear function class). The bonus oracle can also be reduced to a finite\nnumber of calls to the regression oracle.\n\n- We introduce a new type of D [2] -divergence to quantify the uncertainty of an offline dataset, which\nnaturally extends the role of the elliptical norm seen in the linear setting and the D [2] -divergence in\nGentile et al. (2022); Agarwal et al. (2023); Ye et al. (2023) for online RL. We prove an instancedependent regret bound characterized by this new D [2] -divergence. Our regret bound has a tight\ndependence on complexity of the function class, i.e., O [�] ( [√] log N ) with N being the cardinality of\nthe underlying function class, which improves the O [�] (log N ) dependence [1] in (Yin et al., 2022b)\nand resolves the open problem raised in their paper.\n\n\nNotation: In this work, we use lowercase letters to denote scalars and use lower and uppercase\nboldface letters to denote vectors and matrices respectively. For a vector x ∈ R [d] and matrix Σ ∈\nR [d][×][d], we denote by ∥x∥2 the Euclidean norm and ∥x∥Σ = √x [⊤] Σx. For two sequences {an} and\n\n{bn}, we write an = O(bn) if there exists an absolute constant C such that an ≤ Cbn, and we\nwrite an = Ω(bn) if there exists an absolute constant C such that an ≥ Cbn. We use O [�] (·) and Ω( [�] - )\nto further hide the logarithmic factors. For any a ≤ b ∈ R, x ∈ R, let [x][a,b] denote the truncate\nfunction a · `1` (x ≤ a) + x · `1` (a ≤ x ≤ b) + b · `1` (b ≤ x), where `1` (·) is the indicator function. For a\npositive integer n, we use [n] = {1, 2, .., n} to denote the set of integers from 1 to n.\n2 RELATED WORK\n\nRL with function approximation. As one of the simplest function approximation classes, linear\nrepresentation in RL has been extensively studied in recent years (Jiang et al., 2017; Dann et al.,\n2018; Yang & Wang, 2019; Jin et al., 2020; Wang et al., 2020c; Du et al., 2019; Sun et al., 2019;\nZanette et al., 2020a;b; Weisz et al., 2021; Yang & Wang, 2020; Modi et al., 2020; Ayoub et al.,\n2020; Zhou et al., 2021; He et al., 2021; Zhong et al., 2022). Several assumptions on the linear\nstructure of the underlying MDPs have been made in these works, ranging from the linear MDP\nassumption (Yang & Wang, 2019; Jin et al., 2020; Hu et al., 2022; He et al., 2022; Agarwal et al.,\n2023) to the low Bellman-rank assumption (Jiang et al., 2017) and the low inherent Bellman error assumption (Zanette et al., 2020b). Extending the previous theoretical guarantees to more general problem classes, RL with nonlinear function classes has garnered increased attention in recent\nyears (Wang et al., 2020b; Jin et al., 2021a; Foster et al., 2021; Du et al., 2021; Agarwal & Zhang,\n2022; Agarwal et al., 2023). Various complexity measures of function classes have been studied\nincluding Bellman rank (Jiang et al., 2017), Bellman-Eluder dimension (Jin et al., 2021a), DecisionEstimation Coefficient (Foster et al., 2021) and generalized Eluder dimension (Agarwal et al., 2023).\nAmong these works, the setting in our paper is most related to Agarwal et al. (2023) where D [2] divergence (Gentile et al., 2022) was introduced in RL to indicate the uncertainty of a sample with\nrespect to a particular sample batch.\n\n\n1In (Yin et al., 2022b), they denote by d the complexity of the underlying function class, which is essentially\nlog N using our notation.\n\n\n2\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nOffline tabular RL. There is a line of works integrating the principle of pessimism to develop statistically efficient algorithms for offline tabular RL setting (Rashidinejad et al., 2021; Yin & Wang,\n2021; Xie et al., 2021b; Shi et al., 2022; Li et al., 2022). More specifically, Xie et al. (2021b) utilized the variance of transition noise and proposed a nearly optimal algorithm based on pessimism\nand Bernstein-type bonus. Subsequently, Li et al. (2022) proposed a model-based approach that\nachieves minimax-optimal sample complexity without burn-in cost for tabular MDPs. Shi et al.\n(2022) also contributed by proposing the first nearly minimax-optimal model-free offline RL algorithm.\nOffline RL with linear function approximation. Jin et al. (2021b) presented the initial theoretical results on offline linear MDPs. They introduced a pessimism-principled algorithmic framework\nfor offline RL and proposed an algorithm based on LSVI (Jin et al., 2020). Min et al. (2021) subsequently considered offline policy evaluation (OPE) in linear MDPs, assuming independence between\ndata samples across time steps to obtain tighter confidence sets and proposed an algorithm with optimal d dependence. Yin et al. (2022a) took one step further and considered the policy optimization\nin linear MDPs, which implicitly requires the same independence assumption. Zanette et al. (2021)\nproposed an actor-critic-based algorithm that establishes pessimism principle by directly perturbing\nthe parameter vectors in a linear function approximation framework. Recently, Xiong et al. (2023)\nproposed a novel uncertainty decomposition technique via a reference function, and demonstrated\ntheir algorithm matches the performance lower bound up to logarithmic factors.\n\n\nOffline RL with general function approximation. Chen & Jiang (2019) examined the assumptions underlying value-function approximation methods and established an information-theoretic\nlower bound. Xie et al. (2021a) introduced the concept of Bellman-consistent pessimism, which\nenables sample-efficient guarantees by relying solely on the Bellman-completeness assumption.\nUehara & Sun (2021) focused on model-based offline RL with function approximation under partial coverage, demonstrating that realizability in the function class and partial coverage are sufficient\nfor policy learning. Zhan et al. (2022) proposed an algorithm that achieves polynomial sample complexity under the realizability and single-policy concentrability assumptions. Nguyen-Tang & Arora\n(2023) proposed a method of random perturbations and pessimism for neural function approximation. For differentiable function classes, Yin et al. (2022b) made advancements by improving the\nsample complexity with respect to the planing horizon H. However, their result had an additional\ndependence on the dimension d of the parameter space, whereas in linear function approximation,\nthe dependence is typically on √d. Recently, a sequence of works focus on proposing statistically\n\noptimal and practical algorithms under single concentrability assumption. Ozdaglar et al. (2023) provided a new reformulation of linear-programming. Rashidinejad et al. (2022) used the augmented\nLagrangian method and proved augmented Lagrangian is enough for statistically optimal offline RL.\nCheng et al. (2022) proposed an actor-critic algorithm by formulating the offline RL problem into a\nStackelberg game. However, their result is not statistically optimal and Zhu et al. (2023) improved it\nby combining the marginalized importance sampling framework and achieved the optimal statistical\nrate. We leave a comparison of these works in Appendix A.\n3 PRELIMINARIES\n\nIn our work, we consider the inhomogeneous episodic Markov Decision Processes (MDP), which\ncan be denoted by a tuple of M�S, A, H, {rh} [H] h=1 [,][ {][P][h][}][H] h=1�. In specific, S is the state space, A is\nthe finite action space, H is the length of each episode. For each stage h ∈ [H], rh : S × A → [0, 1]\nis the reward function [2] and Ph(s [′] |s, a) is the transition probability function, which denotes the\nprobability for state s to transfer to next state s [′] with current action a. A policy π := {πh} [H] h=1 [is a]\ncollection of mappings πh from a state s ∈S to the simplex of action space A. For simplicity, we\ndenote the state-action pair as z := (s, a). For any policy π and stage h ∈ [H], we define the value\nfunction Vh [π][(][s][)][ and the action-value function][ Q][π] h [(][s, a][)][ as the expected cumulative rewards starting]\nat stage h, which can be denoted as follows:\n\n\nH\nQ [π] h [(][s, a][) =][ r][h][(][s, a][) +][ E] � rh′ [�] sh′, πh′ (sh′)���sh = s, ah = a, Vh [π][(][s][) =][ Q][π] h�s, πh(s)�,\n� h [′] =h+1 �\n\n\nwhere sh′+1 ∼ Ph(·|sh′, ah′) denotes the observed state at stage h [′] + 1. By this definition,\nthe value function Vh [π][(][s][)][ and action-value function][ Q][π] h [(][s, a][)][ are bounded in][ [0][, H][]][.] In addition, we define the optimal value function Vh [∗] [and the optimal action-value function][ Q][∗] h [as]\n\n\n2While we study the deterministic reward functions for simplicity, it is not difficult to generalize our results\nto stochastic reward functions.\n\n\n3\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nVh [∗][(][s][) = max][π][ V] h [ π][(][s][)][ and][ Q][∗] h [(][s, a][) = max][π][ Q][π] h [(][s, a][)][. We denote the corresponding optimal]\npolicy by π [∗] . For any function V : S → R, we denote [PhV ](s, a) = Es′∼Ph(·|s,a)V (s [′] ) and\n\n2\n\n[VarhV ](s, a) = [PhV [2] ](s, a) − �[PhV ](s, a)� for simplicity. For any function f : S × A → R,\nwe define f (s) = maxa f (s, a). For any function f : S → R [3], we define the Bellman operator\nTh as [Thf ](sh, ah) = Esh+1∼Ph(·|sh,ah) [rh(sh, ah) + f (sh+1)]. Based on this definition, for every\nstage h ∈ [H] and policy π, we have the following Bellman equation for value functions Q [π] h [(][s, a][)]\nand Vh [π][(][s][)][, as well as the Bellman optimality equation for optimal value functions:]\nQ [π] h [(][s][h][, a][h][) = [][T][h][V] h [ π] +1 [](][s][h][, a][h][)][, Q] h [∗] [(][s][h][, a][h][) = [][T][h][V] h [ ∗] +1 [](][s][h][, a][h][)][,]\nwhere VH [π] +1 [(][s][) =][ V] H [ ∗] +1 [(][s][) = 0][. We also define the Bellman operator for second moment as]\n\n2 [�]\n\n[T2,hf ](sh, ah) = Esh+1∼Ph(·|sh,ah) ��rh(sh, ah) + f (sh+1)� . For simplicity, we omit the subscripts h in the Bellman operator without causing confusion.\n\n\nOffline Reinforcement Learning: In offline RL, the agent only has access to a batch-dataset\nD = {s [k] h [, a][k] h [, r] h [k] [:][ h][ ∈] [[][H][]][, k][ ∈] [[][K][]][}][, which is collected by a behavior policy][ µ][, and the agent]\ncannot interact with the environment. We also make the compliance assumption of the dataset:\n\nAssumption 3.1. For a dataset D = {(s [k] h [, a][k] h [, r] h [k][)][}][K,H] k,h=1 [, let][ P][D][ be the joint distribution of the data]\ncollecting process. We say D is compliant with an underlying MDP (S, A, H, P, r) if\n\nPD(rh [k] [=][ r][′][, s][k] h+1 [=][ s][′][ | {][(][s][j] h [, a][j] h [)][}][k] j=1 [,][ {][(][r] h [j] [, s][j] h+1 [)][}] j [k] =1 [−][1][)]\n\n= Ph(rh(sh, ah) = r [′], sh+1 = s [′] | sh = s [k] h [, a][h] [=][ a][k] h [)]\nfor all r [′] ∈ [0, 1], s [′] ∈S, h ∈ [H] and k ∈ [K].\nThis assumption is common in offline RL and has also been made in Jin et al. (2021b); Zhong et al.\n(2022). When the dataset originates from one behavior policy, this assumption naturally holds. In\nthis paper, we assume our dataset is generated by a single behavior policy µ. In addition, for each\nstage h, we denote the induced distribution of the state-action pair by d [µ] h [.]\nGiven the batch dataset, the goal of offline RL is finding a near-optimal policy π that minimizes the\nsuboptimality V1 [∗][(][s][)][ −] [V] 1 [ π][(][s][)][. For simplicity, and when there’s no risk of confusion, we will use]\nthe shorthand notation zh [k] [= (][s][k] h [, a][k] h [)][ to denote the state-action pair in the dataset up to stage][ h][ and]\nepisode k.\n\nGeneral Function Approximation: Given a general function class {Fh}h∈[H], where each function class Fh is composed of functions fh : S × A → [0, L]. For simplicity, we assume L = O(H)\nthroughout the paper. We make the following assumptions on the function class.\nAssumption 3.2 (ǫ-realizability under general function approximation). For each stage h ∈ [H],\nthere exists a function fh [∗] [∈F][h][ close to the optimal value function such that][ ∥][f][ ∗] h [−] [Q][∗] h [∥][∞] [≤] [ǫ][.]\nAssumption 3.3 (ǫ-completeness under general function approximation, Agarwal et al. 2023). We\nassume for each stage h ∈ [H], and any function V : S → [0, H], there exists functions fh, f2,h ∈\nFh such that\n\nmax max\n(s,a)∈S×A [|][f][h][(][s, a][)][ −] [[][T][h][V][ ](][s, a][)][| ≤] [ǫ,][ and] (s,a)∈S×A [|][f][2][,h][(][s, a][)][ −] [[][T][2][,h][V][ ](][s, a][)][| ≤] [ǫ.]\n\n\nIn this paper, for simplicity, we assume that the function class is finite and denote its cardinality by\nN = maxh∈[H] |Fh|. For infinite function classes, we can use the covering number to replace the\ncardinality. Note that the covering number will be reduced to the cardinality when the function class\nis finite.\nWe introduce the following definition (D [2] -divergence) to quantify the disparity of a given point\nz = (s, a) from the historical dataset Dh. It is a reflection of the uncertainty of the dataset.\nDefinition 3.4. For a function class Fh consisting of functions fh : S × A → R and Dh =\n{(s [k] h [, a][k] h [, r] h [k][)][}][k][∈][[][K][]][ as a dataset that corresponds to the observations collected up to stage][ h][ in the]\nMDP, we introduce the following D [2] -divergence:\n\n\n\nDF [2] h [(][z][;][ D][h][;][ σ] h [2][) =] sup\nf1,f2∈Fh\n\n\n\n(f1(z) − f2(z)) [2]\n�k∈[K] (σh(1zh [k][))][2][ (][f][1][(][z] h [k][)][ −] [f][2][(][z] h [k][))][2][ +][ λ,]\n\n\n\nwhere σh [2][(][·][,][ ·][) :][ S × A →] [R][ is a weight function.]\n\n3In this paper, we slightly abuse notation f for both f (s) and f (s, a). The context readily clarifies the\nintended meaning, i.e., value function or action-value function.\n\n\n4\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nRemark 3.5. This definition signifies the extent to which the behavior of functions within the function class can deviate at the point z = (s, a), based on their difference in the historical dataset. It can\nbe viewed as the generalization of the weighted elliptical norm ∥φ(s, a)∥Σ−h 1 in linear case, where\n\nφ is the feature map and Σh is defined as [�] k∈[K] [σ] h [−][2][(][s] h [k][, a][k] h [)][φ][(][s][k] h [, a][k] h [)][φ][(][s][k] h [, a][k] h [)][⊤] [+][ λ][I][. Similar]\nideas have been used to define the Generalized Eluder dimension for online RL (Gentile et al., 2022;\nAgarwal et al., 2023; Ye et al., 2023). In these works, the summation is over a sequence up to the\nk-th episode rather than the entire historical dataset.\n\n\nData Coverage Assumption: In offline RL, there exists a discrepancy between the state-action\ndistribution generated by the behavior policy and the distribution from the learned policy. Under\nthis situation, the distribution shift problem can cause the learned policy to perform poorly or even\nfail in offline RL. In this work, we consider the following data coverage assumption to control the\ndistribution shift.\n\nAssumption 3.6 (Uniform Data Coverage). there exists a constant κ > 0, such that for any stage h\nand functions f1, f2 ∈Fh, the following inequality holds,\n\n\n2\nEd [µ] h ��f1(sh, ah) − f2(sh, ah)� � ≥ κ∥f1 − f2∥∞ [2] [,]\n\n\nwhere the state-action pair (at stage h) (sh, ah) is stochasticly generated from the induced distribution d [µ] h [.]\nRemark 3.7. Similar uniform coverage assumptions have also been considered in Wang et al.\n(2020a); Min et al. (2021); Yin et al. (2022a); Xiong et al. (2023); Yin et al. (2022b). Among these\nworks, Yin et al. (2022b) is the most related to ours, proving an instance-dependent regret bound\nunder general function approximation. Here we make a comparison with their assumption. In detail,\nYin et al. (2022b) considered the differentiable function class, which is defined as follows\n\nF := �f �θ, φ(·, ·)� : S × A → R, θ ∈ Θ�.\n\n\nThey introduced the following coverage assumption such that for all stage h ∈ [H], there exists a\nconstant κ,\n\n\n\nEd [µ] h\n\n\n\n2 [�]\n��f (θ1, φ(s, a)) − f (θ2, φ(s, a))� ≥ κ∥θ1 − θ2∥2 [2][,][ ∀][θ][1][,][ θ][2] [∈] [Θ;] (∗)\n\n\n\nEd [µ] h �∇f (θ, φ(s, a))∇f (θ, φ(s, a)) [⊤][�] ≻ κI, ∀θ ∈ Θ. (∗∗)\n\n\nWe can prove that our assumption is weaker than the first assumption (*), and we do not need the\nsecond assumption (**). This suggests that the differentiable function class studied in Yin et al.\n(2022b) is an example covered by our general function class.\nIn addition, in the special case of linear function class, the coverage assumption in Yin et al. (2022b)\nwill reduce to the following linear function coverage assumption (Wang et al., 2020a; Min et al.,\n2021; Yin et al., 2022a; Xiong et al., 2023).\n\nλmin(Ed [µ] h [[][φ][(][s, a][)][φ][(][s, a][)][⊤][]) =][ κ >][ 0][,][ ∀][h][ ∈] [[][H][]][.]\n\n\nTherefore, our assumption is also weaker than the linear function coverage assumption when dealing with the linear function class. Due to space limitations, we defer a detailed comparison to\nAppendix B.\n\nRemark 3.8. Many works such as Uehara & Sun (2021); Xie et al. (2021a); Cheng et al. (2022);\nOzdaglar et al. (2023); Rashidinejad et al. (2022); Zhu et al. (2023) adopted a weaker partial coverage assumption than ours, where the ℓ∞ norm on the right hand is replaced with the expectation\nover a distribution corresponding to a single policy, typically the optimal one. Their assumption,\nhowever, generally confines their results to worst-case scenarios. It is unclear if we can still prove\nthe instance-dependent regret under their assumption. We will explore it in the future.\n\n4 ALGORITHM\n\nIn this section, we provide a comprehensive and detailed description of our algorithm (PNLSVI), as\ndisplayed in Algorithm 1. In the sequel, we introduce the key ideas of the proposed algorithm.\n4.1 PESSIMISTIC VALUE ITERATION BASED PLANNING\nOur algorithm operates in two distinct phases, the Variance Estimate Phase and the Pessimistic\nPlanning Phase. At the beginning of the algorithm, the dataset is divided into two independent\ndisjoint subsets D, D [s] with equal size K, and each is assigned to a specific phase.\n\n\n5\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nAlgorithm 1 Pessimistic Nonlinear Least-Squares Value Iteration (PNLSVI)\n\n\nRequire: Input confidence parameters β [¯] h, βh and ǫ > 0.\n1: Initialize: Split the input dataset into D = {s [k] h [, a][k] h [, r] h [k][}][K,H] k,h=1 [,][ s][D][ =][ {][s][¯] h [k][,][ ¯][a][k] h [,][ ¯][r] h [k][}][K,H] k,h=1 [; Set the]\n\nvalue function f [�] H+1(·) = f [q] H+1(·) = 0.\n2: //Constructing the variance estimator\n3: for stage h = H, . . ., 1 do\n\n4: f¯h = argminfh∈Fh �k∈[K] �fh(¯s [k] h [,][ ¯][a][k] h [)][ −] [r][¯] h [k] [−] [f][q][h][+1][(¯][s][k] h+1 [)] �2.\n\n\n\n5: g¯h = argmingh∈Fh �k∈[K]\n\n\n\n2 [�][2]\n�gh(¯s [k] h [,][ ¯][a][k] h [)][ −] �r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] � .\n\n\n\n6:7: Calculate a bonus functionfqh ←{ ¯fh − ¯bh − ǫ}[0,H−h [¯] b+1]h with confidence parameter; β [¯] h,\n8: Construct the variance estimator\n\n\n\nσ�h [2][(][s, a][) = max] 1, ¯gh(s, a) − ( f [¯] h(s, a)) [2] − O √\n� �\n\n\n\nlog(N·Nb)H [3]\n\n\n\n.\n��\n\n\n\n√\n\n\n\nKκ\n\n\n\n9: end for\n10: //Pessimistic value iteration based planning\n11: for stage h = H, . . ., 1 do\n\n\n\n� 1\n12: fh = argminfh∈Fh �k∈[K] σ�h [2] [(][s][k] h [,a][k] h [)]\n\n\n\n2\n�fh(s [k] h [, a][k] h [)][ −] [r] h [k] [−] [f][�][h][+1][(][s][k] h+1 [)] �\n\n\n\n14:13: fCalculate a bonus function�h ←{ �fh − bh − ǫ}[0,H−h b+1]h with bonus parameter; βh;\n\n\n�\n15: πh(·|s) = argmaxa f [�] h(s, a).\n16: end for\n17: Output: �π = {π�h} [H] h=1 [.]\n\n\nThe basic framework of our algorithm follows the pessimistic value iteration, which was initially\nintroduced by Jin et al. (2021b). In details, for each stage h ∈ [H], we construct the estimator value\nfunction f [�] h by solving the following variance-weighted ridge regression (Line 13):\n\n\n\n2\nfh(s [k] h [, a][k] h [)][ −] [r] h [k] [−] [f][�][h][+1][(][s][k] h+1 [)],\n� �\n\n\n\n�\nfh = argmin\nfh∈Fh\n\n\n\n�\n\nk∈[K]\n\n\n\n1\n\n�\nσh [2][(][s][k] h [, a][k] h [)]\n\n\n\nwhere �σh [2] [is the estimated variance and will be discussed in Section 4.2. In Line 14, we subtract the]\nconfidence bonus function bh from the estimator value function f [�] h to construct the pessimistic value\nfunction f [�] h. With the help of the confidence bonus function bh, the pessimistic value function f [�] h is\nalmost a lower bound for the optimal value function fh [∗][. The details of the bonus function will be]\ndiscussed in Section 4.3.\nBased on the pessimistic value function f [�] h for horizon h, we recursively perform the value iteration\nfor the horizon h − 1. Finally, we use the pessimistic value function f [�] h to do planning and output\nthe greedy policy with respect to the pessimistic value function f [�] h (Lines 15 - 17).\n4.2 VARIANCE ESTIMATOR\nIn this phase, we provide an estimator for the variance �σh in the weighted ridge regression. We\nconstruct this variance estimator with D [s], thus independent of D. Using a larger bonus function [¯] bh,\nwe conduct a pessimistic value iteration process similar to that discussed in Section 4.1 and obtain\na more crude estimated value function {f [q] h}h∈[H]. According to the definition of Bellman operators\nT and T2, the variance of the function f [q] h+1 for each state-action pair (s, a) can be denoted by\n\n\n2\n\n[Varhf [q] h+1](s, a) = [T2,hf [q] h+1](s, a) − [Thf [q] h+1](s, a) .\n� �\n\n\nTherefore, we need to estimate the first-order and second-order moments for f [q] h+1. We perform\nnonlinear least-squares regression separately for each of these moments. Specifically, in Line 4, we\nconduct regression to estimate the first-order moment.\n\n\n\n2\nfh(¯s [k] h [,][ ¯][a][k] h [)][ −] [r][¯] h [k] [−] [f][q][h][+1][(¯][s][k] h+1 [)] .\n� �\n\n\n6\n\n\n\n¯\nfh = argmin\nfh∈Fh\n\n\n\n�\n\nk∈[K]\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nIn Line 5, we perform regression for estimating the second-order moment.\n\n\n\n2 [�][2]\n\n¯\ngh(¯s [k] h [,][ ¯][a] h [k][)][ −] rh [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] .\n� � �\n\n\n\ng¯h = argmin\ngh∈Fh\n\n\n\n�\n\nk∈[K]\n\n\n\nIn this phase, we set the variance function to 1 for each state-action pair (s, a). Combing these two\nregression results and subtracting some perturbing terms (We will discuss in Section 6.1), we create\na pessimistic estimator for the variance function (Lines 7 to 8).\n4.3 NONLINEAR BONUS FUNCTION\nAs we discuss in Sections 4.1 and 4.2, following Wang et al. (2020b); Kong et al. (2021);\nAgarwal et al. (2023), we introduce a bonus function. This function is designed to account for\nthe functional uncertainty, enabling us to develop a pessimistic estimate of the value function. Ideally, we hope to choose bh(·, ·) = βhDFh(·, ·; Dh; �σh [2][)][, where][ β][h][ is the confidence parameter and]\nDFh(·, ·; Dh; �σh [2][)][ is defined in Definition 3.4. However, the][ D][2][-divergence composes a complex]\nfunction class, and its calculation involves solving a complex optimization problem. To address this\nissue, following Agarwal et al. (2023), we assume there exists a function class W with cardinally\n|W| = Nb and can approximate the D [2] -divergence well. For the parameters βh, λ ≥ 0, error parameter ǫ ≥ 0, taking a variance function σh(·, ·) : S × A → R and f [�] h ∈Fh as input, we can get a\nbonus function bh(·, ·) ∈W satisfying the following properties:\n\n\n\n\n- bh(zh) ≥ max |fh(zh) − f [�] h(zh)|, fh ∈Fh : [�]\n�\n\n\n\nk∈[K] (fh(σ(�zhh [k] ( [)] s [−][k] hf [,a][�] h [k] h( [))] zh [k][2][))][2] ≤ (βh) [2][�] for any zh ∈S × A.\n\n\n\n\n- bh(zh) ≤ C · �DFh (zh; Dh; �σh [2][)][ ·] �\n\n\n\n(βh) [2] + λ + ǫβh for all zh ∈S × A with constant 0 < C < ∞.\n�\n\n\n\nWe can implement the function class W with bounded Nb by extending the online subsampling\nframework presented in Wang et al. (2020b); Kong et al. (2021) to an offline dataset. Additionally,\nusing Algorithm 1 of Kong et al. (2021), we can calculate the bonus function with finite calls of the\noracle for solving the variance-weighted ridge regression problem. We leave a detailed discussion\nto Appendix C.\n5 MAIN RESULTS\n\nIn this section, we prove an instance-dependent regret bound of Algorithm 1.\n\n\n� log(N·Nb)H [6]\nTheorem 5.1. Under Assumption 3.6, for K ≥ Ω κ [2], if we set the parameters\n� �\n\nβ1 [′],h [, β] 2 [′],h [=][ �][O][(] �log(N · Nb)H [2] ) and βh = O [�] ( [√] log N ) in Algorithm 1, then with probability\n\nat least 1 − δ, for any state s ∈S, we have\n\nV1 [∗][(][s][)][ −] [V] 1 [ �][π][(][s][)][ ≤] [O][�][(] �log N ) [�][H] h=1 [E][π][∗] [�] DFh(zh; Dh; [VhVh [∗] +1 [](][·][,][ ·][))][|][s][1][ =][ s] �,\n\n\nwhere [VhVh [∗] +1 [](][s, a][) = max][{][1][,][ [][Var][h][V] h [ ∗] +1 [](][s, a][)][}][ is the truncated conditional variance.]\nThis theorem establishes an upper bound for the suboptimality of our policy �π. The bound depends\non the expected uncertainty, which is characterized by the weighted D [2] -divergence along the trajectory, marking itself as an instance-dependent result. It’s noteworthy that both the trajectory and the\nweight function are based on the optimal policy and the optimal value function, respectively. This\nbound necessitates that the dataset size K is sufficiently large. Furthermore, all parameters are determined solely by the complexity of the function class, the horizon length H, and the data coverage\nassumption constant κ, regardless of the dataset’s composition.\nORemark 5.2.�( [√] log N ), which is better than that in Yin et al. (2022b), which scales as In Theorem 5.1, the dependence on the cardinality of the function class scales as �O(log N ). This improved dependence is due to the reference-advantage decomposition (discussed in Section 6.2),\nwhich avoids the unnecessary covering argument. Thus we resolve the open problem raised by\nYin et al. (2022b), i.e., how to achieve the O [�] ( [√] log N ) dependence.\n\nRemark 5.3. When specialized to linear MDPs (Jin et al., 2020), the following function class\n\n\nFh [lin] [=][ {⟨][φ][(][·][,][ ·][)][,][ θ][h][⟩] [:][ θ][h] [∈] [R][d][,][ ∥][θ][h][∥][2] [≤] [B][h][}][ for any][ h][ ∈] [[][H][]][,]\n\n\nsuffices and satisfies the completeness assumption (Assumption 3.3). Let Fh [lin][(][ǫ][)][ be an][ ǫ][-net of the]\nlinear function class Fh [lin][, with][ log][ |F] h [lin][(][ǫ][)][|][ =][ �][O][(][d][)][. The dependency of the function class will]\nreduce to O [�] ( [√] log N ) = O [�] (√d). For linear function class, we can prove the following inequality:\n\nDF linh [(][ǫ][)][(][z][;][ D][h][; [][V][h][V] h [ ∗] +1 [](][·][,][ ·][))][ ≤∥][φ][(][z][)][∥] Σ [∗−] h [1],\n\n\n7\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nwhere Σ [∗] h [=][ �] k∈[K] [φ][(][s] h [k] [, a][k] h [)][φ][(][s][k] h [, a][k] h [)][⊤][/][[][V][h][V] h [ ∗] +1 [](][s] h [k] [, a][k] h [)+][λ][I][. Therefore, our regret guarantee]\nin Theorem 5.1 is reduced to\n\n\n\nV1 [∗][(][s][)][ −] [V] 1 [ �][π][(][s][)][ ≤] [O][�][(] √\n\n\n\nd) ·\n\n\n\nH\n� Eπ∗ �∥φ(sh, ah)∥Σ∗−h 1|s1 = s�,\n\nh=1\n\n\n\nwhich matches the lower bound proved in Xiong et al. (2023). This suggests that our algorithm is\noptimal for linear MDPs.\n\n\n6 KEY TECHNIQUES\nIn this section, we provide an overview of the key techniques in our algorithm design and analysis.\n6.1 VARIANCE ESTIMATOR WITH NONLINEAR FUNCTION CLASS\nIn our work, we extend the technique of variance-weighted ridge regression, first introduced\nin Zhou et al. (2021) for online RL, and later used by Min et al. (2021); Yin et al. (2022a;b);\nXiong et al. (2023) for offline RL with linear MDPs, to general nonlinear function class F . We\nuse the following nonlinear least-squares regression to estimate the underlying value function:\n\n\n\n2\nfh(s [k] h [, a][k] h [)][ −] [r] h [k] [−] [f][�][h][+1][(][s][k] h+1 [)] .\n� �\n\n\n\n�\nfh = argmin\nfh∈Fh\n\n\n\n�\n\nk∈[K]\n\n\n\n1\n\n�\nσh [2][(][s][k] h [, a][k] h [)]\n\n\n\nFor this regression, it is crucial to obtain a reliable evaluation for the variance of the estimated\ncumulative reward rh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][. As discussed in Section 4.2, we use][ s][D][ to construct a variance]\nestimator independent from D. According to the definition of Bellman operators T and T2, the\nvariance of the function f [q] h+1 for each state-action pair (s, a) can be denoted by\n\n\n2\n\n[Varhf [q] h+1](s, a) = [T2,hf [q] h+1](s, a) − [Thf [q] h+1](s, a) .\n� �\n\n\nIn our algorithm, we perform nonlinear least-squares regression on D [s] . For simplicity, we denote\n¯ 2\nthe empirical variance as Bh(s, a) = ¯gh(s, a) − �fh(s, a)�, and the difference between empirical\nvariance Bh(s, a) and actual variance [Varhf [q] h+1](s, a) is upper bound by\n���Bh(s, a) − [Varh qfh+1](s, a)��� ≤ ���g¯h(s, a) − [T2,h qfh+1](s, a)��� + �����fh(s, a)�2 − �[Thf [q] h+1](s, a)�2 [�] ��.\n\n\nFor these nonlinear function estimators, the following lemmas provide coarse concentration properties for the first and second order Bellman operators.\n\nLemma 6.1. For any stage h ∈ [H], let f [q] h+1(·, ·) ≤ H be the estimated value function constructed\nin Algorithm 1 Line 7. By utilizing Assumption 3.3, there exists a function f [¯] h [′] [∈F][h][, satisfying that]\n|f [¯] h [′] [(][z][h][)][ −] [[][T][h][ q][f][h][+1][](][z][h][)][| ≤] [ǫ][ holds for all state-action pair][ z][h][ = (][s][h][, a][h][)][. Then with probability at]\nleast 1 − δ/(4H), it holds that\n�k∈[K]�f¯ ′h [(¯][z] h [k][)][ −] [f][¯][h][(¯][z] h [k][)] �2 ≤ (¯β1,h)2,\n\n\nwhere β [¯] 1,h = O [�] log(N · Nb)H, and f [¯] h is the estimated function for first-moment Bellman\n�� �\n\noperator (Line 4 in Algorithm 1).\n\nLemma 6.2. For any stage h ∈ [H], let f [q] h+1(·, ·) ≤ H be the estimated value function constructed\nin Algorithm 1 Line 7. By utilizing Assumption 3.3, there exists a function ¯gh [′] [∈F][h][, satisfying that]\n|g¯h [′] [(][z][h][)][ −] [[][T][2][,h][ q][f][h][+1][](][z][h][)][| ≤] [ǫ][ holds for all state-action pair][ z][h][ = (][s][h][, a][h][)][. Then with probability]\nat least 1 − δ/(4H), it holds that\n\n�k∈[K]�g¯h [′] [(¯][z] h [k][)][ −] [g][¯][h][(¯][z] h [k][)] �2 ≤ (¯β2,h)2,\n\n\nwhere β [¯] 2,h = O [�] ��log(N · Nb)H [2][�], and ¯gh is the estimated function for second-moment Bellman\n\noperator (Line 5 in Algorithm 1).\n\nNotice that all of the previous analysis focuses on the estimated function f [q] h+1. By leveraging an induction procedure similar to existing works in the linear case (Jin et al., 2021b), we\ncan control the distance between the estimated function f [q] h+1 and the optimal value function\n\n\n8\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nfh [∗] +1 [.] In details, with high probability, for all stage h ∈ [H], the distance is upper bounded\n\n\n\nby O [�]\n��\n\n\n\nKκ . This result allows us to further bound the difference between\n�\n\n\n\nlog(N · Nb)H [3] /√\n\n\n\n\n[Varhf [q] h+1](s, a) and [Varhfh [∗] +1 [](][s, a][)][.]\nTherefore, the concentration properties in Lemmas 6.1 and 6.2 enable us to add some perturbation\nterms and construct a variance estimator, which satisfies the following property:\n\n\n\n\n[VhVh [∗] +1 [](][s, a][)][ −] [O][�]\n� [�]\n\n\n\nlog(N · Nb)H [3]\n\n\n\n�\n≤ σh [2][(][s, a][)][ ≤] [[][V][h][V] h [ ∗] +1 [](][s, a][)][.] (6.1)\n�\n\n\n\n√Kκ\n\n\n\nwhere [VhVh [∗] +1 [](][s, a][) = max][{][1][,][ [][Var][h][V] h [ ∗] +1 [](][s, a][)][}][ is the truncated conditional variance.]\n6.2 REFERENCE-ADVANTAGE DECOMPOSITION\nTo obtain the optimal dependency on the function class complexity, we need to tackle the challenge\nof additional error from uniform concentration over the whole function class Fh. To address this\nproblem, we utilize the so-called reference-advantage decomposition technique, which was used by\n(Xiong et al., 2023) to achieve the optimal regret for offline RL with linear function approximation.\nWe generalize this technique to nonlinear function approximation. We provide detailed insights into\nthis approach as follows:\n\n\nrh(sh, ah) + f [�] h+1(sh+1) − [Thf [�] h+1](sh, ah) = rh(sh, ah) + fh [∗] +1 [(][s][h][+1][)][ −] [[][T][h][f][ ∗] h+1 [](][s][h][, a][h][)]\n� �� �\nReference uncertainty\n\n\n\n+ f [�] h+1(sh+1) − fh [∗] +1 [(][s][h][+1][)][ −] [([][P][h][f][�][h][+1][](][s][h][, a][h][)][ −] [[][P][h][f][ ∗] h+1 [](][s][h][, a][h][))]\n� �� �\nAdvantage uncertainty\n\n\n\n.\n\n\n\nWe decompose the Bellman error into two parts: the Reference uncertainty and the Advantage uncertainty. For the first term, the optimal value function fh [∗] +1 [is fixed and not related to the pre-]\ncollected dataset, which circumvents additional uniform concentration over the whole function class\nand avoids any dependence on the function class complexity. For the second term, it is worth noticing that the distance between the estimated function f [�] h+1 and the optimal value function fh [∗] [de-]\ncreases with the speed of O(1/√Kκ). Though, we still need to maintain the uniform convergence\n\nguarantee, the Advantage uncertainty is dominated by the Reference uncertainty when the number\nof episodes K is large enough. Such an analysis approach has been first studied in the online RL setting (Azar et al., 2017; Zhang et al., 2021; Hu et al., 2022; He et al., 2022; Agarwal et al., 2023) and\nlater in the offline environment by Xiong et al. (2023). Previous works, such as Yin et al. (2022b),\ndidn’t adapt the reference-advantage decomposition analysis to their nonlinear function class, resulting in a parameter space dependence that scales with d, instead of the optimal √d. By integrating\n\nthese results, we can prove a variance-weighted concentration inequality for Bellman operators.\nLemma 6.3.� For each stage h ∈ [H], assuming the variance estimator �σh satisfies (6.1), let\nfh+1(·, ·) ≤ H be the estimated value function constructed in Algorithm 1 Line 14. Then with\n\n2\nprobability at least 1−δ/(4H), it holds that [�] k∈[K] (σ�h(1zh [k][))][2] �[Thf [�] h+1](zh [k][)][ −] [f][�][h][(][z] h [k][)] � ≤ (βh) [2],\n\nwhere βh = O [�] ( [√] log N ) and f [�] h is the estimated function from the weighted ridge regression (Line\n12 in Algorithm 1).\n\nAfter controlling the Bellman error, with a similar argument to Jin et al. (2021b), we obtain the\nfollowing lemma, which provides an upper bound for the regret.\n\n\n�\nLemma 6.4 (Regret Decomposition Property). If [Th �fh+1](z) − fh(z) ≤ bh(z) holds for all stage\n��� ���\n\nh ∈ [H] and state-action pair z = (s, a) ∈S × A, then the suboptimality of the output policy �π in\nAlgorithm 1 can be bounded as\n\n\nV1 [∗][(][s][)][ −] [V] 1 [ �][π][(][s][)][ ≤] [2][�][H] h=1 [E][π][∗] [[][b][h][ (][s][h][, a][h][)][ |][ s][1][ =][ s][]][ .]\n\n\nHere, the expectation Eπ∗ is with respect to the trajectory induced by π [∗] in the underlying MDP.\n\nCombining the results in Lemmas 6.3 and 6.4, we have proved Theorem 5.1.\n7 CONCLUSION AND FUTURE WORK\n\nIn this paper, we present Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), an oracleefficient algorithm for offline RL with non-linear function approximation. Our result matches the\n\n\n9\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nlower bound proved in Xiong et al. (2023) when specialized to linear function approximation. We\nnotice that our uniform coverage assumption can sometimes be strong in practice. In our future\nwork, we plan to relax this assumption by devising algorithms for nonlinear function classes under\na partial coverage assumption.\nACKNOWLEDGEMENTS\n\nWe thank the anonymous reviewers and area chair for their helpful comments. QD, HZ, JH and\nQG are supported in part by the National Science Foundation CAREER Award 1906169, CPS2312094, Amazon Research Award and the Sloan Research Fellowship. JH is also supported in part\nby Amazon PhD Fellowship. The views and conclusions contained in this paper are those of the\nauthors and should not be interpreted as representing any funding agencies.\n\nREFERENCES\n\nAlekh Agarwal and Tong Zhang. Model-based rl with optimistic posterior sampling: Structural\nconditions and sample complexity. arXiv preprint arXiv:2206.07659, 2022.\n\n\nAlekh Agarwal, Yujia Jin, and Tong Zhang. Vo q l: Towards optimal regret in model-free rl with\nnonlinear function approximation. In The Thirty Sixth Annual Conference on Learning Theory,\npp. 987–1063. PMLR, 2023.\n\n\nAlex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement\nlearning with value-targeted regression. In International Conference on Machine Learning, pp.\n463–474. PMLR, 2020.\n\n\nMohammad Gheshlaghi Azar, Ian Osband, and R´emi Munos. Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning, pp. 263–272. PMLR, 2017.\n\n\nNicolo Cesa-Bianchi and G´abor Lugosi. Prediction, learning, and games. Cambridge university\npress, 2006.\n\n\nJinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.\nIn International Conference on Machine Learning, pp. 1042–1051. PMLR, 2019.\n\n\nChing-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic\nfor offline reinforcement learning. In International Conference on Machine Learning, pp. 3852–\n3878. PMLR, 2022.\n\n\nChristoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E\nSchapire. On oracle-efficient pac rl with rich observations. Advances in neural information\nprocessing systems, 31, 2018.\n\n\nJonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco\nCarpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414–\n419, 2022.\n\n\nSimon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong\nWang. Bilinear classes: A structural framework for provable generalization in rl. In International\nConference on Machine Learning, pp. 2826–2836. PMLR, 2021.\n\n\nSimon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufficient\nfor sample efficient reinforcement learning? arXiv preprint arXiv:1910.03016, 2019.\n\n\nDylan Foster, Alekh Agarwal, Miroslav Dud´ık, Haipeng Luo, and Robert Schapire. Practical contextual bandits with regression oracles. In International Conference on Machine Learning, pp.\n1539–1548. PMLR, 2018.\n\n\nDylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of\ninteractive decision making. arXiv preprint arXiv:2112.13487, 2021.\n\n\nClaudio Gentile, Zhilei Wang, and Tong Zhang. Achieving minimax rates in pool-based batch active\nlearning. In International Conference on Machine Learning, pp. 7339–7367. PMLR, 2022.\n\n\n10\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nShixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for\nrobotic manipulation with asynchronous off-policy updates. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 3389–3396. IEEE, 2017.\n\n\nJiafan He, Dongruo Zhou, and Quanquan Gu. Logarithmic regret for reinforcement learning with\nlinear function approximation. In International Conference on Machine Learning, pp. 4171–4180.\nPMLR, 2021.\n\n\nJiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement\nlearning for linear markov decision processes. arXiv preprint arXiv:2212.06132, 2022.\n\n\nPihe Hu, Yu Chen, and Longbo Huang. Nearly minimax optimal reinforcement learning with linear function approximation. In International Conference on Machine Learning, pp. 8971–9019.\nPMLR, 2022.\n\n\nNan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low bellman rank are pac-learnable. In International Conference on\nMachine Learning, pp. 1704–1713. PMLR, 2017.\n\n\nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement\nlearning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.\nPMLR, 2020.\n\n\nChi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of rl\nproblems, and sample-efficient algorithms. Advances in neural information processing systems,\n34:13406–13418, 2021a.\n\n\nYing Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In\nInternational Conference on Machine Learning, pp. 5084–5096. PMLR, 2021b.\n\n\nDingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin F Yang. Online sub-sampling for\nreinforcement learning with general function approximation. arXiv preprint arXiv:2106.07203,\n2021.\n\n\nAkshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daum´e III, and John Langford. Active learning for cost-sensitive classification. In International Conference on Machine Learning,\npp. 1915–1924. PMLR, 2017.\n\n\nSergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning handeye coordination for robotic grasping with deep learning and large-scale data collection. The\nInternational journal of robotics research, 37(4-5):421–436, 2018.\n\n\nGen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei. Settling the sample complexity of\nmodel-based offline reinforcement learning. arXiv preprint arXiv:2204.05275, 2022.\n\n\nYifei Min, Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Variance-aware off-policy evaluation\nwith linear function approximation. Advances in neural information processing systems, 34:7598–\n7610, 2021.\n\n\nAditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement\nlearning using linearly combined model ensembles. In International Conference on Artificial\nIntelligence and Statistics, pp. 2010–2020. PMLR, 2020.\n\n\nThanh Nguyen-Tang and Raman Arora. Viper: Provably efficient algorithm for offline rl with neural\nfunction approximation. In The Eleventh International Conference on Learning Representations,\n2023.\n\n\nThanh Nguyen-Tang, Ming Yin, Sunil Gupta, Svetha Venkatesh, and Raman Arora. On instancedependent bounds for offline reinforcement learning with linear function approximation. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 9310–9318, 2023.\n\n\nAsuman E Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang. Revisiting the linearprogramming framework for offline rl with general function approximation. In International\nConference on Machine Learning, pp. 26769–26791. PMLR, 2023.\n\n\n11\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nParia Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. Advances in Neural Information\nProcessing Systems, 34:11702–11716, 2021.\n\n\nParia Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao. Optimal conservative offline rl with general function approximation via augmented lagrangian. In The Eleventh\nInternational Conference on Learning Representations, 2022.\n\n\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,\ngo, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.\n\n\nLaixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for offline\nreinforcement learning: Towards optimal sample complexity. In International Conference on\nMachine Learning, pp. 19967–20025. PMLR, 2022.\n\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go\nwithout human knowledge. nature, 550(7676):354–359, 2017.\n\n\nWen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based\nrl in contextual decision processes: Pac bounds and exponential improvements over model-free\napproaches. In Conference on learning theory, pp. 2898–2933. PMLR, 2019.\n\n\nMasatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under\npartial coverage. In International Conference on Learning Representations, 2021.\n\n\nRuosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of offline rl with\nlinear function approximation? arXiv preprint arXiv:2010.11895, 2020a.\n\n\nRuosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value\nfunction approximation: Provably efficient approach via bounded eluder dimension. Advances in\nNeural Information Processing Systems, 33:6123–6135, 2020b.\n\n\nYining Wang, Ruosong Wang, Simon Shaolei Du, and Akshay Krishnamurthy. Optimism in reinforcement learning with generalized linear function approximation. In International Conference\non Learning Representations, 2020c.\n\n\nGell´ert Weisz, Philip Amortila, and Csaba Szepesv´ari. Exponential lower bounds for planning in\nmdps with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory,\npp. 1237–1264. PMLR, 2021.\n\n\nTengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent\npessimism for offline reinforcement learning. Advances in neural information processing systems,\n34:6683–6694, 2021a.\n\n\nTengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging\nsample-efficient offline and online reinforcement learning. Advances in neural information processing systems, 34:27395–27407, 2021b.\n\n\nWei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, Liwei Wang, and Tong Zhang. Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent mdp\nand markov game. In International Conference on Learning Representations (ICLR), 2023.\n\n\nLin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.\nIn International Conference on Machine Learning, pp. 6995–7004, 2019.\n\n\nLin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and\nregret bound. In International Conference on Machine Learning, pp. 10746–10756. PMLR, 2020.\n\n\nChenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang. Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In International\nConference on Machine Learning, pp. 39834–39863. PMLR, 2023.\n\n\n12\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nMing Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism. Advances in neural information processing systems, 34:4065–4078, 2021.\n\n\nMing Yin, Yaqi Duan, Mengdi Wang, and Yu-Xiang Wang. Near-optimal offline reinforcement learning with linear representation: Leveraging variance information with pessimism. arXiv preprint\narXiv:2203.05804, 2022a.\n\n\nMing Yin, Mengdi Wang, and Yu-Xiang Wang. Offline reinforcement learning with differentiable\nfunction approximation is provably efficient. In The Eleventh International Conference on Learning Representations, 2022b.\n\n\nAndrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and Alessandro Lazaric.\nFrequentist regret bounds for randomized least-squares value iteration. In International Conference on Artificial Intelligence and Statistics, pp. 1954–1964. PMLR, 2020a.\n\n\nAndrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning,\npp. 10978–10989. PMLR, 2020b.\n\n\nAndrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. Advances in neural information processing systems, 34:\n13626–13640, 2021.\n\n\nWenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement\nlearning with realizability and single-policy concentrability. In Conference on Learning Theory,\npp. 2730–2775. PMLR, 2022.\n\n\nZihan Zhang, Xiangyang Ji, and Simon Du. Is reinforcement learning more difficult than bandits?\na near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory, pp.\n4528–4531. PMLR, 2021.\n\n\nHan Zhong, Wei Xiong, Jiyuan Tan, Liwei Wang, Tong Zhang, Zhaoran Wang, and Zhuoran Yang.\nPessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets.\nIn International Conference on Machine Learning, pp. 27117–27142. PMLR, 2022.\n\n\nDongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In Conference on Learning Theory, pp. 4532–\n4576. PMLR, 2021.\n\n\nHanlin Zhu, Paria Rashidinejad, and Jiantao Jiao. Importance weighted actor-critic for optimal\nconservative offline reinforcement learning. arXiv preprint arXiv:2301.12714, 2023.\n\n\n13\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nA COMPARISON OF OFFLINE RL ALGORITHMS\n\nIn this section, we make a comparison between different offline RL algorithms, concerning their\nalgorithm type, function approximation class, data coverage assumption, used oracles, and regret\ntype. In the upper part are the algorithms with worst-case regret, while in the lower part are those\nwith instance-dependent regret.\n\n\nTable 1: Comparison of offline RL algorithms in terms of algorithm type, function classes, data\ncoverage assumption, types of oracle and regret-type.\n\n\n\nAlgorithm Algorithm\nType\n\n\n\nFunction\n\nClasses\n\n\n\nData Coverage Types of\nOracle\n\n\n\nXie et al.\n(2021a)\n\n\nCPPO-TV\n\nUehara & Sun\n(2021)\n\n\nCORAL\nRashidinejad et al.\n(2022)\n\n\nReformulated LP\nOzdaglar et al.\n(2023)\n\n\nLinPEVI-ADV+\nXiong et al.\n(2023)\n\nPFQL Yin et al.\n(2022b)\n\n\nPNLSVI (Our\nwork)\n\n\n\nBellman\nconsistent\n\nPessimism\n\n\n\nGeneral Partial Optimization on\nPolicy and\nFunction Class\n\n\n\nMLE General Partial Optimization on\nPolicy and\nHypothesis Class\n\n\n\nAugmented\nLagrangian with\nMIS\n\n\n\nGeneral Partial Optimization on\nPolicy and\nFunction Class\n\n\n\nRegret Type\n\n\nWorst-case\n\n\nWorst-case\n\n\nWorst-case\n\n\nWorst-case\n\n\n\nLinear Program General Partial Linear\nProgramming\n\n\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-1.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-2.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-3.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-4.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-5.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-7.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-8.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-9.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-10.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-11.png)\n\nLSVI-type Linear Uniform / Instancedependent\n\n\n\nLSVI-type Differentible Uniform Gradient Oracle\nand Optimization\non the Function\n\nClass\n\n\nLSVI-type General Uniform Regression\nOracle\n\n\n\nInstancedependent\n\n\nInstancedependent\n\n\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-0.png)\n\n![](output/images/5feeabb408da8b4eb0c8bc101064016867ba20d2.pdf-13-6.png)\n\nB COMPARISON OF DATA COVERAGE ASSUMPTIONS\n\nIn Yin et al. (2022b), they studied the general differentiable function class, where the function class\ncan be denoted by\n\nF := �f �θ, φ(·, ·)� : X × A → R, θ ∈ Θ�.\n\n\nIn this definition, Ψ is a compact subset, and φ(·, ·) : X × A → Ψ ⊆ R [m] is a feature map. The\nparameter space Θ is a compact subset Θ ⊆ R [d] . The function f : R [d] × R [m] → R satisfies the\nfollowing smoothness conditions:\n\n   - For any vector φ ∈ R [m], f (θ, φ) is third-time differentiable with respect to the parameter\nθ.\n\n    - Functions f, ∂θf, ∂θ [2],θ [f, ∂] θ [3],θ,θ [f][ are jointly continuous for][ (][θ][,][ φ][)][.]\n\n\nUnder this definition, Yin et al. (2022b) introduce the following coverage assumption (Assumption\n2.3) such that for all stage h ∈ [H], there exists a constant κ,\n\n\n\nEd [µ] h\n\n\n\n2 [�]\n��f (θ1, φ(x, a)) − f (θ2, φ(x, a))� ≥ κ∥θ1 − θ2∥ [2] 2 [,][ ∀][θ][1][,][ θ][2][ ∈] [Θ; (][∗][)]\n\n\n\nEd [µ] h �∇f (θ, φ(x, a))∇f (θ, φ(x, a)) [⊤][�] ≻ κI, ∀θ ∈ Θ.(∗∗)\n\n\nIt is worth noting that our assumption 3.6 is weaker than this assumption. For any compact sets Θ, Ψ\nand continuous function f, there always exist a constant κ0 > 0 such that f is κ0-Lipschitz with\nrespect to the parameter θ,i.e:\n\n\n|f (θ1, φ) − f (θ2, φ)| ≤ κ0∥θ1 − θ2∥2, ∀θ1, θ2 ∈ Θ, φ ∈ Ψ.\n\n\n14\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTherefore, the coverage assumption in Yin et al. (2022b) implies that\n\n\n\nEd [µ] h\n\n\n\n2 [�]\n��f (θ1, φ(·, ·)) − f (θ2, φ(·, ·))� ≥ κ∥θ1 − θ2∥ [2] 2\n\n≥ [κ] sup\n\nκ [2] 0 (x,a)∈X ×A\n\n\n\n2\n�f (θ1, φ(x, a)) − f (θ2, φ(x, a))� .\n\n\n\nOur assumption can be reduced to their first assumption (*). We do not need the second assumption\n(**).\nIn addition, for the linear function class, the coverage assumption in Yin et al. (2022b) will reduce\nto the following linear function coverage assumption(Wang et al., 2020a; Min et al., 2021; Yin et al.,\n2022a; Xiong et al., 2023).\n\nλmin(Eµ,h[φ(x, a)φ(x, a) [⊤] ]) = κ > 0, ∀h ∈ [H].\n\n\nTherefore, our assumption is also weaker than the linear function coverage assumption when dealing\nwith the linear function class.\n\nC DISCUSSION ON THE CALCULATION OF THE BONUS FUNCTION\n\nTo obtain the bonus function satisfying the required properties, we need to consider the constraint\noptimization problem\n\n\n\n\n\n\n(C.1)\n\n\n\n [.]\n\n\n\nmax\n\n\n\n\n\n\n\n\n �\n\n [|][f][1][(][z][h][)][ −] [f][2][(][z][h][)][|][, f][1][, f][2][ ∈F][h][ :] k∈[K\n\n\n\nk∈[K]\n\n\n\n(f1(zh [k][)][ −] [f][2][(][z] h [k][))][2]\n\n� ≤ (βh) [2]\n(σh(s [k] h [, a][k] h [))][2]\n\n\n\nWe utilize the online subsampling framework presented in Wang et al. (2020b); Kong et al. (2021)\nto an offline dataset. Additionally, we generalize the idea of Kong et al. (2021) to the weighted\nproblem. Using the shorthand expression\n\n\n\n�\n∥f ∥ [2] σh,Dh [=] �\n\nk∈[K]\n\n\n\n(f (zh [k][))][2]\n\n\n�\n(σh(s [k] h [, a][k] h [))][2][,]\n\n\n\nwe aim to estimate the solution of the following constraint optimization problem.\n\nfmin1,f2 [∥][f][1][ −] [f][2][∥] σ [2] �h,Dh [+][ w] 2 [(][f][1][(][s, a][)][ −] [f][2][(][s, a][)][ −] [2][L][)][2][.] (C.2)\n\n\nAlgorithm 2 Binary Search\n\n\n1: Input: Dataset Dh = {s [k] h [, a][k] h [, r] h [k][}][K] k=1 [, objective][ z][ = (][s, a][)][,][ β][h][, precision][ α]\n2: G ←Fh −Fh\n3: Define R(g, w) := ∥g∥ [2] σ�h,Dh [+][ w] 2 [(][g][(][s, a][)][ −] [2(][L][ + 1))][2][,][ ∀][g][ ∈G]\n\n4: wL ← 0, wH ← βh/(α(L + 1))\n5: gL ← 0, zL ← 0\n6: gH ← argming∈G R(g, wH ), zH ← gH(s, a)\n7: ∆ ← αβ/(8(L + 1) [3] )\n10:9:8: whilegw�� ← ← |�zargminH(w −H +zLg w|∈G > αL R)/ and(2g, �w |),w �zH ← −�gw(s, aL| >) ∆ do\n11:12: if ∥wgH∥ ← [2] σ�h,Dw, zh� [> β] H ← [h][ then] z�\n13: else\n14: wL ← w, z� L ← z�\n15: end if\n\n16: end while\n17: Output: zH\n\nThe binary search algorithm is similar to Kong et al. (2021), which utilizes the oracle for varianceweighted ridge regression in Lines 6 and 10. It reduces the computational complexity of the constrained optimization problem given by (C.1), limiting it to a finite number of calls to the regression\noracle. If the function class Fh is convex, the binary search algorithm can solve the optimization\nproblem (C.1) up to a precision of α with O(log(1/α)) calls of the regression oracle. We summarize\nthe result in the following theorem. If Fh is not convex, the method of Krishnamurthy et al. (2017)\ncan solve the problem with O(1/α) calls of the regression oracle.\n\n\n15\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTheorem C.1. Assume the optimal solution of the optimization problem (C.1) is g [∗] = f1 [∗] [−] [f][ ∗] 2 [and]\nthe function class Fh is convex and closed under pointwise convergence, then Algorithm 2 terminate\nafter O(log(1/α)) calls of the regression oracle and the returned values satisfy\n\n\n|zH − g [∗] (s, a)| ≤ α.\n\n\nProof of Theorem C.1. Easy to see G = Fh −Fh is also convex. We then follow the proof of\nTheorem 1 of Foster et al. (2018).\n\n\nTo see the solution of optimization problem C.1 satisfies the condition of the bonus function, we\nrecall the definition of the D [2] -divergence\n\n\n\nDF [2] h [(][z][;][ D][h][;][ �][σ] h [2][) =] sup\nf1,f2∈Fh\n\n\n\n(f1(z) − f2(z)) [2]\n\n1\n�k∈[K] (σ�h(zh [k][))][2][ (][f][1][(][z] h [k][)][ −] [f][2][(][z] h [k][))][2][ +][ λ.]\n\n\n\nTherefore, for any f1, f2 ∈Fh satisfying [�] k∈[K] (σ�h(1zh [k][))][2][ (][f][1][(][z] h [k][)][ −] [f][2][(][z] h [k][))][2][ ≤] [(][β] h [2][)][, we have]\n\n\n|f1(zh) − f2(zh)| ≤ DFh(z; Dh; �σh [2][)] �(βh) [2] + λ.\n\n\nD ANALYSIS OF THE VARIANCE ESTIMATOR\n\nIn this section, our main objective is to prove that our variance estimators are close to the truncated\nvariance of the optimal value function [VhVh [∗] +1 [](][s, a][)][. The following lemmas are helpful in the]\nproof of Lemma D.4. To start with, we need an upper bound of the D [2] -divergence for a large\ndataset.\n\n\nLemma D.1. Let Dh be the dataset satisfying Assumption 3.6. When the size of data set satisfies\nK ≥ Ω [�] � logκ [2] N �, with probability at least 1 − δ, for each state-action pair z, we have\n\n\n\n1\nDFh(z, Dh, 1) = O [�] � √Kκ\n\n\n\n.\n�\n\n\n\nLemma 6.1 and Lemma 6.2 show the confidence radius for the first and second-order Bellman error,\nwhich is essential in our proof. Here we restate them with more accurate parameter choices.\n\nLemma D.2 (Restatement of Lemma 6.1). For any stage h ∈ [H], let f [q] h+1(·, ·) ≤ H be the\nestimated value function constructed in Algorithm 1 Line 7. By utilizing Assumption 3.3, there\nexists a function f [¯] h [′] [∈F][h][, satisfying that][ |][ ¯][f][ ′] h [(][z][h][)][ −] [[][T][h][ q][f][h][+1][](][z][h][)][| ≤] [ǫ][ holds for all state-action pair]\nzh = (sh, ah). Then with probability at least 1 − δ/(4H), it holds that\n\n� �f¯ ′h [(¯][z] h [k][)][ −] [f][¯][h][(¯][z] h [k][)] �2 ≤ (¯β1,h)2,\n\nk∈[K]\n\n\nwhere β [¯] 1,h = O [�] log(N · Nb)H, and f [¯] h is the estimated function for first-moment Bellman\n�� �\n\noperator (Line 4 in Algorithm 1).\n\nLemma D.3 (Restatement of Lemma 6.1). For any stage h ∈ [H], let f [q] h+1(·, ·) ≤ H be the\nestimated value function constructed in Algorithm 1 Line 7. By utilizing Assumption 3.3, there\nexists a function ¯gh [′] [∈F][h][, satisfying that][ |][g][¯] h [′] [(][z][h][)][ −] [[][T][2][,h][ q][f][h][+1][](][z][h][)][| ≤] [ǫ][ holds for all state-action]\npair zh = (sh, ah). Then with probability at least 1 − δ/(4H), it holds that\n\n� �g¯h [′] [(¯][z] h [k][)][ −] [g][¯][h][(¯][z] h [k][)] �2 ≤ (¯β2,h)2,\n\nk∈[K]\n\n\nwhere β [¯] 2,h = O [�] ��log(N · Nb)H [2][�], and ¯gh is the estimated function for second-moment Bellman\n\noperator (Line 5 in Algorithm 1).\n\n\nRecall the definition of the variance estimator.\n\n\n\n2\nfh(¯s [k] h [,][ ¯][a][k] h [)][ −] [r][¯] h [k] [−] [f][q][h][+1][(¯][s][k] h+1 [)]\n� �\n\n\n16\n\n\n\n¯\nfh = argmin\nfh∈Fh\n\n\n\n�\n\nk∈[K]\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n2 [�][2]\n\n¯\ngh(¯s [k] h [,][ ¯][a][k] h [)][ −] rh [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] .\n� � �\n\n\n\ng¯h = argmin\ngh∈Fh\n\n\n\n�\n\nk∈[K]\n\n\n\nThe variance estimator is defined as:\n\n\n\n√Kκ\n\n\n\n� [�]\n\n\n\n�\nσh [2][(][s, a][) := max]\n\n\n\n�\n\n\n\n¯ 2 �\n1, ¯gh(s, a) − �fh(s, a)� − O� [�]\n\n\n\nlog(N · Nb)H [3]\n\n\n\n.\n\n\n\nWe can prove the following lemma:\n\n\nLemma D.4. with probability at least 1 − δ/2, for any h ∈ [H], the variance estimator designed\nabove satisfies:\n\n\n\n�\n\n\n\n�\n≤ σh [2][(][s, a][)][ ≤] [[][V][h][V] h [ ∗] +1 [](][s, a][)][.]\n\n\n\n\n[VhVh [∗] +1 [](][s, a][)][ −] [O][�]\n\n\n\n��\n\n\n\nlog(N · Nb)H [3]\n\n\n\n√Kκ\n\n\n\n¯ 2\nProof of Lemma D.4. We write Bh(s, a) = ¯gh(s, a) − �fh(s, a)� . We first bound the difference\nbetween Bh(s, a) and [Varhf [q] h+1](s, a). By the definition of conditional variance, we have\n���Bh(s, a) − [Varh qfh+1](s, a)��� ≤ ���g¯h(s, a) − [T2,h qfh+1](s, a)��� + ����f¯h(s, a)�2 − �[Thf [q] h+1](s, a)�2 [�] ��,\n\nwhere we use our definition of Bellman operators. By Assumption 3.3, there existsg¯h [′] [∈F][h][, such that for all][ (][s, a][)] f [¯] h [′] [∈F][h][,]\n\n��f¯ ′h [(][s, a][)][ −] [[][T][h][f][q][h][+1][](][s, a][)] �� ≤ ǫ (D.1)\n\n¯′\n��gh [(][s, a][)][ −] [[][T][2][,h][f][q][h][+1][](][s, a][)] �� ≤ ǫ. (D.2)\n\n\nThen by Lemma D.2, with probability at least 1 − δ/(4H [2] ), the following inequality holds\n\n\n¯ k 2 2\n\n� �fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] � ≤ (¯β1,h) . (D.3)\n\nk∈[K]\n\n\nSimilarly, for the second-order term, using Lemma D.3, with probability at least 1 − δ/(4H [2] ), the\nfollowing inequality holds\n\n\n¯ 2 2\n\n� �gh(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] � ≤ (¯β2,h) . (D.4)\n\nk∈[K]\n\n\nAfter taking a union bound, with probability at least 1−δ/(2H), (D.3) and (D.4) hold for all h ∈ [H]\nsimultaneously. Consequently, we focus on this high probability event and prove that\n\n¯ ¯ 2 2 [�]\ngh(s, a) − [T2,h qfh+1](s, a) + �fh(s, a)� − �[Thf [q] h+1](s, a)�\n��� ��� ��� ��\n\n\n¯ ¯′ ¯ ¯\n≤ ǫ + ��gh(s, a) − gh [(][s, a][)] �� + O(H) · ���fh(s, a) − f ′h [(][s, a][)] �� + ǫ�\n\n\n\n¯ ¯\n= O(H) · ǫ + |gh(s, a) − gh [′] [(][s, a][)][|] ��k∈[K] �g¯h(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] �2 + λ ��k∈[K]\n\n\n\n�g¯h(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] �2 + λ\n\n\n¯ k 2\n�fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] � + λ\n\n\n\n+ O(H) ·\n\n\n\n¯ ¯\n��fh(s, a) − f ′h [(][s, a][)] ��     ¯ k 2\n��k∈[K] �fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] � + λ ��k∈[K\n\n\n\nk∈[K]\n\n\n\n¯ ¯\n≤ O(H) · ǫ + ��k∈[|Kgh] (�s, ag¯h(¯)z −h [k][)][ −] gh [′] [(][g][¯][s, a] h [′] [(¯][z][)] h [k][|][)] �2 + λ - �(β [¯] 2,h) [2] + λ\n\n\n\n+ O(H) · |f [¯] h(s, a¯ ) −k f [¯] h [′] [(][s, a][)][|] 2 - �(β [¯] 1,h) [2] + λ\n��k∈[K] �fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] � + λ\n\n\n\n≤ O [�] (�log(N · Nb)H [2] ) · DFh(z, Dh [′] [,][ 1)]\n\n\n\n17\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n≤ O [�]\n\n\n\n��\n\n\n\nlog(N · Nb)H [2]\n\n\n\n√Kκ\n\n\n\n�\n\n\n\n,\n\n\n\nwhere the first inequality holds due to the completeness assumption. The second inequality holds\ndue to (D.3) and (D.4). The third inequality holds due to Definition 3.4 . The last inequality holds\ndue to Lemma D.1. Therefore, we have\n\n� log(N · Nb)H [2]\nBh(s, a) − [Varh qfh+1](s, a) ≤ O . (D.5)\n��� ���\n\n�� �\n\n\n\n��\n\n\n\nlog(N · Nb)H [2]\n\n\n\n�\n\n\n\n. (D.5)\n\n\n\n√\n\n\n\nKκ\n\n\n\nq\nTo further bound the difference between �Varhf [q] h+1�(s, a) and [VarhVh [∗] +1 [](][s, a][)][, we prove] ��fh+1 −\n\n\n\n√\nVh [∗] +1��∞ [≤] [O][�]\n�\n\n\n\nlog(N·Nb)H [3]\n\n\n\nby induction.\n�\n\n\n\n√Kκ\n\n\n\nAt horizon H + 1, f [q] H+1 = VH [∗] +1 [= 0][, the inequality holds naturally. At horizon][ H][, we have]\n\n\nQ [∗] H [(][s, a][) = [][T][H] [V] H [ ∗] +1 [](][s, a][)]\n\n= [TH f [q] H+1](s, a)\n\n\n¯\n≥ f [¯] H(s, a) − ��[TH qfH+1](s, a) − fH (s, a)��\n\n≥ f [¯] H(s, a) − (ǫ + |f [¯] H [′] [(][s, a][)][ −] [f][¯][H][(][s, a][)][|][)]\n\n≥ f [¯] H(s, a) − [¯] bH (s, a) − ǫ\n\n= f [q] H(s, a),\n\n\nwhere the first inequality holds due to the triangle inequality. The second inequality holds due to\n(D.1). The third inequality holds due to the property of the bonus function [¯] bH and (D.3). The last\nequality holds due to our definition of f [q] H in Algorithm 1 Line 7. Therefore, VH [∗] [(][s][)][ ≥] [f][q][H] [(][s][)][ for all]\ns ∈S.\nWe denote the policy derived from f [q] H by qπH, i.e. qπH (s) = argmaxa f [q] H (s, a) and then we have\n\nVH [∗] [(][s][)][ −] [f][q][H] [(][s][) =][ ⟨][Q][∗] H [(][s,][ ·][)][ −] [f][q][H][(][s,][ ·][)][, π][∗][(][·|][s][)][⟩][A][ +][ ⟨][f][q][H] [(][s,][ ·][)][, π] H [∗] [(][·|][s][)][ −] [π][q][H] [(][·|][s][)][⟩][A]\n\n≤⟨Q [∗] H [(][s,][ ·][)][ −] [f][q][H][(][s,][ ·][)][, π][∗][(][·|][s][)][⟩][A]\n= ⟨[TH VH [∗] +1 [](][s,][ ·][)][ −] [f][¯][H][(][s,][ ·][) + ¯][b][H] [(][s,][ ·][)][, π][∗][(][·|][s][)][⟩][A]\n\n= ⟨[TH f [q] H+1](s, ·) − f [¯] H (s, ·) + [¯] bH(s, ·), π [∗] (·|s)⟩A\n\n+ ⟨[TH VH [∗] +1 [](][s,][ ·][)][ −] [[][T][H][ q][f][H][+1][](][s,][ ·][)][, π] H [∗] [(][·|][s][)][⟩][A]\n≤ 2⟨ [¯] bH(s, ·), πH [∗] [(][·][, s][)][⟩][A] [+][ ǫ]\n\n\n\n≤ O [�]\n� [�]\n\n\n\nlog(N · Nb)H\n√Kκ\n\n\n\n,\n�\n\n\n\nKκ\n\n\n\nwhere the first inequality holds because the policy qπH takes the action which maximizes f [q] H . The\nsecond inequality holds due to (D.1) and (D.3). For the last inequality, we use the choice of bH such\nthat for all z ∈S × A with constant 0 < C < ∞\n\n\n¯\nbH (z) ≤ C · �DFH (z; DH; 1) · �(β [¯] H ) [2] + λ + ǫβ [¯] H�.\n\n\n\n�\n√\nTherefore, we have maxz [¯] bH (z) ≤ O\n�\n\n\n\nlog(N·Nb)H\n√Kκ\n\n\n\n, which uses Lemma D.1 and βH =\n�\n\n\n\nKκ\n\n\n\nO���log(N · Nb)H�.\n\n\n\n√\nNext, we prove by induction. Let Rh = O [�]\n�\n\n\n\nlog(N·Nb)H\n√Kκ\n\n\n\n\n - (H − h + 1). We define the induction\n�\n\n\n\nKκ\n\n\n\nassumption as follows: Suppose with the probability ofq 1 − δh+1, the event Eh+1 = {0 ≤ Vh [∗] +1 [(][s][)][−]\nfh+1(s) ≤ Rh+1} holds. Then we want to prove that with the probability of 1 − δh (δh will be\ndetermined later), the event Eh = {0 ≤ Vh [∗][(][s][)][ −] [f][q][h][(][s][)][ ≤] [R][h][}][ holds.]\nConditioned on the event Eh+1, using similar argument to stage H, we have\n\n\nQ [∗] h [(][s, a][) = [][T][h][V] h [ ∗] +1 [](][s, a][)]\n\n\n18\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n≥ [Thf [q] h+1](s, a)\n\n\n¯\n≥ f [¯] h(s, a) − [Th qfh+1](s, a) − fh(s, a)\n��� ���\n\n≥ f [¯] h(s, a) − �ǫ + |f [¯] h [′] [(][s, a][)][ −] [f][¯][h][(][s, a][)][|] �\n\n≥ f [¯] h(s, a) − [¯] bh(s, a) − ǫ\n\n= f [q] h(s, a).\n\n\nwhere the first inequality holds due to Eh+1. The second inequality holds due to the triangle inequality. The third inequality holds due to (D.1). The fourth inequality holds due to the property of the\nbonus function [¯] bh and (D.3). The last equality holds due to our definition of f [q] h in Algorithm 1 Line\n7. Therefore, Vh [∗][(][·][)][ ≥] [f][q][h][(][·][)][.]\nOn the other hand, similar to the case at horizon H, we denote the policy derived from f [q] h by qπh, i.e.\n\nq\nπh(s) = argmaxa f [q] h(s, a). Taking a union bound over the event in Lemma D.1 and Eh+1, we have\nwith probability at least 1 − δh+1 − δ/(2H [2] ),\n\n\nVh [∗][(][s][)][ −] [f][q][h][(][s][) =][ ⟨][Q][∗] h [(][s,][ ·][)][ −] [f][q][h][(][s,][ ·][)][, π] h [∗][(][·|][s][)][⟩][A][ +][ ⟨][f][q][h][(][s,][ ·][)][, π] h [∗][(][·|][s][)][ −] [π][q][h][(][·|][s][)][⟩][A]\n\n≤⟨Q [∗] h [(][s,][ ·][)][ −] [f][q][h][(][s,][ ·][)][, π] h [∗][(][·|][s][)][⟩][A]\n= ⟨[ThVh [∗] +1 [](][s,][ ·][)][ −] [f][¯][h][(][s,][ ·][) + ¯][b][h][(][s, a][)][, π] h [∗][(][·|][s][)][⟩][A][ +][ ǫ]\n\n= ⟨[Thf [q] h+1](s, ·) − f [¯] h(s, ·) + [¯] bh(s, a), πh [∗][(][·|][s][)][⟩][A]\n\n+ ⟨[ThVh [∗] +1 [](][s,][ ·][)][ −] [[][T][h][ q][f][h][+1][](][s,][ ·][)][, π] h [∗][(][·|][s][)][⟩][A][ +][ ǫ]\n\n≤ 2⟨ [¯] bh(s, ·), πh [∗][(][·][, s][)][⟩][A] [+ 2][ǫ][ +][ R][h][+1]\n\n\n\n�\n\n\n\n≤ Rh+1 + O [�]\n\n\n\n��\n\n\n\nlog(N · Nb)H\n√Kκ\n\n\n\nKκ\n\n\n\n�\n\n\n\n≤ O [�]\n\n\n\n��\n\n\n\nlog(N · Nb)H\n√Kκ\n\n\n\nKκ\n\n\n\n\n- (H − h + 1) = Rh,\n\n\n\nwhere the first inequality holds because the policy qπH takes the action which maximizes f [q] H . The\nsecond inequality holds due to (D.1) and (D.3). The third inequality holds due to Lemma D.1. The\nlast inequality holds due to the induction assumption. Therefore, we can choose δh = (H − h +\n1)δ/(2H [2] ) ≤ δ/2H. Taking a union bound over all h ∈ [H], we prove that with probability at least\n1 − δ/2, the following inequality holds for all h ∈ [H] simultaneously\n\n\n\n��\n\n\n\n. (D.6)\n\n\n\n0 ≤ Vh [∗] +1 [(][·][)][ −] [f][q][h][+1][(][·][)][ ≤] [O][�]\n\n\n\nlog(N · Nb)H [2]\n\n\n\n√Kκ\n\n\n\n�\n\n\n\nConditioned on this event, we can further bound the difference between [Varhf [q] h+1](s, a) and\n\n[VarhVh [∗] +1 [](][s, a][)][.]\n\n[Varh qfh+1](s, a) − [VarhVh ∗+1 [](][s, a][)]\n���� ����\n\n\n2 2 [�]\n≤ �Phf [q] h [2] +1�(s, a) − �PhVh [∗] +1 [2] �(s, a) + ��Phf [q] h+1�(s, a)� − ��PhVh [∗] +1�(s, a)�\n���� ���� ���� ���\n\n≤ O(H) · Vh ∗+1 [−] [f][q][h][+1]\n��� ���∞\n\n\n\n�\n\n\n\n, (D.7)\n\n\n\n≤ O [�]\n\n\n\n��\n\n\n\nlog(N · Nb)H [3]\n\n\n\n√Kκ\n\n\n\nwhere the first inequality holds due to the triangle inequality. The second inequality holds due to\nVh [∗] +1 [,][ q][f][h][+1][ ≤] [H][. The last inequality holds due to (D.6). Therefore, for any][ (][s, a][)][ ∈S × A][, we]\nhave\n��Bh(s, a) − [VarhVh ∗+1 [](][s, a][)] �� ≤ ���Bh(s, a) − �Varhf [q] h+1�(s, a)���\n\n\n19\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n+ �Varhf [q] h+1�(s, a) − �VarhVh [∗] +1�(s, a)\n��� ���\n\n\n\n≤ O [�]\n\n\n\n��\n\n\n\nlog(N · Nb)H [3]\n\n\n\n√Kκ\n\n\n\n�\n\n\n\n.\n\n\n\nThus, for any (s, a) ∈S × A, we have\n\n\n\n≤ [VarhVh [∗] +1 [](][s, a][)][,]\n\n\n\nBh(s, a) − O [�]\n\n\n\n��\n\n\n\nlog(N · Nb)H [3]\n\n\n\n√Kκ\n\n\n\n�\n\n\n\nwhere the first inequality holds due to the triangle inequality. The second inequality holds due to\n(D.5) and (D.7). Finally, using the fact that the function max{1, ·} is increasing and nonexpansive,\nwe complete the proof of Lemma D.4, which is\n\n\n\n�\n≤ σh [2][(][s, a][)][ ≤] [[][V][h][V] h [ ∗] +1 [](][s, a][)][.]\n\n\n\n\n[VhVh [∗] +1 [](][s, a][)][ −] [O][�]\n\n\n\n��\n\n\n\nlog(N · Nb)H [3]\n\n\n\n√Kκ\n\n\n\n�\n\n\n\nE PROOF OF LEMMAS IN SECTION D\n\nE.1 PROOF OF LEMMA D.1\nProof of Lemma D.1. From the definition of D [2] divergence (Definition 3.4), we have\n\n\n\nDF [2] h [(][z][;][ D][h][; 1) =] sup\nf1,f2∈Fh\n\n\n\n(f1(z) − f2(z)) [2]\n2 (E.1)\n�k∈[K] �f1(zh [k][)][ −] [f][2][(][z] h [k][)] � + λ\n\n\n\nBy the Hoeffding’s inequality (Lemma I.3), with probability at least 1 − δ/(N [2] ), we have\n\n\n2\n\n� �f1(zh [k][)][ −] [f][2][(][z] h [k][)] � − KEµ,h �(f1(zh) − f2(zh)) [2][�] ≥−2�2K log(N [2] /δ) · ∥f1\n\n\n\n2\n�f1(zh [k][)][ −] [f][2][(][z] h [k][)] � − KEµ,h �(f1(zh) − f2(zh)) [2][�] ≥−2�\n\n\n\n2K log(N [2] /δ) · ∥f1 − f2∥∞ [2] [.]\n\n\n\nk∈[K]\n\n\n\nHence, after taking a union bound, we have with probability at least 1 − δ, for all f1, f2 ∈Fh,\n\n\n2\n\n� �f1(zh [k][)][ −] [f][2][(][z] h [k][)] � ≥ KEµ,h �(f1(zh) − f2(zh)) [2][�] − 2�2K log(N [2] /δ) · ∥f1 − f2\n\n\n\n2\n�f1(zh [k][)][ −] [f][2][(][z] h [k][)] � ≥ KEµ,h �(f1(zh) − f2(zh)) [2][�] − 2�\n\n\n\n2K log(N [2] /δ) · ∥f1 − f2∥∞ [2]\n\n\n\nk∈[K]\n\n\n\n≥ K · κ∥f1 − f2∥∞ [2] [−] [2] �2K log(N [2] /δ) · ∥f1 − f2∥∞ [2] [,] (E.2)\n\n\n\nwhere the second inequality holds due to Assumption 3.6. Substituting (E.2) into (E.1), when the\nsize of dataset K ≥ Ω [�] logκ [2] N, we have\n� �\n\n\n\nDF [2] h [(][z][;][ D][h][; 1)][ ≤] sup\nf1,f2∈Fh\n\n\n\n(f1(z) − f2(z)) [2] 1\n\n\n1\n2 [K][ ·][ κ][∥][f][1][ −] [f][2][∥][2][∞] [+][ λ][ =][ �][O] �\n\n\n\n(f1(z) − f2(z)) [2]\n\n\n\nKκ\n\n\n\n.\n�\n\n\n\nE.2 PROOF OF LEMMA D.2\nIn the proof of Lemma D.2, we need to prove the following concentration inequality.\n\nLemma E.1. Based on the dataset D [′] = {s¯ [k] h [,][ ¯][a][k] h [,][ ¯][r] h [k][}][K,H] k,h=1 [, we define the filtration]\n\n\n¯ ¯\nHh [k] [=][ σ] �s [1] 1 [,][ ¯][a][1] 1 [,][ ¯][r] 1 [1][,][ ¯][s][1] 2 [, . . .,][ ¯][r] H [1] [,][ ¯][s][1] H+1 [; ¯][s] 1 [2][,][ ¯][a][2] 1 [,][ ¯][r] 1 [2][,][ ¯][s][2] 2 [, . . .,][ ¯][r] H [2] [,][ ¯][s][2] H+1 [;][ . . .,][ ¯][s] 1 [k][,][ ¯][a][k] 1 [,][ ¯][r] 1 [k][,][ ¯][s][k] 2 [, . . .,][ ¯][r] h [k][,][ ¯][s][k] h+1� .\n\n\nFor any fixed functions f, f [′] : S → [0, H], we make the following definitions:\n\nη¯h [k][[][f][ ′][] :=][ f][ ′][(¯][s][k] h+1 [)][ −] [[][P][h][f][ ′][](¯][s] h [k][,][ ¯][a][k] h [)]\nD¯ h [k][[][f, f][ ′][] := 2¯][η] h [k][[][f][ ′][]] �f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] � .\n\nThen with probability at least 1 − δ/(4H [2] N [2] Nb [2][)][, the following inequality holds,]\n\n\n\n� D¯ h [k][[][f, f][ ′][]][ ≤] [(24][H][ + 5)][i][2][(][δ][) +]\n\nk∈[K]\n\n\n\n2\n�k∈[K] �f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] �\n\n2,\n\n\n\nwhere i(δ) = �\n\n\n\n2 log [(][N·N][b][)][H][(2 log(4][K] δ [)+2)(log(2][L][)+2)] .\n\n\n20\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nProof of Lemma D.2. Let (β [¯] 1,h) [2] = (48H [2] + 10)i [2] (δ) + 16KHǫ. We define the event E [¯] 1,h :=\n\n¯ 2 2 [�]\n\n��k∈[K] �f ′h [(¯][z] h [k][)][ −] [f][¯][h][(¯][z] h [k][)] � - (¯β1,h) . We have the following inequality:\n\n\n\n¯ 2\n\n� �f ′h [(¯][z] h [k][)][ −] [f][¯][h][(¯][z] h [k][)] �\n\nk∈[K]\n\n\n\nr¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] + f¯h(¯zh [k][)][ −] [r][¯] h [k] [−] [f][q][h][+1][(¯][s][k] h+1 [)] 2\n�� � � ��\n\n\n\n=\n�\n\nk∈[K]\n\n\n=\n�\n\nk∈[K]\n\n\n\n2\n�r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] � + �\n\nk∈[K]\n\n\n\n¯ 2\nfh(¯zh [k][)][ −] [r][¯] h [k] [−] [f][q][h][+1][(¯][s][k] h+1 [)]\n� �\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\nr¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] f¯h(¯zh [k][)][ −] [r][¯] h [k] [−] [f][q][h][+1][(¯][s][k] h+1 [)]\n� �� �\n\n\n\n≤ 2\n�\n\nk∈[K]\n\n\n\n2\nr¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [f][¯][ ′] h [(¯][z] h [k][)]\n� �\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\nr¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] f¯h(¯zh [k][)][ −] [r][¯] h [k] [−] [f][q][h][+1][(¯][s][k] h+1 [)]\n� �� �\n\n\n\n= 2 �\n\nk∈[K]\n\n\n\n¯ ¯ k\n�rh [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] ��fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] �, (E.3)\n\n\n\nwhere the first inequality holds due to our choice of f [¯] h, i.e.,\n\n\n\n¯\nfh = argmin �\nfh∈Fh k∈[K]\n\n\n\n2\nfh(¯s [k] h [,][ ¯][a][k] h [)][ −] [r][¯] h [k] [−] [f][q][h][+1][(¯][s][k] h+1 [)] .\n� �\n\n\n\nNext, we will use Lemma E.1. For any fixed h, let f = f [¯] h ∈Fh, f [′] = f [q] h+1 = {f [¯] − ǫ}[0,H−h+1],\nwhere f [¯] = f [¯] h − [¯] bh ∈Fh −W. Following the construction in Lemma E.1, we define\n\n\nη¯h [k][[][f][ ′][] = ¯][r] h [s] [+][ f][ ′][(¯][s][k] h+1 [)][ −] [E] �r¯h [k] [+][ f][ ′][(¯][s][k] h+1 [)][|][z][¯] h [k] �,\n\nand D [¯] h [k][[][f, f][ ′][] = 2¯][η] h [k][[][f][ ′][]] �f (¯zh [k][)][ −] [[][T][h][ ¯][f][ ′][](¯][z] h [k][)] � .\n\n\nDue to the result of Lemma E.1, taking a union bound on the function class, with probability at least\n1 − δ/(4H [2] ), the following inequality holds,\n\n\n\n� D¯ h [k][[][f, f][ ′][]][ ≤] [(24][H] [2][ + 5)][i][2][(][δ][) +]\n\nk∈[K]\n\n\n\n2\n�k∈[K] �f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] �\n\n. (E.4)\n2\n\n\n\nTherefore, with probability at least 1 − δ/(4H [2] ), we have\n\n\n\n2 � �r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] ��f¯h(¯zhk [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] �\n\nk∈[K]\n\n\n\n= 2 �\n\nk∈[K]\n\n\n\n¯ ¯ k\n�rh [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [[][T][h][ q][f][h][+1][](¯][z] h [k][)] ��fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] �\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\n¯ k\n�[Thf [q] h+1](¯zh [k][)][ −] [f][¯][ ′] h [(¯][z] h [k][)] ��fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] � (E.5)\n\n\n\n≤ 2\n�\n\nk∈[K]\n\n\n\n¯ ¯ k\n�rh [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [[][T][h][ q][f][h][+1][](¯][z] h [k][)] ��fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] � + 4KHǫ\n\n\n\n≤ (24H [2] + 5)i [2] (δ) + 4KHǫ +\n\n\n≤ (24H [2] + 5)i [2] (δ) + 8KLǫ +\n\n\n\n¯ 2\n�k∈[K] �fh(¯zh [k][)][ −] [[][T][h][ q][f][h][+1][](¯][z] h [k][)] �\n\n(E.6)\n2\n\n\n¯ 2\n�k∈[K] �f ′h [(¯][z] h [k][)][ −] [f][¯][h][(¯][z] h [k][)] �\n\n\n2\n\n\n21\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n= [(¯][β][1][,h][)][2] +\n\n2\n\n\n\n�k∈[K] �f¯h [′] [(¯][z] h [k][)][ −] [f][�][ ′] h [(¯][z] h [k][)] �2\n\n. (E.7)\n2\n\n\n\nwhere the first and third inequalities hold because of the completeness assumption. The second\ninequality holds due to (E.4). The last equality holds due to the choice of\n\n\n\n2(24L [2] + 5)i [2] (δ) + 16KLǫ = O [�]\n��\n\n\n\n¯\nβ1,h = �\n\n\n\nlog(N · Nb)H .\n�\n\n\n\nHowever, conditioned on the event E [¯] 1,h, we have\n\n\n\n2 �\n\nk∈[K]\n\n\n\n¯ ¯ k\n�rh [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] ��fh(¯zh [)][ −] [f][¯][ ′] h [(¯][z] h [k][)] �\n\n\n\n≥\n�\n\nk∈[K]\n\n\n\n¯ 2\n�f ′h [(¯][z] h [k][)][ −] [f][¯][h][(¯][z] h [k][)] �\n\n\n\n- [(¯][β][1][,h][)][2] +\n\n2\n\n\n\n¯ 2\n�k∈[K] �f ′h [(¯][z] h [k][)][ −] [f][¯][h][(¯][z] h [k][)] �\n\n2 .\n\n\n\nwhere the first inequality holds due to (E.3). The second inequality holds due to E [¯] 1,h. This is\ncontradictory with (E.7). Thus, we have P[ E [¯] 1,h] ≤ δ/(4H [2] ) and complete the proof of Lemma\nD.2.\n\n\nE.3 PROOF OF LEMMA D.3\nTo prove this lemma, we need a lemma similar to Lemma E.1\n\nLemma E.2. On dataset D [′] = {s¯ [k] h [,][ ¯][a][k] h [,][ ¯][r] h [k][}][K,H] k,h=1 [, we define the filtration]\n\n\nH¯h [k] [=][ σ][(¯][s][1] 1 [,][ ¯][a][1] 1 [,][ ¯][r] 1 [1][,][ ¯][s][1] 2 [, . . .,][ ¯][r] H [1] [,][ ¯][s][1] H+1 [; ¯][s] 1 [2][,][ ¯][a][2] 1 [,][ ¯][r] 1 [2][,][ ¯][s][2] 2 [, . . .,][ ¯][r] H [2] [,][ ¯][s][2] H+1 [;][ . . .,][ ¯][s] 1 [k][,][ ¯][a][k] 1 [,][ ¯][r] 1 [k][,][ ¯][s][k] 2 [, . . .,][ ¯][r] h [k][,][ ¯][s][k] h+1 [)][.]\n\n\nFor any fixed function f, f [′] : S → [0, H], we make the following definitions:\n\n\nη¯h [k][[][f][ ′][] :=] �r¯h [k] [+][ f][ ′][(¯][s][k] h+1 [)] �2 − �Ph(¯rh + f [′] ) [2][��] s¯ [k] h [,][ ¯][a][k] h�\n\nD¯ h [k][[][f, f][ ′][] := 2¯][η] h [k][[][f][ ′][]] �f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] � .\n\n\nThen with probability at least 1 − δ/(4H [2] N [2] Nb [2][)][, the following inequality holds,]\n\n\n\n� D¯ h [k][[][f, f][ ′][]][ ≤] [(24][H][ + 5)][i][′][2][(][δ][) +]\n\nk∈[K]\n\n\n\n2\n�k∈[K] �f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] �\n\n2,\n\n\n\nwhere i [′] (δ) = �\n\n\n\n4 log [(][N·N][b][)][H][(2 log(4][LK][)+2)(log(4][L][)+2)]\n\n\n\nδ .\n\n\n\nProof of Lemma D.3. Let (β [¯] 2,h) [2] = (40H [4] + 10)i [′][2] (δ) + 16KLǫ. We define the event E [¯] 2,h :=\n¯ 2 2 [�]\n��k∈[K] �gh [′] [(¯][z] h [k][)][ −] [g][¯][h][(¯][z] h [k][)] � - (¯β2,h) . We can prove the following inequality:\n\n\n\n� �g¯h [′] [(¯][z] h [k][)][ −] [g][¯][h][(¯][z] h [k][)] �2\n\nk∈[K]\n\n\n\n���r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − g¯h′ [(¯][z] h [k][)] � + �g¯h(¯zh [k][)][ −] �r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 [��][2]\n\n\n\n=\n�\n\nk∈[K]\n\n\n=\n�\n\nk∈[K]\n\n\n\n��r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − g¯h′ [(¯][z] h [k][)] �2 + �\n\nk∈[K]\n\n\n\n�g¯h(¯zh [k][)][ −] �r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 [�][2]\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\n��r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − g¯h′ [(¯][z] h [k][)] ��g¯h(¯zh [k][)][ −] �r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 [�]\n\n\n\n≤ 2\n�\n\nk∈[K]\n\n\n\n��r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − g¯h′ [(¯][z] h [k][)] �2\n\n\n22\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\n��r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − g¯h′ [(¯][z] h [k][)] ��g¯h(¯zh [k][)][ −] �r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 [�]\n\n\n\n= 2 �\n\nk∈[K]\n\n\n\n��r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − g¯h′ [(¯][z] h [k][)] ��g¯h(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] �, (E.8)\n\n\n\nwhere the first inequality holds due to our choice of ¯gh, i.e.\n\n\n\ng¯h = argmin �\ngh∈Fh k∈[K]\n\n\n\n�gh(¯s [k] h [,][ ¯][a][k] h [)][ −] [(¯][r] h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [))][2][�][2] .\n\n\n\nNext, we will use Lemma E.2. For any fixed h, let f = ¯gh ∈Fh, f [′] = f [q] h+1 = {f [¯] − ǫ}[0,H−h+1],\nwhere f [¯] = f [¯] h − [¯] bh ∈Fh −W. Following the construction in Lemma E.2, we define\n\nη¯h [k][[][f][ ′][] :=] �r¯h [k] [+][ f][ ′][(¯][s][k] h+1 [)] �2 − �Ph(¯rh + f [′] ) [2][�] (¯s [k] h [,][ ¯][a][k] h [)]\n\nand D [¯] h [k][[][f, f][ ′][] := 2¯][η] h [k][[][f][ ′][]] �f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] � .\n\n\nDue to the result of Lemma E.2, taking a union bound on the function, with probability at least\n1 − δ/(4H [2] ), the following inequality holds,\n\n\n\n� D¯ h [k][[][f, f][ ′][]][ ≤] [(20][H] [4][ + 5)][i][2][(][δ][) +]\n\nk∈[K]\n\n\n\n2\n�k∈[K] �f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] �\n\n. (E.9)\n2\n\n\n\nTherefore, with probability at least 1 − δ/(4H [2] ), we have\n\n\n\n2 �\n\nk∈[K]\n\n\n\n��r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − g¯h′ [(¯][z] h [k][)] ��g¯h(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] �\n\n\n\n= 2 � ��r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − [T2,h qfh+1](¯zhk [)] ��g¯h(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] �\n\nk∈[K]\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\n�[T2,hf [q] h+1](¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] ��g¯h(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] �\n\n\n\n≤ 2 � ��r¯h [k] [+][ q][f][h][+1][(¯][s][k] h+1 [)] �2 − [T2,h qfh+1](¯zhk [)] ��g¯h(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] � + 4KLǫ\n\nk∈[K]\n\n\n\n≤ (20H [4] + 5)i [′][2] (δ) + 4KLǫ +\n\n\n≤ (20H [4] + 5)i [′][2] (δ) + 8KLǫ +\n\n\n\n2\n�k∈[K] �g¯h(¯zh [k][)][ −] [[][T][2][,h][ q][f][h][+1][](¯][z] h [k][)] �\n\n\n2\n\n�k∈[K] �g¯h [′] [(¯][z] h [k][)][ −] [g][¯][h][(¯][z] h [k][)] �2\n\n\n2\n\n\n\n≤ [(¯][β][2][,h][)][2] +\n\n2\n\n\n\n�k∈[K] �g¯h [′] [(¯][z] h [k][)][ −] [g][¯][h][(¯][z] h [k][)] �2\n\n2, (E.10)\n\n\n\nwhere the first and third inequalities hold due to the Bellman completeness assumption. The second\ninequality holds due to (E.9). The last inequality holds due to the choice of\nβ¯2,h = �(40H [4] + 10)i [′][2] (δ) + 16KLǫ = O [�] (�log(N · Nb)H [2] ).\n\n\n\n(40H [4] + 10)i [′][2] (δ) + 16KLǫ = O [�] (�\n\n\n\nlog(N · Nb)H [2] ).\n\n\n\nHowever, conditioned on the event E [¯] 2,h, we have\n\n\n\n2 �\n\nk∈[K]\n\n\n\n�(¯rh [k] [+][ q][f][h][+1][(¯][s][k] h+1 [))][2][ −] [g][¯] h [′] [(¯][z] h [k][)] ��g¯h(¯zh [k][)][ −] [g][¯] h [′] [(¯][z] h [k][)] �\n\n\n\n≥ � �g¯h [′] [(¯][z] h [k][)][ −] [g][¯][h][(¯][z] h [k][)] �2\n\nk∈[K]\n\n\n\n- [(¯][β][2][,h][)][2] +\n\n2\n\n\n\n�k∈[K] �g¯h [′] [(¯][z] h [k][)][ −] [g][¯][h][(¯][z] h [k][)] �2\n\n2,\n\n\n\nwhere the first inequality holds due to (E.8). The last inequality holds due to E [¯] 2,h. It is contradictory\nwith (E.10). Thus, we have P[ E [¯] 2,h] ≤ δ/(4H [2] ) and complete the proof of Lemma D.3.\n\n\n23\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nF PROOF OF THEOREM 5.1\n\nIn this section, we prove Theorem 5.1. The proof idea is similar to that of Section D. To start with,\nwe prove that our data coverage assumption (Assumption 3.6) can lead to an upper bound of the\nweighted D [2] -divergence for a large dataset.\n\nLemma F.1. Let Dh be a dataset satisfying Assumption 3.6. When the size of data set satisfies\nK ≥ Ω [�] � logκ [2] N �, �σh ≤ H, with probability at least 1 − δ, for each state-action pair z, we have\n\n\n\nH\nDFh(z, Dh, �σh [2][) =][ �][O]\n� √Kκ\n\n\n\n.\n�\n\n\n\nWith Lemma D.4, we can prove a variance-weighted version of concentration inequality.\n\nLemma F.2 (Restatement of Lemma 6.3). Suppose the variance function �σh satisfies the inequality\nin Lemma D.4. at stage h ∈ [H], the estimated value function f [�] h+1 in Algorithm 1 is bounded\nby H. According to Assumption 3.3, there exists some function f [¯] h ∈Fh, such that |f [¯] h(zh) −\n\n[Thf [�] h+1](z)| ≤ ǫ for all zh = (sh, ah). Then with probability at least 1 − δ/2, the following\ninequality holds for all stage h ∈ [H] simultaneously,\n\n\n1 ¯ 2\n\n� � [k] [2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] � ≤ (βh) [2] .\n\n\n\n¯ 2\nfh(zh [k][)][ −] [f][�][h][(][z] h [k][)] ≤ (βh) [2] .\n� �\n\n\n\nk∈[K]\n\n\n\n1\n\n�\n(σh(zh [k][))][2]\n\n\n\nWith these lemmas, we can start the proof of Theorem 5.1.\n\n\nProof of Theorem 5.1. For any state-action pair z = (s, a) ∈S × A, we have\n\n� ¯ ¯ �\n\n[Th �fh+1](z) − fh(z) ≤ [Th �fh+1](z) − fh(z) + fh(z) − fh(z)\n��� ��� ��� ��� ��� ���\n� �� �\nI1\n\n\n≤ ǫ + bh(z),\n\n\nwhere we bound I1 with the Bellman completeness assumption. For the second term, we use the\nproperty of the bonus function and Lemma F.2. Using Lemma I.2, we have\n\n\n\nV1 [∗][(][s][)][ −] [V] 1 [ �][π][(][s][)][ ≤] [2]\n\n\n\nH\n� Eπ∗ [bh (sh, ah) | s1 = s] + 2ǫH\n\n\nh=1\n\n\n\n(βh) [2] + λ | s1 = s + 2ǫH\n�\n\n\n\n≤\n\n\n\nH\n�\n\n\n\n� Eπ∗ �DFh(zh; Dh; �σh [2][)][ ·] �\n\nh=1\n\n\n\nlog N � � [H] Eπ∗ [�] DFh(zh; Dh; �σh [2][)][ |][ s][1][ =][ s] �\n\nh=1\n\n\n\nlog N � � [H]\n\n\n\n≤ O [�] ��\n\n\n≤ O [�] ��\n\n\n\nlog N � � [H] Eπ∗ [�] DFh �zh; Dh; [VhVh [∗] +1 [](][·][,][ ·][)] � | s1 = s�,\n\nh=1\n\n\n\nlog N � � [H]\n\n\n\nwhere the first inequality holds due to Lemma I.2. The second inequality holds due to the property\nof the bonus function\n\n\nbh(z) ≤ C · �DFh(z; Dh; �σh [2][)][ ·] �(βh) [2] + λ + ǫβh�.\n\nThe third inequality holds due to our choice of βh = O [�] �√log N �. The last inequality holds due to\nLemma D.4 and the fact that D [2] -divergence is increasing with respect to the variance function. We\ncomplete the proof of Theorem 5.1.\n\n\nG PROOF OF THE LEMMAS IN SECTION F\n\nG.1 PROOF OF LEMMA F.1\nProof of Lemma F.1. From the definition of D [2] divergence, we have\n\n\n\nDF [2] h [(][z][;][ D][h][;][ �][σ] h [2][) =] sup\nf1,f2∈Fh\n\n\n\n(f1(z) − f2(z)) [2]\n1 2 (G.1)\n�k∈[K] (σ�h(zh [k][))][2] �f1(zh [k][)][ −] [f][2][(][z] h [k][)] � + λ\n\n\n24\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\nBy the Hoeffding’s inequality (Lemma I.3), with probability at least 1 − δ/(N [2] ),\n\n\n2\n\n� �f1(zh [k][)][ −] [f][2][(][z] h [k][)] � − KEµ,h �(f1(zh) − f2(zh)) [2][�] ≥−2�2K log(N [2]\n\n\n\n2\n�f1(zh [k][)][ −] [f][2][(][z] h [k][)] � − KEµ,h �(f1(zh) − f2(zh)) [2][�] ≥−2�\n\n\n\n2K log(N [2] /δ) · ∥f1 − f2∥ [2] ∞ [.]\n\n\n\nk∈[K]\n\n\n\nHence, after taking a union bound, we have with probability at least 1 − δ, for all f1, f2 ∈Fh,\n\n\n1 2\n\n� � [k] [2] �f1(zh [k][)][ −] [f][2][(][z] h [k][)] �\n\n\n\nk∈[K]\n\n\n\n1 2\n� �f1(zh [k][)][ −] [f][2][(][z] h [k][)] �\n(σh(zh [k][))][2]\n\n\n\n1\n≥\nH [2]\n\n\n1\n≥\nH [2]\n\n\n\n�KEµ,h �(f1(zh) − f2(zh)) [2][�] − 2�2K log(N [2] /δ) · ∥f1 − f2∥ [2] ∞�\n\n\n\nK · κ∥f1 − f2∥ [2] ∞ [−] [2] �2K log(N [2] /δ) · ∥f1 − f2∥ [2] ∞, (G.2)\n� �\n\n\n\nwhere the last inequality holds due to Assumption 3.6. Substituting (G.2) into (G.1), when K ≥\nΩ� � logκ N �, we have\n\n\n\n.\n�\n\n\n\nDF [2] h [(][z][;][ D][h][;][ �][σ] h [2][)][ ≤] sup\nf1,f2∈Fh\n\n\n\nH [2] (f1(z) − f2(z)) [2] H 2\n\n12 [K][ ·][ κ][∥][f][1][ −] [f][2][∥][2][∞] [+][ λ][ =][ �][O] � Kκ [2]\n\n\n\nH [2] (f1(z) − f2(z)) [2]\n\n\n\nKκ [2]\n\n\n\nG.2 PROOF OF LEMMA F.2\nIn this section, we assume the high probability event in Lemma D.4 holds,i.e., the following inequality holds:\n\n\n\n�\n\n\n\n�\n≤ σh [2][(][s, a][)][ ≤] [[][V][h][V] h [ ∗] +1 [](][s, a][)][.] (G.3)\n\n\n\n\n[VhVh [∗] +1 [](][s, a][)][ −] [O][�]\n\n\n\n��\n\n\n\nlog(N · Nb)H [3]\n\n\n\n√Kκ\n\n\n\nTo prove Lemma F.2, we need the following lemmas.\n\nLemma G.1. Based on the dataset D = {s [k] h [, a][k] h [, r] h [k][}][K,H] k,h=1 [, we define the filtration][ H] h [k] [=]\nσ(s [1] 1 [, a][1] 1 [, r] 1 [1][, s][1] 2 [, . . ., r] H [1] [, s][1] H+1 [;][ x] 1 [2][, a][2] 1 [, r] 1 [2][, s][2] 2 [, . . ., r] H [2] [, s][2] H+1 [;][ · · ·][ s] 1 [k][, a][k] 1 [, r] 1 [k][, s][k] 2 [, . . ., r] h [k][, s][k] h+1 [)][.] For\nany fixed function f, f [′] : S →∈ [0, L], we define the following random variables:\n\n\nηh [k] [:=][ V] h [ ∗] +1 [(][s] h [k] +1 [)][ −] [[][P][h][V] h [ ∗] +1 [](][s] h [k] [, a][k] h [)]\n\nDh [s] [[][f, f][ ′][] := 2] � ηh [k] �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] �,\n(σh(zh [k][))][2]\n\n\nAs the variance function �σh satisfies (G.3), with probability at least 1 − δ/(4H [2] N [2] ), the following\ninequality holds,\n� Dh [k][[][f, f][ ′][]][ ≤] [4] [v][(][δ][)] √λ + √2v(δ) + 30v [2] (δ)\n\n\n\n� Dh [k][[][f, f][ ′][]][ ≤] [4] 3\n\nk∈[K]\n\n\n\n√\n3 [v][(][δ][)]\n\n\n\nλ + √\n\n\n\n2v(δ) + 30v [2] (δ)\n\n\n\n+\n\n\n\n1 2\n�k∈[K] ( [σ][�] h [(][z] h [k][)][)] 2 �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] �\n\n4,\n\n\n\nwhere v(δ) = �\n\n\n\n2 log [H][N] [(2 log(18][LT][ )+2)(log(18][L][)+2)]\n\n\n\nδ .\n\n\n\nK,H\nLemma G.2. Based on the dataset D = �s [k] h [, a][k] h [, r] h [k] �k,h=1 [, we define the following filtration][ H] h [k] [=]\nσ �s [1] 1 [, a][1] 1 [, r] 1 [1][, s][1] 2 [, . . ., r] H [1] [, s][1] H+1 [;][ x] 1 [2][, a][2] 1 [, r] 1 [2][, s][2] 2 [, . . ., r] H [2] [, s][2] H+1 [;][ · · ·][ s] 1 [k][, a][k] 1 [, r] 1 [k][, s][k] 2 [, . . ., r] h [k][, s][k] h+1�. For\nany fixed functions f, f [�] : S → [0, L] and f [′] : S → [0, H], we define the following random\nvariables\n\n\nξh [k][[][f][ ′][] :=][ f][ ′][(][s][k] h+1 [)][ −] [V] h [ ∗] +1 [(][s] h [k] +1 [)][ −] �Ph(f [′] − Vh [∗] +1 [)] � (s [k] h [, a][k] h [)][,]\n\n\n\nξh [k][[][f][ ′][]]\n∆ [k] h f, f, f [�] [′][�] := 2 �\n� (σh(zh [k][))][2]\n\n\n\nf (zh [k][)][ −] [f][�][(][z] h [k][)],\n� �\n\n\n25\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nAs the variance function �σh satisfies (G.3), with probability at least 1−δ/(4H [2] N [3] Nb), the following\ninequality holds,\n\n\n\n4\n\n� ∆ [k] h [[][f,][ �][f, f][ ′][]][ ≤] � 3\n\nk∈[K]\n\n\n\n�\n\n\n\n√\n3 [ι][(][δ][)]\n\n\n\n2ι(δ) ∥f [′] − Vh [∗] +1 [∥] ∞ [2] [+ 2]\n� 3\n\n\n\nλ + √\n\n\n\n3 [ι][2][(][δ][)][/][ log][ N][b]\n\n\n\n+ 30ι [2] (δ)∥f [′] − Vh [∗] +1 [∥] ∞ [2] [+]\n\n\n\n1\n�k∈[K] (σ�h(zh [k][))][2][ (][f] [(][z] h [k][)][ −] [f][ ′][(][z] h [k][))][2]\n\n4 .\n\n\n\nwhere ι(δ) = �\n\n\n\n3 log [H][(][N·N][b][)(2 log(18][LT][ )+2)(log(18][L][)+2)]\n\n\n\nδ .\n\n\n\nWith these lemmas, we can start the proof of Lemma F.2.\n\n\n1 ¯ 2\nProof of Lemma F.2. We define the event Eh := ��k∈[K] (σ�(zh [k][))][2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] � - (βh) [2] �.\n\n\n\nWe have the following inequality:\n\n\n1 ¯\n\n� � [k] [2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k]\n\n\n\n¯ 2\nfh(zh [k][)][ −] [f][�][h][(][z] h [k][)]\n� �\n\n\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\n1\n\n= k�∈[K] (σ�(zh [k][))][2]\n\n\n1\n\n= k�∈[K] (σ�(zh [k][))][2]\n\n\n\n� 2\nrh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [f][¯][h][(][z] h [k][)] + fh(zh [k][)][ −] [r] h [k] [−] [f][�][h][+1][(][s][k] h+1 [)]\n�� � � ��\n\n\n\n2\n�rh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [f][¯][h][(][z] h [k][)] � + �\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\n� 2\nfh(zh [k][)][ −] [r] h [k] [−] [f][�][h][+1][(][s][k] h+1 [)]\n� �\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\n�\nrh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [f][¯][h][(][z] h [k][)] fh(zh [k][)][ −] [r] h [k] [−] [f][�][h][+1][(][s][k] h+1 [)]\n� �� �\n\n\n\n2\n�rh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [f][¯][h][(][z] h [k][)] �\n\n\n\n≤ 2\n�\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\n�\nrh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [f][¯][h][(][z] h [k][)] fh(zh [k][)][ −] [r] h [k] [−] [f][�][h][+1][(][s][k] h+1 [)]\n� �� �\n\n\n\n= 2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\n�\n�rh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [f][¯][h][(][z] h [k][)] ��fh(zh [k][)][ −] [f][¯][h][(][z] h [k][)] � . (G.4)\n\n\n\nwhere the first inequality holds due to our choice of f [�] h in Algorithm 1 Line 12,\n\n\n\n2\nfh(s [k] h [, a][k] h [)][ −] [r] h [k] [−] [f][�][h][+1][(][s][k] h+1 [)] .\n� �\n\n\n\n�\nfh = argmin\nfh∈Fh\n\n\n\n�\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\nWe prove this lemma by induction. At horizon H, we first use Lemma G.1. Let f = f [�] H ∈FH,\nf [′] = f [¯] H ∈FH. We define\n\n\nηH [k] [:=][ V] H [ ∗] +1 [(][s] H [k] +1 [)][ −] [[][P][H] [V] H [ ∗] +1 [](][z] H [k] [)]\n\nDH [k] [[][f, f][ ′][] := 2] � ηH [k] �f (zH [k] [)][ −] [f][ ′][(][z] H [k] [)] � .\n(σH (zH [k] [))][2]\n\n\nTaking a union bound over the function class, with probability at least 1 − δ/(4H [2] ), the following\ninequality holds,\n\n\n\nk�∈[K] 2 (σ�H η�H [k] zH [k] [)] �2 �f�(zH [k] [)][ −] [f][¯][(][z] H [k] [)] � ≤ [4] 3\n\n\n\n�\n\n\n\n√\n3 [v][(][δ][)]\n\n\n\nλ + √2v(δ) + 30v [2] (δ)\n\n\n\n+\n\n\n26\n\n\n\n1\n�k∈[K] ( [σ][�] H [(][z] H [k] [)][)] 2 [(][f][ �][(][z] H [k] [)][ −] [f][¯][(][z] H [k] [))][2]\n\n. (G.5)\n4\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nThen we use Lemma G.2. Let f = f [�] H ∈FH, f [�] = f [¯] H ∈FH, f [′] = f [�] H+1 = 0. We define:\n\n\nξH [k] [[][f][ ′][] :=][ f][ ′][(][s][k] H+1 [)][ −] [V] H [ ∗] +1 [(][s] H [k] +1 [)][ −] [[][P][H][(][f][ ′][ −] [V] H [ ∗] +1 [)](][z] H [k] [)]\n\n\n\nξH [k] [[][f][ ′][]]\n∆ [k] H [[][f,][ �][f, f][ ′][] := 2] �\n(σH (zH [k] [))][2]\n\n\n\nf (zH [k] [)][ −] [f][�][(][z] H [k] [)] .\n� �\n\n\n\nTaking a union bound over the function class, with probability at least 1 − δ/(4H [2] ), we have\n\n\n\nH [[][ �][f][H][+1][]]\nk�∈[K] 2 ( [ξ] σ� [k] H (zH [k] [))][2]\n\n\n\n� 4\nfH(zH [k] [)][ −] [f][¯][H] [(][z] H [k] [)] ≤ √\n� � � 3 [ι][(][δ][)]\n\n\n\n� 4\nfH(zH [k] [)][ −] [f][¯][H] [(][z] H [k] [)] ≤\n� � � 3\n\n\n\nλ + √2ι(δ) ∥f [′] − VH [∗] +1 [∥] ∞ [2]\n\n�\n\n\n\n2ι(δ) ∥f [′] − VH [∗] +1 [∥] ∞ [2]\n�\n\n\n\n+ [2]\n\n\n\nlog Nb + 30ι [2] (δ)∥f [�] H+1 − VH [∗] +1 [∥] ∞ [2] [+]\n\n\n\n1 � 2\n�k∈[K] (σ�H (zH [k] [))][2] �fH (zH [k] [)][ −] [f][¯][H][(][z] H [k] [)] �\n\n4 .\n\n(G.6)\n\n\n\n�\n3 [ι][2][(][δ][)][/]\n\n\n\nCombining (G.5) and (G.6), we have with probability at least 1 − δ/(2H [2] ), the following inequality\nholds:\n\n\n\n2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σH (zH [k] [))][2]\n\n\n\n�\n�rH [k] [+][ �][f][H][+1][(][s][k] H+1 [)][ −] [f][¯][H][(][z] H [k] [)] ��fH (zH [k] [)][ −] [f][¯][H][(][z] H [k] [)] �\n\n\n\n= 2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σH (zH [k] [))][2]\n\n\n\n�\nrH [k] [+][ �][f][H][+1][(][s][k] H+1 [)][ −] [[][T][H] [f][�][H][+1][](][z] H [k] [)] fH(zH [k] [)][ −] [f][¯][H][(][z] H [k] [)]\n� �� �\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σH (zH [k] [))][2]\n\n\n\n�\n\n[TH f [�] H+1](zH [k] [)][ −] [f][¯][H] [(][z] H [k] [)] fH (zH [k] [)][ −] [f][¯][H] [(][z] H [k] [)]\n� �� �\n\n\n\n≤ 2\n�\n\nk∈[K]\n\n\n\n1\n\n�\n(σH (zH [k] [))][2]\n\n\n\n�\n�rH [k] [+][ �][f][H][+1][(][s][k] H+1 [)][ −] [[][T][H][ �][f][H][+1][](][z] H [k] [)] ��fH(zH [k] [)][ −] [f][¯][H][(][z] H [k] [)] � + 4KLǫ\n\n\n\n2ι(δ) ∥f [′] − VH [∗] +1 [∥] ∞ [2] [+ 2]\n� 3\n\n\n\n≤ [4] √\n\n3 [v][(][δ][)]\n\n\n\n4\n2v(δ) + 30v [2] (δ) + √\n� 3 [ι][(][δ][)]\n\n\n\n4\n2v(δ) + 30v [2] (δ) +\n� 3\n\n\n\nλ + √\n\n\n\nλ + √\n\n\n\n3 [ι][2][(][δ][)][/][log][ N][b]\n\n\n\n+ 30ι [2] (δ)∥f [�] H+1 − VH [∗] +1 [∥] ∞ [2] [+ 8][KLǫ][ +]\n\n\n\n1 ¯ 2\n�k∈[K] (σ�H (zH [k] [))][2] �fH (zH [k] [)][ −] [f][�][H] [(][z] H [k] [)] �\n\n\n2\n\n\n\n≤ [(][β][H] [)][2] +\n\n2\n\n\n\n1 ¯ 2\n�k∈[K] (σ�H (zH [k] [))][2] �fH (zH [k] [)][ −] [f][�][H] [(][z] H [k] [)] �\n\n2, (G.7)\n\n\n\nwhere the first inequality holds due to the complete assumption. The second inequality holds due\nto (G.5) and (G.6). The last inequality holds due to the fact f [�] H+1 = VH [∗] +1 [= 0][ and our choice of]\nβH,i.e.,\n\n\n\n�\n\n\n\n2v(δ) + 30v [2] (δ) + [2]\n\n\n\n\n[2]\n\n3 [ι][2][(][δ][)][/][log][ N][b][ + 8][KLǫ] �\n\n\n\nβH =\n\n\n\n4\n2\n� 3\n\n\n\n√\n3 [v][(][δ][)]\n\n\n\nλ + √\n\n\n\n= O [�] (�log N ).\n\n\n\nHowever, conditioned on the event EH, we have\n\n\n1\n\n� � [k] [2] �rH [k] [+][ �][f][H][+1][(][s][k] H\n\n\n\n�\n�rH [k] [+][ �][f][H][+1][(][s][k] H+1 [)][ −] [f][¯][H] [(][z] H [k] [)] ��fH (zH [k] [)][ −] [f][¯][H] [(][z] H [k] [)] �\n\n\n\nk∈[K]\n\n\n\n1\n\n�\n(σH (zH [k] [))][2]\n\n\n\n¯ 2\nfH(zH [k] [)][ −] [f][�][H][(][z] H [k] [)]\n� �\n\n\n\n≥\n�\n\nk∈[K]\n\n\n\n1\n\n�\n(σH (zH [k] [))][2]\n\n\n\n- [(][β][H] [)][2] +\n\n2\n\n\n\n1 ¯ 2\n�k∈[K] (σ�(zH [k] [))][2] �fH(zH [k] [)][ −] [f][�][H] [(][z] H [k] [)] �\n\n2,\n\n\n\nwhere the first inequality holds due to (G.4). The second inequality holds due to EH . It contradicts\nwith (G.7). Therefore, we prove that P[EH ] ≥ 1 − δ/2H [2] .\n\n\n27\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nSuppose the event EH holds, we can prove the following result.\n\n\nQ [∗] H [(][s, a][) = [][T][H] [V] H [ ∗] +1 [](][s, a][)]\n\n= [TH f [�] H+1](s, a)\n\n\n�\n≥ f [�] H(s, a) − [TH �fH+1](s, a) − fH (s, a)\n��� ���\n\n≥ f [�] H(s, a) − ǫ + |f [¯] H(s, a) − f [�] H (s, a)|\n� �\n\n≥ f [�] H(s, a) − bH (s, a) − ǫ\n\n= f [�] H(s, a).\n\n\n\nwhere the first inequality holds due to the triangle inequality. The second inequality holds due to the\ncompleteness assumption. The third inequality holds due to the property of the bonus function and\n\n\n1 ¯ 2\n\n� � [k] [2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] � ≤ (βh) [2] .\n\n\n\n¯ 2\n�fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] � ≤ (βh) [2] .\n\n\n\nk∈[K]\n\n\n\n1\n\n�\n(σh(zh [k][))][2]\n\n\n\nby Lemma F.2. Therefore, VH [∗] [(][s][)][ ≥] [f][�][H][(][s][)][ for all][ s][ ∈S][.]\nWe also have\n\n\nVH [∗] [(][s][)][ −] [f][�][H] [(][s][) =][ ⟨][Q][∗] H [(][s,][ ·][)][ −] [f][�][H][(][s,][ ·][)][, π][∗][(][·|][s][)][⟩][A] [+][ ⟨][f][�][H] [(][s,][ ·][)][, π] H [∗] [(][·|][s][)][ −] [π][�][H] [(][·|][s][)][⟩][A]\n\n≤⟨Q [∗] H [(][s,][ ·][)][ −] [f][�][H][(][s,][ ·][)][, π][∗][(][·|][s][)][⟩][A]\n\n= ⟨[TH VH [∗] [](][s,][ ·][)][ −] [f][�][H][(][s,][ ·][) +][ b][H] [(][s, a][)][, π][∗][(][·|][s][)][⟩][A]\n\n= ⟨[TH f [�] H+1](s, ·) − f [�] H (s, ·) + bH(s, a), π [∗] (·|s)⟩A\n\n+ ⟨[TH VH [∗] +1 [](][s,][ ·][)][ −] [[][T][H] [f][�][H][+1][](][s,][ ·][)][, π] H [∗] [(][·|][s][)][⟩][A]\n≤ 2⟨bH(s, ·), πH [∗] [(][·|][s][)][⟩][A][ +][ ǫ]\n\n\n\n√log N H 2\n≤ O [�]\n� √Kκ\n\n\n\n,\n�\n\n\n\n√\n\n\n\nKκ\n\n\n\nwhere the first inequality holds due to the policy �πH takes the action which maximizes f [�] H. The\nsecond inequality holds due to the Bellman completeness assumption. The last inequality holds due\nto the property of the bonus function\n\n\nbH (z) ≤ C · �DFH (z; DH; �σH ) · �(βH ) [2] + λ + ǫβH�\n\n\n\nand Lemma F.1.\nThen we do the induction step. Let Rh = O [�] � √log√Kκ N H2\n\n\n\n√\n\n\n\nKκ\n\n\n\n\n - (H − h + 1), δh = (H − h + 1)δ/(4H [2] ).\n�\n\n\n\nWe define another event Eh [ind] for induction.\n\n\nEh [ind] = {0 ≤ Vh [∗][(][s][)][ −] [f][�][h][(][s][)][ ≤] [R][h][,][ ∀][s][ ∈S}][.]\n\n\nThe above analysis shows that EH ⊆EH [ind] [and][ P][[][E][H] []][ ≥] [1][ −] [2][δ][H] [. Moreover,][ P][[][E] H [ind][]][ ≥] [1][ −] [2][δ][H]\nWe conduct the induction in the following way. At stage h, if P[Eh+1] ≥ 1 − 2δh+1 and P[Eh [ind] +1 []][ ≥]\n1 − 2δh+1, we prove that P[Eh] ≥ 1 − 2δh and P[Eh [ind][]][ ≥] [1][ −] [2][δ][h][.]\nSuppose at stage h, P[Eh+1] ≥ 1 − 2δh+1 and P[Eh [ind] +1 []][ ≥] [1][ −] [2][δ][h][+1][. We first use Lemma G.1. Let]\nf = f [�] h ∈Fh, f [′] = f [¯] h ∈Fh. We define\n\nηh [k] [:=][ V] h [ ∗] +1 [(][s] h [k] +1 [)][ −] [[][P][h][V] h [ ∗] +1 [](][z] h [k][)]\n\nDh [k][[][f, f][ ′][] := 2] � ηh [k] �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � .\n(σh(zh [k][))][2]\n\n\nAfter taking a union bound, we have with probability at least 1 − δ/(4H [2] ), the following inequality\nholds,\n� 2 � ηh [k][k] [2] �f�(zh [k][)][ −] [f][¯][(][z] h [k][)] � ≤ [4] [v][(][δ][)] √λ + √2v(δ) + 30v [2] (δ)\n\n\n\nk�∈[K] 2 (σ�h(ηzh [k] h [k][))][2]\n\n\n\n�\nf (zh [k][)][ −] [f][¯][(][z] h [k][)] ≤ [4]\n� � 3\n\n\n\n√\n3 [v][(][δ][)]\n\n\n\nλ + √\n\n\n\n2v(δ) + 30v [2] (δ)\n\n\n\n28\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n+\n\n\n\n1 � 2\n�k∈[K] (σ�h(zh [k][))][2] �f (zh [k][)][ −] [f][¯][(][z] h [k][)] �\n\n. (G.8)\n4\n\n\n\nNext, we use Lemma G.2 at stage h. Let f = f [�] h ∈Fh, f [�] = f [¯] h ∈Fh, f [′] = f [�] h+1 = { [�] b}[0,H−h+1],\nwhere [�] b = f [�] h − bh ∈Fh −W. We define:\n\nξh [k][[][f][ ′][] :=][ f][ ′][(][s] h [k] +1 [)][ −] [V] h [ ∗] +1 [(][s] h [k] +1 [)][ −] �Ph(f [′] − Vh [∗] +1 [)] � (zh [k][)]\n\n\n\nξh [k][[][f][ ′][]]\n∆ [k] h [[][f,][ �][f, f][ ′][] := 2] �\n(σh(zh [k][))][2]\n\n\n\n�f (zh [k][)][ −] [f][�][(][z] h [k][)] �,\n\n\n\nAfter taking a union bound, we have with probability at least 1 − δ/(4H [2] ), we have\n\n\n\nh [[][ �][f][h][+1][]]\nk�∈[K] 2 ( [ξ] σ� [k] h(zh [k][))][2]\n\n\n\n� 4\nfh(zh [k][)][ −] [f][¯][h][(][z] h [k][)] ≤ √\n� � � 3 [ι][(][δ][)]\n\n\n\nλ + √2ι(δ) ∥f [′] − Vh [∗] +1 [∥] ∞ [2]\n\n�\n\n\n\n+ [2]\n\n\n\nlog Nb + 30ι [2] (δ)∥f [�] h+1 − Vh [∗] +1 [∥] ∞ [2] [+]\n\n\n\n1\n�k∈[K] (σ�h(zh [k][))][2][ (][f][ �][h][(][z] h [k][)][ −] [f][¯][h][(][z] h [k][))][2]\n\n4 .\n\n(G.9)\n\n\n\n�\n3 [ι][2][(][δ][)][/]\n\n\n\nLet Uh be the event that (G.8) and (G.9) holds simultaneously. On the event Uh ∩Eh [ind] +1 [, which]\nsatisfies P[Uh ∩Eh [ind] +1 []][ ≥] [1][ −] [2][δ][h][+1][ −] [2][δ/H] [2][ = 1][ −] [2][δ][h][, we have]\n\n\n\n2 �\n\nk∈[K]\n\n\n\n1 �\n(σ�h(zh [k][))][2] �rh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [f][¯][h][(][z] h [k][)] ��fh(zh [k][)][ −] [f][¯][h][(][z] h [k][)] �\n\n\n\n= 2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σh(zh [k][))][2]\n\n\n\n�\n�rh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [[][T][h][ �][f][h][+1][](][z] h [k][)] ��fh(zh [k][)][ −] [f][¯][h][(][z] h [k][)] �\n\n\n\n+ 2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σh(zh [k][))][2]\n\n\n\n�\n\n[Thf [�] h+1](zh [k][)][ −] [f][¯][h][(][z] h [k][)] fh(zh [k][)][ −] [f][¯][h][(][z] h [k][)]\n� �� �\n\n\n\n≤ 2\n�\n\nk∈[K]\n\n\n\n1\n\n�\n(σh(zh [k][))][2]\n\n\n\n�\n�rh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [[][T][h][ �][f][h][+1][](][z] h [k][)] ��fh(zh [k][)][ −] [f][¯][h][(][z] h [k][)] � + 4KLǫ\n\n\n\nλ + √2ι(δ) ∥f [�] h+1 − Vh [∗] +1 [∥] ∞ [2]\n\n�\n\n\n\n≤ [4] √\n\n3 [v][(][δ][)]\n\n\n\n4\n2v(δ) + 30v [2] (δ) +\n� 3\n\n\n\n2ι(δ) ∥f [�] h+1 − Vh [∗] +1 [∥] ∞ [2]\n�\n\n\n\nλ + √\n\n\n\n√\n3 [ι][(][δ][)]\n\n\n\n+ [2] h+1 [∥] ∞ [2] [+ 8][KLǫ][ +]\n\n3 [ι][2][(][δ][)][/][ log][ N][b][ + 30][ι][2][(][δ][)][∥][f][�][h][+1][ −] [V][ ∗]\n\n\n\n1 ¯ 2\n�k∈[K] (σ�h(zh [k][))][2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] �\n\n\n2\n\n\n\n≤ [(][β][h][)][2] +\n\n2\n\n\n\n1 ¯ 2\n�k∈[K] (σ�(zh [k][))][2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] �\n\n2, (G.10)\n\n\n\nwhere the first inequality holds due to the completeness assumption. The second inequality holds\ndue to (G.8) and (G.9). The last inequality holds due to the event Eh [ind] +1 [and the choice of][ K][ ≥]\n� ι(δ) [2] H [6]\nΩ � κ �.\n\nHowever, on the event of Eh [c] [, we have]\n\n\n\n2 �\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\n�\n�rh [k] [+][ �][f][h][+1][(][s][k] h+1 [)][ −] [f][¯][h][(][z] h [k][)] ��fh(zh [k][)][ −] [f][¯][h][(][z] h [k][)] �\n\n\n\n¯ 2\nfh(zh [k][)][ −] [f][�][h][(][z] h [k][)]\n� �\n\n\n\n≥\n�\n\nk∈[K]\n\n\n\n1\n\n�\n(σ(zh [k][))][2]\n\n\n\n- [(][β][h][)][2] +\n\n2\n\n\n\n1 ¯ 2\n�k∈[K] (σ�(zh [k][))][2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] �\n\n2 .\n\n\n\nwhere the first inequality holds due to (G.4). The second inequality holds due to event Eh [c] [. However,]\nthis contradicts with (G.10). We conclude that Uh ∩Eh [ind] +1 [⊆E][h][, thus][ P][[][E][h][]][ ≥] [1][ −] [2][δ][h][.]\n\n\n29\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\nNext we prove P[Eh [ind][]][ ≥] [1][ −] [2][δ][h][. Suppose the event][ U][h][ ∩E] h [ind] +1 [holds, the above conclusion shows]\nthat\n\n\n1 ¯ 2\n\n� � [k] [2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] �    - (βh) [2] .\n\n\n\n¯ 2\n�fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] � - (βh) [2] .\n\n\n\nk∈[K]\n\n\n\n1\n\n�\n(σh(zh [k][))][2]\n\n\n\nWe can prove the following result.\n\n\nQ [∗] h [(][s, a][) = [][T][h][V] h [ ∗] +1 [](][s, a][)]\n\n≥ [Thf [�] h+1](s, a)\n\n≥ f [�] h(s, a) −|[Thf [�] h+1](s, a) − f [�] h(s, a)|\n\n≥ f [�] h(s, a) − (ǫ + |f [¯] h(s, a) − f [�] h(s, a)|)\n\n≥ f [�] h(s, a) − bh(s, a)\n\n= f [�] h(s, a),\n\n\n\nwhere the first inequality hold due to the event Eh [ind][. The second inequality holds due to the triangle]\ninequality. The third inequality holds due to the completeness assumption. The last inequality holds\ndue to the property of the bonus function and\n\n\n1 ¯ 2\n\n� � [k] [2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] � ≤ (βh) [2] .\n\n\n\n¯ 2\nfh(zh [k][)][ −] [f][�][h][(][z] h [k][)] ≤ (βh) [2] .\n� �\n\n\n\nk∈[K]\n\n\n\n1\n\n�\n(σh(zh [k][))][2]\n\n\n\nTherefore, Vh [∗][(][s][)][ ≥] [f][�][h][(][s][)][ for all][ s][ ∈S][.]\nWe also have\n\n\nVh [∗][(][s][)][ −] [f][�][h][(][s][) =][ ⟨][Q][∗] h [(][s,][ ·][)][ −] [f][�][h][(][s,][ ·][)][, π][∗][(][·|][s][)][⟩][A][ +][ ⟨][f][�][h][(][s,][ ·][)][, π] h [∗][(][·|][s][)][ −] [π][�][h][(][·|][s][)][⟩][A]\n\n≤⟨Q [∗] h [(][s,][ ·][)][ −] [f][�][h][(][s,][ ·][)][, π][∗][(][·|][s][)][⟩][A]\n\n= ⟨[ThVh [∗][](][s,][ ·][)][ −] [f][�][h][(][s,][ ·][) +][ b][h][(][s, a][)][, π][∗][(][·|][s][)][⟩][A]\n\n= ⟨[Thf [�] h+1](s, ·) − f [�] h(s, ·) + bh(s, a), π [∗] (·|s)⟩A\n\n+ ⟨[ThVh [∗] +1 [](][s,][ ·][)][ −] [[][T][h][ �][f][h][+1][](][s,][ ·][)][, π] h [∗][(][·|][s][)][⟩][A]\n≤ 2⟨bh(s, ·), πh [∗][(][·][, s][)][⟩][A] [+][ ǫ][ +][ R][h][+1]\n\n\n\n√log N H 2\n≤ O [�]\n� √Kκ\n\n\n\n√\n\n\n\nKκ\n\n\n\n\n  - (H − h + 1) = Rh.\n�\n\n\n\nwhere the first inequality holds due to the policy �πh takes the action which maximizes f [�] h. The\nsecond inequality holds due to the Bellman completeness assumption. The second inequality holds\ndue to the property of the bonus function\n\n\nbh(z) ≤ C · �DFh(z; Dh; �σh [2][)][ ·] �(βh) [2] + λ + ǫβh�\n\n\n\nand Lemma F.1. The last inequality holds due to the induction assumption. Therefore, we have\nUh ∩Eh [ind] +1 [⊆E] h [ind] and P[Eh [ind][]][ ≥] [1][ −] [2][δ][h][. Thus we complete the proof of induction.]\nFinally, taking the union bound of all the Eh, we get the result that with probability at least 1 − δ/2,\nthe event ∪ [H] h=1 [E][h][ holds, i.e for any][ h][ ∈] [[][H][]][ simultaneously, we have]\n\n\n1 ¯ 2\n\n� � [k] [2] �fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] � ≤ (βh) [2] .\n\n\n\n¯ 2\n�fh(zh [k][)][ −] [f][�][h][(][z] h [k][)] � ≤ (βh) [2] .\n\n\n\nk∈[K]\n\n\n\n1\n\n�\n(σh(zh [k][))][2]\n\n\n\nTherefore, we complete the proof of Lemma F.2.\n\n\nH PROOF OF LEMMAS IN SECTION E AND G\n\nH.1 PROOF OF LEMMA E.1\nProof of Lemma E.1. We use Lemma I.1, with the following conditions:\n\n\nD¯ h [k][[][f, f][ ′][]][ is adapted to the filtration][ ¯][H] h [k] [and][ E] �D¯ hk [[][f, f][ ′][]][ |][ ¯][H] h [k][−][1] � = 0.\n\n\n30\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n��D¯ hk [[][f, f][ ′][]] �� ≤ 2 ��η¯hk�� maxz |f (z) − [Thf [′] ](z)| ≤ 4H [2] = M.\n� E ��D¯ hk [[][f, f][ ′][]] �2 [�] ��z¯hk� = � E �4 �η¯h [k][[][f][ ′][]] �2 [�] ��z¯hk��f\n\n\n\n� E �4 �η¯h [k][[][f][ ′][]] �2 [�] ��z¯hk��f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] �2 ≤ (4HK)2 = V 2.\n\nk∈[K]\n\n\n\n� E ��D¯ hk [[][f, f][ ′][]] �2 [�] ��z¯hk� = �\n\nk∈[K] k∈[K\n\n\n\nOn the other hand,\n\n� E\n\n\n\n� E �4 �η¯h [k][[][f][ ′][]] �2 [�] ��z¯hk��f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] �2\n\nk∈[K]\n\n\n\n� E ��D¯ hk [[][f, f][ ′][]] �2 [�] ��z¯hk� = �\n\nk∈[K] k∈[K\n\n\n\n2\n\n≤ 8H [2][ �] �f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] � .\n\n\nk∈[K]\n\n\n\nThen using Lemma I.1 with v = 1, m = 1, with at least 1 − δ/(4H [2] N [2] Nb [2][)]\n\n� 2¯ηh [k][[][f][ ′][]] �f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] � ≤ i(δ) 2(2 · 8H [2] ) � �f (¯zh [k][)][ −]\n\n\n\n� 2¯ηh [k][[][f][ ′][]] �f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] � ≤ i(δ)\n\nk∈[K] �\n\n\n\n2(2 · 8H [2] ) �\n\n\n\n2\n�f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] �\n\n\n\nk∈[K]\n\n\n\n\n[2]\n\n3 [i][2][(][δ][) + 4] 3\n\n\n\n+ [2]\n\n\n\n3 [i][2][(][δ][)][ ·][ 4][H] [2]\n\n\n\n≤ (24H [2] + 5)i [2] (δ) +\n\n\n\n2\n�k∈[K] �f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] �\n\n2 .\n\n\n\nWe complete the proof of Lemma E.1.\n\n\nH.2 PROOF OF LEMMA E.2\nProof of Lemma E.2. We use Lemma I.1, with the following conditions:\n\n\n\nD¯ h [k][[][f, f][ ′][]][ is adapted to the filtration][ ¯][H] h [k] [and][ E] �D¯ hk [[][f, f][ ′][]][ |][ ¯][H] h [k][−][1] � = 0.\n��D¯ hk [[][f, f][ ′][]] �� ≤ 2|η¯hk [|][ max] z |f (z) − [T2,hf [′] ](z)| ≤ 4L [2] = M.\n� E ��D¯ hk [[][f, f][ ′][]] �2 [�] ��z¯hk� = � E �4(¯ηh [k][[][f][ ′][])][2][|][z][¯] h [k] ��f (¯zh [k][)][ −] [[][T][2][,h][f]\n\n\n\n� E ��D¯ hk [[][f, f][ ′][]] �2 [�] ��z¯hk� = �\n\nk∈[K] k∈[K\n\n\n\n� E �4(¯ηh [k][[][f][ ′][])][2][|][z][¯] h [k] ��f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] �2 ≤ (4L2K)2 = V 2.\n\nk∈[K]\n\n\n\nOn the other hand,\n\n� E\n\n\n\n� E �4 �η¯h [k][[][f][ ′][]] �2 [�] ��z¯hk��f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] �2\n\nk∈[K]\n\n\n\n� E ��D¯ hk [[][f, f][ ′][]] �2 [�] ��z¯hk� = �\n\nk∈[K] k∈[K\n\n\n\n2\n\n≤ 8L [4][ �] �f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] � .\n\n\nk∈[K]\n\n\n\nThen using Lemma I.1 with v = 1, m = 1, we have:\n\n� 2¯ηh [k][[][f][ ′][]] �f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] � ≤ i [′] (δ)\n\n\n\n� 2¯ηh [k][[][f][ ′][]] �f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] � ≤ i [′] (δ)\n\nk∈[K] �\n\n\n\n2(2 · 8L [4] ) �\n\n\n\n2\n�f (¯zh [k][)][ −] [[][T][2][,h][f][ ′][](¯][z] h [k][)] �\n\n\n\nk∈[K]\n\n\n\n\n[2]\n\n3 [i][′][2][(][δ][) + 4] 3\n\n\n\n+ [2]\n\n\n\n3 [i][′][2][(][δ][)][ ·][ 4][L][2]\n\n\n\n≤ (20L [4] + 5)i [′][2] (δ) +\n\n\n\n2\n�k∈[K] �f (¯zh [k][)][ −] [[][T][h][f][ ′][](¯][z] h [k][)] �\n\n\n\n2\n\nWe complete the proof of Lemma E.2.\n\n\n\nH.3 PROOF OF LEMMA G.1\nProof of Lemma G.1. We use Lemma I.1, with the following conditions:\n\n\nDh [k][[][f, f][ ′][]][ is adapted to the filtration][ H] h [k] [and][ E] �Dh [k][[][f, f][ ′][]][ | H] h [k][−][1] � = 0.\n��Dhk [[][f, f][ ′][]] �� ≤ 2 ��ηhk�� maxz |f (z) − f [′] (z)| ≤ 8LH = M.\n\n\n\n� E ��Dh [k][[][f, f][ ′][]] �2 [�] ��zhk� = 4 �\n\nk∈[K] k∈[K\n\n\n\n�\n\n\n\nk∈[K]\n\n\n\nE �(ηh [k][)][2][��][z] h [k] �\n\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � .\n(σh(zh [k][))][4]\n\n\n\n31\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\nOn the other hand,\n\n�\n\n\n\n� E ��Dh [k][[][f, f][ ′][]] �2 [�] ��zhk� = 4 �\n\nk∈[K] k∈[K\n\n\n\nk∈[K]\n\n\n\nE �(ηh [k][)][2][��][z] h [k] � 2\n\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] �\n(σh(zh [k][))][4]\n\n\n1 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] �,\n(σh(zh [k][))][2]\n\n\n\n≤ 8\n�\n\nk∈[K]\n\n\n\nwhere the last inequality holds because of the inequality in Lemma D.4:\nE �(ηh [k][)][2][|][z] h [k] � = [VarhVh [∗] +1 [](][s] h [k][, a] h [k][)]\n\n≤ [VhVh [∗] +1 [](][s] h [k][, a][k] h [)]\n\n\n\n��\n\n\n\n�\n\n\n\n≤ �σ�h(zh [k][)] �2 + �O\n\n\n≤ 2 �σ�h(zh [k][)] �2,\n\n\n\nlog(N · Nb)H [3]\n\n\n\n√Kκ\n\n\n\nlog(N·Nb)H [6]\nwhere we use the requirement that K ≥ Ω [�] � κ �.\n\n\n\nMoreover, for any k ∈ [K],\n��Dhk [[][f, f][ ′][]] �� ≤ 2 � ηh [k]\n���� (σh(zh [k][))][2]\n\n\n\nk\n��f (zh [)][ −] [f][ ′][(][z] h [k][)] ��\n����\n\n\n\n1 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + λ\n(σh(zh [k][))][2]\n\n\n\n≤ 4H\n\n\n\n�\n�\n�\n�D [2]\nFh [(][z] h [k][,][ D][h][,][ �][σ] h [2][)]\n�\n\n\n\n\n\n\n\nk [�] ∈[K\n\n\n\nk∈[K]\n\n\n\n\n\n\n\n\n\n4H 2\n≤ O [�]\n� √Kκ\n\n\n\n1 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + λ\n(σh(zh [k][))][2]\n\n\n\n√\n\n\n\nKκ\n\n\n\n�\n�\n� [�]\n�\n\n\n\nk∈[K]\n\n\n\n�\n\n\n\n�\n�\n�\n�\n\n\n\n1\n≤\nv(δ)\n\n\n\nk∈[K]\n\n\n\n�\n\n\n\n1 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + λ.\n(σh(zh [k][))][2]\n\n\n\nThe second inequality holds because of the definition of D [2] divergence (Definition 3.4). The\nthird inequality holds due to Lemma F.1. The last inequality holds because of the choice of\nv [2] (δ)H [4]\nK ≥ Ω [�] � κ �.\n\n\n\nv [2] (δ)H [4]\nK ≥ Ω [�] � κ �.\n\nThen using Lemma I.1 with v = 1, m = 1, we have\n\n\n�\n\n� 2 � ηh [k][k] [2] �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � ≤ v(δ)��16\n\n\n\n�\n�\n�16\n�\n\nk∈[K\n\n�\n\n\n\nk�∈[K] 2 (σ�h(ηzh [k] h [k][))][2] �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � ≤ v(δ)\n\n\n\nk∈[K]\n\n\n\n1 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + 2 + 2\n(σh(zh [k][))][2] 3 [v][2][(][δ][)]\n\n\n\n+ [4]\n\n\n\n�\n�\n�\n�\n\n\n\n3 [v][(][δ][)]\n\n\n\n1 2\n\nk�∈[K] (σ�h(zh [k][))][2] �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + λ\n\n\n\n≤ [4] √\n\n3 [v][(][δ][)]\n\n\n\n≤ [4]\n\n\n\n2v(δ) + 30v [2] (δ)\n\n\n\nλ + √\n\n\n\n1 2\n�k∈[K] ( [σ][�] h [(][z] h [k][)][)] 2 �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] �\n\n+ .\n\n4\n\n\n\nWe complete the proof of Lemma G.1.\n\n\nH.4 PROOF OF LEMMA G.2\n\nProof of Lemma G.2. ∆ [k] h [[][f,][ �][f, f][ ′][]][ is adapted to the filtration][ H] h [k] [and][ E] ∆ [k] h [[][f,][ �][f, f][ ′][]][ | H] h [k][−][1] = 0.\n� �\nWe also have\n\n\n\n� E �(∆h [k] [[][f,][ �][f, f][ ′][])][2][���][z] h [k] � = 4 �\n\nk∈[K] k∈[K\n\n\n\nE �(ξh [k][[][f][ ′][])][2][���][z] h [k] � 2\n\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] �\n(σh(zh [k][))][4]\n\n\n\n�\n\n\n\nk∈[K]\n\n\n\n32\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\n≤ 8\n�\n\nk∈[K]\n\n\n\n∥f [′] − Vh [∗] +1 [∥] ∞ [2] 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � .\n(σh(zh [k][))][2]\n\n\n\nMoreover, for any k ∈ [K],\nk ξh [k][[][f][ ′][]] k\n���∆h [[][f,][ �][f, f][ ′][]] ��� ≤ 2 ���� (σ�h(zh [k][))][2] ���� ��f (zh [)][ −] [f][ ′][(][z] h [k][)] ��\n\n\n\n�\n�\n�\n\n≤ 4∥f [′] − Vh [∗] +1 [∥][∞] �DF [2] h [(][z] h [k][,][ D][h][,][ �][σ] h [2][)]\n\n�\n\n\n\n\n\n\n\nk [�] ∈[K\n\n\n\nk∈[K]\n\n\n\n1 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + λ\n(σh(zh [k][))][2] \n\n\n\nk∈[K]\n\n\n\nH\n≤ O [�]\n� √Kκ\n\n\n\nH\n≤ O [�]\n� √\n\n\n\n\n  - ∥f [′] − Vh [∗] +1 [∥][∞]\n�\n\n\n\n�\n�\n�\n�\n\n\n\n�\n\n\n\n1 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + λ\n(σh(zh [k][))][2]\n\n\n\n�\n�\n�\n�\n\n\n\n≤ [∥][f][ ′][ −] [V] h [ ∗] +1 [∥][∞]\nι(δ)\n\n\n\nk∈[K]\n\n\n\n�\n\n\n\n1 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + λ\n(σh(zh [k][))][2]\n\n\n\nThe second inequality holds because of the definition of D [2] divergence (Definition 3.4). The\nthird inequality holds due to Lemma F.1. The last inequality holds because of the choice of\nι [2] (δ)H [4]\nK ≥ Ω [�] κ .\n� �\n\nThen using Lemma I.1 with v = 1, m = 1/ log Nb, with probability at least 1 − δ/(4H [2] N [3] Nb), we\nhave\n\nξh [k][[][f][ ′][]] �� ∥f [′] − Vh [∗] +1 [∥] ∞ [2] 2\n\n� 2 � [k] [2] �f (zh [k][)][ −] [f][�][(][z] h [k][)] � ≤ ι(δ)�8 � � [k] [2] �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + 2\n\n\n\nξh [k][[][f][ ′][]]\nk�∈[K] 2 (σ�h(zh [k][))][2]\n\n\n\n�\n�\n�8 �\n� k∈[K\n\n\n\nf (zh [k][)][ −] [f][�][(][z] h [k][)] ≤ ι(δ)\n� �\n\n\n\nk∈[K]\n\n\n\n∥f [′] − Vh [∗] +1 [∥] ∞ [2] 2\n� �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] � + 2\n(σh(zh [k][))][2]\n\n\n\n\n[2]\n\n3 [ι][2][(][δ][)][/][ log][ N][b][ + 4] 3\n\n\n\n�\n�\n�\n�\n\n\n\n+ [2]\n\n\n\n3 [ι][(][δ][)][∥][f][ ′][ −] [V] h [ ∗] +1 [∥][∞]\n\n\n\n1\n� h [)][ −] [f][ ′][(][z] h [k][))][2][ +][ λ]\n(σh(zh [k][))][2][ (][f] [(][z][k]\n\n\n\n�\n\nk∈[K]\n\n\n\n4\n≤ √\n� 3 [ι][(][δ][)]\n\n\n\nλ + √\n\n\n\n2ι(δ) ∥f [′] − Vh [∗] +1 [∥] ∞ [2] [+ 2] h+1 [∥] ∞ [2]\n� 3 [ι][2][(][δ][)][/][ log][ N][b][ + 30][ι][2][(][δ][)][∥][f][ ′][ −] [V][ ∗]\n\n\n\n+\n\n\n\n1 2\n�k∈[K] (σ�h(zh [k][))][2] �f (zh [k][)][ −] [f][ ′][(][z] h [k][)] �\n\n4 .\n\n\n\nWe complete the proof of Lemma G.2.\n\n\n\nI AUXILIARY LEMMAS\nLemma I.1 (Agarwal et al. 2023). Let M - 0, V - v > 0 be constants, and {xi}i∈[t]\nbe a stochastic process adapted to a filtration {Hi}i∈[t]. Suppose E[xi|Hi−1] = 0, |xi| ≤\nM and [�] i∈[t] [E][[][x] i [2][|H][i][−][1][]] ≤ V [2] almost surely. Then for any δ, ǫ - 0, let ι =\n�log [(2 log(][V/v][)+2)] δ [·][(log(][M/m][)+2)], we have\n\n\n\nlog [(2 log(][V/v][)+2)][·][(log(][M/m][)+2)]\n\n\n\nδ [·], we have\n\n\n\n\n\n[2] 2 max \n\n3 [ι][2] � i∈[t] [|][x][i][|][ +][ m] �\n\n\n\n� E[x [2] i [|H][i][−][1][] +][ v][2]\n\ni∈[t]\n\n\n\n ≤ δ.\n\n\n\n\n�\n�\n�\n�2\n�\n\n\n\n\n\n\n\n+ [2]\n 3\n\n\n\nP\n\n\n\n\n\n\n\n\n�\n\ni∈[t\n\n\n\n\n\nxi > ι\n\ni∈[t]\n\n\n\n\n\n\n\n2 �\n i∈[t\n\n\n\nLemma I.2 (Regret Decomposition Property, Jin et al. 2021b). Suppose the following inequality\nholds,\n\n�\n\n[Th �fh+1](z) − fh(z) ≤ bh(z), ∀z = (s, a) ∈S × A, ∀h ∈ [H],\n��� ���\n\nthe regret of Algorithm 1 can be bounded as\n\n\n\nV1 [∗][(][s][)][ −] [V] 1 [ �][π][(][s][)][ ≤] [2]\n\n\n\nH\n� Eπ∗ [bh (sh, ah) | s1 = s] .\n\n\nh=1\n\n\n\nHere Eπ∗ is with respect to the trajectory induced by π [∗] in the underlying MDP.\n\n\n33\n\n\n\n\nPublished as a conference paper at ICLR 2024\n\n\nLemma I.3 (Azuma-Hoeffding inequality, Cesa-Bianchi & Lugosi 2006). Let {xi} [n] i=1 [be a martin-]\ngale difference sequence with respect to a filtration {Gi} satisfying |xi| ≤ M for some constant M,\nxi is Gi+1-measurable, E[xi|Gi] = 0. Then for any 0 < δ < 1, with probability at least 1 − δ, we\nhave\n\n\n\nn\n�\n\n\n\n� xi ≤ M �\n\n\ni=1\n\n\n\n2n log(1/δ).\n\n\n\n34\n\n\n",
    "ranking": {
      "relevance_score": 0.7293911435018611,
      "citation_score": 0.4805283018867924,
      "recency_score": 0.6857963619207489,
      "final_score": 0.675259097020736
    },
    "citation_key": "Di2023PessimisticNL",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "d367d4de86dce92d30fba888ad190957f9ae14d8",
    "title": "Eligibility Traces and Plasticity on Behavioral Time Scales: Experimental Support of NeoHebbian Three-Factor Learning Rules",
    "published": "2018-01-16",
    "authors": [
      "W. Gerstner",
      "Marco P Lehmann",
      "Vasiliki Liakoni",
      "Dane S. Corneil",
      "Johanni Brea"
    ],
    "summary": "Most elementary behaviors such as moving the arm to grasp an object or walking into the next room to explore a museum evolve on the time scale of seconds; in contrast, neuronal action potentials occur on the time scale of a few milliseconds. Learning rules of the brain must therefore bridge the gap between these two different time scales. Modern theories of synaptic plasticity have postulated that the co-activation of pre- and postsynaptic neurons sets a flag at the synapse, called an eligibility trace, that leads to a weight change only if an additional factor is present while the flag is set. This third factor, signaling reward, punishment, surprise, or novelty, could be implemented by the phasic activity of neuromodulators or specific neuronal inputs signaling special events. While the theoretical framework has been developed over the last decades, experimental evidence in support of eligibility traces on the time scale of seconds has been collected only during the last few years. Here we review, in the context of three-factor rules of synaptic plasticity, four key experiments that support the role of synaptic eligibility traces in combination with a third factor as a biological implementation of neoHebbian three-factor learning rules.",
    "pdf_url": "https://www.frontiersin.org/articles/10.3389/fncir.2018.00053/pdf",
    "doi": "10.3389/fncir.2018.00053",
    "fields_of_study": [
      "Biology",
      "Mathematics",
      "Computer Science",
      "Medicine"
    ],
    "venue": "Front. Neural Circuits",
    "citation_count": 301,
    "bibtex": "@Article{Gerstner2018EligibilityTA,\n author = {W. Gerstner and Marco P Lehmann and Vasiliki Liakoni and Dane S. Corneil and Johanni Brea},\n booktitle = {Front. Neural Circuits},\n journal = {Frontiers in Neural Circuits},\n title = {Eligibility Traces and Plasticity on Behavioral Time Scales: Experimental Support of NeoHebbian Three-Factor Learning Rules},\n volume = {12},\n year = {2018}\n}\n",
    "markdown_text": "Edited by:\n\nEdward S. Ruthazer,\n\nMcGill University, Canada\n\n\nReviewed by:\n\nBlake A. Richards,\n\nUniversity of Toronto Scarborough,\n\nCanada\n\nJoel Zylberberg,\n\nUniversity of Colorado Anschutz\n\nMedical Campus, United States\n\n\n*Correspondence:\n\nWulfram Gerstner\n[wulfram.gerstner@epfl.ch](mailto:wulfram.gerstner@epfl.ch)\n\n\nReceived: 11 January 2018\n\nAccepted: 19 June 2018\n\nPublished: 31 July 2018\n\n\nCitation:\n\nGerstner W, Lehmann M, Liakoni V,\n\nCorneil D and Brea J (2018) Eligibility\n\nTraces and Plasticity on Behavioral\n\nTime Scales: Experimental Support of\n\nNeoHebbian Three-Factor Learning\n\nRules. Front. Neural Circuits 12:53.\n\n[doi: 10.3389/fncir.2018.00053](https://doi.org/10.3389/fncir.2018.00053)\n\n\n\n[REVIEW](https://www.frontiersin.org/journals/neural-circuits#editorial-board)\n[published: 31 July 2018](https://www.frontiersin.org/journals/neural-circuits#editorial-board)\n[doi: 10.3389/fncir.2018.00053](https://doi.org/10.3389/fncir.2018.00053)\n\n# Eligibility Traces and Plasticity on Behavioral Time Scales: Experimental Support of NeoHebbian Three-Factor Learning Rules\n\n\n[Wulfram Gerstner*, Marco Lehmann, Vasiliki Liakoni, Dane Corneil and Johanni Brea](http://loop.frontiersin.org/people/2298/overview)\n\n\nSchool of Computer Science and School of Life Sciences, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland\n\n\nMost elementary behaviors such as moving the arm to grasp an object or walking into\n\nthe next room to explore a museum evolve on the time scale of seconds; in contrast,\n\nneuronal action potentials occur on the time scale of a few milliseconds. Learning rules\n\nof the brain must therefore bridge the gap between these two different time scales.\n\nModern theories of synaptic plasticity have postulated that the co-activation of pre- and\npostsynaptic neurons sets a flag at the synapse, called an eligibility trace, that leads to a\nweight change only if an additional factor is present while the flag is set. This third factor,\n\nsignaling reward, punishment, surprise, or novelty, could be implemented by the phasic\nactivity of neuromodulators or specific neuronal inputs signaling special events. While the\n\ntheoretical framework has been developed over the last decades, experimental evidence\n\nin support of eligibility traces on the time scale of seconds has been collected only during\n\nthe last few years. Here we review, in the context of three-factor rules of synaptic plasticity,\n\nfour key experiments that support the role of synaptic eligibility traces in combination with\n\na third factor as a biological implementation of neoHebbian three-factor learning rules.\n\n\nKeywords: eligibility trace, hebb rule, reinforcement learning, neuromodulators, surprise, synaptic tagging,\n\nsynaptic plasticity, behavioral learning\n\n\n1. INTRODUCTION\n\n\nHumans are able to learn novel behaviors such as pressing a button, swinging a tennis racket,\nor braking at a red traffic light; they are also able to form memories of salient events, learn to\ndistinguish flowers, and to establish a mental map when exploring a novel environment. Memory\nformation and behavioral learning is linked to changes of synaptic connections (Martin et al., 2000).\nLong-lasting synaptic changes, necessary for memory, can be induced by Hebbian protocols that\ncombine the activation of presynaptic terminals with a manipulation of the voltage or the firing\nstate of the postsynaptic neuron (Lisman, 2003). Traditional experimental protocols of long-term\npotentiation (LTP) (Bliss and Lømo, 1973; Bliss and Collingridge, 1993), long-term depression\n(LTD) (Levy and Stewart, 1983; Artola and Singer, 1993) and spike-timing dependent plasticity\n(STDP) (Markram et al., 1997; Zhang et al., 1998; Sjöström et al., 2001) neglect that additional\nfactors such as neuromodulators or other gating signals might be necessary to permit synaptic\nchanges (Gu, 2002; Reynolds and Wickens, 2002; Hasselmo, 2006). Early STDP experiments that\ninvolved neuromodulators mainly focused on tonic bath application of modulatory factors (Pawlak\net al., 2010) with the exception of one study in locusts Cassenaer and Laurent (2012). However, from\nthe perspective of formal learning theories, to be reviewed below, the timing of modulatory factors\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 1 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nis just as crucial (Schultz and Dickinson, 2000; Schultz, 2002).\nFrom the theoretical perspective, STDP under the control of\nneuromodulators leads to the framework of three-factor learning\nrules (Xie and Seung, 2004; Legenstein et al., 2008; Vasilaki et al.,\n2009) where an eligibility trace represents the Hebbian idea of\nco-activation of pre- and postsynaptic neurons (Hebb, 1949)\nwhile modulation of plasticity by additional gating signals is\nrepresented generically by a “third factor” (Crow, 1968; Barto,\n1985; Legenstein et al., 2008). Such a third factor could represent\nvariables such as “reward minus expected reward” (Williams,\n1992; Schultz, 1998; Sutton and Barto, 1998) or the saliency of an\nunexpected event (Ljunberg et al., 1992; Redgrave and Gurney,\n2006).\nIn an earlier paper (Frémaux and Gerstner, 2016) we reviewed\nthe theoretical literature of, and experimental support for, threefactor rules available by the end of 2013. During recent years,\nhowever, the experimental procedures advanced significantly\nand provided direct physiological evidence of eligibility traces\nand three-factor learning rules for the first time, making an\nupdated review of three-factor rules necessary. In the following,\nwe—a group of theoreticians—review five experimental papers\nindicating support of eligibility traces in striatum (Yagishita et al.,\n2014), cortex (He et al., 2015), and hippocampus (Brzosko et al.,\n2015, 2017; Bittner et al., 2017). We will close with a few remarks\non the paradoxical nature of theoretical predictions in the field of\ncomputational neuroscience.\n\n\n2. HEBBIAN RULES VS. THREE-FACTOR\n\nRULES\n\n\nLearning rules describe the change of the strength of a synaptic\ncontact between a presynaptic neuron j and a postsynaptic\nneuron i. The strength of an excitatory synaptic contact can be\ndefined by the amplitude of the postsynaptic potential which\nis closely related to the spine volume and the number of\nAMPA receptors (Matsuzaki et al., 2001). Synapses contain\ncomplex molecular machineries (Lisman, 2003, 2017; Redondo\nand Morris, 2011; Huganir and Nicoll, 2013), but for the sake of\ntransparency of the arguments, we will keep the mathematical\nnotation as simple as possible and characterize the synapse by\ntwo variables only: the first one is the synaptic strength wij,\nmeasured as spine volume or amplitude of postsynaptic potential,\nand the second one is a synapse-internal variable eij which is\nnot directly visible in standard electrophysiological experiments.\nIn our view, the internal variable eij represents a metastable\ntransient state of interacting molecules in the spine head or a\nmulti-molecular substructure in the postsynaptic density which\nserves as a synaptic flag indicating that the synapse is ready for\nan increase or decrease of its spine volume (Bosch et al., 2014).\nThe precise biological nature of eij is not important to understand\nthe theories and experiments that are reviewed below. We refer\nto eij as the “synaptic flag” or the “eligibility trace” and to wij\nas the “synaptic weight,” or “strength” of the synaptic contact. A\nchange of the synaptic flag indicates a ‘candidate weight change’\n(Frémaux et al., 2010) whereas a change of wij indicates an actual,\nmeasurable, change of the synaptic weight. Before we turn to\n\n\n\nthree-factor rules, let us discuss conventional models of Hebbian\nlearning.\n\n\n2.1. Hebbian Learning Rules\nHebbian learning rules are the mathematical summary of\nthe outcome of experimental protocols inducing long-term\npotentiation (LTP) or long-term depression (LTD) of synapses.\nSuitable experimental protocols include strong extracellular\nstimulation of presynaptic fibers (Bliss and Lømo, 1973; Levy\nand Stewart, 1983), manipulation of postsynaptic voltage in the\npresence of presynaptic spike arrivals (Artola and Singer, 1993),\nor spike-timing dependent plasticity (STDP) (Markram et al.,\n1997; Sjöström et al., 2001). In all mathematical formulations\nof Hebbian learning, the synaptic flag variable eij is sensitive to\nthe combination of presynaptic spike arrival and a postsynaptic\nvariable, such as the voltage at the location of the synapse. Under\na Hebbian learning rule, repeated presynaptic spike arrivals at\na synapse of a neuron at rest do not cause a change of the\nsynaptic variable. Similarly, an elevated postsynaptic potential in\nthe absence of a presynaptic spike does not cause a change of\nthe synaptic variable. Thus, Hebbian learning always needs two\nfactors for a synaptic change: a factor caused by a presynaptic\nsignal such as glutamate; and a factor that depends on the state\nof the postsynaptic neuron.\nWhat are these factors? We can think of the presynaptic factor\nas the time course of glutamate available in the synaptic cleft\nor bound to the postsynaptic membrane. Note that the term\n“presynaptic factor” that we will use in the following does not\nimply that the physical location of the presynaptic factor is inside\nthe presynaptic terminal–the factor could very well be located\nin the postsynaptic membrane as long as it only depends on the\namount of available neurotransmitters. The postsynaptic factor\nmight be related to calcium in the synaptic spine (Shouval et al.,\n2002; Rubin et al., 2005), a calcium-related second messenger\nmolecule (Graupner and Brunel, 2007), or simply the voltage at\nthe site of the synapse (Brader et al., 2007; Clopath et al., 2010).\nWe remind the reader that we always use the index j to refer to\nthe presynaptic neuron and the index i to refer to the postsynaptic\none. For the sake of simplicity, let us call the presynaptic factor\nxj (representing the activity of the presynaptic neuron or the\namount of glutamate in the synaptic cleft) and the postsynaptic\nfactor yi (representing the state of the postsynaptic neuron). In a\nHebbian learning rule, changes of the synaptic flag eij need both\nxj and yi\n\n\nd\ndt [e][ij][ =][ η][ f][j][(][x][j][)][ g][i][(][y][i][)][ −] [e][ij][/τ][e] (1)\n\n\nwhere η is the constant learning rate, τe is a decay time constant,\nfj(xj) is an (often linear) function of the presynaptic activity\nxj and gi(yi) is some arbitrary, potentially nonlinear, function\nof the postsynaptic variable yi. The index j of the function fj\nand the index i of the function gi indicate that the rules for\nchanging a synaptic flag can depend on the type of pre- and\npostsynaptic neurons, on the cortical layer and area, but also on\nsome heterogeneity of parameters between one neuron and the\n\nnext.\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 2 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nAccording to Equation 1 the synaptic flag eij acts as a\ncorrelation detector between presynaptic activity xj and the state\nof the postsynaptic neuron yi. In some models, there is no\ndecay or the decay is considered negligible on the time scale\nof one experiment (τe →∞). The flag variable eij could be\nrelated to a calcium-based coincidence detection mechanism in\n\nthe spine such as CaMKII (Lisman, 1989; Shouval et al., 2002)\nor a metastable state of the molecular machinery in the synapse\n(Bosch et al., 2014).\nLet us discuss two examples. In the Bienenstock-Cooper\nMunro (BCM) model of developmental cortical plasticity\n(Bienenstock et al., 1982) the presynaptic factor xj is the firing rate\nof the presynaptic neuron and g(yi) = (yi − θ ) yi is a quadratic\nfunction with yi the postsynaptic firing rate and θ a threshold\nrate. Thus, if both pre- and postsynaptic neurons fire together\nat a high rate xj = yi > θ then the synaptic flag eij increases. In\nthe BCM model, just like in most other conventional models, a\nchange of the synaptic flag (i.e., an internal state of the synapse)\nleads instantaneously to a change of the weight eij −→ wij so that\nan experimental protocol results immediately in a measurable\nweight change. With the BCM rule and other similar rules (Oja,\n1982; Miller and MacKay, 1994), the synaptic weight increases\nif both presynaptic and postsynaptic neuron are highly active,\nimplementing the slogan “fire together, wire together” (Lowel and\nSinger, 1992; Shatz, 1992) cf. **Figure 1Ai** .\nAs a second example, we consider the Clopath model (Clopath\net al., 2010). In this model, there are two correlation detectors\nimplemented as synaptic flags e [+] ij [and][ e][−] ij [for LTP and LTD,]\nrespectively. The synaptic flag e [+] ij [for LTP uses a presynaptic]\nfactor xj [+] (related to the amount of glutamate available in the\nsynaptic cleft) which increases with each presynaptic spike and\ndecays back to zero over the time of a few milliseconds (Clopath\net al., 2010). The postsynaptic factor for LTP depends on the\npostsynaptic voltage yi via a function g(yi) = a+[yi − θ+] ¯yi\nwhere a+ is a positive constant, θ+ a voltage threshold, square\nbrackets denote the rectifying piecewise linear function, and\n\n¯\nyi a running average of the voltage with a time constant of\ntens of milliseconds. An analogous, but simpler, combination of\npresynaptic spikes and postsynaptic voltage defines the second\nsynaptic flag e [−] ij [for LTD (][Clopath et al., 2010][). The total change]\nof the synaptic weight is the combination of the two synaptic\nflags for LTP and LTD: dwij/dt = de [+] ij [/][dt][ −] [de][−] ij [/][dt][. Note that,]\nsince both synaptic flags e [+] ij [and][ e][−] ij [depend on the postsynaptic]\nvoltage, postsynaptic spikes are not a necessary condition for\nchanges, in agreement with voltage-dependent protocols (Artola\nand Singer, 1993; Ngezahayo et al., 2000). Thus, in voltagedependent protocols, and similarly in voltage-dependent models,\n“wiring together” is possible without “firing together”-indicating\nthat the theoretical framework sketched above goes beyond a\nnarrow view of Hebbian learning; cf. **Figure 1Aii** .\nIf we restrict the discussion of the postsynaptic variable\nto super-threshold spikes, then the Clopath model becomes\nidentical to the triplet STDP model (Pfister et al., 2006) which\nis in turn closely related to other nonlinear STDP models (Senn\net al., 2001; Froemke and Dan, 2002; Izhikevich and Desai, 2003)\nas well as to the BCM model discussed above (Pfister et al., 2006;\n\n\n\nGjorjieva et al., 2011). Classic pair-based STDP models (Gerstner\net al., 1996; Kempter et al., 1999; Song et al., 2000; van Rossum\net al., 2000; Rubin et al., 2001) are further examples of the general\ntheoretical framework of Equation (1) and so are some models of\nstructural plasticity (Helias et al., 2008; Deger et al., 2012, 2018;\nFauth et al., 2015). Hebbian models of synaptic consolidation\nhave several hidden flag variables (Fusi et al., 2005; Barrett et al.,\n2009; Benna and Fusi, 2016) but can also be situated as examples\nwithin the general framework of Hebbian rules. Note that in most\nof the examples so far the measured synaptic weight is a linear\nfunction of the synaptic flag variable(s). However, this does not\nneed to be the case. For example, in some voltage-based (Brader\net al., 2007) or calcium-based models (Shouval et al., 2002; Rubin\net al., 2005), the synaptic flag is transformed into a weight change\nonly if eij is above or below some threshold, or only after some\nfurther filtering.\nTo summarize, in the theoretical literature the class of\nHebbian models is a rather general framework encompassing all\nthose models that are driven by a combination of presynaptic\nactivity and the state of the postsynaptic neuron. In this view,\nHebbian models depend on two factors related to the activity\nof the presynaptic and the state of the postsynaptic neuron. The\ncorrelations between the two factors can be extracted on different\ntime scales using one or, if necessary, several flag variables. The\nflag variables trigger a change of the measured synaptic weight.\nIn the following we build on Hebbian learning, but extend the\ntheoretical framework to include a third factor.\n\n\n2.2. Three-Factor Learning Rules\nWe are interested in a framework where a Hebbian co-activation\n\nof two neurons leaves one or several flags (eligibility trace) at\nthe synapse connecting these neurons. The flag is not directly\nvisible and does not automatically trigger a change of the synaptic\nweight. An actual weight change is implemented only if a third\nsignal, e.g., a phasic increase of neuromodulator activity or an\nadditional input (signaling the occurrence of a special event) is\npresent at the same time or in the near future. Theoreticians refer\nto such a plasticity model as a three-factor learning rule (Xie and\nSeung, 2004; Legenstein et al., 2008; Vasilaki et al., 2009; Frémaux\net al., 2013; Frémaux and Gerstner, 2016). Three-factor rules have\nalso been called “neoHebbian” (Lisman et al., 2011; Lisman, 2017)\nor “heterosynaptic (modulatory-input dependent)” (Bailey et al.,\n2000) and can be traced back to the 1960s (Crow, 1968), if not\nearlier. To our knowledge the wording “three factors” was first\nused by (Barto, 1985). The terms eligibility and eligibility traces\nhave been used in (Klopf, 1972; Sutton and Barto, 1981, 1998;\nBarto et al., 1983; Barto, 1985; Williams, 1992; Schultz, 1998) but\nin some of the early studies it remained unclear whether eligibility\ntraces can be set by presynaptic activity alone (Klopf, 1972; Sutton\nand Barto, 1981) or only by Hebbian co-activation of pre- and\npostsynaptic neurons (Barto et al., 1983; Barto, 1985; Williams,\n1992; Schultz, 1998; Sutton and Barto, 1998).\nThe basic idea of a modern eligibility trace is that a synaptic\nflag variable eij is set according to Equation (1) by coincidences\nbetween presynaptic activity xj and a postsynaptic factor yi. The\nupdate of the synaptic weight wij, as measured via the spine\nvolume or the amplitude of the excitatory postsynaptic potential\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 3 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\nFIGURE 1 | (A) Two Hebbian protocols and one three-factor learning protocol. (i) Hebbian STDP protocol with presynaptic spikes (presynaptic factor) followed by a\nburst of postsynaptic spikes (postsynaptic factor). Synapses in the stimulated pathway (green) will typically show LTP while an unstimulated synapse (red) will not\nchange its weight (Markram et al., 1997). (ii) Hebbian voltage pairing protocol of presynaptic spikes (presynaptic factor) with a depolarization of the postsynaptic\nneuron (postsynaptic factor). Depending on the amount of depolarization the stimulated pathway (green) will show LTP or LTD while an unstimulated synapse (red)\ndoes not change its weight (Artola and Singer, 1993; Ngezahayo et al., 2000). (iii) Results of a Hebbian induction protocol are influenced by a third factor (blue) even if\nit is given after a delay d. The third factor could be a neuromodulator such as dopamine, acetylcholine, noreprinephrine, or serotonin (Pawlak et al., 2010; Yagishita\net al., 2014; Brzosko et al., 2015, 2017; He et al., 2015; Bittner et al., 2017). (B) Specificity of three-factor learning rules. (i) Presynaptic input spikes (green) arrive at\ntwo different neurons, but only one of these also shows postsynaptic activity (orange spikes). (ii) A synaptic flag is set only at the synapse with a Hebbian co-activation\nof pre- and postsynaptic factors; the synapse become then eligible to interact with the third factor (blue). Spontaneous spikes of other neurons do not interfere. (iii) The\ninteraction of the synaptic flag with the third factor leads to a strengthening of the synapse (green).\n\n\n\n![](output/images/d367d4de86dce92d30fba888ad190957f9ae14d8.pdf-3-0.png)\n\n(EPSP), is given by\n\n\nd\ndt [w][ij][ =][ e][ij][ M][3][rd][(][t][)] (2)\n\n\nwhere M [3][rd] (t) refers to the global third factor (Izhikevich,\n2007; Legenstein et al., 2008; Frémaux et al., 2013). Therefore,\naccording to Equation 2, a non-zero third factor is needed\nto transform the eligibility trace into a weight change; cf.\n**Figure 1Aiii** . Note that the weight change is proportional to\nM [3][rd] (t). Thus, the third factor influences the speed of learning. In\nthe absence of the third factor (M [3][rd] (t) = 0), the synaptic weight\nis not changed. We emphasize that a positive value of the synaptic\nflag in combination with a negative value M [3][rd] < 0 leads to a\ndecrease of the weight. Therefore, the third factor also influences\nthe direction of change.\nIn the discussion so far, M [3][rd] (t) in Equation (2) can take\npositive and negative values. Such a behavior is typical for\na phasic signal which we may mathematically define as the\ndeviation from a running average. We may, for example, think\nof the third factor as a phasic neuromodulatory signal. However,\nthe third factor could also be biologically implemented by positive\nexcursions of the activity using two different neuromodulators\nwith very low baseline activity. The activity of the first modulator\ncould indicate positive values of the third factor and that of the\nsecond modulator negative ones - similar to ON and OFF cells in\nthe retina. Similarly, the framework of neoHebbian three-factor\nrules is general enough to enable biological implementations with\n\n\n\nseparate eligibility traces for LTP and LTD as discussed above in\nthe context of the Clopath model (Clopath et al., 2010).\nWhat could be the source of such a third factor, be it\na single neuromodulator or several different ones? The third\nfactor could be triggered by attentional processes, surprising\nevents, or reward. Phasic signals of neuromodulators such as\ndopamine, serotonin, acetylcholine, or noradrenaline are obvious\ncandidates for a third factor, but potentially not the only ones.\nNote that axonal branches of most dopaminergic, serotonergic,\ncholinergic, or adrenergic neurons project broadly onto large\nregions of cortex so that a phasic neuromodulator signal arrives\nat many neurons and synapses in parallel (Schultz, 1998). Since\nneuromodulatory information is shared by many neurons, the\nvariable M [3][rd] (t) of the third factor has no neuron-specific index\n(neither i nor j) in our mathematical formulation. Because of\nits nonspecific nature, the theory literature sometimes refers to\nthe third factor as a “global” broadcasting signal, even though\nin practice not every brain region and every synapse is reached\nby each neuromodulator. The learning rule with the global\nmodulator, as formulated in Equation 2 will be called Type 1 for\nthe remainder of this paper.\nTo account for some neuron-specific aspects, three-factor\nlearning rules of Type 2,\n\n\nd\ndt [w][ij][ =][ e][ij][ h][i][(][M][3][rd][(][t][)),] (3)\n\n\ncontain a neuron-specific function hi that determines how the\nglobal third factor M [3][rd] influences synaptic plasticity of the\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 4 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\npostsynaptic neuron i. Including an index i in the function\nhi(M [3][rd] ) keeps the theory flexible enough to set (for example)\nhi(M [3][rd] ) ≡ 0 for the subset of neurons i that are not reached\nby a given neuromodulator. In this case, the classification of a\ngiven learning rule as Type 1 and Type 2 is somewhat arbitrary\nas it depends on whether a population of neurons with a threefactor learning rule is embedded in a larger network with static\nsynapses or not. But there are also existing models, where the\nimplementation of a certain function requires the possibility\nthat one subpopulation has plasticity rules under a third factor\nwhereas another one does not (Brea et al., 2013; Rezende and\nGerstner, 2014).\nThe framework of Equation 3 also includes networks where\nthe distribution of neuromodulatory information to different\nneurons is done with fixed, but random feedback weights bi\n(Lillicrap et al., 2016; Guerguiev et al., 2017); we simply need\n\nto set hi M [3][rd][�] = h bi M [3][rd][�] . It does not, however, include\n� �\nthe general case of supervised learning with backpropagation of\n\nerrors.\n\nOne of the important differences between supervised and\nreinforcement learning is that in most modern supervised\nlearning tasks, such as auto-encoders, the target is, just like the\ninput, a high-dimensional vector. For supervised learning of\nhigh-dimensional targets, we need to generalize Equation (3)\nfurther, to three-factor learning rules of Type 3,\n\n\nd\ndt [w][ij][ =][ e][ij][ M] i [3][rd] (t), (4)\n\n\nwhere Mi [3][rd] is now a neuron-specific (hence non-global) error\nfeedback signal. For the case of standard backpropagation in\nlayered networks, Mi [3][rd] is calculated by a weighted sum over the\nerrors in the next layer closer to the output; a calculation which\nneeds a well-tuned feedback circuit from the output back to\nprevious layers (Roelfsema and van Ooyen, 2005; Lillicrap et al.,\n2016; Roelfsema and Holtmaat, 2018). Interestingly, learning\nsimilar, but not identical, to backpropagation is still possible with\nfeedback circuits, where the direct feedback from the output\nis replaced with fixed random weights (Lillicrap et al., 2016;\nGuerguiev et al., 2017), or in networks with a single hidden layer\nand winner-take-all activity (one-hot coding) in the output layer\n(Roelfsema and van Ooyen, 2005; Rombouts et al., 2015). In the\nlatter case, the neuron-specific third factor Mi [3][rd] further factorizes\ninto a global modulator M [3][rd] and an attention signal Ai, which\nleads to a four-factor learning rule (Roelfsema and Holtmaat,\n2018).\n\n\n2.3. Examples and Theoretical Predictions\nThere are several known examples in the theoretical literature\nof neoHebbian three-factor rules. We briefly present four of\nthese and formulate expectations derived from the theoretical\nframework which we would like to compare to experimental\nresults in the next section.\n\n\n2.3.1. Reward-Based Learning\nAs a first example of a Type 1 three-factor learning rule,\nwe consider the relation of neoHebbian three-factor rules to\n\n\n\nreward-based learning. Temporal Difference (TD) algorithms\nsuch as SARSA(λ) or TD(λ) from the theory of reinforcement\nlearning (Sutton and Barto, 1998) as well as learning rules\nderived from policy gradient theories (Williams, 1992) can be\ninterpreted in neuronal networks in terms of neoHebbian threefactor learning rules. The resulting plasticity rules are applied\nto synapses connecting “state-neurons” (e.g., place cells coding\nfor the current location of an animal) to “action neurons” e.g.,\ncells initiating an action program such as “turn left”) (Brown\nand Sharp, 1995; Suri and Schultz, 1999; Arleo and Gerstner,\n2000; Foster et al., 2000; Xie and Seung, 2004; Loewenstein\nand Seung, 2006; Florian, 2007; Izhikevich, 2007; Legenstein\net al., 2008; Vasilaki et al., 2009; Frémaux et al., 2013); for a\nreview, see (Frémaux and Gerstner, 2016). The eligibility trace\nis increased during the joint activation of “state-neurons” and\n“action-neurons” and decays exponentially thereafter consistent\nwith the framework of Equation (1). The third factor is defined\nas reward minus expected reward where the exact definition of\nexpected reward depends on the implementation details. A long\nline of research by Wolfram Schultz and colleagues (Schultz et al.,\n1997; Schultz, 1998, 2002; Schultz and Dickinson, 2000) indicates\nthat phasic increases of the neuromodulator dopamine have the\nnecessary properties required for a third factor in the theoretical\nframework of reinforcement learning.\nHowever, despite the rich literature on dopamine and rewardbased learning accumulated during the last 25 years, there is scant\ndata on the decay time constant τe of the eligibility trace eij in\nEquation (1) before 2015 (except the locusts study Cassenaer\nand Laurent, 2012). From the mathematical framework of\nneoHebbian three-factor rules it is clear that, in the context of\naction learning, the time constant of the eligibility trace (i.e., the\nduration of the synaptic flag) should roughly match the time span\nfrom the initiation of an action to the delivery of reward. As\nan illustration, let us imagine a baby that attempts to grasp her\nbottle of milk. The typical duration of one grasping movement is\nin the range of a second, but potentially only the third grasping\nattempt might be successful. Let us suppose that each grasping\nmovement corresponds to the co-activation of some neurons in\nthe brain. If the duration of the synaptic flag is much less than a\nsecond, the co-activation of pre- and postsynaptic neurons that\nsets the synaptic flag (eligibility trace) cannot be linked to the\nreward 1 s later and synapses do not change. If the duration of the\nsynaptic flag is much longer than a second, then the two “wrong”\ngrasping attempts are reinforced nearly as strongly as the third,\nsuccessful one which mixes learning of “wrong” co-activations\nwith the correct ones. Hence, the existing theory of three-factor\nlearning rules predicts that the synaptic flag (eligibility trace for\naction learning) should be in the range of a typical elementary\naction, about 200 ms to 2 s; see, for example, p. 15 of Schultz\n(1998) [1], p.3 of Izhikevich (2007) [2], p.3 of Legenstein et al. (2008) [3],\n\n\n1“Learning ... (with dopamine) on striatal synapses ... requires hypothetical traces\nof synaptic activity that last until reinforcement occurs and makes those synapses\neligible for modification ...”\n2“(The eligibility trace c ) decays to c = 0 exponentially with the time constant\nτc = 1 s.”\n3“The time scale of the eligibility trace is assumed in this article to be on the order\nof seconds.”\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 5 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\np. 13327 of Frémaux et al. (2010) [4], or p. 13 of Frémaux et al.\n(2013) [5] . An eligibility trace of <100 ms or more than 10 s would\nbe less useful for learning a typical elementary action or delayed\nreward task than an eligibility trace in the range of 200 ms to 2 s.\nThe expected time scale of the synaptic eligibility trace should\nroughly match the maximal delay of reinforcers in conditioning\nexperiments (Thorndike, 1911; Pavlov, 1927; Black et al., 1985),\nlinking synaptic processes to behavior. For human behavior,\ndelaying a reinforcer by 10 s during ongoing actions decreases\nlearning compared to immediate reinforcement (Okouchi, 2009).\n\n\n2.3.2. Surprise-Based Learning\nAs a second example of a Type 1 three-factor learning rule,\nwe consider situations that go beyond standard reward-based\nlearning. Even in the absence of reward, a surprising event might\ntrigger a combination of neuromodulators such as noradrenaline,\nacetylcholine and dopamine that may act as third factor for\nsynaptic plasticity. Imagine a small baby lying in the cradle\nwith an attractive colorful object swinging above him. He\nspontaneously makes several arm movements until finally he\nsucceeds, by chance, to grasp the object. There is no food reward\nfor this action. However, the fact that he can now turn the\nobject, look at it from different sides, or put it in his mouth is\nsatisfying because it leads to many novel (and exciting!) stimuli.\nThe basic idea is that, in such situations, novelty or surprise\nacts as a reinforcer even in complete absence of food rewards\n(Schmidhuber, 1991; Singh et al., 2004; Oudeyer et al., 2007).\nTheoreticians have studied these ideas in the context of curiosity\n(Schmidhuber, 2010), information gain during active exploration\n(Storck et al., 1995; Schmidhuber, 2006; Sun et al., 2011; Little and\nSommer, 2013; Friston et al., 2016), and via formal definitions\nof surprise (Shannon, 1948; Storck et al., 1995; Itti and Baldi,\n2009; Friston, 2010; Schmidhuber, 2010; Faraji et al., 2018). Note\nthat surprise is not always linked to active exploration but can\nalso occur in a passive situation, e.g., listening to tone beeps or\nviewing simple stimuli (Squires et al., 1976; Kolossa et al., 2013,\n2015; Meyniel et al., 2016). Measurable physiological responses to\nsurprise include pupil dilation (Hess and Polt, 1960) and the P300\ncomponent of the electroencephalogram (Squires et al., 1976).\nIf surprise can play a role similar to reward, then surprisetransmitting broadcast signals should speed-up plasticity. Indeed,\ntheories of surprise as well as hierarchical Bayesian models\npredict a faster change of model parameters for surprising\nstimuli than for known ones (Yu and Dayan, 2005; Nassar\net al., 2010; Mathys et al., 2011, 2014; Faraji et al., 2018)\nsimilar to, but more general than, the well-known Kalman\nfilters (Kalman, 1960). Since the translation of these abstract\nmodels into networks of spiking neurons is still missing,\nprecise predictions for surprise modulation of plasticity in the\nform of three-factor rules are not yet available. However, if\nwe consider noradrenaline, acetylcholine, and/or dopamine as\ncandidate neuromodulators signaling novelty and surprise, we\n\n\n4“Candidate weight changes eij decay to zero with a time constant τe = 500ms.\nThe candidate weight changes eij are known as the eligibility trace in reinforcement\nlearning.”\n5“The time scales of the eligibility traces we propose, (are) on the order of hundreds\nof milliseconds, .. Direct experimental evidence of eligibility traces still lacks, ...”\n\n\n\nexpect that these neuromodulators should have a strong effect\non plasticity so as to boost learning of surprising stimuli. The\ninfluence of tonic applications of various neuromodulators on\nsynaptic plasticity has been shown in numerous studies (Gu,\n2002; Reynolds and Wickens, 2002; Hasselmo, 2006; Pawlak\net al., 2010). However, in the context of the above examples, we\nare interested in phasic neuromodulatory signals. Phasic signals\nconveying moments of surprise are most useful for learning if\nthey are either synchronous with the stimulus to be learned (e.g.,\npassive listening or viewing) or arise with a delay corresponding\nto one exploratory movement (e.g., grasping). Hence, we predict\nfrom these considerations a decay constant τe of the synaptic flag\nin the range of 1 s, but with a pronounced effect for synchronous\nor near-synchronous events.\n\n\n2.3.3. Synaptic Tagging-and-Capture\nAs our third example of a Type 1 three-factor learning rule,\nwe would like to comment on synaptic consolidation. The\nsynaptic tagging-and-capture hypothesis (Frey and Morris, 1997;\nReymann and Frey, 2007; Redondo and Morris, 2011) perfectly\nfits in the framework of three-factor learning rules: The joint\npre- and postsynaptic activity sets the synaptic flag (called “tag”\nin the context of consolidation) which decays back to zero over\nthe time of 1 h. To stabilize synaptic weights beyond 1 h an\nadditional factor is needed to trigger protein synthesis required\nfor long-term maintenance of synaptic weights (Reymann and\nFrey, 2007; Redondo and Morris, 2011). Neuromodulators\nsuch as dopamine have been identified as the necessary third\nfactor for consolidation (Bailey et al., 2000; Reymann and\nFrey, 2007; Redondo and Morris, 2011; Lisman, 2017). Indeed,\nmodern computational models of synaptic consolidation take\ninto account the effect of neuromodulators (Clopath et al., 2008;\nZiegler et al., 2015) in a framework reminiscent of the threefactor rule defined by Equations (1, 2) above. However, there\nare two noteworthy differences. First, in contrast to reward-based\nlearning, the decay time τe of the synaptic tag eij is in the range\nof 1 h rather than 1 s, consistent with slice experiments (Frey and\nMorris, 1997) as well as with behavioral experiments (Moncada\nand Viola, 2007). Second, in slices, the measured synaptic weights\nwij are increased a few minutes after the end of the induction\nprotocol and decay back with the time course of the synaptic tag\nwhereas in the simplest implementation of the three-factor rule\nframework as formulated in Equations (1, 2) the visible weight\nis only updated at the moment when the third factor is present.\nHowever, slightly more involved models where the visible weight\ndepends on both the synaptic tag variable and the long-term\nstable weight (Clopath et al., 2008; Ziegler et al., 2015) correctly\naccount for the time course of the measured synaptic weights in\nconsolidation experiments (Frey and Morris, 1997; Reymann and\nFrey, 2007; Redondo and Morris, 2011).\n\n\n2.3.4. Supervised Learning With Segregated\nDendrites\n\nA recent study proposes a mechanism for implementing 3-factor\nrules of Type 3 in the context of supervised learning (Guerguiev\net al., 2017). Instead of neuromodulators, they propose that topdown feedback signals from the network output to the apical\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 6 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\ndendrites of pyramidal neurons serve as the 3rd factor in the 3factor rule. If the output units have a stationary value yk when\ndriven by the feedforward network input and a stationary value\n\nˆ\nyk when shunted to the target value, then the changes of a weight\nwij from neuron j onto the soma (or basal dendrites) of neuron i is\ngoverned by Equation (4) with a third factor Mi = [�] k [b][ik][(][y][ˆ][k][−][y][k][)]\nwhere bik are random feedback weights from the output neuron\nk to the apical dendrite of neuron i (Guerguiev et al., 2017). The\nauthors assume relatively weak electrical coupling between the\napical dendrite and the soma and suggest that bursts in the apical\ndendrite could transmit the value of the third factor to synapses\nonto the soma or basal dendrites (Guerguiev et al., 2017).\nSimilarly, the Urbanczik-Senn rule for supervised learning can\nbe interpreted as a three-factor rule of Type 3 (Urbanczik and\nSenn, 2014). Target input to a neuron in the output layer is given\nat the soma and leads to a spike-train Si(t) while feedforward\ninput from other neurons in the network arrives in a dendritic\ncompartment where it generates a voltage yi. The third factor\nMi [3][rd] (t) = Si(t) − φ(yi(t)) compares the actual spike train\n(including the somatic drive by the target) with the firing rate\nexpected from dendritic input alone (Urbanczik and Senn, 2014).\nThe authors assume relatively strong electrical coupling between\nthe dendrite and the soma. Interestingly, the same learning rule\ncan also be used in the absence of target information; in this case\nwe prefer to interpret it as a Hebbian two-factor rule, as discussed\nin the next paragraph.\n\n\n2.3.5. Summary\nThe Clopath rule discussed in the paragraph on Hebbian learning\nrules contains terms that combine a presynaptic factor with\ntwo postsynaptic factors, one for instantaneous superthreshold\nvoltage and the other one for low-pass filtered voltage (Clopath\net al., 2010). However, despite the fact that it is possible to\nwrite the Clopath rule as a multiplication of three factors, we do\nnot classify it as a three-factor rule but rather as a two-factor\nrule with a nonlinear postsynaptic factor. The main difference\nto a true three-factor rule is that the third factor M [3][rd] should\n\nbe related to a feedback signal conveying information on the\nperformance of the network as a whole. As we have seen, this\nthird factor can be a global scalar signal related to reward or\nsurprise or a neuron-specific signal related to the error in the\nnetwork output. With this nomenclature, the Urbanczik-Senn\nrule is, just like the Clopath rule (Clopath et al., 2010), a Hebbian\ntwo-factor rule if used in unsupervised learning (Urbanczik and\nSenn, 2014), but the same rule must be seen as a three-factor rule\nwith a neuron-specific (non-global) third factor in the supervised\nsetting.\nIn summary, the neoHebbian three-factor rule framework has\na wide range of applicability. The framework is experimentally\nwell-established in the context of synaptic consolidation where\nthe duration of the flag (“synaptic tag”) extracted from slice\nexperiments (Frey and Morris, 1997) is in the range of 1 h,\nconsistent with fear conditioning experiments (Moncada and\nViola, 2007). This time scale is significantly longer than what\nis needed for behavioral learning of elementary actions or\nfor memorizing surprising events. In the context of rewardbased learning, theoreticians therefore hypothesized that a\n\n\n\nprocess analogous to setting a tag (“eligibility trace”) must\nalso exist on the time scale of 1 s. The next section discusses\n\nsome recent experimental evidence supporting this theoretical\nprediction.\n\n\n3. EXPERIMENTAL EVIDENCE FOR\n\nELIGIBILITY TRACES\n\n\nRecent experimental evidence for eligibility traces in striatum\n(Yagishita et al., 2014), cortex (He et al., 2015), and hippocampus\n(Brzosko et al., 2015, 2017; Bittner et al., 2017) is reviewed in the\nfollowing three subsections.\n\n\n3.1. Eligibility Traces in Dendritic Spines of\nMedium Spiny Striatal Neurons in Nucleus\nAccumbens\n\nIn their elegant imaging experiment of dendritic spines of nucleus\naccumbens neurons, Yagishita et al. (2014) mimicked presynaptic\nspike arrival by glutamate uncaging (presynaptic factor),\npaired it with three postsynaptic spikes immediately afterward\n(postsynaptic factor), repeated this STDP-like pre-beforepost sequence ten times, and combined it with optogenetic\nstimulation of dopamine fibers (3rd factor) at various delays\n(Yagishita et al., 2014). The ten repetitions of the pre-beforepost sequence at 10 Hz took about 1 s while stimulation of\ndopaminergic fibers (10 dopamine pulses at 30 Hz) projecting\nfrom the ventral tegmental area (VTA) to nucleus accumbens\ntook about 0.3 s. In their paper, dopamine was counted as\ndelayed by 1 s if the dopamine stimulation started immediately\nafter the end of the 1 s-long induction period (delay =\ndifference in switch-on time of STDP and dopamine), but for\nconsistency with other data we define the delay d here as\nthe time passed since the end of the STDP protocol. After\n15 complete trials the spine volume, an indicator of synaptic\nstrength (Matsuzaki et al., 2001), was measured and compared\nwith the spine volume before the induction protocol. The\nauthors found that dopamine promoted spine enlargement\nonly if phasic dopamine was given in a narrow time window\nduring or immediately after the 1 s-long STDP protocol; cf.\n**Figure 2A** .\nThe maximum enlargement of spines occurred if the\ndopamine signal started during the STDP protocol (d = −0.4\ns), but even at a delay of d = 1 s LTP was still visible. Giving\ndopamine too early (d = −2 s) or too late (d = +4 s) had\nno effect. Spine enlargement corresponded to an increase in\nthe amplitude of excitatory postsynaptic currents indicating that\nthe synaptic weight was indeed strengthened after the protocol\n(Yagishita et al., 2014). Thus, we can summarize that we have in\nthe striatum a three-factor learning rule for the induction of LTP\nwhere the decay of the eligibility trace occurs on a time scale of 1 s;\ncf. **Figure 2A** .\nTo arrive at these results, Yagishita et al. (2014) concentrated\non medium spiny neurons in the nucleus accumbens core, a part\nof the ventral striatum of the basal ganglia. Functionally, striatum\nis a particularly interesting candidate for reinforcement learning\n(Brown and Sharp, 1995; Schultz, 1998; Arleo and Gerstner,\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 7 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\nFIGURE 2 | Experimental support for synaptic eligibility traces. Fractional weight change (vertical axis) as a function of delay d of third factor (horizontal axis) for\nvarious protocols (schematically indicated at the bottom of each panel). (A) In striatum medium spiny cells, stimulation of presynaptic glutamatergic fibers (green)\nfollowed by three postsynaptic action potentials (STDP with pre-post-post-post at +10 ms) repeated 10 times at 10 Hz yields LTP if dopamine fibers are stimulated\nduring the presentation (d < 0) or shortly afterward (d = 0 s or d = 1 s) but not if dopamine is given with a delay d = 4 s; redrawn after Figure 1 of Yagishita et al.\n(2014), with delay d defined as time since end of STDP protocol. (B) In cortical layer 2/3 pyramidal cells, stimulation of two independent presynaptic pathways (green\nand red) from layer 4 to layer 2/3 by a single pulse combined with a burst of four postsynaptic spikes (orange). If the pre-before-post stimulation was combined with a\npulse of norepinephrine (NE) receptor agonist isoproterenol with a delay of 0 or 5 s, the protocol gave LTP (blue trace). If the post-before-pre stimulation was\ncombined with a pulse of serotonin (5-HT) of a delay of 0 or 2.5 s, the protocol gave LTD (red trace); redrawn after Figure 6 of He et al. (2015). (C) In hippocampus\nCA1, a post-before-pre (�t = -20 ms) induction protocol yields LTP if dopamine is present during induction or given with a delay d of 0 or 1 min, but yields LTD if\ndopamine is absent or given with a delay of 30 min; redrawn after Figures 1F, 2B, and 3C (square data point at delay of 1 min) of Brzosko et al. (2015). (D) In\nhippocampus CA1, 10 extracellular stimuli of presynaptic fibers at 20 Hz cause depolarization of the postsynaptic potential. The timing of a complex spike (calcium\nplateau potential) triggered by current injection (during 300 ms) after a delay d, is crucial for the amount of LTP. If we interpret presynaptic spike arrival as the first, and\npostsynaptic depolarization as the second factor, the complex spike could be associated with a third factor; redrawn after Figure 3 of Bittner et al. (2017). Height of\nboxes gives a very rough estimate of standard deviation - see original papers and figures for details.\n\n\n\n![](output/images/d367d4de86dce92d30fba888ad190957f9ae14d8.pdf-7-0.png)\n\n2000; Doya, 2000a; Daw et al., 2005) for several reasons.\nFirst, striatum receives highly processed sensory information\nfrom neocortex and hippocampus through glutamatergic\nsynapses (Mink, 1996; Middleton and Strick, 2000; Haber\net al., 2006). Second, striatum also receives dopamine input\nassociated with reward processing (Schultz, 1998). Third,\nstriatum is, together with frontal cortex, involved in the\nselection of motor action programs (Mink, 1996; Seo et al.,\n2012).\n\n\n\nOn the molecular level, the striatal three-factor plasticity\ndepended on NMDA, CaMKII, protein synthesis, and dopamine\nD1 receptors (Yagishita et al., 2014; Shindou et al., 2018). CaMKII\nincreases were found to be localized in the spine and to have\nroughly the same time course as the critical window for phasic\ndopamine suggesting that CaMKII could be involved in the\n“synaptic flag” triggered by the STDP-like induction protocol,\nwhile protein kinase A (PKA) was found to have a nonspecific\ncell-wide distribution suggesting an interpretation of PKA as a\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 8 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nmolecule linked to the dopamine-triggered third factor (Yagishita\net al., 2014).\n\n\n3.2. Two Distinct Eligibility Traces for LTP\nand LTD in Cortical Synapses\nIn a recent experiment of He et al. (2015), layer 2/3\npyramidal cells in slices from prefrontal or visual cortex\nwere stimulated by an STDP protocol, either pre-before-post\nfor LTP induction or post-before-pre for LTD induction. A\nneuromodulator was applied with a delay after a single STDP\nsequence before the whole protocol was repeated; cf. **Figure 2B** .\nNeuromodulators, either norepinephrine (NE), serotonin (5HT), dopamine (DA), or acetylcholine (ACh) were ejected\nfrom a pipette for 10 s or from endogenous fibers (using\noptogenetics) for 1 s (He et al., 2015). It was found that NE\nwas necessary for LTP whereas 5-HT was necessary for LTD.\nDA or ACh agonists had no effect in visual cortex but DA had\na positive effect on LTP induction in frontal cortex (He et al.,\n2015).\nFor the STDP protocol, He et al. (2015) used extracellular\nstimulation of two presynaptic pathways from layer 4 to layer\n2/3 (presynaptic factor) combined with a burst of 4 postsynaptic\naction potentials (postsynaptic factor), either pre-before-post or\npost-before-pre. In a first variant of the experiment, the STDP\nstimulation was repeated 200 times at 10 Hz corresponding to\na total stimulation time of 20 s before the NE or 5-HT was\n\ngiven. In a second variant, instead of an STDP protocol, they\npaired presynaptic stimulation (first factor) with postsynaptic\ndepolarization (second factor) to –10 mV to induce LTP, or to –\n40 mV to induce LTD. With both protocols it was found that LTP\ncan be induced if the neuromodulator NE (third factor) arrived\nwith a delay of 5 s or less after the LTP protocol, but not 10 s.\nLTD could be induced if 5-HT (third factor) arrived with a delay\nof 2.5 s or less after the LTD protocol, but not 5 s (He et al., 2015).\nA third variant of the experiment involved optogenetic\nstimulation of the noradrenaline, dopamine, or serotonin\npathway by repeated light pulses during 1 s applied immediately,\nor a few seconds, after a minimal STDP protocol consisting of\na single presynaptic and four postsynaptic pulses (either prebefore-post or post-before-pre), a protocol that is physiologically\nmore plausible. The minimal sequence of STDP pairing and\nneuromodulation was repeated 40 times at intervals of 20 s.\nResults with optogenetic stimulation were consistent with those\nmentioned above and showed in addition that application of NE\nor 5-HT immediately before the STDP stimulus did not induce\nLTP or LTD. Overall these results indicate that in visual and\nfrontal cortex, pre-before-post pairing leaves an eligibility trace\nthat decays over 5–10 s and that can be converted into LTP\nby the neuromodulator noradrenaline. Similarly, post-before-pre\npairing leaves a shorter eligibility trace that decays over 3 s and\ncan be converted into LTD by the neuromodulator serotonin; cf.\n**Figure 2B** .\nFunctionally, a theoretical model in the same paper (He\net al., 2015) showed that the measured three-factor learning\nrules with two separate eligibility traces stabilized and prolonged\nnetwork activity so as to allow “event prediction.” The authors\n\n\n\nhypothesized that these three-factor rules were related to rewardbased learning in cortex such as perceptual learning in monkeys\n(Schoups et al., 2001) or mice (Poort et al., 2015) or reward\nprediction (Shuler and Bear, 2006). The relation to surprise was\nnot discussed but might be a direction for further explorations.\nMolecularly, the transformation of the Hebbian pre-beforepost eligibility trace into LTP involves beta adrenergic receptors\nand intracellular cyclic adenosine monophosphate (cAMP)\nwhereas the transformation of the post-pre eligibility trace\ninto LTD involves the 5-HT2c receptor (He et al., 2015). Both\nreceptors are anchored at the postsynaptic density consistent with\na role in the transformation of an eligibility trace into actual\nweight changes (He et al., 2015).\n\n\n3.3. Eligibility Traces in Hippocampus\nTwo experimental groups studied eligibility traces in CA1\nhippocampal neurons using complementary approaches. In\nthe studies of Brzosko et al. (2015, 2017), CA1 neurons in\nhippocampal slices were stimulated during about 8 min in\nan STDP protocol involving 100 repetitions (at 0.2 Hz) of\npairs of one extracellularly delivered presynaptic stimulation\npulse (presynaptic factor) and one postsynaptic action potential\n(postsynaptic factor) (Brzosko et al., 2015). Repeated prebefore-post with a relative timing +10 ms gave LTP (in the\npresence of natural endogenous dopamine) whereas post-beforepre (–20 ms) gave LTD. However, with additional dopamine\n(third factor) in the bathing solution, post-before-pre at –20 ms\ngave LTP (Zhang et al., 2009). Similarly, an STDP protocol with\npost-before-pre at –10 ms resulted in LTP when endogenous\ndopamine was present, but in LTD when dopamine was blocked\n(Brzosko et al., 2015). Thus dopamine broadens the STDP\nwindow for LTP into the post-before-pre regime (Zhang et al.,\n2009; Pawlak et al., 2010). Moreover, in the presence of ACh\nduring the STDP stimulation protocol, pre-before-post at +10ms\nalso gave LTD (Brzosko et al., 2017). Thus ACh broadens the LTD\nwindow.\n\nThe crucial experiment of Brzosko et al. (2015) involved a\ndelay in the dopamine (Brzosko et al., 2015). Brzosko et al.\nstarted to perfuse dopamine either immediately after the end of\nthe post-before-pre (-20ms) induction protocol or with a delay.\nSince the dopamine was given for about 10 min, it cannot be\nconsidered as a phasic signal – but at least the start of the\ndopamine perfusion was delayed. Brzosko et al. found that the\nstimulus that would normally have given LTD turned into LTP if\nthe delay of dopamine was in the range of 1 min or less, but not\nif dopamine started 10 min after the end of the STDP protocol\n(Brzosko et al., 2015). Note that for the conversion of LTD into\nLTP, it was important that the synapses were weakly stimulated\nat low rate while dopamine was present. Similarly, a prolonged\npre-before-post protocol at +10 ms in the presence of ACh gave\nrise to LTD, but with dopamine given with a delay of <1 min the\nsame protocol gave LTP (Brzosko et al., 2017). To summarize, in\nthe hippocampus a prolonged post-before-pre protocol (or a prebefore-post protocol in the presence of ACh) yields visible LTD, but\nalso sets an invisible synaptic flag for LTP. If dopamine is applied\nwith a delay of <1 min, the synaptic flag is converted into a positive\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 9 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nweight change under continued weak presynaptic stimulation; cf.\n**Figure 2C** .\nMolecularly, the conversion of LTD into LTP after repeated\nstimulation of post-before-pre pulse pairings depended on\nNMDA receptors and on the cAMP - PKA signaling cascade\n(Brzosko et al., 2015). The source of dopamine could be in the\nLocus Coeruleus which would make a link to arousal and novelty\n(Takeuchi et al., 2016) or from other dopamine nuclei linked\nto reward (Schultz, 1998). Since the time scale of the synaptic\nflag reported in Brzosko et al. (2015, 2017) was in the range of\nminutes, the process studied by Brzosko et al. could be related\nto synaptic consolidation (Frey and Morris, 1997; Reymann and\nFrey, 2007; Redondo and Morris, 2011; Lisman, 2017) rather than\neligibility traces in reinforcement learning where shorter time\nconstants are needed (Izhikevich, 2007; Legenstein et al., 2008;\nFrémaux et al., 2010, 2013). The computational study in Brzosko\net al. (2015) used an eligibility trace with a time constant of 2 s\nand showed that dopamine as a reward signal induced learning\nof reward location while ACh during exploration enabled a fast\nrelearning after a shift of the reward location (Brzosko et al.,\n2017).\nThe second study combined in vivo with in vitro data (Bittner\net al., 2017). From in vivo studies it has been known that CA1\nneurons in mouse hippocampus can develop a novel, reliable,\nand rather broadly tuned, place field in a single trial under the\ninfluence of a “calcium plateau potential” (Bittner et al., 2015),\nvisible as a complex spike at the soma. Moreover, an artificially\ninduced complex spike was sufficient to induce such a novel place\nfield in vivo (Bittner et al., 2015, 2017).\nIn additional slice experiments, several input fibers from\nCA3 to CA1 neurons were stimulated by 10 pulses from\nan extracellular electrode during 1 s. The resulting nearly\nsynchronous inputs at, probably, multiple synapses caused a total\nEPSP that was about 10 mV above baseline at the soma, and\npotentially somewhat larger in the dendrite, but did not cause\nsomatic spiking of the CA1 neuron. The stimulated synapses\nshowed LTP if the presynaptic stimulation was paired with a\ncalcium plateau potential (complex spike) in the postsynaptic\nneuron. LTP occurred, even if the presynaptic stimulation\nstopped 1 or 2 s before the start of the plateau potential or if\nthe plateau potential started before the presynaptic stimulation\n(Bittner et al., 2017). The protocol has a remarkable efficiency\nsince potentiation was around 200% after only 5 pairings. Thus,\nthe joint activation of many synapses sets a flag at the activated\nsynapses which is translated into LTP if a calcium plateau potential\n(complex spike) occurs a few seconds before or after the synaptic\nactivation; cf. **Figure 2D** . Molecularly, the plasticity processes\nimplied NMDA receptors and calcium channels (Bittner et al.,\n2017).\nFunctionally, synaptic plasticity in hippocampus is\nparticularly important because of the role of hippocampus\nin spatial memory (O’Keefe and Nadel, 1978). CA1 neurons\nget input from CA3 neurons which have a narrow place field.\nThe emergence of a broad place field in CA1 has therefore\nbeen interpreted as linking several CA3 neurons (that cover for\nexample the 50 cm of the spatial trajectory traversed by the rat\nbefore the current location) to a single CA1 cell that codes for\n\n\n\nthe current location (Bittner et al., 2017). Note that at the typical\nrunning speed of rodents, 50 cm correspond to several seconds\nof running. The broad activity of CA1 cells has therefore been\ninterpreted as a predictive representation of upcoming events or\nplaces (Bittner et al., 2017). What could such an upcoming event\nbe? For a rodent exploring a T-maze it might for example be\nimportant to develop a more precise spatial representation at the\nT-junction than inside one of the long corridors. With a broad\nCA1 place field located at the T-junction, information about the\nupcoming bifurcation could become available several seconds\nbefore the animal reaches the junction.\nBittner et al. interpreted their findings as the signature of\nan unusual form of STDP with a particularly long coincidence\nwindow on the behavioral time scale (Bittner et al., 2017).\nGiven that the time span of several seconds between presynaptic\nstimulation and postsynaptic complex spike is outside the\nrange of a potential causal relation between input and output,\nthey classified the plasticity rule as non-Hebbian because the\npresynaptic neurons do not participate in firing the postsynaptic\none (Bittner et al., 2017). As an alternative view, we propose\nto classify the findings of Bittner et al. as the signature of\nan eligibility trace that was left by the joint occurrence of a\npresynaptic spike arriving from CA3 (presynaptic factor) and a\nsubthreshold depolarization at the location of the synapse in the\npostsynaptic CA1 neuron (postsynaptic factor); cf. **Figure 2D** . In\nthis view, the setting of the synaptic flag is caused by a “Hebbian”type induction, except that on the postsynaptic side there are\nno spikes but just depolarization, consistent with the role of\ndepolarization as a postsynaptic factor (Artola and Singer, 1993;\nNgezahayo et al., 2000; Sjöström et al., 2001; Clopath et al., 2010).\nIn this view, the findings of Bittner et al. suggest that the synaptic\nflag set by the induction protocol leaves an eligibility trace which\ndecays over 2 s. If a plateau potential (related to the third factor)\nis generated during these 2 s, the eligibility trace caused by the\ninduction protocol is transformed into a measurable change of\nthe synaptic weight. The third factor M [3][rd] (t) in Equation (2)\ncould correspond to the complex spike, filtered with a time\nconstant of about 1 s. Importantly, plateau potentials can be\nconsidered as neuron-wide signals (Bittner et al., 2015) triggered\nby surprising, novel or rewarding events (Bittner et al., 2017).\nIn this view, the results of Bittner et al. are consistent with\nthe framework of neoHebbian three-factor learning rules. If the\nplateau potentials are indeed linked to surprising events, the\nthree-factor rule framework predicts that in vivo many neurons\nin CA1 receive such a third input as a broadcast-like signal.\nHowever, only those neurons that also get, at the same time,\nsufficiently strong input from CA3 might develop the visible\nplateau potential (Bittner et al., 2015).\nThe main difference between the two alternative views is that,\nin the model discussed in Bittner et al. (2017), each activated\nsynapse is marked by an eligibility trace (which is independent\nof the state of the postsynaptic neuron) whereas in the view\nof the three-factor rule, the eligibility trace is set only if the\npresynaptic activation coincides with a strong depolarization\nof the postsynaptic membrane. Thus, in the model of Bittner\net al. the eligibility trace is set by the presynaptic factor alone\nwhereas in the three-factor rule description it is set by the\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 10 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\ncombination of pre- and postsynaptic factors. The two models\ncan be distinguished in future experiments where either the\npostsynaptic voltage is controlled during presynaptic stimulation\nor where the number of simultaneously stimulated input fibers is\nminimized. The prediction of the three-factor rule is that spike\narrival at a single synapse, or spike arrival in conjunction with a\nvery small depolarization of <2 mV above rest, is not sufficient\nto set an eligibility trace. Therefore, LTP will not occur in these\ncases even if a calcium plateau potential occurs 1 s later.\n\n\n4. DISCUSSION AND CONCLUSION\n\n\n4.1. Policy Gradient vs. TD-learning\nAlgorithmic models of TD-learning with discrete states and in\ndiscrete time do not need eligibility traces that extend beyond\none time step (Sutton and Barto, 1998). In a scenario where\nthe only reward is given in a target state that is several action\nsteps away from the initial state, reward information shifts, over\nmultiple trials, from the target state backwards, even if the onestep eligibility trace connects only one state to the next (Sutton\nand Barto, 1998). Nevertheless, extended eligibility traces across\nmultiple time steps are considered convenient heuristic tools\nto speed up learning in temporal difference algorithms such as\nTD(λ) or SARSA(λ) (Singh and Sutton, 1996; Sutton and Barto,\n1998).\nIn policy gradient methods (Williams, 1992) as well as\nin continuous space-time TD-learning (Doya, 2000b; Frémaux\net al., 2013) eligibility traces appear naturally in the formulation\nof the problem of reward maximization. Importantly, a large class\nof TD-learning and policy gradient methods can be formulated\nas three-factor rules for spiking neurons where the third factor\nis defined as reward minus expected reward (Frémaux and\nGerstner, 2016). In policy gradient methods and related threefactor rules, expected reward is calculated as a running average\nof the reward (Frémaux et al., 2010) or fixed to zero by choice of\nreward schedule (Florian, 2007; Legenstein et al., 2008). In TDlearning the expected reward in a given time step is defined as\nthe difference of the value of the current state and that of the\nnext state (Sutton and Barto, 1998). In the most recent large-scale\napplications of reinforcement learning the expected immediate\nreward in policy gradient is calculated by a TD-algorithm for\nstate-dependent value estimation (Greensmith et al., 2004; Mnih\net al., 2016). An excellent modern summary of Reinforcement\nLearning Algorithms and their historical predecessors can be\nfound in (Sutton and Barto, 2018).\n\n\n4.2. Supervised Learning vs.\nReinforcement Learning\nThe experiments in Bittner et al. (2015, 2017) provide convincing\nevidence that plateau potentials are relevant for the described\nplasticity events and could be related to the third factor in threefactor rules. But in view of the difference between Equations (2)\nand (4) the question arises whether the third factor in the Bittner\net al. experiments should be considered as a global or as a neuronspecific factor. Obviously, a plateau potential is neuron-specific.\nThe more precise reformulation of this question therefore is\nwhether this specificity is covered by a Type 2 factor written as\n\n\n\nhi(M [3][rd] ) (see Equation 3) or whether it needs the more general\nType 3 formulation with Mi [3][rd] (see Equation 4). We see two\npotential interpretations.\n\n\n(i) A surprise- or novelty-related global (scalar)\nneuromodulator M [3][rd] is capable of pushing all CA1\nneurons into a state ready to generate a plateau potential,\nbut only a fraction of the neurons actually receive this\nmessage and stochastically generate a plateau potential. The\nterm hi(M [3][rd] ) expresses the heterogeneity of this process.\nHowever, amongst the subset of neurons with hi(M [3][rd] ) > 0\nonly those neurons that have a nonzero eligibility trace\nwill implement synaptic plasticity. Thus the third factor is\ninitially global, but triggers in the end very specific plasticity\nevents limited to a few neurons and synapses only.\n(ii) A (potentially high-dimensional) mismatch-related error\nsignal is randomly projected onto different neurons,\nincluding those in CA1. The effect is a neuron-specific\nthird factor Mi [3][rd] with index i. This second possibility\nis particularly intriguing because it relates to theories of\nattention-gated learning (Roelfsema and Holtmaat, 2018)\nand learning with segregated synapses (Guerguiev et al.,\n2017) as instantiations of approximate backpropagation of\nerrors. The high-dimensional signal could be related to the\nmismatch between what the animal expects to see and what it\nactually sees in the next instant. In this second interpretation,\nwe leave the field of generalized reinforcement learning and\nthe experiments in Bittner et al. (2015, 2017) can be seen as a\nmanifestation of supervised learning.\n\n\n4.3. Specificity\nIf phasic scalar neuromodulator signals are broadcasted over\nlarge areas of the brain, the question arises whether synaptic\nplasticity can still be selective. In the framework of three-factor\nrules, specificity is inherited from the synaptic flags which are\nset by the combination of presynaptic spike arrival and an\nelevated postsynaptic voltage at the location of the synapses.\nThe requirement is met only for a small subset of synapses,\nbecause presynaptic activity alone or postsynaptic activity alone\nare not sufficient; cf. **Figure 1B** . Furthermore, among all the\nflagged synapses only those that show, over many trials, a\ncorrelation with the reward signal will be consistently reinforced\n(Loewenstein and Seung, 2006; Legenstein et al., 2008).\nSpecificity can further be enhanced by an attentional feedback\nmechanism (Roelfsema and van Ooyen, 2005; Roelfsema et al.,\n2010) that restricts the number of eligible synapses to the\n“interesting” ones, likely to be involved in the task. Such an\nattentional gating signal acts as an additional factor and turns\nthe three-factor into a four-factor learning rule (Rombouts et al.,\n2015; Roelfsema and Holtmaat, 2018). Additional specificity can\nalso arise from the fact that not all neurons react in the same way\nto a modulator as implemented by the notation hi(M [3][rd] ) of Type\n2 rules (Brea et al., 2013; Rezende and Gerstner, 2014); as well as\nfrom additional factors that indicate whether a specific neuron\nin a population spikes in agreement with the majority of that\npopulation (Urbanczik and Senn, 2009). Maximal specificity is\nachieved with a neuron-specific third factor Mi [3][rd] (Type 3 rules)\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 11 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nas in modern implementations of supervised learning (Lillicrap\net al., 2016; Guerguiev et al., 2017).\n\n\n4.4. Mapping to Neuromodulators\nA global third factor is likely to be related to neuromodulators,\nbut from the perspective of a theoretician there is no need\nto assign one neuromodulator to surprise and another one\nto reward. Indeed, the theoretical framework also works if\neach neuromodulator codes for a different combination of\nvariables such as surprise, novelty or reward, just as we can use\ndifferent coordinate systems to describe the same physical system\n(Frémaux and Gerstner, 2016). Thus, whether dopamine is purely\nreward related or also novelty related (Ljunberg et al., 1992;\nSchultz, 1998; Redgrave and Gurney, 2006) is not critical for the\ndevelopment of three-factor learning rules as long as dimensions\nrelating to novelty, surprise, and reward are all covered by the set\nof neuromodulators.\n\nComplexity in biology is increased by the fact that dopamine\nneurons projecting from the VTA to the striatum can have\nseparate circuits and functions changing from reward in ventral\nstriatum to novelty in the the tail of striatum (Menegas et al.,\n2017). Similarly, dopaminergic fibers starting in the VTA can\nhave a different function than those starting in Locus Coeruleus\n(Takeuchi et al., 2016). Furthermore, findings over the last decade\nindicate that midbrain dopamine neurons generally show a high\ndiversity of responses and input-output mappings (Fiorillo et al.,\n2013; Roeper, 2013). Finally, the time scale of eligibility traces\ncould vary from one brain area to the next, in line with the\ngeneral idea that higher cortical areas show more persistent\nactivity than primary sensory areas (Wang and Kennedy, 2016).\nIf the time scale of eligibility traces is slower in higher areas,\nwe speculate that temporal links between more abstract, slowly\nevolving concepts could be picked up by plasticity rules. The\nframework of three-factor rules is general enough to allow for\nthese, and many other, variations.\n\n\n4.5. Alternatives to Eligibility Traces for\nBridging the Gap Between the Behavioral\nand Neuronal Time Scales\n\nFrom a theoretical point of view, there is nothing—apart from\nconceptual elegance—to favor eligibility traces over alternative\nneuronal mechanisms to associate events that are separated by\na second or more. For example, memory traces hidden in the\nrich firing activity patterns of a recurrent network (Maass et al.,\n2002; Jaeger and Haas, 2004; Buonomano and Maass, 2009;\nSusillo and Abbott, 2009) or short-term synaptic plasticity in\nrecurrent networks (Mongillo et al., 2008) could be involved\nin learning behavioral tasks with delayed feedback. In some\nmodels, neuronal, rather than synaptic, activity traces have\nbeen involved in learning a delayed paired-associate task (Brea\net al., 2016) and a combination of synaptic eligibity traces with\nprolonged single-neuron activity has been used for learning on\nbehavioral time scales (Rombouts et al., 2015). The empirical\nstudies reviewed here support the idea that the brain makes use\nof the elegant solution with synaptic eligibility traces and threefactor learning rules, but do not exclude that other mechanisms\nwork in parallel.\n\n\n\n4.6. The Paradoxical Nature of Predictions\nin Computational Neuroscience\nIf a neuroscientist thinks of a theoretical model, he often\nimagines a couple of assumptions at the beginning, a set\nof results derived from simulations or mathematical analysis,\nand ideally a few novel predictions–but is this the way\nmodeling works? There are at least two types of predictions in\ncomputational neuroscience, detailed predictions and conceptual\npredictions. Well-known examples of detailed predictions have\nbeen generated from variants of multi-channel biophysical\nHodgkin-Huxley type (Hodgkin and Huxley, 1952) models\nsuch as: “if channel X is blocked then we predict that ... ”\nwhere X is a channel with known dynamics and predictions\ninclude depolarization, hyperpolarization, action potential firing,\naction potential backpropagation or failure thereof. All of\nthese are useful predictions readily translated to and tested in\nexperiments.\nConceptual predictions derived from abstract conceptual\nmodels are potentially more interesting, but more difficult to\nformulate. Conceptual models develop ideas and form our\nthinking of how a specific neuronal system could work to solve a\nbehavioral task such as working memory (Mongillo et al., 2008),\naction selection and decision making (Sutton and Barto, 1998),\nlong-term stability of memories (Crick, 1984; Lisman, 1985; Fusi\net al., 2005), memory formation and memory recall (Willshaw\net al., 1969; Hopfield, 1982). Paradoxically these models often\nmake no detailed predictions in the sense indicated above. Rather,\nin these and other conceptual theories, the most relevant model\nfeatures are formulated as assumptions which may be considered,\nin a loose sense, as playing the role of conceptual predictions.\nTo formulate it as a short slogan: Assumptions are predictions.\nLet us return to the conceptual framework of three-factor rules:\nthe purification of rough ideas into the role of three factors is\nthe important conceptual work - and part of the assumptions.\nMoreover, the specific choice of time constant in the range of 1\ns for the eligibility trace has been formulated by theoreticians as\none of the model assumptions, rather than as a prediction; cf. the\nfootnotes in section “Examples and theoretical predictions.” Why\nis this the case?\n\nMost theoreticians shy away from calling their conceptual\nmodeling work a “prediction,” because there is no logical\nnecessity that the brain must work the way they assume in\ntheir model–the brain could have found a less elegant, different,\nbut nevertheless functional solution to the problem under\nconsideration; see the examples in the previous subsection.\nWhat a good conceptual model in computational neuroscience\nshows is that there exists a (nice) solution that should ideally\nnot be in obvious contradiction with too many known facts.\nImportantly, conceptual models necessarily rely on assumptions\nwhich in many cases have not (yet) been shown to be true.\nThe response of referees to modeling work in experimental\njournals therefore often is: “but this has never been shown.”\nIndeed, some assumptions may look far-fetched or even in\ncontradiction with known facts: for example, to come back to\neligibility traces, experiments on synaptic tagging-and-capture\nhave shown in the 1990s that the time scale of a synaptic flag\nis in the range of one hour (Frey and Morris, 1997; Reymann\nand Frey, 2007; Redondo and Morris, 2011; Lisman, 2017),\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 12 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nwhereas the theory of eligibility traces for action learning needs\na synaptic flag on the time scale of one second. Did synaptic\ntagging results imply that three-factor rules for action learning\nwere wrong, because they used the wrong time scale? Or, on\nthe contrary, did these experimental results rather imply that a\nbiological machinery for three-factor rules was indeed in place\nwhich could therefore, for other neuron types and brain areas,\nbe used and re-tuned to a different time scale (Frémaux et al.,\n2013)?\nAs mentioned earlier, the concepts of eligibility traces and\nthree-factor rules can be traced back to the 1960s, from\nmodels formulated in words (Crow, 1968), to firing rate models\nformulated in discrete time and discrete states (Klopf, 1972;\nSutton and Barto, 1981, 1998; Barto et al., 1983; Barto, 1985;\nWilliams, 1992; Schultz, 1998; Bartlett and Baxter, 1999), to\nmodels with spikes in a continuous state space and an explicit\ntime scale for eligibility traces (Xie and Seung, 2004; Loewenstein\nand Seung, 2006; Florian, 2007; Izhikevich, 2007; Legenstein\net al., 2008; Vasilaki et al., 2009; Frémaux et al., 2013). Despite\nthe mismatch with the known time scale of synaptic tagging\nin hippocampus (and lack of experimental support in other\nbrain areas), theoreticians persisted, polished their theories,\n\n\nREFERENCES\n\n\nArleo, A., and Gerstner, W. (2000). Spatial cognition and neuro-mimetic\nnavigation: a model of hippocampal place cell activity. Biol. Cybern. 83,\n[287–299. doi: 10.1007/s004220000171](https://doi.org/10.1007/s004220000171)\n\nArtola, A., and Singer, W. (1993). Long-term depression of excitatory\nsynaptic transmission and its relationship to long-term potentiation. Trends\n[Neurosci. 16, 480–487. doi: 10.1016/0166-2236(93)90081-V](https://doi.org/10.1016/0166-2236(93)90081-V)\nBailey, C. H., Giustetto, M., Huang, Y.-Y., Hawkins, R. D., and Kandel,\nE. R. (2000). Is heterosynaptic modulation essential for stabilizing hebbian\nplasiticity and memory. Nat. Rev. Neurosci. 1, 11–20. doi: 10.1038/350\n\n36191\n\nBarrett, A. B., Billings, G. O., Morris, R. G., and van Rossum, M. C. (2009). State\nbased model of long-term potentiation and synaptic tagging and capture. PLoS\n[Comput. Biol. 5:e1000259. doi: 10.1371/journal.pcbi.1000259](https://doi.org/10.1371/journal.pcbi.1000259)\nBartlett, P. L., and Baxter, J. (1999). Hebbian Synaptic Modification in Spiking\nNeurons That Learn. Technical report, Australian National University.\nBarto, A. (1985). Learning by statistical cooperation of self-interested neuron-like\ncomputing elements. Hum. Neurobiol. 4, 229–256.\nBarto, A., Sutton, R., and Anderson, C. (1983). Neuronlike adaptive elements\nthat can solve difficult learning and control problems. IEEE Trans. Syst. Man\nCybern. 13, 835–846.\nBenna, M. K., and Fusi, S. (2016). Computational principles of synaptic memory\n[consolidation. Nat. Neurosci. 19, 1697–1706. doi: 10.1038/nn.4401](https://doi.org/10.1038/nn.4401)\nBienenstock, E. L., Cooper, L. N., and Munroe, P. W. (1982). Theory\nof the development of neuron selectivity: orientation specificity\nand binocular interaction in visual cortex. J. Neurosci. 2, 32–48.\n[doi: 10.1523/JNEUROSCI.02-01-00032.1982](https://doi.org/10.1523/JNEUROSCI.02-01-00032.1982)\nBittner, K. C., Grienberger, C., Vaidya, S. P., Milstein, A. D., Macklin, J. J.,\nSuh, J., et al. (2015). Conjunctive input processing drives feature selectivity\nin hippocampal CA1 neurons. Nat. Neurosci. 357, 1133–1142. doi: 10.1038/\n\nnn.4062\n\nBittner, K. C., Milstein, A. D., Grienberger, C., Romani, S., and Magee, J. C.\n(2017). Behavioral time scale synaptic plasticity underlies CA1 place fields.\n[Nature 357, 1033–1036. doi: 10.1126/science.aan3846](https://doi.org/10.1126/science.aan3846)\nBliss, T. V., and Lømo, T (1973). Long-lasting potentation of synaptic transmission\nin the dendate area of anaesthetized rabbit following stimulation of the\nperforant path. J. Physiol. 232, 351–356.\n\n\n\ntalked at conferences about these models, until eventually\nthe experimental techniques and the scientific interests of\nexperimentalists were aligned to directly test the assumptions\nof these theories. In view of the long history of three-factor\nlearning rules, the recent elegant experiments (Yagishita et al.,\n2014; Brzosko et al., 2015, 2017; He et al., 2015; Bittner et al.,\n2017) provide an instructive example of how conceptual theories\ncan influence experimental neuroscience.\n\n\nAUTHOR CONTRIBUTIONS\n\n\nAll authors listed have made a substantial, direct and intellectual\ncontribution to the work, and approved it for publication.\n\n\nFUNDING\n\n\nThis project has been funded by the European Research\nCouncil (grant agreement no. 268 689, MultiRules), by the\nEuropean Union Horizon 2020 Framework Program under grant\nagreements no. 720270 and no. 785907 (Human Brain Project\nSGA1 and SGA2), and by the Swiss National Science Foundation\n(no. 200020 165538).\n\n\nBlack, J., Belluzzi, J. D., and Stein, L. (1985). Reinforcement delay of one second\nseverely impairs acquisition of brain self-stimulation. Brain Res. 359, 113–119.\n[doi: 10.1016/0006-8993(85)91418-0](https://doi.org/10.1016/0006-8993(85)91418-0)\nBliss, T. V., and Collingridge, G. L. (1993). A synaptic model of memory: longterm potentiation in the hippocampus. Nature 361, 31–39. doi: 10.1038/361\n\n031a0\n\nBosch, M., Castro, J., Saneyoshi, T., Matsuno, H., Sur, M., and Hayashi, Y. (2014).\nStructural and molecular remodeling of dendritic spine substructures during\n[long-term potentation. Neuron 82, 444–459. doi: 10.1016/j.neuron.2014.03.021](https://doi.org/10.1016/j.neuron.2014.03.021)\nBrader, J. M., Senn, W., and Fusi, S. (2007). Learning real-world stimuli in a neural\nnetwork with spike-driven synaptic dynamics. Neural Comput. 19, 2881–2912.\n[doi: 10.1162/neco.2007.19.11.2881](https://doi.org/10.1162/neco.2007.19.11.2881)\n\nBrea, J., Gaál, A. T., Urbancik, R., and Senn, W. (2016). Prospective\ncoding by spiking neurons. PLoS Comput. Biol. 12:e1005003.\n[doi: 10.1371/journal.pcbi.1005003](https://doi.org/10.1371/journal.pcbi.1005003)\nBrea, J., Senn, W., and Pfister, J.-P. (2013). Matching recall and storage in\nsequence learning with spiking neural networks. J. Neurosci. 33, 9565–9575.\n[doi: 10.1523/JNEUROSCI.4098-12.2013](https://doi.org/10.1523/JNEUROSCI.4098-12.2013)\nBrown, M. A., and Sharp, P. E. (1995). Simulation of spatial learning in the\nMorris water maze by a neural network model of the hippocampal-formation\nand nucleus accumbens. Hippocampus 5, 171–188. doi: 10.1002/hipo.4500\n\n50304\n\nBrzosko, Z., Schultz, W., and Paulsen, O. (2015). Retroactive modulation\nof spike timing-dependent plasticity by dopamine. eLife 4:e09685.\n[doi: 10.7554/eLife.09685](https://doi.org/10.7554/eLife.09685)\n\nBrzosko, Z., Zannone, S., Schultz, W., Clopath, C., and Paulsen, O.\n(2017). Sequential neuromodulation of hebbian plasticity offers mechanism for\n[effective reward-based navigation. eLife 6:e27756. doi: 10.7554/eLife.27756](https://doi.org/10.7554/eLife.27756)\nBuonomano, D. V., and Maass, W. (2009). State-dependent computations:\nspatiotemporal processing in cortical networks. Nat. Rev. Neurosci. 10,\n[113–125. doi: 10.1038/nrn2558](https://doi.org/10.1038/nrn2558)\n\nCassenaer, S., and Laurent, G. (2012). Conditional modulation of spiketiming-dependent plasticity for olfactory learning. Nature 482, 47–52.\n[doi: 10.1038/nature10776](https://doi.org/10.1038/nature10776)\n\nClopath, C., Büsing, L., Vasilaki, E., and Gerstner, W. (2010). Connectivity\nreflects coding: a model of voltage-based spike-timing-dependentplasticity with homeostasis. Nat. Neurosci. 13, 344–352. doi: 10.1038/\n\nnn.2479\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 13 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nClopath, C., Ziegler, L., Vasilaki, E., Büsing, L., and Gerstner, W. (2008). Tagtrigger-consolidation: a model of early and late long-term-potentiation and\n[depression. PLoS Comput. Biol. 4:e1000248. doi: 10.1371/journal.pcbi.1000248](https://doi.org/10.1371/journal.pcbi.1000248)\nCrick, F. (1984). Neurobiology-memory and molecular turnover. Nature 312:101.\n[doi: 10.1038/312101a0](https://doi.org/10.1038/312101a0)\n\nCrow, T. (1968). Cortical synapses and reinforcement: a hypothesis. Nature 219,\n[736–737. doi: 10.1038/219736a0](https://doi.org/10.1038/219736a0)\n\nDaw, N. D., Niv, Y., and Dayan, P. (2005). Uncertainty-based competition\nbetween prefrontal and dorsolateral striatal systems for behavioral control. Nat.\n[Neurosci. 8, 1704–1711. doi: 10.1038/nn1560](https://doi.org/10.1038/nn1560)\nDeger, M., Helias, M., Rotter, S., and Diesmann M., (2012). Spike-timing\ndependence of structural plasticity explains cooperative synapse formation in\nthe neocortex. PLoS Comput. Biol. 8:e1002689. doi: 10.1371/journal.pcbi.10\n\n02689\n\nDeger, M., Seeholzer, A., and Gerstner, W. (2018). Multicontact co-operativity\nin spike-timing-dependent structural plasticity stabilizes networks. Cereb.\n[Cortex 28, 1396–1415. doi: 10.1093/cercor/bhx339](https://doi.org/10.1093/cercor/bhx339)\nDoya, K. (2000a). Complementary roles of basal ganglia and cerebellum\nin learning and motor control. Curr. Opin. Neurobiol. 10, 732–739.\n[doi: 10.1016/S0959-4388(00)00153-7](https://doi.org/10.1016/S0959-4388(00)00153-7)\nDoya, K. (2000b). Temporal difference learning in continuous time and space.\nNeural Comput. 12, 219–245.\nFaraji, M., Preuschoff, K., and Gerstner, W. (2018). Balancing new against old\ninformation: the role of puzzlement surprise in learning. Neural Comput. 30,\n[34–83. doi: 10.1162/neco_a_01025](https://doi.org/10.1162/neco_a_01025)\nFauth, M., Wörgötter, F., and Tetzlaff, C. (2015). The formation of\nmulti-synaptic connections by the interaction of synaptic and structural\nplasticity and their functional consequences. PLoS Comput. Biol. 11:e1004031.\n[doi: 10.1371/journal.pcbi.1004031](https://doi.org/10.1371/journal.pcbi.1004031)\nFiorillo, C. D., Yun, S. R., and Song, M. R. (2013). Diversity and homogeneity\nin responses of midbrain dopamine neurons. J. Neurosci. 33, 4693–4709.\n[doi: 10.1523/JNEUROSCI.3886-12.2013](https://doi.org/10.1523/JNEUROSCI.3886-12.2013)\nFlorian, R. V. (2007). Reinforcement learning through modulation of spiketiming-dependent synaptic plasticity. Neural Comput. 19, 1468–1502.\n[doi: 10.1162/neco.2007.19.6.1468](https://doi.org/10.1162/neco.2007.19.6.1468)\n\nFoster, D. J., Morris, R. G., and Dayan, P. (2000).\nModels of hippocampally dependent navigation using the\ntemporal difference learning rule. Hippocampus 10, 1–16.\n[doi: 10.1002/(SICI)1098-1063(2000)10:1<1::AID-HIPO1>3.0.CO;2-1](https://doi.org/10.1002/(SICI)1098-1063(2000)10:1<1::AID-HIPO1>3.0.CO;2-1)\nFrémaux, N., and Gerstner, W. (2016). Neuromodulated spike-timing dependent\nplasticity and theory of three-factor learning rules. Front. Neural Circ. 9:85.\n[doi: 10.3389/fncir.2015.00085](https://doi.org/10.3389/fncir.2015.00085)\n\nFrémaux, N., Sprekeler, H., and Gerstner, W. (2010). Functional requirements\nfor reward-modulated spike-timing-dependent plasticity,. J. Neurosci. 40,\n[13326–13337. doi: 10.1523/JNEUROSCI.6249-09.2010](https://doi.org/10.1523/JNEUROSCI.6249-09.2010)\nFrémaux, N., Sprekeler, H., and Gerstner, W. (2013). Reinforcement learning\nusing continuous time actor-critic framework with spiking neurons. PLoS\n[Comput. Biol. 9:e1003024. doi: 10.1371/journal.pcbi.1003024](https://doi.org/10.1371/journal.pcbi.1003024)\nFrey, U., and Morris, R. (1997). Synaptic tagging and long-term potentiation.\n[Nature 385, 533–536. doi: 10.1038/385533a0](https://doi.org/10.1038/385533a0)\nFriston, K. (2010). The free-energy principle: a unified brain theory? Nat. Rev.\n[Neurosci. 11, 127–138. doi: 10.1038/nrn2787](https://doi.org/10.1038/nrn2787)\nFriston, K., Fitzgerald, T., Rigoli, F., Schwartenbeck, P., O’Doherty, J., and\nPezzulo, G. (2016). Active inference and learning. Neurosci. and Behav. Rev. 68,\n[862–879. doi: 10.1016/j.neubiorev.2016.06.022](https://doi.org/10.1016/j.neubiorev.2016.06.022)\nFroemke, R. C., and Dan, Y. (2002). Spike-timing dependent plasticity induced by\n[natural spike trains. Nature 416, 433–438. doi: 10.1038/416433a](https://doi.org/10.1038/416433a)\nFusi, S., Drew, P. J., and Abbott, L. F. (2005). Cascade models of\nsynaptically stored memories. Neuron 45, 599–611. doi: 10.1016/j.neuron.2005.\n\n02.001\n\nGerstner, W., Kempter, R., van Hemmen, J. L., and Wagner, H. (1996). A\nneuronal learning rule for sub-millisecond temporal coding. Nature 383, 76–78.\n[doi: 10.1038/383076a0](https://doi.org/10.1038/383076a0)\n\nGjorjieva, J., Clopath, C., Audet, J., and Pfister, J. P. (2011). A triplet spike-timing\ndependent plasticity model generalizes the Bienenstock-Cooper-Munro rule\nto higher-order spatiotemporal correlations. Proc. Natl. Sci. Acad. U.S.A. 108,\n[19383–19388. doi: 10.1073/pnas.1105933108](https://doi.org/10.1073/pnas.1105933108)\n\n\n\nGraupner, M., and Brunel, N. (2007). STDP in a bistable synapse model based\non CaMKII and associate signaling pathways. PLoS Comput. Biol. 3:e221.\n[doi: 10.1371/journal.pcbi.0030221](https://doi.org/10.1371/journal.pcbi.0030221)\nGreensmith, E., Bartlett, P., and Baxter, J. (2004). Variance reduction\ntechniques for gradient estimates in reinforcement learning. J. Machine Learn.\nRes/ 5, 1471–1530.\nGu, Q. (2002). Neuromodulatory transmitter systems in the cortex\nand their role in cortical plasticity. Neuroscience 111, 815–835.\n[doi: 10.1016/S0306-4522(02)00026-X](https://doi.org/10.1016/S0306-4522(02)00026-X)\nGuerguiev, J., Lillicrap, T. P., and Richards, B. A. (2017). Towards deep learning\n[with segregated dendrites. elife 6:e22901. doi: 10.7554/eLife.22901](https://doi.org/10.7554/eLife.22901)\nHaber, S. N., Kim, K.-S., Mailly, P., and Calzavara, R. (2006). Reward-related\ncortical inputs define a large striatal region in primates that interface with\nassociative cortical connections, providing a substrate for incentive-based\n[learning. J. Neurosci. 26, 8368–8376. doi: 10.1523/JNEUROSCI.0271-06.2006](https://doi.org/10.1523/JNEUROSCI.0271-06.2006)\nHasselmo, M. (2006). The role of acetylcholine in learning and memory. Curr.\n[Opin. Neurobiol. 16, 710–715. doi: 10.1016/j.conb.2006.09.002](https://doi.org/10.1016/j.conb.2006.09.002)\nHe, K., Huertas, M., Hong, S. Z., Tie, X., Hell, J.W., Shouval, H., et al. (2015).\nDistinct eligibility traces for LTP and LTD in cortical synapses. Neuron 88,\n[528–538. doi: 10.1016/j.neuron.2015.09.037](https://doi.org/10.1016/j.neuron.2015.09.037)\nHebb, D. O. (1949). The Organization of Behavior. New York, NY: Wiley.\nHelias, M., Rotter, S., Gewaltig, M.-O., and Diesmann, M. (2008). Structural\nplasticity controlled by calcium based correlation detection. Front. Comput.\n[Neurosci. 2:7. doi: 10.3389/neuro.10.007.2008](https://doi.org/10.3389/neuro.10.007.2008)\n\nHess, E. H., and Polt, J. M. (1960). Pupil size as related to interest value of visual\n[stimuli. Science 132, 349–350. doi: 10.1126/science.132.3423.349](https://doi.org/10.1126/science.132.3423.349)\nHodgkin, A. L., and Huxley, A. F. (1952). A quantitative description of membrane\ncurrent and its application to conduction and excitation in nerve. J Physiol 117,\n[500–544. doi: 10.1113/jphysiol.1952.sp004764](https://doi.org/10.1113/jphysiol.1952.sp004764)\nHopfield, J. J. (1982). Neural networks and physical systems with emergent\ncollective computational abilities. Proc. Natl. Acad. Sci. U.S.A. 79, 2554–2558.\n[doi: 10.1073/pnas.79.8.2554](https://doi.org/10.1073/pnas.79.8.2554)\nHuganir, R. L., and Nicoll, R. A. (2013). AMPARs and synaptic plasticity: the last\n[25 years. Neuron 80, 704–717. doi: 10.1016/j.neuron.2013.10.025](https://doi.org/10.1016/j.neuron.2013.10.025)\nItti, L., and Baldi, P. (2009). Bayesian surprise attracts human attention. Vis.\n[Res. 49, 1295–1306. doi: 10.1016/j.visres.2008.09.007](https://doi.org/10.1016/j.visres.2008.09.007)\nIzhikevich, E. (2007). Solving the distal reward problem through linkage\nof STDP and dopamine signaling. Cereb. Cortex 17, 2443–2452.\n[doi: 10.1093/cercor/bhl152](https://doi.org/10.1093/cercor/bhl152)\n\nIzhikevich, E. M., and Desai, N. S. (2003). Relating STDP to BCM. Neural\n[Comput. 15, 1511–1523. doi: 10.1162/089976603321891783](https://doi.org/10.1162/089976603321891783)\nJaeger, H., and Haas, H. (2004). Harnessing nonlinearity: predicting chaotic\nsystems and saving energy in wireless communication. Science 304, 78–80.\n[doi: 10.1126/science.1091277](https://doi.org/10.1126/science.1091277)\n\nKalman, R. (1960). A new approach to linear filtering and prediction problems. J.\n[Basic Eng. 82, 35–45. doi: 10.1115/1.3662552](https://doi.org/10.1115/1.3662552)\nKempter, R., Gerstner, W., and van Hemmen, J. L. (1999). Hebbian learning and\n[spiking neurons. Phys. Rev. E 59, 4498–4514. doi: 10.1103/PhysRevE.59.4498](https://doi.org/10.1103/PhysRevE.59.4498)\nKlopf, A. (1972). Brain Function and Adaptive Systems-a Heterostatic Theory. Air\nForce Cambridge research laboratories, special Reports Vol. 133, 1–70.\nKolossa, A., Fingscheidt, T., Wessel, K., and Kopp, B. (2013). A modelbased approach to trial-by-trial p300 amplitude fluctuations. Front. Hum.\n[Neurosci. 6:359. doi: 10.3389/fnhum.2012.00359](https://doi.org/10.3389/fnhum.2012.00359)\n\nKolossa, A., Kopp, B., and Finscheidt, T. (2015). A computational analysis\nof the neural bases of bayesian inference. NeuroImage 106, 222–237.\n[doi: 10.1016/j.neuroimage.2014.11.007](https://doi.org/10.1016/j.neuroimage.2014.11.007)\nLegenstein, R., Pecevski, D., and Maass, W. (2008). A learning theory\nfor reward-modulated spike-timing-dependent plasticity with application to\nbiofeedback. PLoS Comput. Biol. 4:e1000180. doi: 10.1371/journal.pcbi.10\n\n00180\n\nLevy, W. B., and Stewart, O. (1983). Temporal contiguity requirements for\nlong-term associative potentiation/depression in hippocampus. Neurosci, 8,\n[791–797. doi: 10.1016/0306-4522(83)90010-6](https://doi.org/10.1016/0306-4522(83)90010-6)\nLillicrap, T. P., Cownden, D. B., Tweed, D., and Akerman, C. J. (2016).\nRandom synaptic feedback weights support error backpropagation\nfor deep learning. Nat. Commun., 7:13276. doi: 10.1038/ncomms\n\n13276\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 14 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nLisman, J. (1985). A mechanism for memory storage insensitive to molecular\nturnover: a bistable autophosphorylating kinase. Proc. Natl. Acad. Sci. U.S.A. 82,\n[3055–3057. doi: 10.1073/pnas.82.9.3055](https://doi.org/10.1073/pnas.82.9.3055)\nLisman, J. (1989). A mechanism for Hebb and anti-Hebb processes underlying\nlearning and memory. Proc. Natl. Acad. Sci. U.S.A. 86, 9574–9578.\n[doi: 10.1073/pnas.86.23.9574](https://doi.org/10.1073/pnas.86.23.9574)\nLisman, J. (2003). Long-term potentiation: outstanding questions and\nattempted synthesis. Phil. Trans. R. Soc. Lond B Biol. Sci. 358, 829–842.\n[doi: 10.1098/rstb.2002.1242](https://doi.org/10.1098/rstb.2002.1242)\n\nLisman, J. (2017). Glutamatergic synapses are structurally and biochemically\ncomplex because of multiple plasticity processes: long-term potentiation, longterm depression, short-term potentiation and scaling. Phil. Trans. Roy. Soc.\n[B 372:20160260. doi: 10.1098/rstb.2016.0260](https://doi.org/10.1098/rstb.2016.0260)\n\nLisman, J., Grace, A. A., and Duzel, E. (2011). A neoHebbian framework for\nepisodic memory; role of dopamine-dependent late LTP. Trends Neurosci. 34,\n[536–547. doi: 10.1016/j.tins.2011.07.006](https://doi.org/10.1016/j.tins.2011.07.006)\nLittle, D. J., and Sommer, F. T. (2013). Learning and exploration in action[perception loops. Front. Neural Circ. 7:37. doi: 10.3389/fncir.2013.00037](https://doi.org/10.3389/fncir.2013.00037)\nLjunberg, T., Apicella, P., and Schultz, W. (1992). Responses of monkey dopamine\nneurons during learning of behavioral interactions. J. Neurophysiol. 67,\n[145–163. doi: 10.1152/jn.1992.67.1.145](https://doi.org/10.1152/jn.1992.67.1.145)\nLoewenstein, Y., and Seung, H. (2006). Operant matching is a generic outcome of\nsynaptic plasticity based on the covariance between reward and neural activity.\n[Proc. Natl. Acad. Sci. U.S.A. 103, 15224–15229. doi: 10.1073/pnas.0505220103](https://doi.org/10.1073/pnas.0505220103)\nLöwel, S., and Singer, W. (1992). Selection of intrinsic horizontal connections\nin the visual cortex by correlated neuronal activity. Science 255, 209–212.\n[doi: 10.1126/science.1372754](https://doi.org/10.1126/science.1372754)\n\nMaass, W., Natschläger, T., and Markram, H. (2002). Real-time computing\nwithout stable states: a new framework for neural computation\nbased on perturbations. Neural Comput. 14, 2531–2560. doi: 10.1162/\n\n089976602760407955\n\nMarkram, H., Lübke, J., Frotscher, M., and Sakmann, B. (1997). Regulation of\nsynaptic efficacy by coincidence of postysnaptic AP and EPSP. Science 275,\n[213–215. doi: 10.1126/science.275.5297.213](https://doi.org/10.1126/science.275.5297.213)\n\nMartin, S. J., Grimwood, P. D., and Morris, R. G. (2000). Synaptic plasticity and\nmemory: an evaluation of the hypothesis. Ann. Rev. Neurosci. 23, 649–711.\n[doi: 10.1146/annurev.neuro.23.1.649](https://doi.org/10.1146/annurev.neuro.23.1.649)\n\nMathys, C., Daunizeau, J., Friston, K. J., and Stephan, K. E. (2011). A\nbayesian foundation for individual learning under uncertainty. Front. Hum.\n[Neurosci. 5:39. doi: 10.3389/fnhum.2011.00039](https://doi.org/10.3389/fnhum.2011.00039)\n\nMathys, C. D., Lomakina, E. I., Daunizeau, K. H., Iglesias, S., Brodersen, K.\nJ., Friston, K., et al. (2014). Uncertainty in perception and the hierarchical\n[gaussian filter. Front. Hum. Neurosci. 8:825. doi: 10.3389/fnhum.2014.00825](https://doi.org/10.3389/fnhum.2014.00825)\nMatsuzaki, M., Ellis-Davies, G. C., Nemoto, T., Iione, M., Miyashita,\nY., and Kasai, H. (2001). Dendritic spine geometry is critical for AMPA\nreceptor expression in hippocampal CA1 pyramidal neurons. Nat. Neurosci. 4,\n[1086–1092. doi: 10.1038/nn736](https://doi.org/10.1038/nn736)\n\nMenegas, W., Babayan, B. M., Uchida, N., and Watabe-Uchida, M. (2017).\nOpposite initialization to novel cues in dopamine signaling in ventral and\n[posterior striatum in mice. eLife 6:e21886. doi: 10.7554/eLife.21886](https://doi.org/10.7554/eLife.21886)\nMeyniel, F., Maheu, M., and Dehaene, S. (2016). Human inferences\nabout sequences: a minimal transition probability model. PLoS Comput.\n[Biol. 12:e1005260. doi: 10.1371/journal.pcbi.1005260](https://doi.org/10.1371/journal.pcbi.1005260)\nMiddleton, F. A., and Strick, P. (2000). Basal ganglia and cerebellar\nloops: motor and cognitive circuits. Brain Res. Rev. 31, 236–250.\n[doi: 10.1016/S0165-0173(99)00040-5](https://doi.org/10.1016/S0165-0173(99)00040-5)\nMiller, K. D., and MacKay, D. J. C. (1994). The role of constraints in hebbian\n[learning. Neural Comput. 6, 100–126. doi: 10.1162/neco.1994.6.1.100](https://doi.org/10.1162/neco.1994.6.1.100)\nMink, J. (1996). The basal ganglia: focused selection and inhibition\nof competing motor programs. Progr. Neurobiol. 50, 381–425.\n[doi: 10.1016/S0301-0082(96)00042-1](https://doi.org/10.1016/S0301-0082(96)00042-1)\nMnih, V., Badia, A., Mirza, M., Graves, A., Harley, T., Lillicrap, T., et al. (2016).\n“Asynchronous methods for deep reinforcement learning,” in Proceedings of the\n33rd International Conference on Machine Learning, eds M. F. Balcan and K. Q.\nWeinberger (New York, NY), 1928–1937.\nMoncada, D., and Viola, H. (2007). Induction of long-term memory by\nexposure to novelty requires protein synthesis: Evidence for a behavioral\n\n\n\ntagging. J. Neurosci. 27, 7476–7481. doi: 10.1523/JNEUROSCI.1083-0\n\n7.2007\n\nMongillo, G., Barak, O., and Tsodyks, M. (2008). Synaptic theory of working\n[memory. Science 319, 1543–1546. doi: 10.1126/science.1150769](https://doi.org/10.1126/science.1150769)\nNassar, M. R, Wilson, R. C., Heasly, B., and Gold, J. I. (2010). An approximately\nbayesian delta-rule model explains the dynamics of belief updating in\na changing environment. J. Neurosci. 30, 12366–12378. doi: 10.1523/\nJNEUROSCI.0822-10.2010\nNgezahayo, A., Schachner, M., and Artola, A. (2000). Synaptic activation\nmodulates the induction of bidirectional synaptic changes in adult mouse\nhippocampus. J. Neurosci. 20, 2451–2458. doi: 10.1523/JNEUROSCI\n\n.20-07-02451.2000\n\nOja, E. (1982). A simplified neuron model as a principal component analyzer. J.\n[Math. Biol. 15, 267–273. doi: 10.1007/BF00275687](https://doi.org/10.1007/BF00275687)\nO’Keefe, J., and Nadel, L. (1978). The Hippocampus as a Cognitive Map, Vol. 3.\nOxford: Clarendon Press Oxford.\n\nOkouchi, H. (2009). Response acquisition by humans with delayed reinforcement.\n[J. Exp. Anal. Behav. 91, 377–390. doi: 10.1901/jeab.2009.91-377](https://doi.org/10.1901/jeab.2009.91-377)\nOudeyer, P., Kaplan, F., and Hafner, V. (2007). Intrinsic motivation systems\nfor autonomous mental development. IEEE Trans. Evol. Comput. 11, 265–286.\n[doi: 10.1109/TEVC.2006.890271](https://doi.org/10.1109/TEVC.2006.890271)\n\nPavlov, I. P. (1927). Conditioned Reflexes: An Investigation of the Physiological\nActivity of the Cerebral Cortex. Oxford, UK: Oxford University Press; Humpfrey\nMilford.\n\nPawlak, V., Wickens, J., Kirkwood, A., and Kerr, J. (2010). Timing is\nnot everything: neuromodulation opens the STDP gate. Front. Synaptic\n[Neurosci. 2:146. doi: 10.3389/fnsyn.2010.00146](https://doi.org/10.3389/fnsyn.2010.00146)\nPfister, J.-P., Toyoizumi, T., Barber, D., and Gerstner, W. (2006). Optimal spiketiming-dependent plasticity for precise action potential firing in supervised\n[learning. Neural Comput. 18, 1318–1348. doi: 10.1162/neco.2006.18.6.1318](https://doi.org/10.1162/neco.2006.18.6.1318)\nPoort, J., Khan, A. G., Pachitariu, M., Nemri, A., Orsolic, I.,\nKrupic, J., et al. (2015). Learning enhances sensory and multiple nonsensory representations in primary visual cortex. Neuron 86, 1478–1490.\n[doi: 10.1016/j.neuron.2015.05.037](https://doi.org/10.1016/j.neuron.2015.05.037)\nRedgrave, P., and Gurney, K. (2006). The short-latency dopamine signal: a role in\n[discovering novel actions? Nat. Rev. Neurosci. 7, 967–975. doi: 10.1038/nrn2022](https://doi.org/10.1038/nrn2022)\nRedondo, R. L., and Morris, R. G. (2011). Making memories last: the\nsynaptic tagging and capture hypothesis. Nat. Rev. Neurosci. 12, 17–30.\n[doi: 10.1038/nrn2963](https://doi.org/10.1038/nrn2963)\n\nReymann, K. G., and Frey, J. U. (2007). The late maintenance of hippocampal\nLTP: requirements, phases,synaptic tagging, late associativity and implications.\n[Neuropharmacology 52, 24–40. doi: 10.1016/j.neuropharm.2006.07.026](https://doi.org/10.1016/j.neuropharm.2006.07.026)\nReynolds, J. N., and Wickens, J. R. (2002). Dopamine-dependent\nplasticity of corticostriatal synapses. Neural Netw. 15, 507–521.\n[doi: 10.1016/S0893-6080(02)00045-X](https://doi.org/10.1016/S0893-6080(02)00045-X)\nRezende, D., and Gerstner, W. (2014). Stochastic variational learning\nin recurrent spiking networks. Front. Comput. Neurosci. 8:38.\n[doi: 10.3389/fncom.2014.00038](https://doi.org/10.3389/fncom.2014.00038)\n\nRoelfsema, P. R., and Holtmaat, A. (2018). Control of synaptic plasticity in deep\n[cortical networks. Nat. Rev. Neurosci. 19, 166–180. doi: 10.1038/nrn.2018.6](https://doi.org/10.1038/nrn.2018.6)\nRoelfsema, P. R., and van Ooyen, A. (2005). Attention-gated reinforcement\nlearning of internal representations for classification. Neural Comput. 17,\n[2176–2214. doi: 10.1162/0899766054615699](https://doi.org/10.1162/0899766054615699)\n\nRoelfsema, P. R., van Ooyen, A., and Watanabe, T. (2010). Perceptual learning\nrules based on reinforcers and attention. Trends Cogn. Sci. 14, 64–71.\n[doi: 10.1016/j.tics.2009.11.005](https://doi.org/10.1016/j.tics.2009.11.005)\nRoeper, J. (2013). Dissecting the diversity of midbrain dopamine neurons. Trends\n[Neurosci. 36, 336–342. doi: 10.1016/j.tins.2013.03.003](https://doi.org/10.1016/j.tins.2013.03.003)\nRombouts, J. O., Bothe, S. M., and Roelfsema, P. R. (2015). How attention can\ncreate synaptic tags for the learning of working memories in sequential tasks.\n[PLoS Comput. Biol. 11:e1004060. doi: 10.1371/journal.pcbi.1004060](https://doi.org/10.1371/journal.pcbi.1004060)\nRubin, J. E., Gerkin, R. C., Bi, G.-Q., and Chow, C. C. (2005). Calcium time\ncourse as a signal for spike-timing-dependent plasticity. J. Neurophysiol. 93,\n[2600–2613. doi: 10.1152/jn.00803.2004](https://doi.org/10.1152/jn.00803.2004)\nRubin, J., Lee, D. D., and Sompolinsky, H. (2001). Equilibrium properties\nof temporally asymmetric Hebbian plasticity. Phys. Rev. Lett. 86, 364–367.\n[doi: 10.1103/PhysRevLett.86.364](https://doi.org/10.1103/PhysRevLett.86.364)\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 15 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n\n\nGerstner et al. Synaptic Eligibility Traces for Learning\n\n\n\nSchmidhuber, J. (1991). “Curious model-building control systems,” in Proceedings\nof the International Joint Conference on Neural Networks, Vol. 2, (Singapore:\nIEEE press), 1458–1463.\nSchmidhuber, J. (2006). Developmental robotics, optimal artificial\ncuriosity, creativity, music, and the fine arts. Connect. Sci. 18, 173–187.\n[doi: 10.1080/09540090600768658](https://doi.org/10.1080/09540090600768658)\n\nSchmidhuber, J. (2010). Formal theory of creativity, fun, and intrinsic\nmotivation (1990–2010). IEEE Trans. Auton. Mental Dev. 2, 230–247.\n\n[doi: 10.1109/TAMD.2010.2056368](https://doi.org/10.1109/TAMD.2010.2056368)\n\nSchoups, A., Vogels, R., Quian, N., and Orban, G. (2001). Practising orientation\nidentification improves orientation coding in V1 neurons. Nature 412, 549–553.\n[doi: 10.1038/35087601](https://doi.org/10.1038/35087601)\n\nSchultz, W. (1998). Predictive reward signal of dopamine neurons. J.\n[Neurophysiol. 80, 1–27. doi: 10.1152/jn.1998.80.1.1](https://doi.org/10.1152/jn.1998.80.1.1)\nSchultz, W. (2002). Getting formal with dopamine and reward. Neuron 36,\n[241–263. doi: 10.1016/S0896-6273(02)00967-4](https://doi.org/10.1016/S0896-6273(02)00967-4)\nSchultz, W., Dayan, P., and Montague, P. R. (1997). A neural substrate\nfor prediction and reward. Science 275, 1593–1599. doi: 10.1126/\n\nscience.275.5306.1593\n\nSchultz, W., and Dickinson, A. (2000). Neuronal coding of prediction errors. Ann.\n[Rev. Neurosci. 23, 472–500. doi: 10.1146/annurev.neuro.23.1.473](https://doi.org/10.1146/annurev.neuro.23.1.473)\nSenn, W., Tsodyks, M., and Markram, H. (2001). An algorithm for modifying\nneurotransmitter release probability based on pre- and postsynaptic spike\n[timing. Neural Comput. 13, 35–67. doi: 10.1162/089976601300014628](https://doi.org/10.1162/089976601300014628)\nSeo, M., Lee, E., and Averbeck, B. (2012). Action selection and action value in\n[frontal-striatal circuits. Neuron 74, 947–960. doi: 10.1016/j.neuron.2012.03.037](https://doi.org/10.1016/j.neuron.2012.03.037)\nShannon, C. (1948). A mathematical theory of communication. Bell Syst. Techn.\n[J. 27, 37–423. doi: 10.1002/j.1538-7305.1948.tb01338.x](https://doi.org/10.1002/j.1538-7305.1948.tb01338.x)\nShatz, C. (1992). The developing brain. Sci. Am. 267, 60–67.\n[doi: 10.1038/scientificamerican0992-60](https://doi.org/10.1038/scientificamerican0992-60)\nShindou, T., Shindou, M., Watanabe, S., and Wickens, J. (2018). A silent eligibility\ntrace enables dopamine-dependent synaptic plasticity for reinforcement\n[learning in the mouse striatum. Eur. J. Neurosci. doi: 10.1111/ejn.13921. [Epub](https://doi.org/10.1111/ejn.13921)\nahead of print].\nShouval, H. Z., Bear, M. F., and Cooper, L. N. (2002). A unified model of NMDA\nreceptor dependent bidirectional synaptic plasticity. Proc. Natl. Acad. Sci.\n[U.S.A. 99, 10831–10836. doi: 10.1073/pnas.152343099](https://doi.org/10.1073/pnas.152343099)\nShuler, M., and Bear, M. (2006). Reward timing in the primary visual cortex.\n[Science 311, 1606–1609. doi: 10.1126/science.1123513](https://doi.org/10.1126/science.1123513)\nSingh, S., Barto, A., and Chentanez, N. (2004). Intrinsically\nmotivated reinforcement learning. Adv. Neural Inform. Proc. Syst. 17,\n\n1281–1288.\n\nSingh, S., and Sutton, R. (1996). Reinforcement learning with replacing eligibility\n[traces. Mach. Learn. 22, 123–158. doi: 10.1007/BF00114726](https://doi.org/10.1007/BF00114726)\nSjöström, P. J., Turrigiano, G. G., and Nelson, S. B. (2001). Rate, timing,\nand cooperativity jointly determine cortical synaptic plasticity. Neuron 32,\n[1149–1164. doi: 10.1016/S0896-6273(01)00542-6](https://doi.org/10.1016/S0896-6273(01)00542-6)\nSong, S., Miller, K., and Abbott, L. (2000). Competitive Hebbian learning\nthrough spike-time-dependent synaptic plasticity. Nat. Neurosci. 3, 919–926.\n[doi: 10.1038/78829](https://doi.org/10.1038/78829)\n\nSquires, K. C., Wickens, C., Squires, N. K., and Donchin, E. (1976). The effect\nof stimulus sequence on the waveform of the cortical event-related potential.\n[Science 193, 1141–1146. doi: 10.1126/science.959831](https://doi.org/10.1126/science.959831)\nStorck, J., Hochreiter, S., and Schmidhuber, J. (1995). “Reinforcement-driven\ninformation acquisition in non-deterministic environments,” in Proceedings of\nICANN’95, Vol.2 (Paris: EC2-CIE), 159–164.\nSun, Y., Gomez, F., and Schmidhuber, J. (2011). “Planning to be surprised:\noptimal Bayesian exploration in dynamic environments,” in Artificial General\nIntelligence, Lecture Notes in Computer Science, Vol. 6830, eds J. Schmidhuber,\nK. R. Thorisson, and M. Looks (Springer), 41–51.\nSuri, R. E., and Schultz, W. (1999). A neural network with dopaminelike reinforcement signal that learns a spatial delayed response task.\n[Neuroscience 91, 871–890. doi: 10.1016/S0306-4522(98)00697-6](https://doi.org/10.1016/S0306-4522(98)00697-6)\n\n\n\nSusillo, D., and Abbott, L. F. (2009). Generating coherent patterns\nof activity from chaotic neural networks. Neuron 63, 544–557.\n[doi: 10.1016/j.neuron.2009.07.018](https://doi.org/10.1016/j.neuron.2009.07.018)\nSutton, R., and Barto, A. (1998). Reinforcement Learning. Cambridge: MIT Press.\nSutton, R., and Barto, A. (2018). Reinforcement Learning: an introduction (2nd\nEdn.) Cambridge, MA: MIT Press.\nSutton, R. S., and Barto, A. G. (1981). Towards a modern theory of\nadaptive networks: expectation and prediction. Psychol. Rev. 88, 135–171.\n[doi: 10.1037/0033-295X.88.2.135](https://doi.org/10.1037/0033-295X.88.2.135)\n\nTakeuchi, T., Duszkiewicz, A. J., Sonneborn, A., Spooner, P. A., Yamasaki,\nM., Watanabe, M., et al. (2016). Locus coeruleus and dopaminergic\nconsolidation of everyday memory. Nature 537, 357–362. doi: 10.1038/nature\n\n19325\n\nThorndike, E. (1911). Animal Intelligence. Darien, CT: Hafner.\nUrbanczik, R., and Senn, W. (2009). Reinforcement learning in populations of\n[spiking neurons. Nat. Neurosci. 12, 250–252. doi: 10.1038/nn.2264](https://doi.org/10.1038/nn.2264)\nUrbanczik, R., and Senn, W. (2014). Learning by the dendritic prediction of\n[somatic spiking. Neuron 81, 521–528. doi: 10.1016/j.neuron.2013.11.030](https://doi.org/10.1016/j.neuron.2013.11.030)\nvan Rossum, M. C. W., Bi, G. Q., and Turrigiano, G. G. (2000). Stable Hebbian\nlearning from spike timing-dependent plasticity. J. Neurosci. 20, 8812–8821.\n[doi: 10.1523/JNEUROSCI.20-23-08812.2000](https://doi.org/10.1523/JNEUROSCI.20-23-08812.2000)\nVasilaki, E., Frémaux, N., Urbanczik, R., Senn, W., and Gerstner, W.\n(2009). Spike-based reinforcement learning in continuous state and action\nspace: When policy gradient methods fail. PLoS Comput.Biol. 5:e1000586.\n[doi: 10.1371/journal.pcbi.1000586](https://doi.org/10.1371/journal.pcbi.1000586)\nWang, X.-J., and Kennedy, H. (2016). Brain structure and dynamics\nacross scales: in search of rules. Curr. Opin. Neurobiol. 37, 92–98.\n[doi: 10.1016/j.conb.2015.12.010](https://doi.org/10.1016/j.conb.2015.12.010)\nWilliams, R. (1992). Simple statistical gradient-following methods for\nconnectionist reinforcement learning. Mach. Learn. 8, 229–256.\n[doi: 10.1007/BF00992696](https://doi.org/10.1007/BF00992696)\n\nWillshaw, D. J., Bunemann, O. P., and Longuet-Higgins, H. C. (1969). Non[holographic associative memory. Nature 222, 960–962. doi: 10.1038/222960a0](https://doi.org/10.1038/222960a0)\nXie, X., and Seung, H. S. (2004). Learning in neural networks by reinforcement of\n[irregular spiking. Phys. Rev. E 69:41909. doi: 10.1103/PhysRevE.69.041909](https://doi.org/10.1103/PhysRevE.69.041909)\nYagishita, S., Hayashi-Takagi, A., Ellis-Davies, G. C., Urakubo, H., Ishii,\nS., and Kasai, H. (2014). A critical time window for dopamine actions\non the structural plasticity of dendritic spines. Science 345, 1616–1620.\n[doi: 10.1126/science.1255514](https://doi.org/10.1126/science.1255514)\n\nYu, A. J., and Dayan, P. (2005). Uncertainty, neuromodulation, and attention.\n[Neuron 46, 681–692. doi: 10.1016/j.neuron.2005.04.026](https://doi.org/10.1016/j.neuron.2005.04.026)\nZhang, J. C., Lau, P. M., and Bi,G. Q. (2009). Gain in sensitivity\nand loss in temporal contrast of STDP by dopaminergic modulation at\nhippocampal synapses. Proc. Natl. Acad. Sci. U.S.A. 106, 13028–13033.\n[doi: 10.1073/pnas.0900546106](https://doi.org/10.1073/pnas.0900546106)\nZhang, L. I., Tao, H. W., Holt, C. E., Harris, W. A., and Poo, M. (1998). A critical\nwindow for cooperation and competition among developing retinotectal\n[synapses. Nature 395, 37–44. doi: 10.1038/25665](https://doi.org/10.1038/25665)\nZiegler, L., Zenke, F., Kastner, D. B., and Gerstner, W. (2015). Synaptic\nconsolidation: from synapses to behavioral modeling. J. Neurosci. 35, 1319–\n[1334. doi: 10.1523/JNEUROSCI.3989-14.2015](https://doi.org/10.1523/JNEUROSCI.3989-14.2015)\n\n\n**Conflict of Interest Statement:** The authors declare that the research was\nconducted in the absence of any commercial or financial relationships that could\nbe construed as a potential conflict of interest.\n\n\nCopyright © 2018 Gerstner, Lehmann, Liakoni, Corneil and Brea. This is an open[access article distributed under the terms of the Creative Commons Attribution](http://creativecommons.org/licenses/by/4.0/)\n[License (CC BY). The use, distribution or reproduction in other forums is permitted,](http://creativecommons.org/licenses/by/4.0/)\nprovided the original author(s) and the copyright owner(s) are credited and that the\noriginal publication in this journal is cited, in accordance with accepted academic\npractice. No use, distribution or reproduction is permitted which does not comply\nwith these terms.\n\n\n\n[Frontiers in Neural Circuits | www.frontiersin.org](https://www.frontiersin.org/journals/neural-circuits) 16 [July 2018 | Volume 12 | Article 53](https://www.frontiersin.org/journals/neural-circuits#articles)\n\n\n",
    "ranking": {
      "relevance_score": 0.6667028428741371,
      "citation_score": 0.9,
      "recency_score": 0.25503151357713194,
      "final_score": 0.6721951413696091
    },
    "citation_key": "Gerstner2018EligibilityTA",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
    "title": "Q-learning",
    "published": "1992-05-01",
    "authors": [
      "C. Watkins",
      "P. Dayan"
    ],
    "summary": null,
    "pdf_url": "https://link.springer.com/content/pdf/10.1007/BF00992698.pdf",
    "doi": "10.1007/BF00992698",
    "fields_of_study": [
      "Computer Science"
    ],
    "venue": "Machine-mediated learning",
    "citation_count": 11746,
    "bibtex": "@Article{Watkins1992Qlearning,\n author = {C. Watkins and P. Dayan},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {279-292},\n title = {Q-learning},\n volume = {8},\n year = {1992}\n}\n",
    "markdown_text": "![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-0-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-1-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-2-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-3-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-4-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-5-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-6-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-7-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-8-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-9-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-10-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-11-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-12-full.png)\n\n\n![](output/images/03b7e51c52084ac1db5118342a00b5fbcfc587aa.pdf-13-full.png)\n",
    "ranking": {
      "relevance_score": 0.690268251239643,
      "citation_score": 0.9,
      "recency_score": 0.0029623508790907987,
      "final_score": 0.6634840109556591
    },
    "citation_key": "Watkins1992Qlearning",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  },
  {
    "id": "8c38397d18116c9985349355e3bcae8326e2cf56",
    "title": "Final Iteration Convergence Bound of Q-Learning: Switching System Approach",
    "published": "2022-05-11",
    "authors": [
      "Donghwan Lee"
    ],
    "summary": "Q-learning is known as one of the fundamental reinforcement learning (RL) algorithms. Its convergence has been the focus of extensive research over the past several decades. Recently, a new finite-time error bound and analysis for Q-learning was introduced using a switching system framework. This approach views the dynamics of Q-learning as a discrete-time stochastic switching system. The prior study established a finite-time error bound on the averaged iterates using Lyapunov functions, offering further insights into Q-learning. While valuable, the analysis focuses on error bounds of the averaged iterate, which comes with the inherent disadvantages: It necessitates extra averaging steps, which can decelerate the convergence rate. Moreover, the final iterate, being the original format of Q-learning, is more commonly used and is often regarded as a more intuitive and natural form in the majority of iterative algorithms. In this article, we present a finite-time error bound on the final iterate of Q-learning based on the switching system framework. The proposed error bounds have different features compared to the previous works, and cover different scenarios. Finally, we expect that the proposed results provide additional insights on Q-learning via connections with discrete-time switching systems, and can potentially present a new template for finite-time analysis of more general RL algorithms.",
    "pdf_url": "https://ieeexplore.ieee.org/ielx7/9/4601496/10402068.pdf",
    "doi": "10.1109/TAC.2024.3355326",
    "fields_of_study": [
      "Engineering",
      "Computer Science"
    ],
    "venue": "IEEE Transactions on Automatic Control",
    "citation_count": 7,
    "bibtex": "@Article{Lee2022FinalIC,\n author = {Donghwan Lee},\n booktitle = {IEEE Transactions on Automatic Control},\n journal = {IEEE Transactions on Automatic Control},\n pages = {4765-4772},\n title = {Final Iteration Convergence Bound of Q-Learning: Switching System Approach},\n volume = {69},\n year = {2022}\n}\n",
    "markdown_text": "This article has been accepted for publication in IEEE Transactions on Automatic Control. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/TAC.2024.3355326\n\n# Final Iteration Convergence Bound of Q-Learning: Switching System Approach\n\n\nDonghwan Lee\n\n\n\n1\n\n\n\n_**Abstract**_ **— Q-learning is known as one of the fundamental re-**\n**inforcement learning (RL) algorithms. Its convergence has been**\n**the focus of extensive research over the past several decades.**\n**Recently, a new finite-time error bound and analysis for Q-**\n**learning was introduced using a switching system framework.**\n**This approach views the dynamics of Q-learning as a discrete-**\n**time stochastic switching system. The prior study established a**\n**finite-time error bound on the averaged iterates using Lyapunov**\n**functions, offering further insights into Q-learning. While valuable,**\n**the analysis focuses on error bounds of the averaged iterate, which**\n**comes with the inherent disadvantages: it necessitates extra aver-**\n**aging steps, which can decelerate the convergence rate. Moreover,**\n**the final iterate, being the original format of Q-learning, is more**\n**commonly used and is often regarded as a more intuitive and**\n**natural form in the majority of iterative algorithms. In this paper, we**\n**present a finite-time error bound on the final iterate of Q-learning**\n**based on the switching system framework. The proposed error**\n**bounds have different features compared to the previous works,**\n**and cover different scenarios. Finally, we expect that the proposed**\n**results provide additional insights on Q-learning via connections**\n**with discrete-time switching systems, and can potentially present**\n**a new template for finite-time analysis of more general RL algo-**\n**rithms.**\n\n\n_**Index Terms**_ **— Reinforcement learning, Q-learning, switching**\n**system, convergence, finite-time analysis**\n\n\nI. INTRODUCTION\n\n\nReinforcement learning (RL) addresses the optimal sequential decision making problem for unknown systems through experiences [1].\nRecent successes of RL algorithms outperforming humans in several challenging tasks [2]–[9] have triggered a surge of interests\nin RL both theoretically and experimentally. Among many others,\nQ-learning [10] is one of the most fundamental and popular RL\nalgorithms, and its convergence has been extensively studied over\nthe past decades. Classical analysis mostly focuses on asymptotic\nconvergence [11]–[17]. While crucial, the asymptotic convergence\ncannot measure the speed at which iterations approach a solution.\nConsequently, the efficiency of the related algorithms cannot be\nprecisely assessed. For this reason, finite-time convergence analysis,\nwhich quantifies how fast the iterations progress toward the solution,\nhas gained increasing attention recently.\nRecently, advances have been made in finite-time convergence\nanalysis [18]–[28]. Most of the existing results treat the Q-learning\ndynamics as nonlinear stochastic approximations [29], and use the\ncontraction property of the Bellman equation. Recently, [28] proposed\na new perspective of Q-learning based on discrete-time switching\nsystem models [30], [31], and established a finite-time analysis based\non tools in control theory [32], [33]. The switching system perspective\n\n\nD. Lee is with the Department of Electrical Engineering, KAIST,\nDaejeon, 34141, South Korea donghwan@kaist.ac.kr.\n_∗_ The work was supported by the BK21 FOUR from the Ministry of\nEducation (Republic of Korea) (Corresponding author: Donghwan Lee).\nThis work was supported by Institute of Information communications\nTechnology Planning Evaluation (IITP) grant funded by the Korea government (MSIT)(No.2022-0-00469)\n\n\n\ncaptures unique features of Q-learning dynamics, and allows us to\nconvert the notion of finite-time convergence analysis into the stability\nanalysis of dynamic control systems. While valuable, the analysis\nin [28] focuses on error bounds of the _average iterate_, which comes\nwith the inherent disadvantages: it necessitates extra averaging steps,\nwhich can decelerate the convergence rate. Moreover, the final iterate,\nbeing the original format of Q-learning, is more commonly used and\nis often regarded as a more intuitive and natural form in the majority\nof iterative algorithms. Therefore, it is more interesting to study the\nconvergence and finite-time analysis of the final iterate.\nGiven these considerations, the main goal of this paper is to present\na finite-time error bound on the _final iterate_ of Q-learning based on\nthe switching system framework in [28] for additional insights and\ncomplementary analysis. In particular, we improve the analysis in [28]\nby replacing the average iterate with the final iterate and deriving the\nfollowing bound:\n\n\n\nwhere _O_ ˜ ignores the constant and polylogarithmic factors. We note that this _d_ max is the maximum state-action occupation frequency, and\nextension is not trivial, and significantly different approaches have\nbeen adopted in this paper.\nThe proposed analysis relies on propagations of the autocorrelation\nmatrix instead of the Lyapunov function analysis used in [28], which\nallows conceptually simpler analysis. It also provides additional\ninsights on Q-learning via connections with discrete-time switching\nsystems, and can potentially present a new template for finite-time\nanalysis of more general RL algorithms. Moreover, the new perspective can potentially stimulate synergy between control theory and\nRL, and open up opportunities to the design of new RL algorithms.\nThe proposed error bounds have different features compared to the\nprevious works, and cover different cases detailed throughout this\npaper. Finally, we note that this paper only covers an i.i.d. observation\nmodel with constant step-sizes for simplicity of the overall analysis.\nExtensions to more complicated scenarios are not the main purpose\nof this paper.\n_Related Works:_ Recently, some progresses have been made\nin finite-time analysis of Q-learning [18]–[28]. In particular, [18]\n\n\n\nE[�� _Qk −_ _Q∗_ �� _∞_ []] _[ ≤]_ [9] _[d]_ [max][3] [2] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n\n\n_ρ_ _[k]_\n1 _−_ _γ_\n\n\n\n\n[max] _[|][S × A][|][α]_ [1] _[/]_ [2] + [2] _[|][S × A][|]_ [3] _[/]_ [2]\n\n_d_ [3] _[/]_ [2] 1 _−_ _γ_\nmin [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n\n\n1\n\n+ [4] _[γ][d]_ [max] _[|][S × A][|]_ [2] _[/]_ [3]\n\n1 _−_ _γ_ _d_ min(1 _−_ _γ_ ) _[ρ][k/]_ [2] _[−]_ [1] _[,]_\n\n\n\n(1)\n\n\nwhere _|S×A|_ is the number of the state-action pairs, _γ_ is the discount\nfactor, _d_ min is the minimum state-action occupation frequency, _α ∈_\n(0 _,_ 1) is the constant step-size, _Q_ _[∗]_ is a vectorized optimal Q-function,\n_Qk_ is its estimation at the current time _k_, and _ρ_ := 1 _−_ _αd_ min(1 _−_\n_γ_ ) _∈_ (0 _,_ 1) is the exponential decay rate. Moreover, the sample\n\n\n\ncomplexity, _O_ [˜] � _ε_ [4] _dd_ [4] max ~~[6]~~ min _[|]_ [(1] _[S×A][−][γ]_ [)] _[|]_ [4][10]\n\n\n\n, in [28] for _ε_ -optimal solution can\n�\n\n\n\nbe improved to _O_ [˜] � _γε_ [2][2] _dd_ [2] max ~~[4]~~ min [(1] _[|][S×A][−][γ]_ [)] _[|]_ [6][2]\n\n\n\nfrom the proposed approach,\n�\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\n\n\nThis article has been accepted for publication in IEEE Transactions on Automatic Control. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/TAC.2024.3355326\n\n\n\n2\n\n\nprovided a finite-time convergence rate with state-action dependent\ndiminishing step-sizes. The authors in [19] analyzed a batch version\nof synchronous Q-learning, called phased Q-learning, with finitetime bounds. The authors of [20] developed convergence rates for\nboth synchronous and asynchronous Q-learning with polynomial\nand linear step-sizes. [21] proposed a variant of synchronous Qlearning called speedy Q-learning by adding a momentum term,\nand obtained an accelerated learning rate. A finite-time analysis of\nasynchronous Q-learning with constant step-sizes was considered\nin [22]. Afterwards, many advances have been made recently in\nfinite-time analysis. The paper [27] developed the so-called periodic\nQ-learning mimicking the stochastic gradient-based training scheme\nin [2] with periodic target updates. The paper [23] provided finitetime bounds for general synchronous stochastic approximation, and\napplied it to a synchronous Q-learning with state-independent diminishing step-sizes. In [24], a finite-time convergence rate of general\nasynchronous stochastic approximation scheme was derived, and\nit was applied to asynchronous Q-learning with diminishing stepsizes. Subsequently, [25] obtained sharper bounds under constant\nstep-sizes, [26] provided a Lyapunov method-based analysis for\ngeneral stochastic approximations and Q-learning with both constant\nand diminishing step-sizes, and [28] proposed a switching system\nperspective of Q-learning, and established a finite-time analysis.\n\n\nII. PRELIMINARIES\n\n_A. Notation_\n\n\nThe adopted notation is as follows: R: set of real numbers; R _[n]_ : _n_  dimensional Euclidean space; R _[n][×][m]_ : set of all _n × m_ real matrices;\n_A_ _[T]_ : transpose of matrix _A_ ; _A ≻_ 0 ( _A ≺_ 0, _A ⪰_ 0, and _A ⪯_ 0,\nrespectively): symmetric positive definite (negative definite, positive\nsemi-definite, and negative semi-definite, respectively) matrix _A_ ; _I_ :\nidentity matrix with appropriate dimensions; _λ_ min( _A_ ) and _λ_ max( _A_ )\nfor any symmetric matrix _A_ : the minimum and maximum eigenvalues\nof _A_ ; _|S|_ : cardinality of a finite set _S_ ; tr( _A_ ): trace of any matrix _A_ ;\n_A ⊗_ _B_ : Kronecker product of matrices _A_ and _B_ .\n\n\n_B. Markov decision problem_\n\n\nWe consider the infinite-horizon discounted Markov decision problem (MDP) and Markov decision process, where the agent sequentially takes actions to maximize cumulative discounted rewards. In a\nMarkov decision process with the state-space _S_ := _{_ 1 _,_ 2 _, . . ., |S|}_\nand action-space _A_ := _{_ 1 _,_ 2 _, . . ., |A|}_, the decision maker selects\nan action _a ∈A_ at the current state _s_, then the state transits\nto the next state _s_ _[′]_ with probability _P_ ( _s, a, s_ _[′]_ ), and the transition\nincurs a reward _r_ ( _s, a, s_ _[′]_ ), where _P_ ( _s, a, s_ _[′]_ ) is the state transition\nprobability from the current state _s ∈S_ to the next state _s_ _[′]_ _∈S_\nunder action _a ∈A_, and _r_ ( _s, a, s_ _[′]_ ) is the reward function. For\nconvenience, we consider a deterministic reward function and simply\nwrite _r_ ( _sk, ak, sk_ +1) =: _rk, k ∈{_ 0 _,_ 1 _, . . .}_ .\nA deterministic policy, _π_ : _S →A_, maps a state _s ∈S_ to an action\n_π_ ( _s_ ) _∈A_ . The objective of the Markov decision problem (MDP) is\nto find a deterministic optimal policy, _π_ _[∗]_, such that the cumulative\ndiscounted rewards over infinite time horizons is maximized, i.e.,\n\n\n\n_π_\n�����\n\n\n\n�\n\n\n\n_π_ _[∗]_ := arg max _π∈_ Θ E\n\n\n\n_∞_\n� _γ_ _[k]_ _rk_\n� _k_ =0\n\n\n\n_∞_\n�\n� _k_ =0\n\n\n\n_,_\n\n\n\nwhere _γ ∈_ [0 _,_ 1) is the discount factor, Θ is the set of all deterministic\npolicies, ( _s_ 0 _, a_ 0 _, s_ 1 _, a_ 1 _, . . ._ ) is a state-action trajectory generated\nby the Markov chain under policy _π_, and E[ _·|π_ ] is an expectation\nconditioned on the policy _π_ . Q-function under policy _π_ is defined as\n\n\n\n**Algorithm 1** Q-Learning with a constant step-size\n\n1: Initialize _Q_ 0 _∈_ R _[|S||A|]_ randomly such that _∥Q_ 0 _∥∞_ _≤_ 1.\n2: Sample _s_ 0 _∼_ _p_\n3: **for** iteration _k_ = 0 _,_ 1 _, . . ._ **do**\n4: Sample _ak ∼_ _β_ ( _·|sk_ ) and _sk ∼_ _p_ ( _·_ )\n5: Sample _s_ _[′]_ _k_ _[∼]_ _[P]_ [(] _[s][k][, a][k][,][ ·]_ [)][ and] _[ r][k]_ [=] _[ r]_ [(] _[s][k][, a][k][, s][′]_ _k_ [)]\n6: Update _Qk_ +1( _sk, ak_ ) = _Qk_ ( _sk, ak_ ) + _α{rk_ +\n_γ_ max _u∈A Qk_ ( _s_ _[′]_ _k_ _[, u]_ [)] _[ −]_ _[Q][k]_ [(] _[s][k][, a][k]_ [)] _[}]_\n7: **end for**\n\n\nand the optimal Q-function is defined as _Q_ _[∗]_ ( _s, a_ ) = _Q_ _[π][∗]_ ( _s, a_ ) for\nall ( _s, a_ ) _∈S × A_ . Once _Q_ _[∗]_ is known, then an optimal policy can\nbe retrieved by the greedy policy _π_ _[∗]_ ( _s_ ) = arg max _a∈A Q_ _[∗]_ ( _s, a_ ).\nThroughout, we assume that the MDP is ergodic so that the stationary\nstate distribution exists and the Markov decision problem is well\nposed.\n\n\n_C. Switching system_\n\n\nSince a switching system [30], [31] is a special form of nonlinear\nsystems [33], we first consider the nonlinear system\n\n\n_xk_ +1 = _f_ ( _xk_ ) _,_ _x_ 0 = _z ∈_ R _[n]_ _,_ _k ∈{_ 0 _,_ 1 _, . . .},_ (2)\n\n\nwhere _xk ∈_ R _[n]_ is the state and _f_ : R _[n]_ _→_ R _[n]_ is a nonlinear\nmapping. An important concept in dealing with the nonlinear system\nis the equilibrium point. A point _x_ = _x_ _[∗]_ in the state-space is said to\nbe an equilibrium point of (2) if it has the property that whenever the\nstate of the system starts at _x_ _[∗]_, it will remain at _x_ _[∗]_ [33]. For (2), the\nequilibrium points are the real solutions of the equation _f_ ( _x_ ) = _x_ .\nThe equilibrium point _x_ _[∗]_ is said to be globally asymptotically stable\nif for any initial state _x_ 0 _∈_ R _[n]_, _xk →_ _x_ _[∗]_ as _k →∞_ .\nNext, let us consider the particular system, called the _linear_\n_switching system_,\n\n\n_xk_ +1 = _Aσk_ _xk,_ _x_ 0 = _z ∈_ R _[n]_ _,_ _k ∈{_ 0 _,_ 1 _, . . .},_ (3)\n\n\nwhere _xk ∈_ R _[n]_ is the state, _σ ∈M_ := _{_ 1 _,_ 2 _, . . ., M_ _}_ is called\nthe mode, _σk ∈M_ is called the switching signal, and _{Aσ, σ ∈_\n_M}_ are called the subsystem matrices. The switching signal can be\neither arbitrary or controlled by the user under a certain switching\npolicy. Especially, a state-feedback switching policy is denoted by\n_σk_ = _σ_ ( _xk_ ). A more general class of systems is the _affine switching_\n\n_system_\n\n\n_xk_ +1 = _Aσk_ _xk_ + _bσk_ _,_ _x_ 0 = _z ∈_ R _[n]_ _,_ _k ∈{_ 0 _,_ 1 _, . . .},_ (4)\n\n\nwhere _bσk ∈_ R _[n]_ is the additional input vector, which also switches\naccording to _σk_ . Due to the additional input _bσk_, its stabilization\nbecomes much more challenging.\n\n\n_D. Assumptions and definitions_\n\n\nIn this paper, we focus on the standard Q-learning in Algorithm 1\nwith a constant step-size _α ∈_ (0 _,_ 1) under the following setting:\n_{_ ( _sk, ak, s_ _[′]_ _k_ [)] _[}][∞]_ _k_ =0 [are i.i.d. samples under the behavior policy] _[ β]_ [,]\nwhere the time-invariant behavior policy is the policy by which\nthe RL agent actually behaves to collect experiences. Note that the\nnotation _s_ _[′]_ _k_ [implies the next state sampled at the time step] _[ k]_ [, which]\nis used instead of _sk_ +1 in order to distinguish _s_ _[′]_ _k_ [from] _[ s][k]_ [+1][. In this]\npaper, the notation _sk_ +1 indicate the current state at the iteration\nstep _k_ + 1, while it does not depend on _sk_ . For simplicity, we\nassume that the state at each time is sampled from the stationary\nstate distribution _p_, and in this case, the state-action distribution at\neach time is identically given by\n\n\n_d_ ( _s, a_ ) = _p_ ( _s_ ) _β_ ( _a|s_ ) _,_ ( _s, a_ ) _∈S × A._\n\n\n\n_s_ 0 = _s, a_ 0 = _a, π_\n�����\n\n\n\n_,_ ( _s, a_ ) _∈S × A,_\n\n�\n\n\n\n_Q_ _[π]_ ( _s, a_ ) = E\n\n\n\n_∞_\n� _γ_ _[k]_ _rk_\n� _k_ =0\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\n\n\nThis article has been accepted for publication in IEEE Transactions on Automatic Control. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/TAC.2024.3355326\n\n\n\n_Remark 1:_ In this paper, we assume that the behavior policy _β_\nis time-invariant, and this scenario excludes the common method of\nusing the _ε_ -greedy behavior policy with _ε >_ 0 because the _ε_ -greedy\nbehavior policy depends on the current Q-iterate, and hence is timevarying. Moreover, the proposed analysis cannot be easily extended\nto the analysis of Q-learning with the _ε_ -greedy behavior policy due\nto reasons that will appear later in this paper.\nThroughout, we make the following assumptions for convenience.\n_Assumption 1: d_ ( _s, a_ ) _>_ 0 holds for all ( _s, a_ ) _∈S × A_ .\n_Assumption 2:_ The step-size is a constant _α ∈_ (0 _,_ 1).\n_Assumption 3:_ The reward is bounded as follows:\n\n( _s,a,s_ _[′]_ max) _∈S×A×S_ _[|][r]_ [(] _[s, a, s][′]_ [)] _[|]_ [ =:] _[ R]_ [max] _[ ≤]_ [1] _[.]_\n\n_Assumption 4:_ The initial iterate _Q_ 0 satisfies _∥Q_ 0 _∥∞_ _≤_ 1.\n_Remark 2:_ All these assumptions will be used throughout this\npaper for convergence proof. Assumption 1 guarantees that every\nstate-action pair is visited infinitely often for sufficient exploration.\nThis assumption is used when the state-action occupation frequency\nis given. It has been also considered in [25] and [26]. The work\nin [22] considers another exploration condition, called the cover\ntime condition, which states that there is a certain time period,\nwithin which every state-action pair is expected to be visited at least\nonce. Slightly different cover time conditions have been used in [20]\nand [25] for convergence rate analysis. The unit bounds imposed on\n_R_ max and _Q_ 0 are just for simplicity of analysis. The constant stepsize in Assumption 2 has been also studied in [22] and [26] using\ndifferent approaches.\nThe following quantities will be frequently used in this paper;\nhence, we define the corresponding notations for convenience.\n\n_Definition 1:_ 1) Maximum state-action occupation frequency:\n\n\n_d_ max := max\n( _s,a_ ) _∈S×A_ _[d]_ [(] _[s, a]_ [)] _[ ∈]_ [(0] _[,]_ [ 1)] _[.]_\n\n\n2) Minimum state-action occupation frequency:\n\n\n_d_ min := ( _s,a_ min) _∈S×A_ _[d]_ [(] _[s, a]_ [)] _[ ∈]_ [(0] _[,]_ [ 1)] _[.]_\n\n\n3) Exponential decay rate:\n\n\n_ρ_ := 1 _−_ _αd_ min(1 _−_ _γ_ ) _._ (5)\n\n\nUnder Assumption 2, the decay rate satisfies _ρ ∈_ (0 _,_ 1).\nThroughout the paper, we will use the following matrix notations\nfor compact dynamical system representations:\n\n\n\n3\n\n\nrespectively. Note also that under Assumption 1, _D_ is a nonsingular\ndiagonal matrix with strictly positive diagonal elements.\nFor any stochastic policy, _π_ : _S →_ ∆ _|A|_, where ∆ _|A|_ is the set\nof all probability distributions over _A_, we define the corresponding\naction transition matrix as\n\n\n\n_π_ (1) _[T]_ _⊗_ _e_ _[T]_ 1\n_π_ (2) _[T]_ _⊗_ _e_ _[T]_ 2\n\n...\n_π_ ( _|S|_ ) _[T]_ _⊗_ _e_ _[T]_ _|S|_\n\n\n\n\n\n_∈_ R _[|S|×|S×A|]_ _,_ (6)\n\n\n\n\nΠ _[π]_ :=\n\n\n\n\n\n\n\n\n\n_P_ 1\n\n...\n\n_P_\n_|A|_\n\n\n\n\n\n\n\n\n\n_R_ 1\n\n...\n\n_R_\n_|A|_\n\n\n\n\n\n\n\n\n\n\n\n\n\n_Q_ ( _·,_ 1)\n\n...\n_Q_ ( _·, |A|_ )\n\n\n\n\n\n\n\n_,_\n\n\n\n\n\n\n _, R_ :=\n\n\n\n\n\n _, Q_ :=\n\n\n\n_P_ :=\n\n\n_Da_ :=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_d_ (1 _, a_ )\n\n...\n\n_d_ ( _|S|, a_ )\n\n\n\n\n\n\n\n _, D_ :=\n\n\n\n_D_ 1\n\n...\n\n_D_\n_|A|_\n\n\n\n\n\n\n_,_\n\n\n\n\n\n\n\n\n\n\nwhere _Pa_ = _P_ ( _·, a, ·_ ) _∈_ R _[|S|×|S|]_, _Q_ ( _·, a_ ) _∈_ R _[|S|]_ _, a ∈A_\nand _Ra_ ( _s_ ) := E[ _r_ ( _s, a, s_ _[′]_ ) _|s, a_ ]. Note that _P_ _∈_ R _[|S×A|×|S|]_,\n_R ∈_ R _[|S×A|]_, _Q ∈_ R _[|S×A|]_, and _D ∈_ R _[|S×A|×|S×A|]_ . In this\nnotation, Q-function is encoded as a single vector _Q ∈_ R _[|S×A|]_,\nwhich enumerates _Q_ ( _s, a_ ) for all _s ∈S_ and _a ∈A_ . In particular,\nthe single value _Q_ ( _s, a_ ) can be written as\n\n\n_Q_ ( _s, a_ ) = ( _ea ⊗_ _es_ ) _[T]_ _Q,_\n\n\nwhere _es ∈_ R _[|S|]_ and _ea ∈_ R _[|A|]_ are _s_ -th basis vector (all components\nare 0 except for the _s_ -th component which is 1) and _a_ -th basis vector,\n\n\n\nwhere _es_ _∈_ R _[|S|]_ . Then, it is well known that _P_ Π _[π]_ _∈_\nR _[|S×A|×|S×A|]_ is the transition probability matrix of the state-action\npair under policy _π_ . If we consider a deterministic policy, _π_ : _S →A_,\nthe stochastic policy can be replaced with the corresponding one-hot\nencoding vector _⃗π_ ( _s_ ) := _eπ_ ( _s_ ) _∈_ ∆ _|A|,_ where _ea ∈_ R _[|A|]_, and\nthe corresponding action transition matrix is identical to (6) with _π_\nreplaced with _⃗π_ . For any given _Q ∈_ R _[|S×A|]_, denote the greedy\npolicy w.r.t. _Q_ as _πQ_ ( _s_ ) := arg max _a∈A Q_ ( _s, a_ ) _∈A_ . We will use\nthe following shorthand frequently: Π _Q_ := Π _[π][Q]_ _._\nThe boundedness of Q-learning iterates [34] plays an important\nrole in our analysis.\n\n_Lemma 1 (Boundedness of Q-learning iterates [34]):_ If the stepsize is less than one, then for all _k ≥_ 0,\n\n\nmax _{R_ max _,_ max( _s,a_ ) _∈S×A Q_ 0( _s, a_ ) _}_\n_∥Qk∥∞_ _≤_ _Q_ max := _._\n\n1 _−_ _γ_\n\n\nFrom Assumption 3 and Assumption 4, we can easily see that\n1\n_Q_ max _≤_ 1 _−γ_ [.]\n\n\nIII. FINITE-TIME ANALYSIS OF Q-LEARNING FROM\n\nSWITCHING SYSTEM THEORY\n\n\nIn this section, we study a discrete-time switching system model of\nQ-learning in Algorithm 1, and establish its finite-time convergence\nbound based on the stability analysis of switching system.\n\n\n_A. Q-learning as a stochastic affine switching system_\n\n\nUsing the notation introduced, the update in Algorithm 1 can be\nrewritten as\n\n\n_Qk_ +1 = _Qk_ + _α{DR_ + _γDP_ Π _Qk_ _Qk −_ _DQk_ + _wk},_ (7)\n\n\nwhere\n\n\n_wk_ =( _eak ⊗_ _esk_ ) _rk_ + _γ_ ( _eak ⊗_ _esk_ )( _es′_\n_k_ [)] _[T]_ [ Π] _[Q][k]_ _[Q][k]_\n\n_−_ ( _eak ⊗_ _esk_ )( _eak ⊗_ _esk_ ) _[T]_ _Qk_\n_−_ ( _DR_ + _γDP_ Π _Qk_ _Qk −_ _DQk_ )\n\n=( _ea ⊗_ _es_ ) _δk −_ ( _DR_ + _γDP_ Π _Qk_ _Qk −_ _DQk_ ) _,_ (8)\n\n\nand\n\n\n_δk_ := _rk_ + _γ_ ( _es′_ ) _[T]_ Π _Qk_ _Qk −_ ( _ea ⊗_ _es_ ) _[T]_ _Qk_ (9)\n\n\nis the so-called temporal-difference (TD) error [35], and\n( _sk, ak, rk, s_ _[′]_ _k_ [)][ is the sample in the] _[ k]_ [-th time-step. Note that]\nby definition, the noise term has a zero mean conditioned on _Qk_,\ni.e., E[ _wk|Qk_ ] = 0. Recall the definitions _πQ_ ( _s_ ) and Π _Q_ . Invoking\nthe optimal Bellman equation ( _γDP_ Π _Q∗_ _−_ _D_ ) _Q_ _[∗]_ + _DR_ = 0, (7)\ncan be further rewritten by\n\n\n( _Qk_ +1 _−_ _Q_ _[∗]_ ) = _{I_ + _α_ ( _γDP_ Π _Qk −_ _D_ ) _}_ ( _Qk −_ _Q_ _[∗]_ )\n\n+ _γDP_ (Π _Qk −_ Π _Q∗_ ) _Q_ _[∗]_ + _αwk._ (10)\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\n\n\nThis article has been accepted for publication in IEEE Transactions on Automatic Control. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/TAC.2024.3355326\n\n\n\n4\n\n\nwhich is a linear switching system with an extra affine term,\n_γDP_ (Π _Qk −_ Π _Q∗_ ) _Q_ _[∗]_, and a stochastic noise vector, _wk_ . For any\n_Q ∈_ R _[|S×A|]_, define\n\n\n_AQ_ := _I_ + _α_ ( _γDP_ Π _Q −_ _D_ ) _,_ _bQ_ := _γDP_ (Π _Q −_ Π _Q∗_ ) _Q_ _[∗]_ _._\n\n\nUsing the notation, the Q-learning iteration can be concisely represented as the _stochastic affine switching system_\n\n\n_Qk_ +1 _−_ _Q_ _[∗]_ = _AQk_ ( _Qk −_ _Q_ _[∗]_ ) + _bQk_ + _αwk,_ (11)\n\nwhere _AQk ∈_ R _[|S×A|×|S×A|]_ and _bQk ∈_ R _[|S×A|]_ switch among\nmatrices from _{I_ + _α_ ( _γDP_ Π _[π]_ _−_ _D_ ) : _π ∈_ Θ _}_ and vectors from\n_{γDP_ (Π _[π]_ _−_ Π _[π][∗]_ ) _Q_ _[∗]_ : _π ∈_ Θ _}_ . In particular, let us define a oneto-one mapping _ϕ_ : Θ _→{_ 1 _,_ 2 _, . . ., |_ Θ _|}_ from a deterministic policy\n_π ∈_ Θ to an integer in _{_ 1 _,_ 2 _, . . ., |_ Θ _|}_, and define\n\n_Ai_ = _I_ + _α_ ( _γDP_ Π _[π]_ _−_ _D_ ) _∈_ R _[|S×A|×|S×A|]_ _,_ (12)\n\n_bi_ = _γDP_ (Π _[π]_ _−_ Π _[π][∗]_ ) _Q_ _[∗]_ _∈_ R _[|S×A|]_ _,_\n\n\nfor all _i_ = _ϕ_ ( _π_ ) and _π ∈_ Θ. Then, (11) can be written by\nthe affine switching system (4) with the switching signal _σk ∈_\n_{_ 1 _,_ 2 _, . . ., |_ Θ _|}_ at time _k ≥_ 0 determined by _σk_ = _ϕ_ ( _πk_ ) with\n_πk_ ( _·_ ) := arg min _a∈AQk_ ( _·, a_ ) _∈_ Θ.\nConsequently, the convergence of Q-learning is now reduced to\nanalyzing the stability of the above switching system. A main obstacle\nin proving the stability arises from the presence of the affine and\nstochastic terms. Without these terms, we can easily establish the\nexponential stability of the corresponding deterministic switching\nsystem, under arbitrary switching policy. Specifically, we have the\nfollowing result.\n\n_Proposition 1 ( [28]):_ For arbitrary _Hk ∈_ R _[|S×A|]_ _, k ≥_ 0, the\nlinear switching system\n\n_Qk_ +1 _−_ _Q_ _[∗]_ = _AHk_ ( _Qk −_ _Q_ _[∗]_ ) _,_ _Q_ 0 _−_ _Q_ _[∗]_ _∈_ R _[|S||A|]_ _,_\n\n\nis exponentially stable with _∥Qk_ _−Q_ _[∗]_ _∥∞_ _≤_ _ρ_ _[k]_ _∥Q_ 0 _−Q_ _[∗]_ _∥∞, k ≥_ 0,\nwhere _ρ_ is defined in (5).\nThe above result follows immediately from the key fact that\n_∥AQ∥∞_ _≤_ _ρ_, which we formally state in the lemma below.\n\n_Lemma 2 ( [28]):_ For any _Q ∈_ R _[|S×A|]_, _∥AQ∥∞_ _≤_ _ρ_, where\n_∥A∥∞_ := max1 _≤i≤m_ � _nj_ =1 _[|][A][ij]_ _[|]_ [ and] _[ A][ij]_ [ is the element of] _[ A]_ [ in]\n_i_ -th row and _j_ -th column.\nHowever, because of the additional affine term and stochastic\nnoises in the original switching system (11), it is not obvious how\nto directly derive its finite-time convergence bound. To circumvent\nthe difficulty with the affine term, we will resort to two simpler\ncomparison systems, whose trajectories upper and lower bound that of\nthe original system, and can be more easily analyzed. These systems\nwill be called the upper and lower comparison systems, which\ncapture important behaviors of Q-learning. The upper comparison\nsystem, denoted by _Q_ _[U]_ _k_ [, upper bounds Q-learning iterate] _[ Q][k]_ [, while]\nthe lower comparison system, denoted by _Q_ _[L]_ _k_ [, lower bounds] _[ Q][k]_ [.]\nThe construction of these comparison systems is partly inspired\nby [16] and exploits the special structure of the Q-learning algorithm.\nUnlike [16], here we focus on the discrete-time domain and a finitetime analysis. To address the difficulty with the stochastic noise, we\nintroduce a two-phase analysis: the first phase captures the noise\neffect of the lower comparison system, while the second phase\ncaptures the difference between the two comparison systems when\nnoise effect vanishes.\n\n\n_Remark 3:_ When an _ε_ -greedy strategy is utilized for the behavior\npolicy, it results in the behavior policy that is time-varying and\ndepends on the current Q-iterate, _Qk_ . This implies that the matrix\n_D_ in (10) becomes a time-varying matrix depending on _Qk_, introducing further nonlinearity and probabilistic dependencies within the\n\n\n\nswitching system dynamics in (10). Consequently, it is not feasible to\nstraightforwardly extend the proposed analysis to Q-learning with the\n_ε_ -greedy behavior policy. A more extensive analysis is necessary for\nthis extension, which is left as a subject for future research endeavors.\nBefore closing this section, we present the following result which\nwill be useful throughout the paper.\n\n_Lemma 3:_ For any _Q ∈_ R _[|S×A|]_, _AQ_ is a nonnegative matrix (all\nentries are nonnegative).\n_Proof:_ Recalling the definition _AQ_ := _I_ + _α_ ( _γDP_ Π _Q −_\n_D_ ), one can easily see that for any _i, j ∈{_ 1 _,_ 2 _, . . ., |S × A|}_,\nwe have [ _AQ_ ] _ij_ = [ _I −_ _αD_ + _αγDP_ Π _Q_ ] _ij_ = [ _I −_ _αD_ ] _ij_ +\n_αγ_ [ _DP_ Π _Q_ ] _ij ≥_ 0, where [ _·_ ] _ij_ denotes the element of a matrix [ _·_ ]\nin the _i_ th row and _j_ th column, and the inequality follows from the\nfact that both _I −_ _αD_ and _DP_ Π _Q_ are nonnegative matrices. This\ncompletes the proof.\n\n\n_B. Lower comparison system_\n\n\nLet us consider the stochastic linear system [28]\n\n_Q_ _[L]_ _k_ +1 _[−]_ _[Q][∗]_ [=] _[ A]_ _Q_ _[∗]_ [(] _[Q]_ _k_ _[L]_ _[−]_ _[Q][∗]_ [) +] _[ αw]_ _k_ _[,]_ _Q_ _[L]_ 0 _[−]_ _[Q][∗]_ _[∈]_ [R] _[|S×A|][,]_\n(13)\n\n\nwhere the stochastic noise _wk_ is the same as the original system (10).\nWe call it the _lower comparison system_ .\n\n_Proposition 2 ( [28]):_ Suppose _Q_ _[L]_ 0 _[−]_ _[Q][∗]_ _[≤]_ _[Q]_ [0] _[−]_ _[Q][∗]_ [, where] _[ ≤]_\nis used as the element-wise inequality. Then, _Q_ _[L]_ _k_ _[−]_ _[Q][∗]_ _[≤]_ _[Q][k]_ _[−]_ _[Q][∗]_\n\nfor all _k ≥_ 0.\n_Proof:_ The proof is done by an induction argument. Suppose\nthe result holds for some _k ≥_ 0. Then, we have\n\n\n( _Qk_ +1 _−_ _Q_ _[∗]_ ) = _AQ∗_ ( _Qk −_ _Q_ _[∗]_ ) + ( _AQk −_ _AQ∗_ )( _Qk −_ _Q_ _[∗]_ )\n\n+ _bQk_ + _αwk_\n= _AQ∗_ ( _Qk −_ _Q_ _[∗]_ ) + _αγDP_ (Π _Qk −_ Π _Q∗_ ) _Qk_\n+ _αwk ≥_ _AQ∗_ ( _Qk −_ _Q_ _[∗]_ ) + _αwk_\n_≥AQ∗_ ( _Q_ _[L]_ _k_ _[−]_ _[Q][∗]_ [) +] _[ αw]_ _k_\n= _Q_ _[L]_ _k_ +1 _[−]_ _[Q][∗][,]_\n\n\nwhere the first inequality is due to _DP_ (Π _Qk −_ Π _Q∗_ ) _Qk_ _≥_\n_DP_ (Π _Q∗_ _−_ Π _Q∗_ ) _Qk_ = 0 and the second inequality is due to\nthe hypothesis _Q_ _[L]_ _k_ _[−]_ _[Q][∗]_ _[≤]_ _[Q][k]_ _[−]_ _[Q][∗]_ [and the fact that] _[ A][Q][∗]_ [is]\na nonnegative matrix (all elements are nonnegative). The proof is\ncompleted by induction.\n\nDefining _xk_ := _Q_ _[L]_ _k_ _[−]_ _[Q][∗]_ [and] _[ A]_ [ :=] _[ A][Q][∗]_ [, (13) can be concisely]\nrepresented as the _stochastic linear system_\n\n\n_xk_ +1 = _Axk_ + _αwk,_ _x_ 0 _∈_ R _[n]_ _,_ _∀k ≥_ 0 _,_ (14)\n\n\nwhere _n_ := _|S × A|_, and _wk ∈_ R _[n]_ is a stochastic noise. The noise\n_wk_ has the zero mean, and is bounded. It is formally proved in the\nfollowing lemma.\n\n_Lemma 4:_ We have\n\n1) E[ _wk_ ] = 0;\n2) E[ _∥wk∥∞_ ] _≤_ ~~_[√]_~~ _W_ max;\n3) E[ _∥wk∥_ 2] _≤_ ~~_[√]_~~ _W_ max;\n4) E[ _wk_ _[T]_ _[w][k]_ []] _[ ≤]_ _−_ 9 [=:] _[ W]_ [max][.]\n(1 _γ_ ) ~~[2]~~\n\nfor all _k ≥_ 0.\n_Proof:_ For the first statement, we take the conditional expectation on (8) to have E[ _wk|xk_ ] = 0. Taking the total expectation\nagain with the law of total expectation leads to the first conclusion.\nMoreover, the conditional expectation, E[ _wk_ _[T]_ _[w][k][|][Q][k]_ []][, is bounded as]\n\n\nE[ _wk_ _[T]_ _[w]_ _k_ _[|][Q]_ _k_ []]\n\n=E[ _∥wk∥_ [2] 2 _[|][Q][k]_ []]\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\n\n\nThis article has been accepted for publication in IEEE Transactions on Automatic Control. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/TAC.2024.3355326\n\n\n\n5\n\n\n\n2\n=E[��( _eak ⊗_ _esk_ ) _δk −_ ( _DR_ + _γDP_ Π _Qk_ _Qk −_ _DQk_ )��2 _[|][Q][k]_ []]\n\n\n2\n=E[ _δk_ [2] _[|][Q]_ _k_ []] _[ −]_ �� _DR_ + _γDP_ Π _Qk_ _Qk −_ _DQk_ ��2\n_≤_ E[ _δk_ [2] _[|][Q]_ _k_ []]\n\n=E[ _rk_ [2] _[|][Q]_ _k_ [] +][ E][[2] _[r]_ _k_ _[γ]_ [(] _[e]_ _s_ _[′]_ [)] _[T]_ [ Π] _Qk_ _[Q]_ _k_ _[|][Q]_ _k_ []]\n\n+ E[ _−_ 2 _rk_ ( _eak ⊗_ _esk_ ) _[T]_ _Qk |Qk_ ]\n\n+ E[ _−_ 2 _γ_ ( _es′k_ [)] _[T]_ [ Π] _[Q][k]_ _[Q][k]_ [(] _[e][a][k][ ⊗]_ _[e][s][k]_ [)] _[T][ Q][k][ |][Q][k]_ []]\n\n+ E[ _γ_ ( _es′k_ [)] _[T]_ [ Π] _[Q][k]_ _[Q][k][γ]_ [(] _[e][s][′]_ _k_ [)] _[T]_ [ Π] _[Q][k]_ _[Q][k][ |][Q][k]_ []]\n\n+ E[( _eak ⊗_ _esk_ ) _[T]_ _Qk_ ( _eak ⊗_ _esk_ ) _[T]_ _Qk |Qk_ ]\n\n_≤_ 1 + 2 _γ_ E[ _|rk| × |_ ( _es′k_ [)] _[T]_ [ Π] _[Q][k]_ _[Q][k][| |][Q][k]_ []]\n\n+ 2E[ _|rk| × |_ ( _eak ⊗_ _esk_ ) _[T]_ _Qk| |Qk_ ]\n\n+ 2 _γ_ E[ _|_ ( _es′k_ [)] _[T]_ [ Π] _[Q][k]_ _[Q][k][| × |]_ [(] _[e][a][k][ ⊗]_ _[e][s][k]_ [)] _[T][ Q][k][| |][Q][k]_ []]\n\n+ _γ_ [2] E[ _|_ ( _es′k_ [)] _[T]_ [ Π] _[Q][k]_ _[Q][k][| × |]_ [(] _[e][s][′]_ _k_ [)] _[T]_ [ Π] _[Q][k]_ _[Q][k][| |][Q][k]_ []]\n\n+ E[ _|_ ( _eak ⊗_ _esk_ ) _[T]_ _Qk| × |_ ( _eak ⊗_ _esk_ ) _[T]_ _Qk| |Qk_ ]\n\n9\n_≤_ (1 _−_ _γ_ ) [2] [=:] _[ W]_ [max] _[,]_\n\n\nwhere _δk_ is defined in (9), and the last inequality comes from\nAssumptions 3-4, and Lemma 1. Taking the total expectation, we\nhave the fourth result. Next, taking the square root on both sides\nof E[ _∥wk∥_ [2] 2 []] _[ ≤]_ _[W]_ [max][, one gets][ E][[] _[∥][w][k][∥]_ _∞_ []] _[ ≤]_ [E][[] _[∥][w][k][∥]_ 2 []] _[ ≤]_\n~~�~~ E[ _∥wk∥_ [2] 2 []] _[ ≤]_ ~~_[√]_~~ _[W]_ [max][, where the first inequality comes from]\n\n_∥·∥∞_ _≤∥·∥_ 2. This completes the proof.\n\nTo proceed further, let us define the covariance of the noise\n\n\nE[ _wkwk_ _[T]_ [] =:] _[ W]_ _k_ [=] _[ W]_ _k_ _[ T]_ _[⪰]_ [0] _[.]_\n\n\nAn important quantity we use in the main result is the maximum\neigenvalue, _λ_ max( _Wk_ ), whose bound can be easily established as\nfollows.\n\n\n_Lemma 5:_ The maximum eigenvalue of _Wk_ is bounded as\n\n\n_λ_ max( _Wk_ ) _≤_ _W_ max\n\n\nfor all _k ≥_ 0, where _W_ max _>_ 0 is given in Lemma 4.\n_Proof:_ The proof is completed by noting _λ_ max( _Wk_ ) _≤_\ntr( _Wk_ ) = tr(E[ _wkwk_ _[T]_ []) =][ E][[tr(] _[w][k][w]_ _k_ _[T]_ [)] =][ E][[] _[w]_ _k_ _[T]_ _[w][k]_ []] _[ ≤]_ _[W]_ [max][,]\nwhere the last inequality comes from Lemma 4, and the second\nequality uses the fact that the trace is a linear function. This completes\nthe proof.\n\nAs a next step, we investigate how the autocorrelation matrix,\nE[ _xkx_ _[T]_ _k_ []][, propagates over the time. In particular, the autocorrelation]\nmatrix is updated through the linear recursion\n\n\nE[ _xk_ +1 _x_ _[T]_ _k_ +1 [] =] _[ A]_ [E][[] _[x]_ _k_ _[x][T]_ _k_ []] _[A][T]_ [ +] _[ α]_ [2] _[W]_ _k_ _[,]_\n\n\nwhere E[ _wkwk_ _[T]_ [] =] _[ W][k]_ [. Defining] _[ X][k]_ [:=][ E][[] _[x][k][x][T]_ _k_ []] _[, k][ ≥]_ [0][, it is]\nequivalently written as\n\n\n_Xk_ +1 = _AXkA_ _[T]_ + _α_ [2] _Wk,_ _∀k ≥_ 0 _,_ (15)\n\n\nwith _X_ 0 := _x_ 0 _x_ _[T]_ 0 [. The following lemma proves the fact that the trace]\nof _Xk_ is bounded, which will be used for the main development.\n\n_Lemma 6 (Bounded trace):_ We have the following bound:\n\n\n9 _n_ [2] _α_\ntr( _Xk_ ) _≤_ _d_ min(1 _−_ _γ_ ) [3] [+] _[ ∥][x]_ [0] _[∥]_ 2 [2] _[n]_ [2] _[ρ]_ [2] _[k][.]_\n\n_Proof:_\nWe first bound _λ_ max( _Xk_ ) as follows:\n\n\n_k−_ 1\n_λ_ max( _Xk_ ) _≤α_ [2] � _λ_ max( _A_ _[i]_ _Wk−i−_ 1( _A_ _[T]_ ) _[i]_ ) + _λ_ max( _A_ _[k]_ _X_ 0( _A_ _[T]_ ) _[k]_ )\n\n_i_ =0\n\n\n\n_≤α_ [2] sup _λ_ max( _Wj_ )\n_j≥_ 0\n\n\n\n_k−_ 1\n� _λ_ max( _A_ _[i]_ ( _A_ _[T]_ ) _[i]_ )\n\n_i_ =0\n\n\n\n+ _λ_ max( _X_ 0) _λ_ max( _A_ _[k]_ ( _A_ _[T]_ ) _[k]_ )\n\n\n\n= _α_ [2] sup _λ_ max( _Wj_ )\n_j≥_ 0\n\n\n\n_k−_ 1\n� _∥A_ _[i]_ _∥_ 2 [2] [+] _[ λ]_ [max][(] _[X]_ 0 [)] _[∥][A][k][∥]_ [2] 2\n\n_i_ =0\n\n\n\n_≤α_ [2] _W_ max _n_\n\n\n_≤α_ [2] _W_ max _n_\n\n\n\n_k−_ 1\n� _∥A_ _[i]_ _∥∞_ [2] [+] _[ nλ]_ max [(] _[X]_ 0 [)] _[∥][A][k][∥]_ [2] _∞_\n\n_i_ =0\n\n\n_k−_ 1\n� _ρ_ [2] _[i]_ + _nλ_ max( _X_ 0) _ρ_ [2] _[k]_\n\n_i_ =0\n\n\n\n_≤α_ [2] _W_ max _n_ lim\n_k→∞_\n\n\n\n_k−_ 1\n� _ρ_ [2] _[i]_ + _nλ_ max( _X_ 0) _ρ_ [2] _[k]_\n\n_i_ =0\n\n\n\n_≤_ _[α]_ [2] _[W]_ [max] _[n]_ + _nλ_ max( _X_ 0) _ρ_ [2] _[k]_\n\n1 _−_ _ρ_ [2]\n\n_≤_ _[α]_ [2] _[W]_ [max] _[n]_ + _nλ_ max( _X_ 0) _ρ_ [2] _[k]_ _,_\n\n1 _−_ _ρ_\n\n\nwhere the first inequality is due to _A_ _[i]_ _Wk−i−_ 1( _A_ _[T]_ ) _[i]_ _⪰_ 0 and\n_A_ _[k]_ _X_ 0( _A_ _[T]_ ) _[k]_ _⪰_ 0, the third inequality comes from Lemma 5 and\n_∥·∥_ 2 _≤_ ~~_[√]_~~ ~~_n_~~ _∥·∥∞_, the fourth inequality is due to Lemma 2, the sixth\nand last inequalities come from _ρ ∈_ (0 _,_ 1). On the other hand, since\n_Xk ⪰_ 0, the diagonal elements are nonnegative. Therefore, we have\ntr( _Xk_ ) _≤_ _nλ_ max( _Xk_ ). Combining the last two inequalities lead to\n\ntr( _Xk_ ) _≤_ _nλ_ max( _Xk_ ) _≤_ _[α]_ [2] _[W]_ [max] _[n]_ [2] + _n_ [2] _λ_ max( _X_ 0) _ρ_ [2] _[k]_ _._\n\n1 _−_ _ρ_\n\n\nMoreover, noting the inequality _λ_ max( _X_ 0) _≤_ tr( _X_ 0) =\ntr( _x_ 0 _x_ _[T]_ 0 [) =] _[ ∥][x]_ [0] _[∥]_ [2] 2 [, and plugging] _[ ρ]_ [ = 1] _[ −]_ _[αd]_ [min][(1] _[ −]_ _[γ]_ [)][ into] _[ ρ]_\nin the last inequality, one gets the desired conclusion.\n\nNow, we are ready to present the main results. In the first result, we\nprovide a finite-time bound on the state error of the lower comparison\n\nsystem.\n\n_Theorem 1:_ For any _k ≥_ 0, we have\n\nE[ _∥Q_ _[L]_ _k_ _[−]_ _[Q][∗][∥]_ 2 []] _[ ≤]_ _d_ [3][1] _[α][/]_ [2][1] _[/]_ [2] _[|][S × A][|]_ [+] _[ |S × A|∥][Q]_ 0 _[L]_ _[−]_ _[Q][∗][∥]_ 2 _[ρ][k][.]_\n\nmin [(1] _[ −]_ _[γ]_ [)][3] _[/]_ [2]\n\n(16)\n_Proof:_ Noting the relations\n\n\nE[ _∥Q_ _[L]_ _k_ _[−]_ _[Q][∗][∥]_ 2 [2][] =][E][[(] _[Q][L]_ _k_ _[−]_ _[Q][∗]_ [)] _[T]_ [ (] _[Q][L]_ _k_ _[−]_ _[Q][∗]_ [)]]\n\n=E[tr(( _Q_ _[L]_ _k_ _[−]_ _[Q][∗]_ [)] _[T]_ [ (] _[Q][L]_ _k_ _[−]_ _[Q][∗]_ [))]]\n\n=E[tr(( _Q_ _[L]_ _k_ _[−]_ _[Q][∗]_ [)(] _[Q][L]_ _k_ _[−]_ _[Q][∗]_ [)] _[T]_ [ )]]\n\n\n=E[tr( _Xk_ )] _,_\n\n\nand using the bound in Lemma 6, one gets\n\n\n9 _αn_ [2]\nE[ _∥Q_ _[L]_ _k_ _[−]_ _[Q][∗][∥]_ 2 [2][]] _[ ≤]_ _d_ min(1 _−_ _γ_ ) [3] [+] _[ n]_ [2] _[∥][Q]_ 0 _[L]_ _[−]_ _[Q][∗][∥]_ [2] 2 _[ρ]_ [2] _[k][.]_ (17)\n\n\nTaking the square root on both side of the last inequality, using\nthe subadditivity of the square root function, the Jensen inequality,\nand the concavity of the square root function, we have the desired\nconclusion.\n\n\nThe first term on the right-hand side of (16) can be made arbitrarily\nsmall by reducing the step-size _α ∈_ (0 _,_ 1). The second bound\nexponentially vanishes as _k →∞_ at the rate of _ρ_ = 1 _−_ _αd_ min(1 _−_\n_γ_ ) _∈_ (0 _,_ 1). Therefore, it proves the exponential convergence of the\nmean-squared error of the lower comparison system up to a constant\nbias. In the next subsection, we will investigate an analysis of an\nupper comparison system.\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\n\n\nThis article has been accepted for publication in IEEE Transactions on Automatic Control. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/TAC.2024.3355326\n\n\n\n6\n\n\n_C. Upper comparison system_\n\n\nNow, let us consider the stochastic linear switching system [28]\n\n_Q_ _[U]_ _k_ +1 _[−]_ _[Q][∗]_ [=] _[ A]_ _Qk_ [(] _[Q]_ _k_ _[U]_ _[−]_ _[Q][∗]_ [) +] _[ αw]_ _k_ _[,]_ _Q_ _[U]_ 0 _[−]_ _[Q][∗]_ _[∈]_ [R] _[|S×A|][,]_\n(18)\n\n\nwhere the stochastic noise _wk_ is kept the same as the original system.\nWe will call it the _upper comparison system_ .\n\n_Proposition 3 ( [28]):_ Suppose _Q_ _[U]_ 0 _[−]_ _[Q][∗]_ _[≥]_ _[Q]_ [0] _[−]_ _[Q][∗]_ [, where] _[ ≥]_\nis used as the element-wise inequality. Then, _Q_ _[U]_ _k_ _[−]_ _[Q][∗]_ _[≥]_ _[Q][k]_ _[−]_ _[Q][∗]_\n\nfor all _k ≥_ 0.\n_Proof:_ Suppose the result holds for some _k ≥_ 0. Then, we have\n\n\n( _Qk_ +1 _−_ _Q_ _[∗]_ ) = _AQk_ ( _Qk −_ _Q_ _[∗]_ ) + _bQk_ + _αwk_\n_≤AQk_ ( _Qk −_ _Q_ _[∗]_ ) + _αwk_\n\n_≤AQk_ ( _Q_ _[U]_ _k_ _[−]_ _[Q][∗]_ [) +] _[ αw]_ _k_\n\n= _Q_ _[U]_ _k_ +1 _[−]_ _[Q][∗][,]_\n\n\nwhere we used the fact that _bQk_ = _D_ ( _γP_ Π _Qk_ _Q_ _[∗]_ _−_ _γP_ Π _Q∗_ _Q_ _[∗]_ ) _≤_\n_D_ ( _γP_ Π _Q∗_ _Q_ _[∗]_ _−_ _γP_ Π _Q∗_ _Q_ _[∗]_ ) = 0 in the first inequality. The second\ninequality is due to the hypothesis _Q_ _[U]_ _k_ _[−]_ _[Q][∗]_ _[≥]_ _[Q][k]_ _[−]_ _[Q][∗]_ [and the]\nfact that _AQk_ is a nonnegative matrix. The proof is completed by\ninduction.\n\n\nAccording to Proposition 3, the trajectory of the stochastic linear\nswitching system in (18) bounds that of the original system (11) from\nabove. Then, with the notation _xk_ := _Q_ _[U]_ _k_ _[−][Q][∗]_ [, (18) can be concisely]\nrepresented as the _stochastic switching linear system_\n\n\n_xk_ +1 = _Aσk_ _xk_ + _αwk,_ _x_ 0 _∈_ R _[n]_ _,_ _∀k ≥_ 0 _,_ (19)\n\n\nwhere _n_ := _|S × A|_, _σk_ _∈{_ 1 _,_ 2 _, . . ., |_ Θ _|}_ is the switching\nsignal at time _k ≥_ 0 determined by _σk_ = _ϕ_ ( _πk_ ) with _πk_ ( _·_ ) :=\narg min _a∈AQk_ ( _·, a_ ) _∈_ Θ, _ϕ_ : Θ _→{_ 1 _,_ 2 _, . . ., |_ Θ _|}_ is a oneto-one mapping from a deterministic policy _π ∈_ Θ to an integer\nin _{_ 1 _,_ 2 _, . . ., |_ Θ _|}_, and matrices _Ai, i ∈{_ 1 _,_ 2 _, . . ., |_ Θ _|}_ are defined\nin (12).\nCompared to the lower comparison system (14), which is linear, (19) is a switching system, which is much more complicated\ndue to the dependency on _Qk_ and _Q_ _[U]_ _k_ [. In particular, the system]\nmatrix _AQk_ switches according to the change of _Qk_, which depends\nprobabilistically on _Q_ _[U]_ _k_ [. Therefore, if we take the expectation on both]\nsides, it is not possible to separate _AQk_ and the state _Q_ _[U]_ _k_ _[−][Q][∗]_ [unlike]\nthe lower comparison system, making it much harder to analyze the\nstability of the upper comparison system. Therefore, the analysis used\nfor the upper comparison system cannot be directly applied, i.e.,\nthe autocorrelation matrix E[ _xkx_ _[T]_ _k_ []][ cannot be obtained by using the]\nsimple linear recursion given in (15). To overcome this difficulty, in\nthe next subsection, we instead study an error system by subtracting\nthe lower comparison system [28] from the upper comparison system.\n\n\n_D. Analysis of original system_\n\n\nIn the previous subsections, we have introduced upper and lower\ncomparison systems, and provided bounds on the corresponding\nexpected state errors in Theorem 1. Since the states of the lower\nand upper comparison systems bound the state of the original system\nfrom below and above, respectively, i.e.,\n\n\n_Q_ _[L]_ _k_ _[−]_ _[Q][∗]_ _[≤]_ _[Q]_ _k_ _[−]_ _[Q][∗]_ _[≤]_ _[Q][U]_ _k_ _[−]_ _[Q][∗][,]_ (20)\n\n\none can prove that the mean state error of the original system is\nalso bounded in terms of those of the upper and lower comparison\n\nsystems.\nHowever, as discussed previously, compared to the lower comparison system (14), which is linear, (19) is a switching system, which\nis much more complicated due to the dependency on _Qk_ and _Q_ _[U]_ _k_ [.]\n\n\n\nTo circumvent such a difficulty, we instead study an error system\nby subtracting the lower comparison system [28] from the upper\ncomparison system:\n\n_Q_ _[U]_ _k_ +1 _[−]_ _[Q][L]_ _k_ +1 [=] _[ A]_ _Qk_ [(] _[Q]_ _k_ _[U]_ _[−]_ _[Q][L]_ _k_ [) +] _[ B]_ _Qk_ [(] _[Q]_ _k_ _[L]_ _[−]_ _[Q][∗]_ [)] _[,]_ (21)\n\n\nwhere\n\n\n_BQk_ := _AQk −_ _AQ∗_ = _αγDP_ (Π _Qk −_ Π _Q∗_ ) _._ (22)\n\n\nHere, the stochastic noise _αwk_ is canceled out in the error system.\nMatrices ( _AQk_ _, BQk_ ) switch according to the external signal _Qk_,\nand _Q_ _[L]_ _k_ _[−]_ _[Q][∗]_ [can be seen as an external disturbance. The key insight]\nis as follows: if we can prove the stability of the error system, i.e.,\n_Q_ _[U]_ _k_ _[−][Q][L]_ _k_ _[→]_ [0][ as] _[ k][ →∞]_ [, then since] _[ Q][L]_ _k_ _[→]_ _[Q][∗]_ [as] _[ k][ →∞]_ [, we have]\n_Q_ _[U]_ _k_ _[→]_ _[Q][∗]_ [as well. Keeping this picture in mind, we can establish]\nthe following bound on the expected error E[ _∥Qk −_ _Q_ _[∗]_ _∥∞_ ].\n\n_Theorem 2 (Convergence):_ For all _k ≥_ 0, we have\n\n\n\nE[�� _Qk −_ _Q∗_ �� _∞_ []] _[ ≤]_ [9] _[d]_ [max][3] [2] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n\n\n_ρ_ _[k]_\n1 _−_ _γ_\n\n\n\n\n[max] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n+ [2] _[|][S × A][|]_ [3] _[/]_ [2]\n_d_ [3] _[/]_ [2] 1 _−_ _γ_\nmin [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n\n\n+ [4] _[α][γ][d]_ [max] _[|][S × A][|]_ [2] _[/]_ [3]\n\n\n\n_kρ_ _[k][−]_ [1] (23)\n1 _−_ _γ_\n\n\n\n_Proof:_ Taking norm on the error system in (21), we get\n\n\n\n_∥Q_ _[U]_ _k_ +1 _[−]_ _[Q][L]_ _k_ +1 _[∥][∞]_ _[≤∥][A]_ _Qk_ _[∥][∞][∥][Q]_ _k_ _[U]_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_\n\n+ _∥BQk_ _∥∞∥Q_ _[L]_ _k_ _[−]_ _[Q][∗][∥][∞]_\n\n_≤ρ∥Q_ _[U]_ _k_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_ [+ 2] _[αγd]_ [max] _[∥][Q][L]_ _k_ _[−]_ _[Q][∗][∥][∞]_\n\n\nwhere the second inequality is due to Lemma 2 and the definition\nin (22). Combining the last inequality with that in Theorem 1 and\n_∥Q_ _[L]_ _k_ _[−]_ _[Q][∗][∥][∞]_ _[≤∥][Q][L]_ _k_ _[−]_ _[Q][∗][∥]_ [2][ yields]\n\n\nE[ _∥Q_ _[U]_ _i_ +1 _[−]_ _[Q][L]_ _i_ +1 _[∥][∞]_ []] _[ ≤][ρ]_ [E][[] _[∥][Q]_ _i_ _[U]_ _[−]_ _[Q][L]_ _i_ _[∥][∞]_ []]\n\n+ 2 _αγd_ max [3] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n_d_ [1] _[/]_ [2]\nmin [(1] _[ −]_ _[γ]_ [)][3] _[/]_ [2]\n\n+ 2 _αγd_ max _∥Q_ _[L]_ 0 _[−]_ _[Q][∗][∥]_ 2 _[|S × A|][ρ][i]_\n\n\nfor all _i ≥_ 0. Applying the inequality successively for these values\nfrom _i_ = 0 to _i_ = _k_ leads to\n\n\nE[ _∥Q_ _[U]_ _k_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_ []] _[ ≤][ρ][k]_ [E][[] _[∥][Q]_ 0 _[U]_ _[−]_ _[Q][L]_ 0 _[∥][∞]_ []]\n\n+ [6] _[γ][d]_ [max] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n_d_ [3] _[/]_ [2]\nmin [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n+ _kρ_ _[k][−]_ [1] 2 _αγd_ max _∥Q_ _[L]_ 0 _[−]_ _[Q][∗][∥]_ 2 _[|S × A|][.]_\n\n\nNext, letting _Q_ _[U]_ 0 [=] _[ Q][L]_ 0 [=] _[ Q]_ [0] [yields]\n\nE[ _∥Q_ _[U]_ _k_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_ []] _[ ≤]_ [6] _[γ][d]_ [max] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n_d_ [3] _[/]_ [2]\nmin [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n+ _kρ_ _[k][−]_ [1] 2 _αγd_ max _∥Q_ 0 _−_ _Q_ _[∗]_ _∥_ 2 _|S × A|._\n(24)\n\n\n2\nUsing _∥Q_ 0 _−_ _Q_ _[∗]_ _∥_ 2 _≤|S × A|_ [1] _[/]_ [2] _∥Q_ 0 _−_ _Q_ _[∗]_ _∥∞_ _≤|S × A|_ [1] _[/]_ [2] 1 _−γ_\nfurther leads to\n\nE[ _∥Q_ _[U]_ _k_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_ []] _[ ≤]_ [6] _[γ][d]_ [max] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n_d_ [3] _[/]_ [2]\nmin [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n+ _kρ_ _[k][−]_ [1] 4 _αγd_ max _[|][S × A][|]_ [2] _[/]_ [3] (25)\n\n1 _−_ _γ_\n\n\nOn the other hand, one can prove the inequalities\n\nE[�� _Qk −_ _Q∗_ �� _∞_ [] =][E][[] _[∥][Q][k][ −]_ _[Q]_ _k_ _[L]_ [+] _[ Q][L]_ _k_ _[−]_ _[Q][∗][∥][∞]_ []]\n\n_≤_ E[ _∥Q_ _[L]_ _k_ _[−]_ _[Q][∗][∥][∞]_ [] +][ E][[] _[∥][Q]_ _k_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_ []]\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\n\n\nThis article has been accepted for publication in IEEE Transactions on Automatic Control. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/TAC.2024.3355326\n\n\n\n_≤_ E[ _∥Q_ _[L]_ _k_ _[−]_ _[Q][∗][∥][∞]_ [] +][ E][[] _[∥][Q][U]_ _k_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_ []] (26)\n\n\nwhere the last inequality comes from the fact that _Q_ _[U]_ _k_ _[−]_ _[Q][L]_ _k_ _[≥]_ _[Q][k]_ _[−]_\n_Q_ _[L]_ _k_ _[≥]_ [0][ holds. Combining the last inequality with that in Theorem 1]\nleads to\n\n\nE[ _∥Qk −_ _Q_ _[∗]_ _∥∞_ ] _≤_ E[ _∥Q_ _[L]_ _k_ _[−]_ _[Q][∗][∥][∞]_ [] +][ E][[] _[∥][Q][U]_ _k_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_ []]\n\n_≤_ [3] _[α]_ [1] _[/]_ [2] _[|][S × A][|]_\n\n_d_ [1] _[/]_ [2]\nmin [(1] _[ −]_ _[γ]_ [)][3] _[/]_ [2]\n\n+ [2] _[|][S × A][|]_ [3] _[/]_ [2] _ρ_ _[k]_ + E[ _∥Q_ _[U]_ _k_ _[−]_ _[Q][L]_ _k_ _[∥][∞]_ []] _[.]_\n\n1 _−_ _γ_\n\n\nMoreover, combining the above inequality with (25) yields the desired\nconclusion.\n\n\nNote that the first term in (23) is the constant error due to the\nconstant step-size, which is scaled according to _α ∈_ (0 _,_ 1). The\nsecond term in (23) is due to the gap between lower comparison\nsystem and original system, and the third term in (23) is due to the gap\nbetween upper comparison system and original system. The second\nterm _O_ ( _ρ_ _[k]_ ) exponentially decays, and the third term _O_ ( _kρ_ _[k][−]_ [1] ) also\nexponentially decays while the speed is slower than the second term\ndue to the additional linearly increasing factor. The upper bound\nin (23) can be converted to looser but more interpretable forms as\nfollows.\n\n\n_Corollary 1:_ For any _k ≥_ 0, we have\n\n\n\n7\n\n\nwhere the last inequality uses _α ∈_ (0 _,_ 1) in Assumption 2. Combining\nthe above bound with (27), (28) follows. This completes the proof.\n\n\n_Remark 4:_ A probabilistic error bound can be derived from the\nexpected error bound by leveraging various concentration inequalities,\nsuch as the Markov inequality. For instance, using the Markov\ninequality, we have\n\nP[�� _Qk −_ _Q∗_ �� _∞_ _[< ε]_ []] _[ ≥]_ [1] _[ −]_ [9] _[γ][d]_ [max] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n_εd_ [3] min _[/]_ [2] [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n_−_ [2] _[|][S × A][|]_ [3] _[/]_ [2]\n\n_ρ_ _[k]_\n_ε_ (1 _−_ _γ_ )\n\n\n1\n\n_−_ [8] _[γ][d]_ [max] _[|][S × A][|]_ [2] _[/]_ [3]\n\n_ε_ (1 _−_ _γ_ ) _d_ min(1 _−_ _γ_ ) _[ρ][k/]_ [2] _[−]_ [1] _[.]_\n\n\nThe right-hand side converges to one as _k →∞_ and _α →_ 0.\n\n\nIV. COMPARATIVE ANALYSIS\n\n\nTABLE I\n\nCOMPARATIVE ANALYSIS OF SEVERAL RESULTS: _**t**_ **cover** IS THE COVER\n\nTIME; _**t**_ **mix** IS THE MIXING TIME; _**O**_ **[˜]** IGNORES THE POLYLOGARITHMIC\n\n\nFACTORS\n\n\nMethod Sample complexity Observation model\n\n\n\nOurs _O_ ˜\n\n\nLee et. al. [28] _O_ ˜\n\n\n\n_γ_ ~~[2]~~ _d_ ~~[2]~~ max _[|][S×A][|]_ ~~[2]~~\n~~�~~ _ε_ [2] _d_ ~~[4]~~ min [(1] _[−][γ]_ [)6]\n\n\n\n_ε_ [2] _d_ ~~[4]~~ min [(1] _[−][γ]_ [)6]\n\n\n\n_ε_ [4] _d_ ~~[6]~~ min [(1] _[−][γ]_ [)10]\n\n\n\n~~�~~\n\n�\n\n\n\n\n[max] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n+ [2] _[|][S × A][|]_ [3] _[/]_ [2]\n_d_ [3] _[/]_ [2] 1 _−_ _γ_\nmin [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n\n\nE[ _∥Qk −_ _Q_ _[∗]_ _∥∞_ ] _≤_ [9] _[d]_ [max] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n\n\n_ρ_ _[k]_\n1 _−_ _γ_\n\n\n\nBeck et. al. [22] _O_ ˜\n\n\n\n_d_ [4] max _[|][S×A][|]_ [4]\n� _ε_ [4] _d_ ~~[6]~~ min [(1] _[−][γ]_ [)10]\n\n\n\n_t_ [3] cover _[|][S×A][|]_\n� (1 _−γ_ ) ~~[5]~~ _ε_ ~~[2]~~\n\n\n\n(1 _−γ_ ) ~~[5]~~ _ε_ ~~[2]~~\n\n\n\n�\n\n\n\n�\n\n\n\ni.i.d.\n\n\ni.i.d.\n\n\nnon-i.i.d.\n\n\nnon-i.i.d.\n\n\nnon-i.i.d.\n\n\nnon-i.i.d.\n\n\n\n1 _t_ mix\n� _d_ min(1 _−γ_ ) ~~[5]~~ _ε_ ~~[2 ]~~ [+] _d_ min(1 _−γ_ )\n\n\n\n_−_ 2 _−_ 1\n\n+ [4] _[α][γ][d]_ [max] _[|][S × A][|]_ [2] _[/]_ [3] ln( _ρ_ ) _[−]_ [1] _ρ_ _[k/]_ [2]\n\n1 _−_ _γ_ ln( _ρ_ ) _[ρ]_\n\n\n\nLi et. al. [25] _O_ ˜\n\n\n\nChen et. al. [26] _O_ ˜\n\n\n\n�\n\n\n\n(27)\n\n\nand\n\nE[�� _Qk −_ _Q∗_ �� _∞_ []] _[ ≤]_ [9] _[d]_ [max] _[|][S × A][|][α]_ [1] _[/]_ [2]\n\n_d_ [3] _[/]_ [2]\nmin [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n+ [2] _[|][S × A][|]_ [3] _[/]_ [2] _ρ_ _[k]_\n\n1 _−_ _γ_\n\n\n1\n\n+ [8] _[γ][d]_ [max] _[|][S × A][|]_ [2] _[/]_ [3]\n\n1 _−_ _γ_ _d_ min(1 _−_ _γ_ ) _[ρ][k/]_ [2] _[−]_ [1]\n\n(28)\n_Proof:_ In (23), we focus on the term _kρ_ _[k][−]_ [1] = _kρ_ _[k/]_ [2+] _[k/]_ [2] _[−]_ [1] =\n_kρ_ _[k/]_ [2] _[−]_ [1] _ρ_ _[k/]_ [2] . Let _f_ ( _x_ ) = _xρ_ _[x/]_ [2] = _xρ_ _[x/]_ [2] . Checking the first-order\noptimality condition\n\n\n\n1\n� _d_ ~~[3]~~ min [(1] _[−][γ]_ [)5] _[ε]_ [2]\n\n\n\n�\n\n\n\nQu et. al. [24] _O_ ˜\n\n\nEven-Dar et. al. [20] _O_ ˜\n\n\n\n_t_ mix _|S×A|_ [2]\n� (1 _−γ_ ) ~~[5]~~ _ε_ ~~[2]~~\n\n\n\n(1 _−γ_ ) ~~[4]~~ _ε_ ~~[2]~~\n\n\n\n(1 _−γ_ ) ~~[5]~~ _ε_ ~~[2]~~\n\n\n\n1\n\n [(] _[t]_ (1 [cover)] _−γ_ ) ~~[4]~~ 1 _ε−_ ~~[2]~~ _γ_\n\n\n\n non-i.i.d.\n\n\n\n\n\n\n\n_df_ ( _x_ )\n\n\n\n\n_[d]_\n\n_dx_ _[xρ][x/]_ [2][ =] _[ ρ][x/]_ [2][ +] _[ x]_ 2 [1]\n\n\n\n2 _[ρ][x/]_ [2][ ln(] _[ρ]_ [) = 0] _[,]_\n\n\n\n( _x_ ) = _[d]_\n\n_dx_\n\n\n\n_−_ 2\nit follows that its maximum point is _x_ =\nln( _ρ_ ) [, and the corresponding]\n\n\n\nThe sample complexities of Q-learning, as analyzed and reported\nin various existing works [20], [22], [24]–[26], [28], are summarized\nin Table I, where _t_ cover represents the cover time, _t_ mix denotes\nthe mixing time, and _O_ [˜] omits the polylogarithmic factors, and the\nproof of the sample complexity based on the proposed convergence\nbound in Corollary 1 is given in Appendix. It is worth noting\nthat most of these analyses adopt non-i.i.d. observation models.\nTo account for these non-i.i.d. observation models, the cover time\nassumptions are considered in [20], [22], while the mixing time\nassumptions are employed in [24]–[26]. These sample complexity\nbounds are derived under various assumptions and conditions, making\nit generally impractical to make direct comparisons among them.\nHowever, it is worth noting that the proposed sample complexity\ndoes not appear to be consistently tighter than existing approaches,\nwhich represents a limitation of our method. Regarding the step-size\nconditions, our proposed finite-time analysis allows for a step-size\n_α ∈_ (0 _,_ 1), which is more flexible compared to [22], [25], [26]. This\nis because the constant step-sizes used in [22], [25], [26] impose\nmore restrictive ranges for the finite-time analysis. In our view, the\nmain advantage of the proposed approach lies in its introduction of\na unique switching system and control viewpoints. Building upon\nthese perspectives, we have developed clear and simpler analysis\nframeworks for finite-time error bounds. These perspectives not only\noffer simplicity but also yield valuable insights into Q-learning. The\nproposed techniques, based on the foundational principles of systems\nand control theory, render the overall analysis more accessible and\nintuitive particularly for people with a background in control theory.\n\n\n\nmaximum value is _f_ _−_ 2\n� ln( _ρ_ )\n\n\n\n_−_ 1\n= _−_ 2 ln( _ρ_ ) . Therefore, we have the\n� ln( _ρ_ ) _[ρ]_\n\n\n\n_−_ 1\nbounds _kρ_ _[k][−]_ [1] = _kρ_ _[k/]_ [2] _ρ_ _[−]_ [1] _ρ_ _[k/]_ [2] _≤_ _−_ 2 ln( _ρ_ ) _[−]_ [1] _ρ_ _[k/]_ [2] . Combining\nln( _ρ_ ) _[ρ]_\nthis bound with (23), one gets the first bound in (27). To obtain\nthe second inequality in (28), we use the relation 1 _−_ _x_ [1] _[≤]_ [ln] _[ x][ ≤]_\n\n_x −_ 1 _, ∀x >_ 0 to obtain\n\n\n\n1\nln( _ρ_ _[−]_ [1] ) _[ρ]_\n\n\n\n1 1\nln( _ρ_ ~~_[−]_~~ ~~[1]~~ ) _≤_ 1 _−_ 1 _ρ_ _[ρ][−]_ [1] _[−]_ [1]\n_ρ_ ~~_[−]_~~ ~~[1]~~\n\n_≤_ 1 1 _−αd_ min(11 _−γ_ ) _[−]_ [1]\n_αd_ min(1 _−_ _γ_ ) _[ρ]_\n\n1\n_≤_\n_αd_ min(1 _−_ _γ_ ) _[,]_\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\n\n\nThis article has been accepted for publication in IEEE Transactions on Automatic Control. This is the author's version which has not been fully edited and\n\n\ncontent may change prior to final publication. Citation information: DOI 10.1109/TAC.2024.3355326\n\n\n\n8\n\n\nV. CONCLUSION\n\n\nIn this paper, we have revisited the switching system framework\nin [28] to analyze the finite-time convergence bound of Q-learning.\nWe have improved the analysis in [28] by replacing the average\niterate with the final iterate, which is simpler and more common\nin the literature. The proposed finite-time error bounds are more\ngeneral than most existing bounds for the constant step-size Qlearning in terms of the allowable range of step-sizes. Besides, the\nproposed analysis potentially offers additional insights on analysis\nof Q-learning, and complements existing approaches. Potential future\ntopics include finite-time analysis of SARSA, double Q-learning, and\nactor-critic using similar dynamic system viewpoints.\n\n\nREFERENCES\n\n\n[1] R. S. Sutton and A. G. Barto, _Reinforcement learning: An introduction_ .\nMIT Press, 1998.\n\n[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\n_et al._, “Human-level control through deep reinforcement learning,”\n_Nature_, vol. 518, no. 7540, p. 529, 2015.\n\n[3] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,\n“Dueling network architectures for deep reinforcement learning,” in\n_International conference on machine learning_, 2016, pp. 1995–2003.\n\n[4] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning.” in _International Conference on learning representations_, 2016.\n\n[5] N. Heess, J. J. Hunt, T. P. Lillicrap, and D. Silver, “Memory-based control with recurrent neural networks,” _arXiv preprint arXiv:1512.04455_,\n2015.\n\n[6] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning\nwith double Q-learning,” in _Proceedings of the AAAI conference on_\n_artificial intelligence_, vol. 30, no. 1, 2016.\n\n[7] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective on reinforcement learning,” in _International Conference on Machine_\n_Learning_, 2017, pp. 449–458.\n\n[8] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in _International conference on machine_\n_learning_, 2015, pp. 1889–1897.\n\n[9] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” _arXiv preprint arXiv:1707.06347_,\n2017.\n\n[10] C. J. C. H. Watkins and P. Dayan, “Q-learning,” _Machine learning_, vol. 8,\nno. 3-4, pp. 279–292, 1992.\n\n[11] J. N. Tsitsiklis, “Asynchronous stochastic approximation and Qlearning,” _Machine learning_, vol. 16, no. 3, pp. 185–202, 1994.\n\n[12] T. Jaakkola, M. I. Jordan, and S. P. Singh, “Convergence of stochastic\niterative dynamic programming algorithms,” in _Advances in neural_\n_information processing systems_, 1994, pp. 703–710.\n\n[13] V. S. Borkar and S. P. Meyn, “The ODE method for convergence of\nstochastic approximation and reinforcement learning,” _SIAM Journal on_\n_Control and Optimization_, vol. 38, no. 2, pp. 447–469, 2000.\n\n[14] H. V. Hasselt, “Double Q-learning,” in _Advances in Neural Information_\n_Processing Systems_, 2010, pp. 2613–2621.\n\n[15] F. S. Melo, S. P. Meyn, and M. I. Ribeiro, “An analysis of reinforcement\nlearning with function approximation,” in _Proceedings of the 25th_\n_international conference on Machine learning_, 2008, pp. 664–671.\n\n[16] D. Lee and N. He, “A unified switching system perspective and convergence analysis of Q-learning algorithms,” in _34th Conference on Neural_\n_Information Processing Systems, NeurIPS 2020_, 2020.\n\n[17] A. M. Devraj and S. P. Meyn, “Zap Q-learning,” in _Proceedings of_\n_the 31st International Conference on Neural Information Processing_\n_Systems_, 2017, pp. 2232–2241.\n\n[18] C. Szepesv´ari, “The asymptotic convergence-rate of Q-learning,” in\n_Advances in Neural Information Processing Systems_, 1998, pp. 1064–\n1070.\n\n[19] M. J. Kearns and S. P. Singh, “Finite-sample convergence rates for Qlearning and indirect algorithms,” in _Advances in neural information_\n_processing systems_, 1999, pp. 996–1002.\n\n[20] E. Even-Dar and Y. Mansour, “Learning rates for Q-learning,” _Journal_\n_of machine learning Research_, vol. 5, no. Dec, pp. 1–25, 2003.\n\n[21] M. G. Azar, R. Munos, M. Ghavamzadeh, and H. J. Kappen, “Speedy Qlearning,” in _Proceedings of the 24th International Conference on Neural_\n_Information Processing Systems_, 2011, pp. 2411–2419.\n\n\n\n\n[22] C. L. Beck and R. Srikant, “Error bounds for constant step-size Qlearning,” _Systems & Control letters_, vol. 61, no. 12, pp. 1203–1208,\n2012.\n\n[23] M. J. Wainwright, “Stochastic approximation with cone-contractive\noperators: Sharp _ℓ∞_ -bounds for Q-learning,” _arXiv_ _preprint_\n_arXiv:1905.06265_, 2019.\n\n[24] G. Qu and A. Wierman, “Finite-time analysis of asynchronous stochastic\napproximation and Q-learning,” _arXiv preprint arXiv:2002.00260_, 2020.\n\n[25] G. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen, “Sample complexity\nof asynchronous Q-learning: Sharper analysis and variance reduction,”\n_arXiv preprint arXiv:2006.03041_, 2020.\n\n[26] Z. Chen, S. T. Maguluri, S. Shakkottai, and K. Shanmugam, “A Lyapunov theory for finite-sample guarantees of asynchronous Q-learning\nand TD-learning variants,” _arXiv preprint arXiv:2102.01567_, 2021.\n\n[27] D. Lee and N. He, “Periodic Q-learning,” in _Learning for dynamics and_\n_control_, 2020, pp. 582–598.\n\n[28] D. Lee, J. Hu, and N. He, “A discrete-time switching system analysis\nof Q-learning,” _SIAM Journal on Control and Optimization (accepted)_,\n2022.\n\n[29] H. Kushner and G. G. Yin, _Stochastic approximation and recursive_\n_algorithms and applications_ . Springer Science & Business Media, 2003,\nvol. 35.\n\n[30] D. Liberzon, _Switching in systems and control_ . Springer Science &\nBusiness Media, 2003.\n\n[31] H. Lin and P. J. Antsaklis, “Stability and stabilizability of switched linear\nsystems: a survey of recent results,” _IEEE Transactions on Automatic_\n_control_, vol. 54, no. 2, pp. 308–322, 2009.\n\n[32] C.-T. Chen, _Linear System Theory and Design_ . Oxford University Press,\nInc., 1995.\n\n[33] H. K. Khalil, _Nonlinear systems_, 2002.\n\n[34] A. Gosavi, “Boundedness of iterates in Q-learning,” _Systems & Control_\n_letters_, vol. 55, no. 4, pp. 347–349, 2006.\n\n[35] R. S. Sutton, “Learning to predict by the methods of temporal differences,” _Machine learning_, vol. 3, no. 1, pp. 9–44, 1988.\n\n\nVI. APPENDIX\n\n\nUsing the bound in (28), to achieve E[ _∥Qk −_ _Q_ _[∗]_ _∥∞_ ] _< ε_, a\nsufficient condition is\n\n\n\n9 _γd_ max _|S × A|α_ [1] _[/]_ [2]\n\n\n\n_ρ_ _[k]_\n1 _−_ _γ_\n\n\n\n_d_ max _|S × A|α_ [1] _[/]_ [2] + [2] _[|][S × A][|]_ [3] _[/]_ [2]\n\n_d_ [3] _[/]_ [2] 1 _−_ _γ_\nmin [(1] _[ −]_ _[γ]_ [)][5] _[/]_ [2]\n\n\n\n1\n\n+ [8] _[γ][d]_ [max] _[|][S × A][|]_ [2] _[/]_ [3]\n\n1 _−_ _γ_ _d_ min(1 _−_ _γ_ ) _[ρ][k/]_ [2] _[−]_ [1] _[ ≤]_ _[ε.]_\n\n\n\nThe inequality holds if each of the three terms is bounded by _ε/_ 3.\n\n\n\n9 _γd_ max _|S×A|α_ [1] _[/]_ [2]\nFor the first term, the bound\n\n\n\n_α ≤_ _ε_ [2] _d_ [3] min [(][1] _[−][γ]_ [)][5]\n\n\n\nmax _≤_ _ε/_ 3 leads to\n\n_d_ [3] min _[/]_ [2] [(1] _[−][γ]_ [)][5] _[/]_ [2]\n\n\n\n_α_ hand side of the above inequality. For the second term, the bound _≤_ 729 _γ_ ~~[2]~~ min _d_ ~~[2]~~ max _|S×A|_ ~~[2]~~ [. Therefore, we first let] _[ α]_ [ equal to the right-]\n\n\n\nln _ε_ (1 _−γ_ )\n1 _−γ|_ [3] _[/]_ [2] _ρ_ _[k]_ _≤_ _ε/_ 3 leads to _k ≥_ � 6 _|S×A_ ln( _ρ_ ) _|_ [3]\n\n\n\n2 _|S×A|_ [3] _[/]_ [2]\n\n\n\n6 _|S×A|_ [3] _[/]_ [2]\n\n\n\n�\n\n\n\nrelation 1 _−_ [1]\n\n\n\n. Using the\nln( _ρ_ )\n\n\n\nrelation 1 _−_ _x_ _[≤]_ [ln] _[ x][ ≤]_ _[x][ −]_ [1] _[,][ ∀][x >]_ [ 0][, a sufficient condition]\n\nfor the above condition is\n\n\n\n�\n\n\n\n_k ≥_ [729] _[γ]_ [2] _[d]_ [2max] _[|][S × A][|]_ [2] ln\n\n_ε_ [2] _d_ [4] min [(1] _[ −]_ _[γ]_ [)][6]\n\n\n\n6 _|S × A|_ [3] _[/]_ [2]\n� _ε_ (1 _−_ _γ_ )\n\n\n\n_._\n\n\n\nFor the last term, the bound [8] _[γ][d]_ [max] _[|][S×A][|]_ [2] _[/]_ [3]\n\n\n\n2 ln _εd_ min(1 _−γ_ ) [2]\n_k_ _≥_ � 24 _γd_ max _|S×A|_\n\n\n\n_d_ [max] min(1 _[|][S×A]_ _−γ_ ) _[|]_ ~~[2]~~ _ρ_ _[k/]_ [2] _[−]_ [1] _≤_ 3 _ε_ [yields]\n\n\n\n�\n\n\n\n24 _γd_ max _|S×A|_ [2] _[/]_ [3]\n\n\n\n_k_ _≥_ max . Again, using the relation 1 _−_\n\nln( _ρ_ )\n_x_ 1 _[≤]_ [ln] _[ x][ ≤]_ _[x][ −]_ [1] _[,][ ∀][x >]_ [ 0][ results in the sufficient condition]\n\n\n\n_k ≥_ ln � 24 _γεdd_ maxmin(1 _|S×A−γ_ ) _|_ ~~[2]~~ [2] _[/]_ [3]\n\n\n\n1458 _γ_ [2] _d_ [2] max _[|][S×A][|]_ [2]\n� _ε_ [2] _d_ ~~[4]~~ min [(1] _[−][γ]_ [)][6]\n\n\n\n_εd_ min(1 _−γ_ ) ~~[2]~~\n\n\n\n_ε_ [2] _d_ ~~[4]~~ minmax [(1] _[−][γ]_ [)][6] . Combining the\n\n\n\ntwo bounds leads to the desired conclusion.\n\n\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n\n\n",
    "ranking": {
      "relevance_score": 0.7337049301283651,
      "citation_score": 0.41685582822085887,
      "recency_score": 0.5386651530152541,
      "final_score": 0.6508311320355528
    },
    "citation_key": "Lee2022FinalIC",
    "is_open_access": true,
    "user_provided": false,
    "pdf_path": null
  }
]