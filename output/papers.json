[
  {
    "id": "http://arxiv.org/abs/2307.10493v1",
    "title": "An Analysis of Bugs In Persistent Memory Application",
    "published": "2023-07-19T23:12:01Z",
    "updated": "2023-07-19T23:12:01Z",
    "authors": [
      "Jahid Hasan"
    ],
    "summary": "Over the years of challenges on detecting the crash consistency of non-volatile persistent memory (PM) bugs and developing new tools to identify those bugs are quite stretching due to its inconsistent behavior on the file or storage systems. In this paper, we evaluated an open-sourced automatic bug detector tool (i.e. AGAMOTTO) to test NVM level hashing PM application to identify performance and correctness PM bugs in the persistent (main) memory. Furthermore, our faithful validation tool able to discovered 65 new NVM level hashing bugs on PMDK library and it outperformed the number of bugs (i.e. 40 bugs) that WITCHER framework was able to identified. Finally, we will propose a Deep-Q Learning search heuristic algorithm over the PM-Aware search algorithm in the state selection process to improve the searching strategy efficiently.",
    "pdf_url": "https://arxiv.org/pdf/2307.10493v1",
    "doi": null,
    "categories": [
      "cs.SE",
      "cs.CR",
      "cs.IT"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages, 4 figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Hasan2023AnAO,\n author = {J. Hasan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Analysis of Bugs In Persistent Memory Application},\n volume = {abs/2307.10493},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hasan2023"
  },
  {
    "id": "http://arxiv.org/abs/2509.17152v1",
    "title": "Criticality of a stochastic modern Hopfield network model with exponential interaction function",
    "published": "2025-09-21T16:48:35Z",
    "updated": "2025-09-21T16:48:35Z",
    "authors": [
      "Marco Cafiso",
      "Paolo Paradisi"
    ],
    "summary": "The Hopfield network (HN) is a classical model of associative memory whose dynamics are closely related to the Ising spin system with 2-body interactions. Stored patterns are encoded as minima of an energy function shaped by a Hebbian learning rule, and retrieval corresponds to convergence towards these minima. Modern Hopfield Networks (MHNs) introduce p-body interactions among neurons with p greater than 2 and, more recently, also exponential interaction functions, which significantly improve network's storing and retrieval capacity. While the criticality of HNs and p-body MHNs were extensively studied since the 1980s, the investigation of critical behavior in exponential MHNs is still in its early stages. Here, we study a stochastic exponential MHN (SMHN) with a multiplicative salt-and-pepper noise. While taking the noise probability p as control parameter, the average overlap parameter Q and a diffusion scaling H are taken as order parameters. In particular, H is related to the time correlation features of the network, with H greater than 0.5 signaling the emergence of persistent time memory. We found the emergence of a critical transition in both Q and H, with the critical noise level weakly decreasing as the load N increases. Notably, for each load N, the diffusion scaling H highlights a transition between a sub- and a super-critical regime, both with short-range correlated dynamics. Conversely, the critical regime, which is found in the range of p around 0.23-0.3, displays a long-range correlated dynamics with highly persistent temporal memory marked by the high value H around 1.3.",
    "pdf_url": "https://arxiv.org/pdf/2509.17152v1",
    "doi": null,
    "categories": [
      "physics.app-ph",
      "nlin.CD",
      "physics.comp-ph"
    ],
    "primary_category": "physics.app-ph",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Cafiso2025CriticalityOA,\n author = {Marco Cafiso and Paolo Paradisi},\n title = {Criticality of a stochastic modern Hopfield network model with exponential interaction function},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "cafiso2025"
  },
  {
    "id": "http://arxiv.org/abs/2404.15822v1",
    "title": "Recursive Backwards Q-Learning in Deterministic Environments",
    "published": "2024-04-24T11:54:53Z",
    "updated": "2024-04-24T11:54:53Z",
    "authors": [
      "Jan Diekhoff",
      "Jörn Fischer"
    ],
    "summary": "Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.",
    "pdf_url": "https://arxiv.org/pdf/2404.15822v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Diekhoff2024RecursiveBQ,\n author = {Jan Diekhoff and Jorn Fischer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Recursive Backwards Q-Learning in Deterministic Environments},\n volume = {abs/2404.15822},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "diekhoff2024"
  },
  {
    "id": "http://arxiv.org/abs/1805.12375v3",
    "title": "Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update",
    "published": "2018-05-31T08:28:24Z",
    "updated": "2019-11-12T04:16:25Z",
    "authors": [
      "Su Young Lee",
      "Sungik Choi",
      "Sae-Young Chung"
    ],
    "summary": "We propose Episodic Backward Update (EBU) - a novel deep reinforcement learning algorithm with a direct value propagation. In contrast to the conventional use of the experience replay with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state to its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate directly through all transitions of the sampled episode. We theoretically prove the convergence of the EBU method and experimentally demonstrate its performance in both deterministic and stochastic environments. Especially in 49 games of Atari 2600 domain, EBU achieves the same mean and median human normalized performance of DQN by using only 5% and 10% of samples, respectively.",
    "pdf_url": "https://arxiv.org/pdf/1805.12375v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2019",
    "journal_ref": null,
    "citation_count": 78,
    "bibtex": "@Article{Lee2018SampleEfficientDR,\n author = {Su Young Lee and Sung-Ik Choi and Sae-Young Chung},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update},\n volume = {abs/1805.12375},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "lee2018"
  },
  {
    "id": "http://arxiv.org/abs/2501.08109v4",
    "title": "Data-driven inventory management for new products: An adjusted Dyna-$Q$ approach with transfer learning",
    "published": "2025-01-14T13:40:08Z",
    "updated": "2025-06-09T11:45:53Z",
    "authors": [
      "Xinye Qu",
      "Longxiao Liu",
      "Wenjie Huang"
    ],
    "summary": "In this paper, we propose a novel reinforcement learning algorithm for inventory management of newly launched products with no historical demand information. The algorithm follows the classic Dyna-$Q$ structure, balancing the model-free and model-based approaches, while accelerating the training process of Dyna-$Q$ and mitigating the model discrepancy generated by the model-based feedback. Based on the idea of transfer learning, warm-start information from the demand data of existing similar products can be incorporated into the algorithm to further stabilize the early-stage training and reduce the variance of the estimated optimal policy. Our approach is validated through a case study of bakery inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\\% reduction in average daily cost compared with $Q$-learning, and up to a 77.5\\% reduction in training time within the same horizon compared with classic Dyna-$Q$. By using transfer learning, it can be found that the adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and relatively low shortage percentages among all the benchmarking algorithms under a 30-day testing.",
    "pdf_url": "https://arxiv.org/pdf/2501.08109v4",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Qu2025DatadrivenIM,\n author = {Xinye Qu and Longxiao Liu and Wenjie Huang},\n booktitle = {2025 IEEE 21st International Conference on Automation Science and Engineering (CASE)},\n journal = {2025 IEEE 21st International Conference on Automation Science and Engineering (CASE)},\n pages = {1031-1037},\n title = {Data-driven inventory management for new products: An adjusted Dyna-Q approach with transfer learning},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "qu2025"
  },
  {
    "id": "http://arxiv.org/abs/1101.4003v3",
    "title": "Dyna-H: a heuristic planning reinforcement learning algorithm applied to role-playing-game strategy decision systems",
    "published": "2011-01-20T19:51:58Z",
    "updated": "2011-07-30T09:56:22Z",
    "authors": [
      "Matilde Santos",
      "Jose Antonio Martin H.",
      "Victoria Lopez",
      "Guillermo Botella"
    ],
    "summary": "In a Role-Playing Game, finding optimal trajectories is one of the most important tasks. In fact, the strategy decision system becomes a key component of a game engine. Determining the way in which decisions are taken (online, batch or simulated) and the consumed resources in decision making (e.g. execution time, memory) will influence, in mayor degree, the game performance. When classical search algorithms such as A* can be used, they are the very first option. Nevertheless, such methods rely on precise and complete models of the search space, and there are many interesting scenarios where their application is not possible. Then, model free methods for sequential decision making under uncertainty are the best choice. In this paper, we propose a heuristic planning strategy to incorporate the ability of heuristic-search in path-finding into a Dyna agent. The proposed Dyna-H algorithm, as A* does, selects branches more likely to produce outcomes than other branches. Besides, it has the advantages of being a model-free online reinforcement learning algorithm. The proposal was evaluated against the one-step Q-Learning and Dyna-Q algorithms obtaining excellent experimental results: Dyna-H significantly overcomes both methods in all experiments. We suggest also, a functional analogy between the proposed sampling from worst trajectories heuristic and the role of dreams (e.g. nightmares) in human behavior.",
    "pdf_url": "https://arxiv.org/pdf/1101.4003v3",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 54,
    "bibtex": "@Article{Santos2011DynaHAH,\n author = {Matilde Santos and Jos´e Antonio and Victoria L´opez and Guillermo Botella},\n booktitle = {Knowledge-Based Systems},\n journal = {Knowl. Based Syst.},\n pages = {28-36},\n title = {Dyna-H: A heuristic planning reinforcement learning algorithm applied to role-playing game strategy decision systems},\n volume = {32},\n year = {2011}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "santos2011"
  },
  {
    "id": "http://arxiv.org/abs/1901.01977v1",
    "title": "Accelerating Goal-Directed Reinforcement Learning by Model Characterization",
    "published": "2019-01-04T19:04:37Z",
    "updated": "2019-01-04T19:04:37Z",
    "authors": [
      "Shoubhik Debnath",
      "Gaurav Sukhatme",
      "Lantao Liu"
    ],
    "summary": "We propose a hybrid approach aimed at improving the sample efficiency in goal-directed reinforcement learning. We do this via a two-step mechanism where firstly, we approximate a model from Model-Free reinforcement learning. Then, we leverage this approximate model along with a notion of reachability using Mean First Passage Times to perform Model-Based reinforcement learning. Built on such a novel observation, we design two new algorithms - Mean First Passage Time based Q-Learning (MFPT-Q) and Mean First Passage Time based DYNA (MFPT-DYNA), that have been fundamentally modified from the state-of-the-art reinforcement learning techniques. Preliminary results have shown that our hybrid approaches converge with much fewer iterations than their corresponding state-of-the-art counterparts and therefore requiring much fewer samples and much fewer training trials to converge.",
    "pdf_url": "https://arxiv.org/pdf/1901.01977v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper was published in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Debnath2018AcceleratingGR,\n author = {Shoubhik Debnath and G. Sukhatme and Lantao Liu},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {1-9},\n title = {Accelerating Goal-Directed Reinforcement Learning by Model Characterization},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "debnath2019"
  },
  {
    "id": "http://arxiv.org/abs/2009.02602v2",
    "title": "A Hybrid PAC Reinforcement Learning Algorithm",
    "published": "2020-09-05T21:32:42Z",
    "updated": "2021-01-28T05:24:39Z",
    "authors": [
      "Ashkan Zehfroosh",
      "Herbert G. Tanner"
    ],
    "summary": "This paper offers a new hybrid probably approximately correct (PAC) reinforcement learning (RL) algorithm for Markov decision processes (MDPs) that intelligently maintains favorable features of its parents. The designed algorithm, referred to as the Dyna-Delayed Q-learning (DDQ) algorithm, combines model-free and model-based learning approaches while outperforming both in most cases. The paper includes a PAC analysis of the DDQ algorithm and a derivation of its sample complexity. Numerical results are provided to support the claim regarding the new algorithm's sample efficiency compared to its parents as well as the best known model-free and model-based algorithms in application.",
    "pdf_url": "https://arxiv.org/pdf/2009.02602v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Zehfroosh2020AHP,\n author = {A. Zehfroosh and H. Tanner},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Hybrid PAC Reinforcement Learning Algorithm},\n volume = {abs/2009.02602},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "zehfroosh2020"
  },
  {
    "id": "http://arxiv.org/abs/2012.09737v2",
    "title": "Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL",
    "published": "2020-12-17T16:57:27Z",
    "updated": "2022-01-26T14:03:48Z",
    "authors": [
      "Simon Hirlaender",
      "Niky Bruchon"
    ],
    "summary": "Reinforcement learning holds tremendous promise in accelerator controls. The primary goal of this paper is to show how this approach can be utilised on an operational level on accelerator physics problems. Despite the success of model-free reinforcement learning in several domains, sample-efficiency still is a bottle-neck, which might be encompassed by model-based methods. We compare well-suited purely model-based to model-free reinforcement learning applied to the intensity optimisation on the FERMI FEL system. We find that the model-based approach demonstrates higher representational power and sample-efficiency, while the asymptotic performance of the model-free method is slightly superior. The model-based algorithm is implemented in a DYNA-style using an uncertainty aware model, and the model-free algorithm is based on tailored deep Q-learning. In both cases, the algorithms were implemented in a way, which presents increased noise robustness as omnipresent in accelerator control problems. Code is released in https://github.com/MathPhysSim/FERMI_RL_Paper.",
    "pdf_url": "https://arxiv.org/pdf/2012.09737v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY",
      "physics.acc-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 17 figures - minor changes and adaption to physical review journals",
    "journal_ref": null,
    "citation_count": 25,
    "bibtex": "@Article{Hirlaender2020ModelfreeAB,\n author = {Simon Hirlaender and N. Bruchon},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Model-free and Bayesian Ensembling Model-based Deep Reinforcement Learning for Particle Accelerator Control Demonstrated on the FERMI FEL},\n volume = {abs/2012.09737},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hirlaender2020"
  },
  {
    "id": "http://arxiv.org/abs/0904.0546v1",
    "title": "Eligibility Propagation to Speed up Time Hopping for Reinforcement Learning",
    "published": "2009-04-03T10:42:28Z",
    "updated": "2009-04-03T10:42:28Z",
    "authors": [
      "Petar Kormushev",
      "Kohei Nomoto",
      "Fangyan Dong",
      "Kaoru Hirota"
    ],
    "summary": "A mechanism called Eligibility Propagation is proposed to speed up the Time Hopping technique used for faster Reinforcement Learning in simulations. Eligibility Propagation provides for Time Hopping similar abilities to what eligibility traces provide for conventional Reinforcement Learning. It propagates values from one state to all of its temporal predecessors using a state transitions graph. Experiments on a simulated biped crawling robot confirm that Eligibility Propagation accelerates the learning process more than 3 times.",
    "pdf_url": "https://arxiv.org/pdf/0904.0546v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Kormushev2009EligibilityPT,\n author = {Petar Kormushev and K. Nomoto and F. Dong and K. Hirota},\n booktitle = {Journal of Advanced Computational Intelligence and Intelligent Informatics},\n journal = {J. Adv. Comput. Intell. Intell. Informatics},\n pages = {600-607},\n title = {Eligibility Propagation to Speed up Time Hopping for Reinforcement Learning},\n volume = {13},\n year = {2009}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "kormushev2009"
  },
  {
    "id": "http://arxiv.org/abs/2103.04529v3",
    "title": "Self-Supervised Online Reward Shaping in Sparse-Reward Environments",
    "published": "2021-03-08T03:28:04Z",
    "updated": "2021-07-26T00:30:07Z",
    "authors": [
      "Farzan Memarian",
      "Wonjoon Goo",
      "Rudolf Lioutikov",
      "Scott Niekum",
      "Ufuk Topcu"
    ],
    "summary": "We introduce Self-supervised Online Reward Shaping (SORS), which aims to improve the sample efficiency of any RL algorithm in sparse-reward environments by automatically densifying rewards. The proposed framework alternates between classification-based reward inference and policy update steps -- the original sparse reward provides a self-supervisory signal for reward inference by ranking trajectories that the agent observes, while the policy update is performed with the newly inferred, typically dense reward function. We introduce theory that shows that, under certain conditions, this alteration of the reward function will not change the optimal policy of the original MDP, while potentially increasing learning speed significantly. Experimental results on several sparse-reward environments demonstrate that, across multiple domains, the proposed algorithm is not only significantly more sample efficient than a standard RL baseline using sparse rewards, but, at times, also achieves similar sample efficiency compared to when hand-designed dense reward functions are used.",
    "pdf_url": "https://arxiv.org/pdf/2103.04529v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication in IROS 2021",
    "journal_ref": null,
    "citation_count": 58,
    "bibtex": "@Article{Memarian2021SelfSupervisedOR,\n author = {F. Memarian and Wonjoon Goo and Rudolf Lioutikov and U. Topcu and S. Niekum},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {2369-2375},\n title = {Self-Supervised Online Reward Shaping in Sparse-Reward Environments},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "memarian2021"
  },
  {
    "id": "http://arxiv.org/abs/2311.01450v2",
    "title": "DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing",
    "published": "2023-11-02T17:57:38Z",
    "updated": "2024-02-18T00:59:14Z",
    "authors": [
      "Vint Lee",
      "Pieter Abbeel",
      "Youngwoon Lee"
    ],
    "summary": "Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a bottleneck of MBRL, especially for sparse rewards that are challenging (or even ambiguous) to predict. Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, DreamSmooth, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks, such as Deepmind Control Suite and Atari benchmarks.",
    "pdf_url": "https://arxiv.org/pdf/2311.01450v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "For code and website, see https://vint-1.github.io/dreamsmooth/",
    "journal_ref": null,
    "citation_count": 7,
    "bibtex": "@Article{Lee2023DreamSmoothIM,\n author = {Vint Lee and Pieter Abbeel and Youngwoon Lee},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing},\n volume = {abs/2311.01450},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "lee2023"
  },
  {
    "id": "http://arxiv.org/abs/2505.19769v2",
    "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning",
    "published": "2025-05-26T09:52:25Z",
    "updated": "2025-06-24T05:29:35Z",
    "authors": [
      "Yuhui Chen",
      "Haoran Li",
      "Zhennan Jiang",
      "Haowei Wen",
      "Dongbin Zhao"
    ],
    "summary": "Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViR's ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation.",
    "pdf_url": "https://arxiv.org/pdf/2505.19769v2",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Chen2025TeViRTR,\n author = {Yuhui Chen and Haoran Li and Zhen Jiang and Haowei Wen and Dongbin Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning},\n volume = {abs/2505.19769},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "chen2025"
  },
  {
    "id": "http://arxiv.org/abs/2401.14226v1",
    "title": "Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks",
    "published": "2024-01-25T15:06:40Z",
    "updated": "2024-01-25T15:06:40Z",
    "authors": [
      "Shuai Han",
      "Mehdi Dastani",
      "Shihan Wang"
    ],
    "summary": "Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse. Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency. Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks. The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies. In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks. Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task. We evaluate our algorithm in a variety of sparse-reward environments. The experiment results show that our approach significantly outperforms the state-of-art baselines as the difficulty of the task increases.",
    "pdf_url": "https://arxiv.org/pdf/2401.14226v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Han2024SampleER,\n author = {Shuai Han and M. Dastani and Shihan Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks},\n volume = {abs/2401.14226},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "han2024"
  },
  {
    "id": "http://arxiv.org/abs/2312.05787v1",
    "title": "Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization",
    "published": "2023-12-10T06:30:19Z",
    "updated": "2023-12-10T06:30:19Z",
    "authors": [
      "Takuya Hiraoka"
    ],
    "summary": "Reinforcement learning (RL) methods with a high replay ratio (RR) and regularization have gained interest due to their superior sample efficiency. However, these methods have mainly been developed for dense-reward tasks. In this paper, we aim to extend these RL methods to sparse-reward goal-conditioned tasks. We use Randomized Ensemble Double Q-learning (REDQ) (Chen et al., 2021), an RL method with a high RR and regularization. To apply REDQ to sparse-reward goal-conditioned tasks, we make the following modifications to it: (i) using hindsight experience replay and (ii) bounding target Q-values. We evaluate REDQ with these modifications on 12 sparse-reward goal-conditioned tasks of Robotics (Plappert et al., 2018), and show that it achieves about $2 \\times$ better sample efficiency than previous state-of-the-art (SoTA) RL methods. Furthermore, we reconsider the necessity of specific components of REDQ and simplify it by removing unnecessary ones. The simplified REDQ with our modifications achieves $\\sim 8 \\times$ better sample efficiency than the SoTA methods in 4 Fetch tasks of Robotics.",
    "pdf_url": "https://arxiv.org/pdf/2312.05787v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Source code: https://github.com/TakuyaHiraoka/Efficient-SRGC-RL-with-a-High-RR-and-Regularization Demo video: https://drive.google.com/file/d/1UHd7JVPCwFLNFhy1QcycQfwU_nll_yII/view?usp=drive_link",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Hiraoka2023EfficientSG,\n author = {Takuya Hiraoka},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization},\n volume = {abs/2312.05787},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hiraoka2023"
  },
  {
    "id": "http://arxiv.org/abs/2408.03029v4",
    "title": "Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning",
    "published": "2024-08-06T08:22:16Z",
    "updated": "2025-02-28T12:21:00Z",
    "authors": [
      "Haozhe Ma",
      "Zhengding Luo",
      "Thanh Vinh Vo",
      "Kuankuan Sima",
      "Tze-Yun Leong"
    ],
    "summary": "Reward shaping is a technique in reinforcement learning that addresses the sparse-reward problem by providing more frequent and informative rewards. We introduce a self-adaptive and highly efficient reward shaping mechanism that incorporates success rates derived from historical experiences as shaped rewards. The success rates are sampled from Beta distributions, which dynamically evolve from uncertain to reliable values as data accumulates. Initially, the shaped rewards exhibit more randomness to encourage exploration, while over time, the increasing certainty enhances exploitation, naturally balancing exploration and exploitation. Our approach employs Kernel Density Estimation (KDE) combined with Random Fourier Features (RFF) to derive the Beta distributions, providing a computationally efficient, non-parametric, and learning-free solution for high-dimensional continuous state spaces. Our method is validated on various tasks with extremely sparse rewards, demonstrating notable improvements in sample efficiency and convergence stability over relevant baselines.",
    "pdf_url": "https://arxiv.org/pdf/2408.03029v4",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 14,
    "bibtex": "@Article{Ma2024HighlyES,\n author = {Haozhe Ma and Zhengding Luo and Thanh Vinh Vo and Kuankuan Sima and Tze-Yun Leong},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning},\n volume = {abs/2408.03029},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ma2024"
  },
  {
    "id": "http://arxiv.org/abs/2206.05652v1",
    "title": "Dealing with Sparse Rewards in Continuous Control Robotics via Heavy-Tailed Policies",
    "published": "2022-06-12T04:09:39Z",
    "updated": "2022-06-12T04:09:39Z",
    "authors": [
      "Souradip Chakraborty",
      "Amrit Singh Bedi",
      "Alec Koppel",
      "Pratap Tokekar",
      "Dinesh Manocha"
    ],
    "summary": "In this paper, we present a novel Heavy-Tailed Stochastic Policy Gradient (HT-PSG) algorithm to deal with the challenges of sparse rewards in continuous control problems. Sparse reward is common in continuous control robotics tasks such as manipulation and navigation, and makes the learning problem hard due to non-trivial estimation of value functions over the state space. This demands either reward shaping or expert demonstrations for the sparse reward environment. However, obtaining high-quality demonstrations is quite expensive and sometimes even impossible. We propose a heavy-tailed policy parametrization along with a modified momentum-based policy gradient tracking scheme (HT-SPG) to induce a stable exploratory behavior to the algorithm. The proposed algorithm does not require access to expert demonstrations. We test the performance of HT-SPG on various benchmark tasks of continuous control with sparse rewards such as 1D Mario, Pathological Mountain Car, Sparse Pendulum in OpenAI Gym, and Sparse MuJoCo environments (Hopper-v2). We show consistent performance improvement across all tasks in terms of high average cumulative reward. HT-SPG also demonstrates improved convergence speed with minimum samples, thereby emphasizing the sample efficiency of our proposed algorithm.",
    "pdf_url": "https://arxiv.org/pdf/2206.05652v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Chakraborty2022DealingWS,\n author = {Souradip Chakraborty and A. S. Bedi and Alec Koppel and Pratap Tokekar and Dinesh Manocha},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Dealing with Sparse Rewards in Continuous Control Robotics via Heavy-Tailed Policies},\n volume = {abs/2206.05652},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "chakraborty2022"
  },
  {
    "id": "http://arxiv.org/abs/2205.09448v1",
    "title": "Image Augmentation Based Momentum Memory Intrinsic Reward for Sparse Reward Visual Scenes",
    "published": "2022-05-19T10:08:16Z",
    "updated": "2022-05-19T10:08:16Z",
    "authors": [
      "Zheng Fang",
      "Biao Zhao",
      "Guizhong Liu"
    ],
    "summary": "Many scenes in real life can be abstracted to the sparse reward visual scenes, where it is difficult for an agent to tackle the task under the condition of only accepting images and sparse rewards. We propose to decompose this problem into two sub-problems: the visual representation and the sparse reward. To address them, a novel framework IAMMIR combining the self-supervised representation learning with the intrinsic motivation is presented. For visual representation, a representation driven by a combination of the imageaugmented forward dynamics and the reward is acquired. For sparse rewards, a new type of intrinsic reward is designed, the Momentum Memory Intrinsic Reward (MMIR). It utilizes the difference of the outputs from the current model (online network) and the historical model (target network) to present the agent's state familiarity. Our method is evaluated on the visual navigation task with sparse rewards in Vizdoom. Experiments demonstrate that our method achieves the state of the art performance in sample efficiency, at least 2 times faster than the existing methods reaching 100% success rate.",
    "pdf_url": "https://arxiv.org/pdf/2205.09448v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Fang2022ImageAM,\n author = {Zheng Fang and Biao Zhao and Guizhong Liu},\n booktitle = {IEEE Transactions on Games},\n journal = {IEEE Transactions on Games},\n pages = {509-517},\n title = {Image Augmentation-Based Momentum Memory Intrinsic Reward for Sparse Reward Visual Scenes},\n volume = {16},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "fang2022"
  },
  {
    "id": "http://arxiv.org/abs/2503.18234v2",
    "title": "KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies",
    "published": "2025-03-23T23:00:05Z",
    "updated": "2025-06-07T17:23:47Z",
    "authors": [
      "Shih-Min Yang",
      "Martin Magnusson",
      "Johannes A. Stork",
      "Todor Stoyanov"
    ],
    "summary": "Soft Actor-Critic (SAC) has achieved notable success in continuous control tasks but struggles in sparse reward settings, where infrequent rewards make efficient exploration challenging. While novelty-based exploration methods address this issue by encouraging the agent to explore novel states, they are not trivial to apply to SAC. In particular, managing the interaction between novelty-based exploration and SAC's stochastic policy can lead to inefficient exploration and redundant sample collection. In this paper, we propose KEA (Keeping Exploration Alive) which tackles the inefficiencies in balancing exploration strategies when combining SAC with novelty-based exploration. KEA integrates a novelty-augmented SAC with a standard SAC agent, proactively coordinated via a switching mechanism. This coordination allows the agent to maintain stochasticity in high-novelty regions, enhancing exploration efficiency and reducing repeated sample collection. We first analyze this potential issue in a 2D navigation task, and then evaluate KEA on the DeepSea hard-exploration benchmark as well as sparse reward control tasks from the DeepMind Control Suite. Compared to state-of-the-art novelty-based exploration baselines, our experiments show that KEA significantly improves learning efficiency and robustness in sparse reward setups.",
    "pdf_url": "https://arxiv.org/pdf/2503.18234v2",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2025",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Yang2025KEAKE,\n author = {Shih-Min Yang and Martin Magnusson and J. A. Stork and Todor Stoyanov},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies},\n volume = {abs/2503.18234},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yang2025"
  },
  {
    "id": "http://arxiv.org/abs/2302.10825v1",
    "title": "Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning",
    "published": "2023-02-21T17:00:05Z",
    "updated": "2023-02-21T17:00:05Z",
    "authors": [
      "Jiong Li",
      "Pratik Gajane"
    ],
    "summary": "Sparsity of rewards while applying a deep reinforcement learning method negatively affects its sample-efficiency. A viable solution to deal with the sparsity of rewards is to learn via intrinsic motivation which advocates for adding an intrinsic reward to the reward function to encourage the agent to explore the environment and expand the sample space. Though intrinsic motivation methods are widely used to improve data-efficient learning in the reinforcement learning model, they also suffer from the so-called detachment problem. In this article, we discuss the limitations of intrinsic curiosity module in sparse-reward multi-agent reinforcement learning and propose a method called I-Go-Explore that combines the intrinsic curiosity module with the Go-Explore framework to alleviate the detachment problem.",
    "pdf_url": "https://arxiv.org/pdf/2302.10825v1",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 5,
    "bibtex": "@Article{Li2023CuriositydrivenEI,\n author = {Jiong Li and Pratik Gajane},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Curiosity-driven Exploration in Sparse-reward Multi-agent Reinforcement Learning},\n volume = {abs/2302.10825},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "li2023"
  },
  {
    "id": "http://arxiv.org/abs/2010.03956v1",
    "title": "Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games",
    "published": "2020-10-05T03:43:06Z",
    "updated": "2020-10-05T03:43:06Z",
    "authors": [
      "Shengyi Huang",
      "Santiago Ontañón"
    ],
    "summary": "Training agents using Reinforcement Learning in games with sparse rewards is a challenging problem, since large amounts of exploration are required to retrieve even the first reward. To tackle this problem, a common approach is to use reward shaping to help exploration. However, an important drawback of reward shaping is that agents sometimes learn to optimize the shaped reward instead of the true objective. In this paper, we present a novel technique that we call action guidance that successfully trains agents to eventually optimize the true objective in games with sparse rewards while maintaining most of the sample efficiency that comes with reward shaping. We evaluate our approach in a simplified real-time strategy (RTS) game simulator called $μ$RTS.",
    "pdf_url": "https://arxiv.org/pdf/2010.03956v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Huang2020ActionGG,\n author = {Shengyi Huang and Santiago Ontan'on},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games},\n volume = {abs/2010.03956},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "huang2020"
  },
  {
    "id": "http://arxiv.org/abs/2009.13579v3",
    "title": "Novelty Search in Representational Space for Sample Efficient Exploration",
    "published": "2020-09-28T18:51:52Z",
    "updated": "2022-04-15T16:11:46Z",
    "authors": [
      "Ruo Yu Tao",
      "Vincent François-Lavet",
      "Joelle Pineau"
    ],
    "summary": "We present a new approach for efficient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. Our approach uses intrinsic rewards that are based on the distance of nearest neighbors in the low dimensional representational space to gauge novelty. We then leverage these intrinsic rewards for sample-efficient exploration with planning routines in representational space for hard exploration tasks with sparse rewards. One key element of our approach is the use of information theoretic principles to shape our representations in a way so that our novelty reward goes beyond pixel similarity. We test our approach on a number of maze tasks, as well as a control problem and show that our exploration approach is more sample-efficient compared to strong baselines.",
    "pdf_url": "https://arxiv.org/pdf/2009.13579v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages + references + appendix. Oral presentation at NeurIPS 2020",
    "journal_ref": null,
    "citation_count": 48,
    "bibtex": "@Article{Tao2020NoveltySI,\n author = {Ruo Yu Tao and Vincent François-Lavet and Joelle Pineau},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Novelty Search in representational space for sample efficient exploration},\n volume = {abs/2009.13579},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "tao2020"
  },
  {
    "id": "http://arxiv.org/abs/2403.06880v2",
    "title": "Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning",
    "published": "2024-03-11T16:34:23Z",
    "updated": "2024-03-18T09:43:20Z",
    "authors": [
      "Junseok Park",
      "Yoonsung Kim",
      "Hee Bin Yoo",
      "Min Whoo Lee",
      "Kibeom Kim",
      "Won-Seok Choi",
      "Minsu Lee",
      "Byoung-Tak Zhang"
    ],
    "summary": "Toddlers evolve from free exploration with sparse feedback to exploiting prior experiences for goal-directed learning with denser rewards. Drawing inspiration from this Toddler-Inspired Reward Transition, we set out to explore the implications of varying reward transitions when incorporated into Reinforcement Learning (RL) tasks. Central to our inquiry is the transition from sparse to potential-based dense rewards, which share optimal strategies regardless of reward changes. Through various experiments, including those in egocentric navigation and robotic arm manipulation tasks, we found that proper reward transitions significantly influence sample efficiency and success rates. Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense (S2D) transition. Beyond these performance metrics, using Cross-Density Visualizer technique, we observed that transitions, especially the S2D, smooth the policy loss landscape, promoting wide minima that enhance generalization in RL models.",
    "pdf_url": "https://arxiv.org/pdf/2403.06880v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as a full paper at AAAI 2024 (Oral presentation): 7 pages (main paper), 2 pages (references), 17 pages (appendix) each",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Park2024UnveilingTS,\n author = {Junseok Park and Yoonsung Kim and H. Yoo and Min Whoo Lee and Kibeom Kim and Won-Seok Choi and Min Whoo Lee and Byoung-Tak Zhang},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {592-600},\n title = {Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "park2024"
  },
  {
    "id": "http://arxiv.org/abs/2305.10865v2",
    "title": "Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning",
    "published": "2023-05-18T10:37:54Z",
    "updated": "2023-09-30T08:27:28Z",
    "authors": [
      "Wenhao Li",
      "Dan Qiao",
      "Baoxiang Wang",
      "Xiangfeng Wang",
      "Bo Jin",
      "Hongyuan Zha"
    ],
    "summary": "The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the \"over-representation\" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel \"disentangled\" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thought that can suggest potential goals, provide suitable goal decomposition and subgoal allocation as well as self-reflection-based replanning. Additionally, SAMA incorporates language-grounded RL to train each agent's subgoal-conditioned policy. SAMA demonstrates considerable advantages in sample efficiency compared to state-of-the-art ASG methods, as evidenced by its performance on two challenging sparse-reward tasks, Overcooked and MiniRTS.",
    "pdf_url": "https://arxiv.org/pdf/2305.10865v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "54 pages, 16 figures",
    "journal_ref": null,
    "citation_count": 10,
    "bibtex": "@Article{Li2023SemanticallyAT,\n author = {Wenhao Li and Dan Qiao and Baoxiang Wang and Xiangfeng Wang and Bo Jin and H. Zha},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning},\n volume = {abs/2305.10865},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "li2023"
  },
  {
    "id": "http://arxiv.org/abs/2109.14830v2",
    "title": "Reinforcement Learning for Classical Planning: Viewing Heuristics as Dense Reward Generators",
    "published": "2021-09-30T03:36:01Z",
    "updated": "2022-03-07T18:51:01Z",
    "authors": [
      "Clement Gehring",
      "Masataro Asai",
      "Rohan Chitnis",
      "Tom Silver",
      "Leslie Pack Kaelbling",
      "Shirin Sohrabi",
      "Michael Katz"
    ],
    "summary": "Recent advances in reinforcement learning (RL) have led to a growing interest in applying RL to classical planning domains or applying classical planning methods to some complex RL domains. However, the long-horizon goal-based problems found in classical planning lead to sparse rewards for RL, making direct application inefficient. In this paper, we propose to leverage domain-independent heuristic functions commonly used in the classical planning literature to improve the sample efficiency of RL. These classical heuristics act as dense reward generators to alleviate the sparse-rewards issue and enable our RL agent to learn domain-specific value functions as residuals on these heuristics, making learning easier. Correct application of this technique requires consolidating the discounted metric used in RL and the non-discounted metric used in heuristics. We implement the value functions using Neural Logic Machines, a neural network architecture designed for grounded first-order logic inputs. We demonstrate on several classical planning domains that using classical heuristics for RL allows for good sample efficiency compared to sparse-reward RL. We further show that our learned value functions generalize to novel problem instances in the same domain.",
    "pdf_url": "https://arxiv.org/pdf/2109.14830v2",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Equal contributions by the first two authors. This manuscript is a camera-ready version accepted in ICAPS-2022. It is significantly updated from past versions (e.g., in the ICAPS PRL (Planning and RL) workshop) with additional experiments comparing existing work (STRIPS-HGN (Shen, Trevizan, and Thiebaux 2020) and GBFS-GNN (Rivlin, Hazan, and Karpas 2019))",
    "journal_ref": null,
    "citation_count": 43,
    "bibtex": "@Article{Gehring2021ReinforcementLF,\n author = {Clement Gehring and Masataro Asai and Rohan Chitnis and Tom Silver and L. Kaelbling and Shirin Sohrabi and Michael Katz},\n booktitle = {International Conference on Automated Planning and Scheduling},\n pages = {588-596},\n title = {Reinforcement Learning for Classical Planning: Viewing Heuristics as Dense Reward Generators},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "gehring2021"
  },
  {
    "id": "http://arxiv.org/abs/2007.15543v2",
    "title": "PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping Pixels to Rewards",
    "published": "2020-07-30T15:50:38Z",
    "updated": "2020-11-19T13:42:41Z",
    "authors": [
      "Prasoon Goyal",
      "Scott Niekum",
      "Raymond J. Mooney"
    ],
    "summary": "Reinforcement learning (RL), particularly in sparse reward settings, often requires prohibitively large numbers of interactions with the environment, thereby limiting its applicability to complex problems. To address this, several prior approaches have used natural language to guide the agent's exploration. However, these approaches typically operate on structured representations of the environment, and/or assume some structure in the natural language commands. In this work, we propose a model that directly maps pixels to rewards, given a free-form natural language description of the task, which can then be used for policy learning. Our experiments on the Meta-World robot manipulation domain show that language-based rewards significantly improves the sample efficiency of policy learning, both in sparse and dense reward settings.",
    "pdf_url": "https://arxiv.org/pdf/2007.15543v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Conference on Robot Learning (CoRL), 2020",
    "journal_ref": null,
    "citation_count": 60,
    "bibtex": "@Article{Goyal2020PixL2RGR,\n author = {Prasoon Goyal and S. Niekum and R. Mooney},\n booktitle = {Conference on Robot Learning},\n journal = {ArXiv},\n title = {PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping Pixels to Rewards},\n volume = {abs/2007.15543},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "goyal2020"
  },
  {
    "id": "http://arxiv.org/abs/2409.08724v1",
    "title": "Quasimetric Value Functions with Dense Rewards",
    "published": "2024-09-13T11:26:05Z",
    "updated": "2024-09-13T11:26:05Z",
    "authors": [
      "Khadichabonu Valieva",
      "Bikramjit Banerjee"
    ],
    "summary": "As a generalization of reinforcement learning (RL) to parametrizable goals, goal conditioned RL (GCRL) has a broad range of applications, particularly in challenging tasks in robotics. Recent work has established that the optimal value function of GCRL $Q^\\ast(s,a,g)$ has a quasimetric structure, leading to targetted neural architectures that respect such structure. However, the relevant analyses assume a sparse reward setting -- a known aggravating factor to sample complexity. We show that the key property underpinning a quasimetric, viz., the triangle inequality, is preserved under a dense reward setting as well. Contrary to earlier findings where dense rewards were shown to be detrimental to GCRL, we identify the key condition necessary for triangle inequality. Dense reward functions that satisfy this condition can only improve, never worsen, sample complexity. This opens up opportunities to train efficient neural architectures with dense rewards, compounding their benefits to sample complexity. We evaluate this proposal in 12 standard benchmark environments in GCRL featuring challenging continuous control tasks. Our empirical results confirm that training a quasimetric value function in our dense reward setting indeed outperforms training with sparse rewards.",
    "pdf_url": "https://arxiv.org/pdf/2409.08724v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Valieva2024QuasimetricVF,\n author = {Khadichabonu Valieva and Bikramjit Banerjee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Quasimetric Value Functions with Dense Rewards},\n volume = {abs/2409.08724},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "valieva2024"
  },
  {
    "id": "http://arxiv.org/abs/2501.17842v1",
    "title": "From Sparse to Dense: Toddler-inspired Reward Transition in Goal-Oriented Reinforcement Learning",
    "published": "2025-01-29T18:46:35Z",
    "updated": "2025-01-29T18:46:35Z",
    "authors": [
      "Junseok Park",
      "Hyeonseo Yang",
      "Min Whoo Lee",
      "Won-Seok Choi",
      "Minsu Lee",
      "Byoung-Tak Zhang"
    ],
    "summary": "Reinforcement learning (RL) agents often face challenges in balancing exploration and exploitation, particularly in environments where sparse or dense rewards bias learning. Biological systems, such as human toddlers, naturally navigate this balance by transitioning from free exploration with sparse rewards to goal-directed behavior guided by increasingly dense rewards. Inspired by this natural progression, we investigate the Toddler-Inspired Reward Transition in goal-oriented RL tasks. Our study focuses on transitioning from sparse to potential-based dense (S2D) rewards while preserving optimal strategies. Through experiments on dynamic robotic arm manipulation and egocentric 3D navigation tasks, we demonstrate that effective S2D reward transitions significantly enhance learning performance and sample efficiency. Additionally, using a Cross-Density Visualizer, we show that S2D transitions smooth the policy loss landscape, resulting in wider minima that improve generalization in RL models. In addition, we reinterpret Tolman's maze experiments, underscoring the critical role of early free exploratory learning in the context of S2D rewards.",
    "pdf_url": "https://arxiv.org/pdf/2501.17842v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Extended version of AAAI 2024 paper: Unveiling the Significance of Toddler-Inspired Reward Transition in Goal-Oriented Reinforcement Learning. This manuscript is currently being prepared for journal submission",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Park2025FromST,\n author = {Junseok Park and Hyeonseo Yang and Min Whoo Lee and Won-Seok Choi and Min Whoo Lee and Byoung-Tak Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Sparse to Dense: Toddler-inspired Reward Transition in Goal-Oriented Reinforcement Learning},\n volume = {abs/2501.17842},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "park2025"
  },
  {
    "id": "http://arxiv.org/abs/2204.07696v1",
    "title": "Efficient Reinforcement Learning for Unsupervised Controlled Text Generation",
    "published": "2022-04-16T01:54:24Z",
    "updated": "2022-04-16T01:54:24Z",
    "authors": [
      "Bhargav Upadhyay",
      "Akhilesh Sudhakar",
      "Arjun Maheswaran"
    ],
    "summary": "Controlled text generation tasks such as unsupervised text style transfer have increasingly adopted the use of Reinforcement Learning (RL). A major challenge in applying RL to such tasks is the sparse reward, which is available only after the full text is generated. Sparse rewards, combined with a large action space make RL training sample-inefficient and difficult to converge. Recently proposed reward-shaping strategies to address this issue have shown only negligible gains. In contrast, this work proposes a novel approach that provides dense rewards to each generated token. We evaluate our approach by its usage in unsupervised text style transfer. Averaged across datasets, our style transfer system improves upon current state-of-art systems by 21\\% on human evaluation and 12\\% on automatic evaluation. Upon ablated comparison with the current reward shaping approach (the `roll-out strategy'), using dense rewards improves the overall style transfer quality by 22\\% based on human evaluation. Further the RL training is 2.5 times as sample efficient, and 7 times faster.",
    "pdf_url": "https://arxiv.org/pdf/2204.07696v1",
    "doi": null,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 2 figures, 4 tables",
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Upadhyay2022EfficientRL,\n author = {Bhargav Upadhyay and A. Sudhakar and Arjun Maheswaran},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Efficient Reinforcement Learning for Unsupervised Controlled Text Generation},\n volume = {abs/2204.07696},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "upadhyay2022"
  },
  {
    "id": "http://arxiv.org/abs/2509.20570v1",
    "title": "PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models",
    "published": "2025-09-24T21:23:03Z",
    "updated": "2025-09-24T21:23:03Z",
    "authors": [
      "Mingze Yuan",
      "Pengfei Jin",
      "Na Li",
      "Quanzheng Li"
    ],
    "summary": "Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.",
    "pdf_url": "https://arxiv.org/pdf/2509.20570v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 6 figures; NeurIPS 2025 AI for science workshop",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Yuan2025PIRFPR,\n author = {Mingze Yuan and Pengfei Jin and Na Li and Quanzheng Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models},\n volume = {abs/2509.20570},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yuan2025"
  },
  {
    "id": "http://arxiv.org/abs/2506.01096v2",
    "title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning",
    "published": "2025-06-01T17:43:54Z",
    "updated": "2025-08-08T08:03:03Z",
    "authors": [
      "Yihao Liu",
      "Shuocheng Li",
      "Lang Cao",
      "Yuhang Xie",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Xiaojun Ma",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "summary": "Large language models are increasingly used for complex reasoning tasks where high-quality offline data such as expert-annotated solutions and distilled reasoning traces are often available. However, in environments with sparse rewards, reinforcement learning struggles to sample successful trajectories, leading to inefficient learning. At the same time, these offline trajectories that represent correct reasoning paths are not utilized by standard on-policy reinforcement learning methods. We introduce SuperRL, a unified training framework that adaptively alternates between RL and SFT. Whenever every rollout for a given instance receives zero reward, indicating the absence of a learning signal, SuperRL falls back to SFT on the curated offline data. Extensive experiments across diverse reasoning benchmarks show that SuperRL surpasses vanilla RL by delivering higher sample efficiency, stronger generalization, and improved robustness under sparse rewards.",
    "pdf_url": "https://arxiv.org/pdf/2506.01096v2",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 5,
    "bibtex": "@Article{Liu2025SuperRLRL,\n author = {Yihao Liu and Shuocheng Li and Lang Cao and Yuhang Xie and Mengyu Zhou and Haoyu Dong and Xiaojun Ma and Shi Han and Dongmei Zhang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning},\n volume = {abs/2506.01096},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "liu2025"
  },
  {
    "id": "http://arxiv.org/abs/1809.02070v2",
    "title": "ARCHER: Aggressive Rewards to Counter bias in Hindsight Experience Replay",
    "published": "2018-09-06T16:08:39Z",
    "updated": "2018-09-07T00:31:16Z",
    "authors": [
      "Sameera Lanka",
      "Tianfu Wu"
    ],
    "summary": "Experience replay is an important technique for addressing sample-inefficiency in deep reinforcement learning (RL), but faces difficulty in learning from binary and sparse rewards due to disproportionately few successful experiences in the replay buffer. Hindsight experience replay (HER) was recently proposed to tackle this difficulty by manipulating unsuccessful transitions, but in doing so, HER introduces a significant bias in the replay buffer experiences and therefore achieves a suboptimal improvement in sample-efficiency. In this paper, we present an analysis on the source of bias in HER, and propose a simple and effective method to counter the bias, to most effectively harness the sample-efficiency provided by HER. Our method, motivated by counter-factual reasoning and called ARCHER, extends HER with a trade-off to make rewards calculated for hindsight experiences numerically greater than real rewards. We validate our algorithm on two continuous control environments from DeepMind Control Suite - Reacher and Finger, which simulate manipulation tasks with a robotic arm - in combination with various reward functions, task complexities and goal sampling strategies. Our experiments consistently demonstrate that countering bias using more aggressive hindsight rewards increases sample efficiency, thus establishing the greater benefit of ARCHER in RL applications with limited computing budget.",
    "pdf_url": "https://arxiv.org/pdf/1809.02070v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages",
    "journal_ref": null,
    "citation_count": 32,
    "bibtex": "@Article{Lanka2018ARCHERAR,\n author = {Sameera Lanka and Tianfu Wu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ARCHER: Aggressive Rewards to Counter bias in Hindsight Experience Replay},\n volume = {abs/1809.02070},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "lanka2018"
  },
  {
    "id": "http://arxiv.org/abs/2203.00874v2",
    "title": "Follow your Nose: Using General Value Functions for Directed Exploration in Reinforcement Learning",
    "published": "2022-03-02T05:14:11Z",
    "updated": "2023-02-27T16:58:18Z",
    "authors": [
      "Durgesh Kalwar",
      "Omkar Shelke",
      "Somjit Nath",
      "Hardik Meisheri",
      "Harshad Khadilkar"
    ],
    "summary": "Improving sample efficiency is a key challenge in reinforcement learning, especially in environments with large state spaces and sparse rewards. In literature, this is resolved either through the use of auxiliary tasks (subgoals) or through clever exploration strategies. Exploration methods have been used to sample better trajectories in large environments while auxiliary tasks have been incorporated where the reward is sparse. However, few studies have attempted to tackle both large scale and reward sparsity at the same time. This paper explores the idea of combining exploration with auxiliary task learning using General Value Functions (GVFs) and a directed exploration strategy. We present a way to learn value functions which can be used to sample actions and provide directed exploration. Experiments on navigation tasks with varying grid sizes demonstrate the performance advantages over several competitive baselines.",
    "pdf_url": "https://arxiv.org/pdf/2203.00874v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Nath2022FollowYN,\n author = {Somjit Nath and Omkar Shelke and Durgesh Kalwar and Hardik Meisheri and H. Khadilkar},\n booktitle = {Adaptive Agents and Multi-Agent Systems},\n journal = {ArXiv},\n title = {Follow your Nose: Using General Value Functions for Directed Exploration in Reinforcement Learning},\n volume = {abs/2203.00874},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "kalwar2022"
  },
  {
    "id": "http://arxiv.org/abs/2404.00651v1",
    "title": "Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration",
    "published": "2024-03-31T11:39:11Z",
    "updated": "2024-03-31T11:39:11Z",
    "authors": [
      "Yibo Wang",
      "Jiang Zhao"
    ],
    "summary": "Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive experiments, our method shows competitive or even superior performance compared to prior works, especially the sparse reward cases.",
    "pdf_url": "https://arxiv.org/pdf/2404.00651v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Wang2024LearningOW,\n author = {Yibo Wang and Jiang Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration},\n volume = {abs/2404.00651},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wang2024"
  },
  {
    "id": "http://arxiv.org/abs/2004.00530v1",
    "title": "Learning Sparse Rewarded Tasks from Sub-Optimal Demonstrations",
    "published": "2020-04-01T15:57:15Z",
    "updated": "2020-04-01T15:57:15Z",
    "authors": [
      "Zhuangdi Zhu",
      "Kaixiang Lin",
      "Bo Dai",
      "Jiayu Zhou"
    ],
    "summary": "Model-free deep reinforcement learning (RL) has demonstrated its superiority on many complex sequential decision-making problems. However, heavy dependence on dense rewards and high sample-complexity impedes the wide adoption of these methods in real-world scenarios. On the other hand, imitation learning (IL) learns effectively in sparse-rewarded tasks by leveraging the existing expert demonstrations. In practice, collecting a sufficient amount of expert demonstrations can be prohibitively expensive, and the quality of demonstrations typically limits the performance of the learning policy. In this work, we propose Self-Adaptive Imitation Learning (SAIL) that can achieve (near) optimal performance given only a limited number of sub-optimal demonstrations for highly challenging sparse reward tasks. SAIL bridges the advantages of IL and RL to reduce the sample complexity substantially, by effectively exploiting sup-optimal demonstrations and efficiently exploring the environment to surpass the demonstrated performance. Extensive empirical results show that not only does SAIL significantly improve the sample-efficiency but also leads to much better final performance across different continuous control tasks, comparing to the state-of-the-art.",
    "pdf_url": "https://arxiv.org/pdf/2004.00530v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 15,
    "bibtex": "@Article{Zhu2020LearningSR,\n author = {Zhuangdi Zhu and Kaixiang Lin and Bo Dai and Jiayu Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning Sparse Rewarded Tasks from Sub-Optimal Demonstrations},\n volume = {abs/2004.00530},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "zhu2020"
  },
  {
    "id": "http://arxiv.org/abs/1711.06006v3",
    "title": "Hindsight policy gradients",
    "published": "2017-11-16T10:05:31Z",
    "updated": "2019-02-20T10:46:44Z",
    "authors": [
      "Paulo Rauber",
      "Avinash Ummadisingu",
      "Filipe Mutz",
      "Juergen Schmidhuber"
    ],
    "summary": "A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen goals, such policies may also enable higher-level planning based on subgoals. In sparse-reward environments, the capacity to exploit information about the degree to which an arbitrary goal has been achieved while another goal was intended appears crucial to enable sample efficient learning. However, reinforcement learning agents have only recently been endowed with such capacity for hindsight. In this paper, we demonstrate how hindsight can be introduced to policy gradient methods, generalizing this idea to a broad class of successful algorithms. Our experiments on a diverse selection of sparse-reward environments show that hindsight leads to a remarkable increase in sample efficiency.",
    "pdf_url": "https://arxiv.org/pdf/1711.06006v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR 2019",
    "journal_ref": null,
    "citation_count": 74,
    "bibtex": "@Article{Rauber2017HindsightPG,\n author = {Paulo E. Rauber and Filipe Wall Mutz and J. Schmidhuber},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Hindsight policy gradients},\n volume = {abs/1711.06006},\n year = {2017}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "rauber2017"
  },
  {
    "id": "http://arxiv.org/abs/2207.09071v2",
    "title": "Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks",
    "published": "2022-07-19T04:58:06Z",
    "updated": "2022-07-20T14:30:50Z",
    "authors": [
      "Yijie Guo",
      "Qiucheng Wu",
      "Honglak Lee"
    ],
    "summary": "Meta reinforcement learning (meta-RL) aims to learn a policy solving a set of training tasks simultaneously and quickly adapting to new tasks. It requires massive amounts of data drawn from training tasks to infer the common structure shared among tasks. Without heavy reward engineering, the sparse rewards in long-horizon tasks exacerbate the problem of sample efficiency in meta-RL. Another challenge in meta-RL is the discrepancy of difficulty level among tasks, which might cause one easy task dominating learning of the shared policy and thus preclude policy adaptation to new tasks. This work introduces a novel objective function to learn an action translator among training tasks. We theoretically verify that the value of the transferred policy with the action translator can be close to the value of the source policy and our objective function (approximately) upper bounds the value difference. We propose to combine the action translator with context-based meta-RL algorithms for better data collection and more efficient exploration during meta-training. Our approach empirically improves the sample efficiency and performance of meta-RL algorithms on sparse-reward tasks.",
    "pdf_url": "https://arxiv.org/pdf/2207.09071v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in AAAI 2022",
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Guo2022LearningAT,\n author = {Yijie Guo and Qiucheng Wu and Honglak Lee},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Learning Action Translator for Meta Reinforcement Learning on Sparse-Reward Tasks},\n volume = {abs/2207.09071},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "guo2022"
  },
  {
    "id": "http://arxiv.org/abs/2408.07877v5",
    "title": "BCR-DRL: Behavior- and Context-aware Reward for Deep Reinforcement Learning in Human-AI Coordination",
    "published": "2024-08-15T01:33:06Z",
    "updated": "2025-08-01T00:09:26Z",
    "authors": [
      "Xin Hao",
      "Bahareh Nakisa",
      "Mohmmad Naim Rastgoo",
      "Gaoyang Pang"
    ],
    "summary": "Deep reinforcement Learning (DRL) offers a powerful framework for training AI agents to coordinate with human partners. However, DRL faces two critical challenges in human-AI coordination (HAIC): sparse rewards and unpredictable human behaviors. These challenges significantly limit DRL to identify effective coordination policies, due to its impaired capability of optimizing exploration and exploitation. To address these limitations, we propose an innovative behavior- and context-aware reward (BCR) for DRL, which optimizes exploration and exploitation by leveraging human behaviors and contextual information in HAIC. Our BCR consists of two components: (i) A novel dual intrinsic rewarding scheme to enhance exploration. This scheme composes an AI self-motivated intrinsic reward and a human-motivated intrinsic reward, which are designed to increase the capture of sparse rewards by a logarithmic-based strategy; and (ii) A new context-aware weighting mechanism for the designed rewards to improve exploitation. This mechanism helps the AI agent prioritize actions that better coordinate with the human partner by utilizing contextual information that can reflect the evolution of learning. Extensive simulations in the Overcooked environment demonstrate that our approach can increase the cumulative sparse rewards by approximately 20%, and improve the sample efficiency by around 38% compared to state-of-the-art baselines.",
    "pdf_url": "https://arxiv.org/pdf/2408.07877v5",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": "The 28th European Conference on Artificial Intelligence (ECAI 2025)",
    "citation_count": 1,
    "bibtex": "@Inproceedings{Hao2024BCRDRLBA,\n author = {Xin Hao and Bahareh Nakisa and Mohmmad Naim Rastgoo and Gaoyang Pang},\n title = {BCR-DRL: Behavior- and Context-aware Reward for Deep Reinforcement Learning in Human-AI Coordination},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hao2024"
  },
  {
    "id": "http://arxiv.org/abs/2508.18420v1",
    "title": "LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning",
    "published": "2025-08-25T19:10:58Z",
    "updated": "2025-08-25T19:10:58Z",
    "authors": [
      "André Quadros",
      "Cassio Silva",
      "Ronnie Alves"
    ],
    "summary": "This paper explores the combination of two intrinsic motivation strategies to improve the efficiency of reinforcement learning (RL) agents in environments with extreme sparse rewards, where traditional learning struggles due to infrequent positive feedback. We propose integrating Variational State as Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward state novelty, with an intrinsic reward approach derived from Large Language Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward signals based on environment and goal descriptions, guiding the agent. We implemented this combined approach with an Actor-Critic (A2C) agent in the MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical results show that this combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent, which failed to learn. Analysis of learning curves indicates that the combination effectively complements different aspects of the environment and task: VSIMR drives exploration of new states, while the LLM-derived rewards facilitate progressive exploitation towards goals.",
    "pdf_url": "https://arxiv.org/pdf/2508.18420v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 5 figures, Accepted to the ENIAC 2025 conference",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Quadros2025LLMDrivenIM,\n author = {Andr'e Quadros and Cassio Silva and Ronnie Alves},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning},\n volume = {abs/2508.18420},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "quadros2025"
  },
  {
    "id": "http://arxiv.org/abs/2212.02190v3",
    "title": "L2SR: Learning to Sample and Reconstruct for Accelerated MRI via Reinforcement Learning",
    "published": "2022-12-05T11:54:12Z",
    "updated": "2024-04-06T04:59:12Z",
    "authors": [
      "Pu Yang",
      "Bin Dong"
    ],
    "summary": "Magnetic Resonance Imaging (MRI) is a widely used medical imaging technique, but its long acquisition time can be a limiting factor in clinical settings. To address this issue, researchers have been exploring ways to reduce the acquisition time while maintaining the reconstruction quality. Previous works have focused on finding either sparse samplers with a fixed reconstructor or finding reconstructors with a fixed sampler. However, these approaches do not fully utilize the potential of joint learning of samplers and reconstructors. In this paper, we propose an alternating training framework for jointly learning a good pair of samplers and reconstructors via deep reinforcement learning (RL). In particular, we consider the process of MRI sampling as a sampling trajectory controlled by a sampler, and introduce a novel sparse-reward Partially Observed Markov Decision Process (POMDP) to formulate the MRI sampling trajectory. Compared to the dense-reward POMDP used in existing works, the proposed sparse-reward POMDP is more computationally efficient and has a provable advantage. Moreover, the proposed framework, called L2SR (Learning to Sample and Reconstruct), overcomes the training mismatch problem that arises in previous methods that use dense-reward POMDP. By alternately updating samplers and reconstructors, L2SR learns a pair of samplers and reconstructors that achieve state-of-the-art reconstruction performances on the fastMRI dataset. Codes are available at \\url{https://github.com/yangpuPKU/L2SR-Learning-to-Sample-and-Reconstruct}.",
    "pdf_url": "https://arxiv.org/pdf/2212.02190v3",
    "doi": null,
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Yang2022L2SRLT,\n author = {Pu Yang and Bin Dong},\n booktitle = {Inverse Problems},\n journal = {Inverse Problems},\n title = {L2SR: learning to sample and reconstruct for accelerated MRI via reinforcement learning},\n volume = {40},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yang2022"
  },
  {
    "id": "http://arxiv.org/abs/1309.4291v2",
    "title": "Models and algorithms for skip-free Markov decision processes on trees",
    "published": "2013-09-17T12:58:40Z",
    "updated": "2013-11-08T11:30:43Z",
    "authors": [
      "E. J. Collins"
    ],
    "summary": "We introduce a class of models for multidimensional control problems which we call skip-free Markov decision processes on trees. We describe and analyse an algorithm applicable to Markov decision processes of this type that are skip-free in the negative direction. Starting with the finite average cost case, we show that the algorithm combines the advantages of both value iteration and policy iteration -- it is guaranteed to converge to an optimal policy and optimal value function after a finite number of iterations but the computational effort required for each iteration step is comparable with that for value iteration. We show that the algorithm can also be used to solve discounted cost models and continuous time models, and that a suitably modified algorithm can be used to solve communicating models.",
    "pdf_url": "https://arxiv.org/pdf/1309.4291v2",
    "doi": null,
    "categories": [
      "math.OC",
      "cs.AI",
      "math.PR"
    ],
    "primary_category": "math.OC",
    "comment": "v1: 20 pages Accepted for publication subject to minor changes by the Journal of the Operational Research Society (JORS); v2: 22 pages, 1 figure, revised title, example added",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Collins2013ModelsAA,\n author = {E. J. Collins},\n booktitle = {Journal of the Operational Research Society},\n journal = {Journal of the Operational Research Society},\n pages = {1595-1604},\n title = {Models and algorithms for skip-free Markov decision processes on trees},\n volume = {66},\n year = {2013}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "collins2013"
  },
  {
    "id": "http://arxiv.org/abs/2110.05442v1",
    "title": "Neural Algorithmic Reasoners are Implicit Planners",
    "published": "2021-10-11T17:29:20Z",
    "updated": "2021-10-11T17:29:20Z",
    "authors": [
      "Andreea Deac",
      "Petar Veličković",
      "Ognjen Milinković",
      "Pierre-Luc Bacon",
      "Jian Tang",
      "Mladen Nikolić"
    ],
    "summary": "Implicit planning has emerged as an elegant technique for combining learned models of the world with end-to-end model-free reinforcement learning. We study the class of implicit planners inspired by value iteration, an algorithm that is guaranteed to yield perfect policies in fully-specified tabular environments. We find that prior approaches either assume that the environment is provided in such a tabular form -- which is highly restrictive -- or infer \"local neighbourhoods\" of states to run value iteration over -- for which we discover an algorithmic bottleneck effect. This effect is caused by explicitly running the planning algorithm based on scalar predictions in every state, which can be harmful to data efficiency if such scalars are improperly predicted. We propose eXecuted Latent Value Iteration Networks (XLVINs), which alleviate the above limitations. Our method performs all planning computations in a high-dimensional latent space, breaking the algorithmic bottleneck. It maintains alignment with value iteration by carefully leveraging neural graph-algorithmic reasoning and contrastive self-supervised learning. Across eight low-data settings -- including classical control, navigation and Atari -- XLVINs provide significant improvements to data efficiency against value iteration-based implicit planners, as well as relevant model-free baselines. Lastly, we empirically verify that XLVINs can closely align with value iteration.",
    "pdf_url": "https://arxiv.org/pdf/2110.05442v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at NeurIPS 2021 (Spotlight talk). 20 pages, 10 figures. arXiv admin note: text overlap with arXiv:2010.13146",
    "journal_ref": null,
    "citation_count": 26,
    "bibtex": "@Article{Deac2021NeuralAR,\n author = {Andreea Deac and Petar Velivckovi'c and Ognjen Milinkovi'c and Pierre-Luc Bacon and Jian Tang and Mladen Nikolic},\n booktitle = {Neural Information Processing Systems},\n pages = {15529-15542},\n title = {Neural Algorithmic Reasoners are Implicit Planners},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "deac2021"
  },
  {
    "id": "http://arxiv.org/abs/1807.05692v6",
    "title": "On SDEs with Lipschitz coefficients, driven by continuous, model-free martingales",
    "published": "2018-07-16T06:30:29Z",
    "updated": "2022-02-14T16:19:32Z",
    "authors": [
      "Lesiba Ch. Galane",
      "Rafał M. Łochowski",
      "Farai J. Mhlanga"
    ],
    "summary": "We prove the existence and uniqueness of solutions of SDEs with Lipschitz coefficients, driven by continuous, model-free martingales. The main tool in our reasoning is Picard's iterative procedure and a model-free version of the Burkholder-Davis-Gundy inequality for integrals driven by model-free, continuous martingales. We work with a new outer measure which assigns zero value exactly to those properties which are instantly blockable.",
    "pdf_url": "https://arxiv.org/pdf/1807.05692v6",
    "doi": null,
    "categories": [
      "q-fin.MF",
      "math.PR"
    ],
    "primary_category": "q-fin.MF",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Galane2018OnSW,\n author = {Lesiba. Ch. Galane and R. Lochowski and F. J. Mhlanga},\n booktitle = {Electronic Communications in Probability},\n journal = {Electronic Communications in Probability},\n title = {On SDEs with Lipschitz coefficients, driven by continuous, model-free martingales},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "galane2018"
  },
  {
    "id": "http://arxiv.org/abs/2507.00531v1",
    "title": "An inverse-free fixed-time stable dynamical system and its forward-Euler discretization for solving generalized absolute value equations",
    "published": "2025-07-01T07:49:39Z",
    "updated": "2025-07-01T07:49:39Z",
    "authors": [
      "Xuehua Li",
      "Linjie Chen",
      "Dongmei Yu",
      "Cairong Chen",
      "Deren Han"
    ],
    "summary": "An inverse-free dynamical system is proposed to solve the generalized absolute value equation (GAVE) within a fixed time, where the time of convergence is finite and is uniformly bounded for all initial points. Moreover, an iterative method obtained by using the forward-Euler discretization of the proposed dynamic model are developed and sufficient conditions which guarantee that the discrete iteration globally converge to an arbitrarily small neighborhood of the unique solution of GAVE within a finite number of iterative steps are given.",
    "pdf_url": "https://arxiv.org/pdf/2507.00531v1",
    "doi": null,
    "categories": [
      "math.NA",
      "math.OC"
    ],
    "primary_category": "math.NA",
    "comment": "14 pages",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Li2025AnIF,\n author = {Xuehua Li and Linjie Chen and D. Yu and Cairong Chen and Deren Han},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An inverse-free fixed-time stable dynamical system and its forward-Euler discretization for solving generalized absolute value equations},\n volume = {abs/2507.00531},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "li2025"
  },
  {
    "id": "http://arxiv.org/abs/2306.17320v1",
    "title": "Monotone iteration scheme for nonlinear PDEs in risk models",
    "published": "2023-06-29T22:07:32Z",
    "updated": "2023-06-29T22:07:32Z",
    "authors": [
      "Falko Baustian",
      "Jan Pospíšil",
      "Vladimír Švígler"
    ],
    "summary": "In this paper we study nonlinear partial differential equations (PDEs) that are used to model different value adjustments denoted generally as xVA. These adjustments are nowadays commonly added to the risk-free financial derivative values and the PDE approach allows their easy incorporation. The aim of this paper is to apply the method of monotone iterations with sub- and supersolutions to the nonlinear Black-Scholes-type equation that occurs especially in the counterparty risk models. We introduce a monotone iteration scheme with semi-explicit solution formulas for each iteration step. Moreover, we show that the problem greatly simplifies for contracts with non-negative payoffs. To show the viability of the approach we apply our method to the call option, the forward, and the gap option.",
    "pdf_url": "https://arxiv.org/pdf/2306.17320v1",
    "doi": null,
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Baustian2023MonotoneIS,\n author = {F. Baustian and Jan Posp'ivsil and Vladim'ir vSv'igler},\n title = {Monotone iteration scheme for nonlinear PDEs in risk models},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "baustian2023"
  },
  {
    "id": "http://arxiv.org/abs/2205.14211v1",
    "title": "KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal",
    "published": "2022-05-27T19:39:24Z",
    "updated": "2022-05-27T19:39:24Z",
    "authors": [
      "Tadashi Kozuno",
      "Wenhao Yang",
      "Nino Vieillard",
      "Toshinori Kitamura",
      "Yunhao Tang",
      "Jincheng Mei",
      "Pierre Ménard",
      "Mohammad Gheshlaghi Azar",
      "Michal Valko",
      "Rémi Munos",
      "Olivier Pietquin",
      "Matthieu Geist",
      "Csaba Szepesvári"
    ],
    "summary": "In this work, we consider and analyze the sample complexity of model-free reinforcement learning with a generative model. Particularly, we analyze mirror descent value iteration (MDVI) by Geist et al. (2019) and Vieillard et al. (2020a), which uses the Kullback-Leibler divergence and entropy regularization in its value and policy updates. Our analysis shows that it is nearly minimax-optimal for finding an $\\varepsilon$-optimal policy when $\\varepsilon$ is sufficiently small. This is the first theoretical result that demonstrates that a simple model-free algorithm without variance-reduction can be nearly minimax-optimal under the considered setting.",
    "pdf_url": "https://arxiv.org/pdf/2205.14211v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 6 figures",
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Kozuno2022KLEntropyRegularizedRW,\n author = {Tadashi Kozuno and Wenhao Yang and Nino Vieillard and Toshinori Kitamura and Yunhao Tang and Jincheng Mei and Pierre M'enard and M. G. Azar and M. Vaĺko and R. Munos and O. Pietquin and M. Geist and Csaba Szepesvari},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {KL-Entropy-Regularized RL with a Generative Model is Minimax Optimal},\n volume = {abs/2205.14211},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "kozuno2022"
  },
  {
    "id": "http://arxiv.org/abs/2310.19563v1",
    "title": "Data-driven optimal control via linear programming: boundedness guarantees",
    "published": "2023-10-30T14:17:54Z",
    "updated": "2023-10-30T14:17:54Z",
    "authors": [
      "Lucia Falconi",
      "Andrea Martinelli",
      "John Lygeros"
    ],
    "summary": "The linear programming (LP) approach is, together with value iteration and policy iteration, one of the three fundamental methods to solve optimal control problems in a dynamic programming setting. Despite its simple formulation, versatility, and predisposition to be employed in model-free settings, the LP approach has not enjoyed the same popularity as the other methods. The reason is the often poor scalability of the exact LP approach and the difficulty to obtain bounded solutions for a reasonable amount of constraints. We mitigate these issues here, by investigating fundamental geometric features of the LP and developing sufficient conditions to guarantee finite solutions with minimal constraints. In the model-free context, we show that boundedness can be guaranteed by a suitable choice of dataset and objective function.",
    "pdf_url": "https://arxiv.org/pdf/2310.19563v1",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Falconi2023DataDrivenOC,\n author = {Lucia Falconi and Andrea Martinelli and J. Lygeros},\n booktitle = {IEEE Transactions on Automatic Control},\n journal = {IEEE Transactions on Automatic Control},\n pages = {1683-1697},\n title = {Data-Driven Optimal Control via Linear Programming: Boundedness Guarantees},\n volume = {70},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "falconi2023"
  },
  {
    "id": "http://arxiv.org/abs/1412.8570v1",
    "title": "Stochastic Shortest Path Games and Q-Learning",
    "published": "2014-12-30T06:44:50Z",
    "updated": "2014-12-30T06:44:50Z",
    "authors": [
      "Huizhen Yu"
    ],
    "summary": "We consider a class of two-player zero-sum stochastic games with finite state and compact control spaces, which we call stochastic shortest path (SSP) games. They are undiscounted total cost stochastic dynamic games that have a cost-free termination state. Exploiting the close connection of these games to single-player SSP problems, we introduce novel model conditions under which we show that the SSP games have strong optimality properties, including the existence of a unique solution to the dynamic programming equation, the existence of optimal stationary policies, and the convergence of value and policy iteration. We then focus on finite state and control SSP games and the classical Q-learning algorithm for computing the value function. Q-learning is a model-free, asynchronous stochastic iterative algorithm. By the theory of stochastic approximation involving monotone nonexpansive mappings, it is known to converge when its associated dynamic programming equation has a unique solution and its iterates are bounded with probability one. For the SSP case, as the main result of this paper, we prove the boundedness of the Q-learning iterates under our proposed model conditions, thereby establishing completely the convergence of Q-learning for a broad class of total cost finite-space stochastic games.",
    "pdf_url": "https://arxiv.org/pdf/1412.8570v1",
    "doi": null,
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC",
    "comment": "Revision of an old technical report written in 2011; 39 pages",
    "journal_ref": null,
    "citation_count": 13,
    "bibtex": "@Article{Yu2014StochasticSP,\n author = {Huizhen Yu},\n journal = {arXiv: Optimization and Control},\n title = {Stochastic Shortest Path Games and Q-Learning},\n year = {2014}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yu2014"
  },
  {
    "id": "http://arxiv.org/abs/2011.14212v3",
    "title": "Approximate Midpoint Policy Iteration for Linear Quadratic Control",
    "published": "2020-11-28T20:22:10Z",
    "updated": "2022-02-15T18:58:42Z",
    "authors": [
      "Benjamin Gravell",
      "Iman Shames",
      "Tyler Summers"
    ],
    "summary": "We present a midpoint policy iteration algorithm to solve linear quadratic optimal control problems in both model-based and model-free settings. The algorithm is a variation of Newton's method, and we show that in the model-based setting it achieves cubic convergence, which is superior to standard policy iteration and policy gradient algorithms that achieve quadratic and linear convergence, respectively. We also demonstrate that the algorithm can be approximately implemented without knowledge of the dynamics model by using least-squares estimates of the state-action value function from trajectory data, from which policy improvements can be obtained. With sufficient trajectory data, the policy iterates converge cubically to approximately optimal policies, and this occurs with the same available sample budget as the approximate standard policy iteration. Numerical experiments demonstrate effectiveness of the proposed algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2011.14212v3",
    "doi": null,
    "categories": [
      "math.OC",
      "cs.LG",
      "eess.SY",
      "math.DS"
    ],
    "primary_category": "math.OC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Gravell2020ApproximateMP,\n author = {Benjamin J. Gravell and I. Shames and T. Summers},\n booktitle = {Conference on Learning for Dynamics & Control},\n journal = {ArXiv},\n title = {Approximate Midpoint Policy Iteration for Linear Quadratic Control},\n volume = {abs/2011.14212},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "gravell2020"
  },
  {
    "id": "http://arxiv.org/abs/2010.08920v2",
    "title": "Average-reward model-free reinforcement learning: a systematic review and literature mapping",
    "published": "2020-10-18T05:06:01Z",
    "updated": "2021-08-03T11:20:08Z",
    "authors": [
      "Vektor Dewanto",
      "George Dunn",
      "Ali Eshragh",
      "Marcus Gallagher",
      "Fred Roosta"
    ],
    "summary": "Reinforcement learning is important part of artificial intelligence. In this paper, we review model-free reinforcement learning that utilizes the average reward optimality criterion in the infinite horizon setting. Motivated by the solo survey by Mahadevan (1996a), we provide an updated review of work in this area and extend it to cover policy-iteration and function approximation methods (in addition to the value-iteration and tabular counterparts). We present a comprehensive literature mapping. We also identify and discuss opportunities for future work.",
    "pdf_url": "https://arxiv.org/pdf/2010.08920v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "36 pages, refined prelim and politer sections",
    "journal_ref": null,
    "citation_count": 36,
    "bibtex": "@Article{Dewanto2020AveragerewardMR,\n author = {Vektor Dewanto and George Dunn and A. Eshragh and M. Gallagher and Fred Roosta},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Average-reward model-free reinforcement learning: a systematic review and literature mapping},\n volume = {abs/2010.08920},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "dewanto2020"
  },
  {
    "id": "http://arxiv.org/abs/1910.02919v3",
    "title": "Multi-step Greedy Reinforcement Learning Algorithms",
    "published": "2019-10-07T17:20:25Z",
    "updated": "2020-07-13T00:00:32Z",
    "authors": [
      "Manan Tomar",
      "Yonathan Efroni",
      "Mohammad Ghavamzadeh"
    ],
    "summary": "Multi-step greedy policies have been extensively used in model-based reinforcement learning (RL), both when a model of the environment is available (e.g.,~in the game of Go) and when it is learned. In this paper, we explore their benefits in model-free RL, when employed using multi-step dynamic programming algorithms: $κ$-Policy Iteration ($κ$-PI) and $κ$-Value Iteration ($κ$-VI). These methods iteratively compute the next policy ($κ$-PI) and value function ($κ$-VI) by solving a surrogate decision problem with a shaped reward and a smaller discount factor. We derive model-free RL algorithms based on $κ$-PI and $κ$-VI in which the surrogate problem can be solved by any discrete or continuous action RL method, such as DQN and TRPO. We identify the importance of a hyper-parameter that controls the extent to which the surrogate problem is solved and suggest a way to set this parameter. When evaluated on a range of Atari and MuJoCo benchmark tasks, our results indicate that for the right range of $κ$, our algorithms outperform DQN and TRPO. This shows that our multi-step greedy algorithms are general enough to be applied over any existing RL algorithm and can significantly improve its performance.",
    "pdf_url": "https://arxiv.org/pdf/1910.02919v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2020",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Tomar2019MultistepGP,\n author = {Manan Tomar and Yonathan Efroni and M. Ghavamzadeh},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning},\n volume = {abs/1910.02919},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "tomar2019"
  },
  {
    "id": "http://arxiv.org/abs/1310.2075v1",
    "title": "Numerical Methods for a Nonlinear BVP Arising in Physical Oceanography",
    "published": "2013-10-08T10:23:55Z",
    "updated": "2013-10-08T10:23:55Z",
    "authors": [
      "Riccardo Fazio",
      "Alessandra Jannelli"
    ],
    "summary": "In this paper we report and compare the numerical results for an ocean circulation model obtained by the classical truncated boundary formulation, the free boundary approach and a quasi-uniform grid treatment of the problem. We apply a shooting method to the truncated boundary formulation and finite difference methods to both the free boundary approach and the quasi-uniform grid treatment. Using the shooting method, supplemented by the Newton's iterations, we show that the ocean circulation model cannot be considered as a simple test case. In fact, for this method we are forced to use as initial iterate a value close to the correct missing initial condition in order to be able to get a convergent numerical solution. The reported numerical results allow us to point out how the finite difference method with a quasi-uniform grid is the less demanding approach and that the free boundary approach provides a more reliable formulation than the classical truncated boundary formulation.",
    "pdf_url": "https://arxiv.org/pdf/1310.2075v1",
    "doi": null,
    "categories": [
      "math.NA",
      "math-ph"
    ],
    "primary_category": "math.NA",
    "comment": "25 pages, 12 figures, 5 tables",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Fazio2013NumericalMF,\n author = {Riccardo Fazio and Alessandra Jannelli},\n journal = {arXiv: Numerical Analysis},\n title = {Numerical Methods for a Nonlinear BVP Arising in Physical Oceanography},\n year = {2013}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "fazio2013"
  },
  {
    "id": "http://arxiv.org/abs/2203.06547v1",
    "title": "Model-free Value Iteration Algorithm for Continuous-time Stochastic Linear Quadratic Optimal Control Problems",
    "published": "2022-03-13T02:43:13Z",
    "updated": "2022-03-13T02:43:13Z",
    "authors": [
      "Guangchen Wang",
      "Heng Zhang"
    ],
    "summary": "This paper presents a novel value iteration (VI) algorithm for finding the optimal control for a kind of infinite-horizon stochastic linear quadratic (SLQ) problem with unknown systems. First, an off-line algorithm is estabilished to obtain the optimal feedback control of our problem. Then, based on the off-line algorithm, the VI-based model-free algorithm and its convergence proof is provided. The main feature of the model-free algorithm is that a stabilizing control is not needed to initiate the algorithm. Finally, we validate our results with a simulation example.",
    "pdf_url": "https://arxiv.org/pdf/2203.06547v1",
    "doi": null,
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Inproceedings{Wang2022ModelfreeVI,\n author = {Guangchen Wang and Heng Zhang},\n title = {Model-free Value Iteration Algorithm for Continuous-time Stochastic Linear Quadratic Optimal Control Problems},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wang2022"
  },
  {
    "id": "http://arxiv.org/abs/1804.06021v3",
    "title": "Model-Free Linear Quadratic Control via Reduction to Expert Prediction",
    "published": "2018-04-17T02:52:38Z",
    "updated": "2018-10-05T19:50:23Z",
    "authors": [
      "Yasin Abbasi-Yadkori",
      "Nevena Lazic",
      "Csaba Szepesvari"
    ],
    "summary": "Model-free approaches for reinforcement learning (RL) and continuous control find policies based only on past states and rewards, without fitting a model of the system dynamics. They are appealing as they are general purpose and easy to implement; however, they also come with fewer theoretical guarantees than model-based RL. In this work, we present a new model-free algorithm for controlling linear quadratic (LQ) systems, and show that its regret scales as $O(T^{ξ+2/3})$ for any small $ξ>0$ if time horizon satisfies $T>C^{1/ξ}$ for a constant $C$. The algorithm is based on a reduction of control of Markov decision processes to an expert prediction problem. In practice, it corresponds to a variant of policy iteration with forced exploration, where the policy in each phase is greedy with respect to the average of all previous value functions. This is the first model-free algorithm for adaptive control of LQ systems that provably achieves sublinear regret and has a polynomial computation cost. Empirically, our algorithm dramatically outperforms standard policy iteration, but performs worse than a model-based approach.",
    "pdf_url": "https://arxiv.org/pdf/1804.06021v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 96,
    "bibtex": "@Article{Abbasi-Yadkori2018ModelFreeLQ,\n author = {Yasin Abbasi-Yadkori and N. Lazic and Csaba Szepesvari},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {3108-3117},\n title = {Model-Free Linear Quadratic Control via Reduction to Expert Prediction},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "abbasiyadkori2018"
  },
  {
    "id": "http://arxiv.org/abs/2006.11901v5",
    "title": "Free-rider Attacks on Model Aggregation in Federated Learning",
    "published": "2020-06-21T20:20:38Z",
    "updated": "2021-02-22T14:33:08Z",
    "authors": [
      "Yann Fraboni",
      "Richard Vidal",
      "Marco Lorenzi"
    ],
    "summary": "Free-rider attacks against federated learning consist in dissimulating participation to the federated learning process with the goal of obtaining the final aggregated model without actually contributing with any data. This kind of attacks is critical in sensitive applications of federated learning, where data is scarce and the model has high commercial value. We introduce here the first theoretical and experimental analysis of free-rider attacks on federated learning schemes based on iterative parameters aggregation, such as FedAvg or FedProx, and provide formal guarantees for these attacks to converge to the aggregated models of the fair participants. We first show that a straightforward implementation of this attack can be simply achieved by not updating the local parameters during the iterative federated optimization. As this attack can be detected by adopting simple countermeasures at the server level, we subsequently study more complex disguising schemes based on stochastic updates of the free-rider parameters. We demonstrate the proposed strategies on a number of experimental scenarios, in both iid and non-iid settings. We conclude by providing recommendations to avoid free-rider attacks in real world applications of federated learning, especially in sensitive domains where security of data and models is critical.",
    "pdf_url": "https://arxiv.org/pdf/2006.11901v5",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 150,
    "bibtex": "@Article{Fraboni2020FreeriderAO,\n author = {Yann Fraboni and Richard Vidal and Marco Lorenzi},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n journal = {ArXiv},\n title = {Free-rider Attacks on Model Aggregation in Federated Learning},\n volume = {abs/2006.11901},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "fraboni2020"
  },
  {
    "id": "http://arxiv.org/abs/1904.03535v1",
    "title": "Randomised Bayesian Least-Squares Policy Iteration",
    "published": "2019-04-06T21:50:24Z",
    "updated": "2019-04-06T21:50:24Z",
    "authors": [
      "Nikolaos Tziortziotis",
      "Christos Dimitrakakis",
      "Michalis Vazirgiannis"
    ],
    "summary": "We introduce Bayesian least-squares policy iteration (BLSPI), an off-policy, model-free, policy iteration algorithm that uses the Bayesian least-squares temporal-difference (BLSTD) learning algorithm to evaluate policies. An online variant of BLSPI has been also proposed, called randomised BLSPI (RBLSPI), that improves its policy based on an incomplete policy evaluation step. In online setting, the exploration-exploitation dilemma should be addressed as we try to discover the optimal policy by using samples collected by ourselves. RBLSPI exploits the advantage of BLSTD to quantify our uncertainty about the value function. Inspired by Thompson sampling, RBLSPI first samples a value function from a posterior distribution over value functions, and then selects actions based on the sampled value function. The effectiveness and the exploration abilities of RBLSPI are demonstrated experimentally in several environments.",
    "pdf_url": "https://arxiv.org/pdf/1904.03535v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "European Workshop on Reinforcement Learning 14, October 2018, Lille, France",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Tziortziotis2019RandomisedBL,\n author = {Nikolaos Tziortziotis and Christos Dimitrakakis and M. Vazirgiannis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Randomised Bayesian Least-Squares Policy Iteration},\n volume = {abs/1904.03535},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "tziortziotis2019"
  },
  {
    "id": "http://arxiv.org/abs/1604.02080v1",
    "title": "Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes",
    "published": "2016-04-07T17:12:07Z",
    "updated": "2016-04-07T17:12:07Z",
    "authors": [
      "Jordi Grau-Moya",
      "Felix Leibfried",
      "Tim Genewein",
      "Daniel A. Braun"
    ],
    "summary": "Information-theoretic principles for learning and acting have been proposed to solve particular classes of Markov Decision Problems. Mathematically, such approaches are governed by a variational free energy principle and allow solving MDP planning problems with information-processing constraints expressed in terms of a Kullback-Leibler divergence with respect to a reference distribution. Here we consider a generalization of such MDP planners by taking model uncertainty into account. As model uncertainty can also be formalized as an information-processing constraint, we can derive a unified solution from a single generalized variational principle. We provide a generalized value iteration scheme together with a convergence proof. As limit cases, this generalized scheme includes standard value iteration with a known model, Bayesian MDP planning, and robust planning. We demonstrate the benefits of this approach in a grid world simulation.",
    "pdf_url": "https://arxiv.org/pdf/1604.02080v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 28,
    "bibtex": "@Article{Grau-Moya2016PlanningWI,\n author = {Jordi Grau-Moya and Felix Leibfried and Tim Genewein and Daniel A. Braun},\n booktitle = {ECML/PKDD},\n journal = {ArXiv},\n title = {Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes},\n volume = {abs/1604.02080},\n year = {2016}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "graumoya2016"
  },
  {
    "id": "http://arxiv.org/abs/2003.00779v2",
    "title": "Data-Driven Control of Unknown Systems: A Linear Programming Approach",
    "published": "2020-03-02T11:44:25Z",
    "updated": "2020-03-30T14:52:48Z",
    "authors": [
      "Alexandros Tanzanakis",
      "John Lygeros"
    ],
    "summary": "We consider the problem of discounted optimal state-feedback regulation for general unknown deterministic discrete-time systems. It is well known that open-loop instability of systems, non-quadratic cost functions and complex nonlinear dynamics, as well as the on-policy behavior of many reinforcement learning (RL) algorithms, make the design of model-free optimal adaptive controllers a challenging task. We depart from commonly used least-squares and neural network approximation methods in conventional model-free control theory, and propose a novel family of data-driven optimization algorithms based on linear programming, off-policy Q-learning and randomized experience replay. We develop both policy iteration (PI) and value iteration (VI) methods to compute an approximate optimal feedback controller with high precision and without the knowledge of a system model and stage cost function. Simulation studies confirm the effectiveness of the proposed methods.",
    "pdf_url": "https://arxiv.org/pdf/2003.00779v2",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": null,
    "journal_ref": null,
    "citation_count": 12,
    "bibtex": "@Article{Tanzanakis2020DataDrivenCO,\n author = {Alexandros Tanzanakis and J. Lygeros},\n booktitle = {IFAC-PapersOnLine},\n journal = {ArXiv},\n title = {Data-Driven Control of Unknown Systems: A Linear Programming Approach},\n volume = {abs/2003.00779},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "tanzanakis2020"
  },
  {
    "id": "http://arxiv.org/abs/2010.13146v2",
    "title": "XLVIN: eXecuted Latent Value Iteration Nets",
    "published": "2020-10-25T16:04:30Z",
    "updated": "2020-12-06T16:59:01Z",
    "authors": [
      "Andreea Deac",
      "Petar Veličković",
      "Ognjen Milinković",
      "Pierre-Luc Bacon",
      "Jian Tang",
      "Mladen Nikolić"
    ],
    "summary": "Value Iteration Networks (VINs) have emerged as a popular method to incorporate planning algorithms within deep reinforcement learning, enabling performance improvements on tasks requiring long-range reasoning and understanding of environment dynamics. This came with several limitations, however: the model is not incentivised in any way to perform meaningful planning computations, the underlying state space is assumed to be discrete, and the Markov decision process (MDP) is assumed fixed and known. We propose eXecuted Latent Value Iteration Networks (XLVINs), which combine recent developments across contrastive self-supervised learning, graph representation learning and neural algorithmic reasoning to alleviate all of the above limitations, successfully deploying VIN-style models on generic environments. XLVINs match the performance of VIN-like models when the underlying MDP is discrete, fixed and known, and provides significant improvements to model-free baselines across three general MDP setups.",
    "pdf_url": "https://arxiv.org/pdf/2010.13146v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2020 Deep Reinforcement Learning Workshop",
    "journal_ref": null,
    "citation_count": 20,
    "bibtex": "@Article{Deac2020XLVINEL,\n author = {Andreea Deac and Petar Velivckovi'c and Ognjen Milinkovi'c and Pierre-Luc Bacon and Jian Tang and Mladen Nikolic},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {XLVIN: eXecuted Latent Value Iteration Nets},\n volume = {abs/2010.13146},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "deac2020"
  },
  {
    "id": "http://arxiv.org/abs/1810.06764v2",
    "title": "Simple Policy Evaluation for Data-Rich Iterative Tasks",
    "published": "2018-10-16T00:22:30Z",
    "updated": "2019-03-20T19:20:09Z",
    "authors": [
      "Ugo Rosolia",
      "Xiaojing Zhang",
      "Francesco Borrelli"
    ],
    "summary": "A data-based policy for iterative control task is presented. The proposed strategy is model-free and can be applied whenever safe input and state trajectories of a system performing an iterative task are available. These trajectories, together with a user-defined cost function, are exploited to construct a piecewise affine approximation to the value function. Approximated value functions are then used to evaluate the control policy by solving a linear program. We show that for linear system subject to convex cost and constraints, the proposed strategy guarantees closed-loop constraint satisfaction and performance bounds on the closed-loop trajectory. We evaluate the proposed strategy in simulations and experiments, the latter carried out on the Berkeley Autonomous Race Car (BARC) platform. We show that the proposed strategy is able to reduce the computation time by one order of magnitude while achieving the same performance as our model-based control algorithm.",
    "pdf_url": "https://arxiv.org/pdf/1810.06764v2",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "submitted for conference publication",
    "journal_ref": null,
    "citation_count": 5,
    "bibtex": "@Article{Rosolia2018SimplePE,\n author = {Ugo Rosolia and Xiaojing Zhang and F. Borrelli},\n booktitle = {American Control Conference},\n journal = {2019 American Control Conference (ACC)},\n pages = {2855-2860},\n title = {Simple Policy Evaluation for Data-Rich Iterative Tasks},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "rosolia2018"
  },
  {
    "id": "http://arxiv.org/abs/2406.03884v2",
    "title": "Steady supersonic combustion flows with a contact discontinuity in two-dimensional finitely long nozzles",
    "published": "2024-06-06T09:23:20Z",
    "updated": "2024-06-09T13:46:29Z",
    "authors": [
      "Junlei Gao",
      "Feimin Huang",
      "Jie Kuang",
      "Dehua Wang",
      "Wei Xiang"
    ],
    "summary": "In this paper, we are concerned with the two-dimensional steady supersonic combustion flows with a contact discontinuity moving through a nozzle of finite length. Mathematically, it can be formulated as a free boundary value problem governed by the two -dimensional steady combustion Euler equations with a contact discontinuity as the free boundary. The main mathematical difficulties are that the contact discontinuity is a characteristic free boundary and the equations for all states are coupled with each other due to the combustion process. We first employ the Lagrangian coordinate transformation to fix the free boundary. Then by introducing the flow slope and Bernoulli function, we further reduce the fixed boundary value problem into an initial boundary value problem for a first order hyperbolic system coupled with several ordinary differential equations. A new iteration scheme is developed near the background states by employing the intrinsic structure of the equation for the mass fraction of the non-combustion gas. We show that there is a fixed point for the iteration by deriving some novel $C^{1,α}$-estimates of the solutions and applying the fixed point theorem, and then the uniqueness of the fixed point is proved by a contraction argument. On the other hand, a quasi-one-dimensional approximate system is often used to simplify the two-dimensional steady supersonic combustion model. The error between these two systems is estimated. Finally, given a piece-wise $C^{1,α}$-solution containing a contact discontinuity with piece-wise constant states on the entrance of the nozzle, we can show that the solution is the piece-wise constant states with a straight contact discontinuity.",
    "pdf_url": "https://arxiv.org/pdf/2406.03884v2",
    "doi": null,
    "categories": [
      "math.AP",
      "math-ph"
    ],
    "primary_category": "math.AP",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Gao2024SteadySC,\n author = {Junlei Gao and Feimin Huang and Jie Kuang and Dehua Wang and Wei Xiang},\n title = {Steady supersonic combustion flows with a contact discontinuity in two-dimensional finitely long nozzles},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "gao2024"
  },
  {
    "id": "http://arxiv.org/abs/2311.17855v1",
    "title": "Maximum Entropy Model Correction in Reinforcement Learning",
    "published": "2023-11-29T18:00:41Z",
    "updated": "2023-11-29T18:00:41Z",
    "authors": [
      "Amin Rakhsha",
      "Mete Kemertas",
      "Mohammad Ghavamzadeh",
      "Amir-massoud Farahmand"
    ],
    "summary": "We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the MaxEnt Model Correction (MoCo) procedure that corrects the model's next-state distributions based on a Maximum Entropy density estimation formulation. Based on MoCo, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna's convergence can be much faster than the conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.",
    "pdf_url": "https://arxiv.org/pdf/2311.17855v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Rakhsha2023MaximumEM,\n author = {Amin Rakhsha and Mete Kemertas and M. Ghavamzadeh and Amir-massoud Farahmand},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Maximum Entropy Model Correction in Reinforcement Learning},\n volume = {abs/2311.17855},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "rakhsha2023"
  },
  {
    "id": "http://arxiv.org/abs/2312.02498v2",
    "title": "Provable Reinforcement Learning for Networked Control Systems with Stochastic Packet Disordering",
    "published": "2023-12-05T04:59:28Z",
    "updated": "2023-12-12T02:34:50Z",
    "authors": [
      "Wenqian Xue",
      "Yi Jiang",
      "Frank L. Lewis",
      "Bosen Lian"
    ],
    "summary": "This paper formulates a stochastic optimal control problem for linear networked control systems featuring stochastic packet disordering with a unique stabilizing solution certified. The problem is solved by proposing reinforcement learning algorithms. A measurement method is first presented to deal with PD and calculate the newest control input. The NCSs with stochastic PD are modeled as stochastic NCSs. Then, given a cost function, a modified algebraic Riccati equation is derived within the formulation. We propose offline policy iteration and value iteration algorithms to solve the MARE associated with provable convergence. These two algorithms require knowledge of NCS dynamics and PD probabilities. To release that, we further design online model-free off-policy and Q-learning algorithms with an online estimation method for PD probability. Both model-free algorithms solve the optimal control problem using real-time system states, control inputs, and PD probability estimates. Simulation results verify the proposed formulation and algorithms at last.",
    "pdf_url": "https://arxiv.org/pdf/2312.02498v2",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "This is a wrong version with problem setting and description errors in main sections",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Xue2023ProvableRL,\n author = {Wenqian Xue and Yi Jiang and F. L. Lewis and Bosen Lian},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Provable Reinforcement Learning for Networked Control Systems with Stochastic Packet Disordering},\n volume = {abs/2312.02498},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "xue2023"
  },
  {
    "id": "http://arxiv.org/abs/2105.09006v1",
    "title": "Online Adaptive Optimal Control Algorithm Based on Synchronous Integral Reinforcement Learning With Explorations",
    "published": "2021-05-19T09:15:50Z",
    "updated": "2021-05-19T09:15:50Z",
    "authors": [
      "Lei Guo",
      "Han Zhao"
    ],
    "summary": "In this paper, we present a novel algorithm named synchronous integral Q-learning, which is based on synchronous policy iteration, to solve the continuous-time infinite horizon optimal control problems of input-affine system dynamics. The integral reinforcement is measured as an excitation signal in this method to estimate the solution to the Hamilton-Jacobi-Bellman equation. Moreover, the proposed method is completely model-free, i.e. no a priori knowledge of the system is required. Using policy iteration, the actor and critic neural networks can simultaneously approximate the optimal value function and policy. The persistence of excitation condition is required to guarantee the convergence of the two networks. Unlike in traditional policy iteration algorithms, the restriction of the initial admissible policy is relaxed in this method. The effectiveness of the proposed algorithm is verified through numerical simulations.",
    "pdf_url": "https://arxiv.org/pdf/2105.09006v1",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": null,
    "journal_ref": null,
    "citation_count": 16,
    "bibtex": "@Article{Guo2021OnlineAO,\n author = {Lei Guo and Han Zhao},\n booktitle = {Neurocomputing},\n journal = {Neurocomputing},\n pages = {250-261},\n title = {Online Adaptive Optimal Control Algorithm Based on Synchronous Integral Reinforcement Learning With Explorations},\n volume = {520},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "guo2021"
  },
  {
    "id": "http://arxiv.org/abs/2403.19834v2",
    "title": "Online Feedback Optimization over Networks: A Distributed Model-free Approach",
    "published": "2024-03-28T21:07:27Z",
    "updated": "2024-09-12T12:03:58Z",
    "authors": [
      "Wenbin Wang",
      "Zhiyu He",
      "Giuseppe Belgioioso",
      "Saverio Bolognani",
      "Florian Dörfler"
    ],
    "summary": "Online feedback optimization (OFO) enables optimal steady-state operations of a physical system by employing an iterative optimization algorithm as a dynamic feedback controller. When the plant consists of several interconnected sub-systems, centralized implementations become impractical due to the heavy computational burden and the need to pre-compute system-wide sensitivities, which may not be easily accessible in practice. Motivated by these challenges, we develop a fully distributed model-free OFO controller, featuring consensus-based tracking of the global objective value and local iterative (projected) updates that use stochastic gradient estimates. We characterize how the closed-loop performance depends on the size of the network, the number of iterations, and the level of accuracy of consensus. Numerical simulations on a voltage control problem in a direct current power grid corroborate the theoretical findings.",
    "pdf_url": "https://arxiv.org/pdf/2403.19834v2",
    "doi": null,
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Wang2024OnlineFO,\n author = {Wenbin Wang and Zhiyu He and Giuseppe Belgioioso and S. Bolognani and Florian Dörfler},\n booktitle = {IEEE Conference on Decision and Control},\n journal = {2024 IEEE 63rd Conference on Decision and Control (CDC)},\n pages = {2403-2408},\n title = {Online Feedback Optimization over Networks: A Distributed Model-free Approach},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wang2024"
  },
  {
    "id": "http://arxiv.org/abs/2103.14606v1",
    "title": "A Convex Programming Approach to Data-Driven Risk-Averse Reinforcement Learning",
    "published": "2021-03-26T17:16:39Z",
    "updated": "2021-03-26T17:16:39Z",
    "authors": [
      "Yuzhen Han",
      "Majid Mazouchi",
      "Subramanya Nageshrao",
      "Hamidreza Modares"
    ],
    "summary": "This paper presents a model-free reinforcement learning (RL) algorithm to solve the risk-averse optimal control (RAOC) problem for discrete-time nonlinear systems. While successful RL algorithms have been presented to learn optimal control solutions under epistemic uncertainties (i.e., lack of knowledge of system dynamics), they do so by optimizing the expected utility of outcomes, which ignores the variance of cost under aleatory uncertainties (i.e., randomness). Performance-critical systems, however, must not only optimize the expected performance, but also reduce its variance to avoid performance fluctuation during RL's course of operation. To solve the RAOC problem, this paper presents the following three variants of RL algorithms and analyze their advantages and preferences for different situations/systems: 1) a one-shot static convex program -based RL, 2) an iterative value iteration (VI) algorithm that solves a linear programming (LP) optimization at each iteration, and 3) an iterative policy iteration (PI) algorithm that solves a convex optimization at each iteration and guarantees the stability of the consecutive control policies. Convergence of the exact optimization problems, which are infinite-dimensional in all three cases, to the optimal risk-averse value function is shown. To turn these optimization problems into standard optimization problems with finite decision variables and constraints, function approximation for value estimations as well as constraint sampling are leveraged. Data-driven implementations of these algorithms are provided based on Q-function which enables learning the optimal value without any knowledge of the system dynamics. The performance of the approximated solutions is also verified through a weighted sup-norm bound and the Lyapunov bound. A simulation example is provided to verify the effectiveness of the presented approach.",
    "pdf_url": "https://arxiv.org/pdf/2103.14606v1",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": null,
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Han2021ACP,\n author = {Yuzhen Han and Majid Mazouchi and S. Nageshrao and H. Modares},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Convex Programming Approach to Data-Driven Risk-Averse Reinforcement Learning},\n volume = {abs/2103.14606},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "han2021"
  },
  {
    "id": "http://arxiv.org/abs/1802.07935v2",
    "title": "Asynchronous stochastic approximations with asymptotically biased errors and deep multi-agent learning",
    "published": "2018-02-22T08:13:53Z",
    "updated": "2019-05-02T08:24:49Z",
    "authors": [
      "Arunselvan Ramaswamy",
      "Shalabh Bhatnagar",
      "Daniel E. Quevedo"
    ],
    "summary": "Asynchronous stochastic approximations (SAs) are an important class of model-free algorithms, tools and techniques that are popular in multi-agent and distributed control scenarios. To counter Bellman's curse of dimensionality, such algorithms are coupled with function approximations. Although the learning/ control problem becomes more tractable, function approximations affect stability and convergence. In this paper, we present verifiable sufficient conditions for stability and convergence of asynchronous SAs with biased approximation errors. The theory developed herein is used to analyze Policy Gradient methods and noisy Value Iteration schemes. Specifically, we analyze the asynchronous approximate counterparts of the policy gradient (A2PG) and value iteration (A2VI) schemes. It is shown that the stability of these algorithms is unaffected by biased approximation errors, provided they are asymptotically bounded. With respect to convergence (of A2VI and A2PG), a relationship between the limiting set and the approximation errors is established. Finally, experimental results are presented that support the theory.",
    "pdf_url": "https://arxiv.org/pdf/1802.07935v2",
    "doi": null,
    "categories": [
      "math.OC",
      "math.DS",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Inproceedings{Ramaswamy2018AsynchronousSA,\n author = {Arunselvan Ramaswamy and S. Bhatnagar and D. Quevedo},\n title = {Asynchronous stochastic approximations with asymptotically biased errors and deep multi-agent learning},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ramaswamy2018"
  },
  {
    "id": "http://arxiv.org/abs/1706.04711v2",
    "title": "Reinforcement Learning under Model Mismatch",
    "published": "2017-06-15T01:06:05Z",
    "updated": "2017-11-09T01:09:29Z",
    "authors": [
      "Aurko Roy",
      "Huan Xu",
      "Sebastian Pokutta"
    ],
    "summary": "We study reinforcement learning under model misspecification, where we do not have access to the true environment but only to a reasonably close approximation to it. We address this problem by extending the framework of robust MDPs to the model-free Reinforcement Learning setting, where we do not have access to the model parameters, but can only sample states from it. We define robust versions of Q-learning, SARSA, and TD-learning and prove convergence to an approximately optimal robust policy and approximate value function respectively. We scale up the robust algorithms to large MDPs via function approximation and prove convergence under two different settings. We prove convergence of robust approximate policy iteration and robust approximate value iteration for linear architectures (under mild assumptions). We also define a robust loss function, the mean squared robust projected Bellman error and give stochastic gradient descent algorithms that are guaranteed to converge to a local minimum.",
    "pdf_url": "https://arxiv.org/pdf/1706.04711v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in Proceedings of NIPS 2017",
    "journal_ref": null,
    "citation_count": 86,
    "bibtex": "@Article{Roy2017ReinforcementLU,\n author = {Aurko Roy and Huan Xu and S. Pokutta},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Reinforcement Learning under Model Mismatch},\n volume = {abs/1706.04711},\n year = {2017}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "roy2017"
  },
  {
    "id": "http://arxiv.org/abs/2506.19940v2",
    "title": "Strong convergence to operator-valued semicirculars",
    "published": "2025-06-24T18:31:23Z",
    "updated": "2025-09-27T07:04:51Z",
    "authors": [
      "David Jekel",
      "Yoonkyeong Lee",
      "Brent Nelson",
      "Jennifer Pi"
    ],
    "summary": "We establish a framework for weak and strong convergence of matrix models to operator-valued semicircular systems parametrized by operator-valued covariance matrices $η= (η_{i,j})_{i,j \\in I}$. Non-commutative polynomials are replaced by covariance polynomials that can involve iterated applications of $η_{i,j}$, leading to the notion of covariance laws. We give sufficient conditions for weak and strong convergence of general Gaussian random matrices and deterministic matrices to a $B$-valued semicircular family and generators of the base algebra $B$. In particular, we obtain operator-valued strong convergence for continuously weighted Gaussian Wigner matrices, such as Gaussian band matrices with a continuous cutoff, and we construct natural strongly convergent matrix models for interpolated free group factors.",
    "pdf_url": "https://arxiv.org/pdf/2506.19940v2",
    "doi": null,
    "categories": [
      "math.OA",
      "math.PR"
    ],
    "primary_category": "math.OA",
    "comment": "37 pages; updated article includes additional applications",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Inproceedings{Jekel2025StrongCT,\n author = {David Jekel and Yoonkyeong Lee and Brent Nelson and Jennifer Pi},\n title = {Strong convergence to operator-valued semicirculars},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "jekel2025"
  },
  {
    "id": "http://arxiv.org/abs/2305.10504v1",
    "title": "Model-Free Robust Average-Reward Reinforcement Learning",
    "published": "2023-05-17T18:19:23Z",
    "updated": "2023-05-17T18:19:23Z",
    "authors": [
      "Yue Wang",
      "Alvaro Velasquez",
      "George Atia",
      "Ashley Prater-Bennette",
      "Shaofeng Zou"
    ],
    "summary": "Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence and Wasserstein distance.",
    "pdf_url": "https://arxiv.org/pdf/2305.10504v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2023",
    "journal_ref": null,
    "citation_count": 21,
    "bibtex": "@Article{Wang2023ModelFreeRA,\n author = {Yue Wang and Alvaro Velasquez and George K. Atia and Ashley Prater-Bennette and Shaofeng Zou},\n booktitle = {International Conference on Machine Learning},\n pages = {36431-36469},\n title = {Model-Free Robust Average-Reward Reinforcement Learning},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wang2023"
  },
  {
    "id": "http://arxiv.org/abs/2304.10098v2",
    "title": "Two-Memory Reinforcement Learning",
    "published": "2023-04-20T05:39:25Z",
    "updated": "2023-04-23T09:29:57Z",
    "authors": [
      "Zhao Yang",
      "Thomas. M. Moerland",
      "Mike Preuss",
      "Aske Plaat"
    ],
    "summary": "While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that the 2M agent is more data efficient and outperforms both pure episodic memory and pure reinforcement learning, as well as a state-of-the-art memory-augmented RL agent. Moreover, the proposed approach provides a general framework that can be used to combine any episodic memory agent with other off-policy reinforcement learning algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2304.10098v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Yang2023TwoMemoryRL,\n author = {Zhao Yang and T. Moerland and M. Preuss and A. Plaat},\n booktitle = {2023 IEEE Conference on Games (CoG)},\n journal = {2023 IEEE Conference on Games (CoG)},\n pages = {1-9},\n title = {Two-Memory Reinforcement Learning},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yang2023"
  },
  {
    "id": "http://arxiv.org/abs/2111.02104v2",
    "title": "Model-Based Episodic Memory Induces Dynamic Hybrid Controls",
    "published": "2021-11-03T09:52:33Z",
    "updated": "2021-11-06T08:04:18Z",
    "authors": [
      "Hung Le",
      "Thommen Karimpanal George",
      "Majid Abdolshah",
      "Truyen Tran",
      "Svetha Venkatesh"
    ],
    "summary": "Episodic control enables sample efficiency in reinforcement learning by recalling past experiences from an episodic memory. We propose a new model-based episodic memory of trajectories addressing current limitations of episodic control. Our memory estimates trajectory values, guiding the agent towards good policies. Built upon the memory, we construct a complementary learning model via a dynamic hybrid control unifying model-based, episodic and habitual learning into a single architecture. Experiments demonstrate that our model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments including stochastic and non-Markovian settings.",
    "pdf_url": "https://arxiv.org/pdf/2111.02104v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages",
    "journal_ref": null,
    "citation_count": 23,
    "bibtex": "@Article{Le2021ModelBasedEM,\n author = {Hung Le and T. G. Karimpanal and Majid Abdolshah and T. Tran and S. Venkatesh},\n booktitle = {Neural Information Processing Systems},\n pages = {30313-30325},\n title = {Model-Based Episodic Memory Induces Dynamic Hybrid Controls},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "le2021"
  },
  {
    "id": "http://arxiv.org/abs/2112.14734v2",
    "title": "Sequential memory improves sample and memory efficiency in Episodic Control",
    "published": "2021-12-29T18:42:15Z",
    "updated": "2024-06-06T15:50:45Z",
    "authors": [
      "Ismael T. Freire",
      "Adrián F. Amil",
      "Paul F. M. J. Verschure"
    ],
    "summary": "State of the art deep reinforcement learning algorithms are sample inefficient due to the large number of episodes they require to achieve asymptotic performance. Episodic Reinforcement Learning (ERL) algorithms, inspired by the mammalian hippocampus, typically use extended memory systems to bootstrap learning from past events to overcome this sample-inefficiency problem. However, such memory augmentations are often used as mere buffers, from which isolated past experiences are drawn to learn from in an offline fashion (e.g., replay). Here, we demonstrate that including a bias in the acquired memory content derived from the order of episodic sampling improves both the sample and memory efficiency of an episodic control algorithm. We test our Sequential Episodic Control (SEC) model in a foraging task to show that storing and using integrated episodes as event sequences leads to faster learning with fewer memory requirements as opposed to a standard ERL benchmark, Model-Free Episodic Control, that buffers isolated events only. We also study the effect of memory constraints and forgetting on the sequential and non-sequential version of the SEC algorithm. Furthermore, we discuss how a hippocampal-like fast memory system could bootstrap slow cortical and subcortical learning subserving habit formation in the mammalian brain.",
    "pdf_url": "https://arxiv.org/pdf/2112.14734v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 8 figures",
    "journal_ref": null,
    "citation_count": 5,
    "bibtex": "@Article{Freire2021SequentialMI,\n author = {Ismael T. Freire and A. F. Amil and P. Verschure},\n booktitle = {Nature Machine Intelligence},\n journal = {Nature Machine Intelligence},\n pages = {43 - 55},\n title = {Sequential memory improves sample and memory efficiency in episodic control},\n volume = {7},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "freire2021"
  },
  {
    "id": "http://arxiv.org/abs/2211.15183v3",
    "title": "Continuous Episodic Control",
    "published": "2022-11-28T09:48:42Z",
    "updated": "2023-04-23T09:21:14Z",
    "authors": [
      "Zhao Yang",
      "Thomas M. Moerland",
      "Mike Preuss",
      "Aske Plaat"
    ],
    "summary": "Non-parametric episodic memory can be used to quickly latch onto high-rewarded experience in reinforcement learning tasks. In contrast to parametric deep reinforcement learning approaches in which reward signals need to be back-propagated slowly, these methods only need to discover the solution once, and may then repeatedly solve the task. However, episodic control solutions are stored in discrete tables, and this approach has so far only been applied to discrete action space problems. Therefore, this paper introduces Continuous Episodic Control (CEC), a novel non-parametric episodic memory algorithm for sequential decision making in problems with a continuous action space. Results on several sparse-reward continuous control environments show that our proposed method learns faster than state-of-the-art model-free RL and memory-augmented RL algorithms, while maintaining good long-run performance as well. In short, CEC can be a fast approach for learning in continuous control tasks.",
    "pdf_url": "https://arxiv.org/pdf/2211.15183v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Yang2022ContinuousEC,\n author = {Zhao Yang and Thomas M. Moerland and M. Preuss and Aske Plaat},\n booktitle = {2023 IEEE Conference on Games (CoG)},\n journal = {2023 IEEE Conference on Games (CoG)},\n pages = {1-8},\n title = {Continuous Episodic Control},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yang2022"
  },
  {
    "id": "http://arxiv.org/abs/2008.09685v1",
    "title": "Model-Free Episodic Control with State Aggregation",
    "published": "2020-08-21T21:20:49Z",
    "updated": "2020-08-21T21:20:49Z",
    "authors": [
      "Rafael Pinto"
    ],
    "summary": "Episodic control provides a highly sample-efficient method for reinforcement learning while enforcing high memory and computational requirements. This work proposes a simple heuristic for reducing these requirements, and an application to Model-Free Episodic Control (MFEC) is presented. Experiments on Atari games show that this heuristic successfully reduces MFEC computational demands while producing no significant loss of performance when conservative choices of hyperparameters are used. Consequently, episodic control becomes a more feasible option when dealing with reinforcement learning tasks.",
    "pdf_url": "https://arxiv.org/pdf/2008.09685v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 21 figures",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Pinto2020ModelFreeEC,\n author = {R. Pinto},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Model-Free Episodic Control with State Aggregation},\n volume = {abs/2008.09685},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "pinto2020"
  },
  {
    "id": "http://arxiv.org/abs/2111.03110v2",
    "title": "Successor Feature Neural Episodic Control",
    "published": "2021-11-04T19:14:43Z",
    "updated": "2023-08-02T20:21:49Z",
    "authors": [
      "David Emukpere",
      "Xavier Alameda-Pineda",
      "Chris Reinke"
    ],
    "summary": "A longstanding goal in reinforcement learning is to build intelligent agents that show fast learning and a flexible transfer of skills akin to humans and animals. This paper investigates the integration of two frameworks for tackling those goals: episodic control and successor features. Episodic control is a cognitively inspired approach relying on episodic memory, an instance-based memory model of an agent's experiences. Meanwhile, successor features and generalized policy improvement (SF&GPI) is a meta and transfer learning framework allowing to learn policies for tasks that can be efficiently reused for later tasks which have a different reward function. Individually, these two techniques have shown impressive results in vastly improving sample efficiency and the elegant reuse of previously learned policies. Thus, we outline a combination of both approaches in a single reinforcement learning framework and empirically illustrate its benefits.",
    "pdf_url": "https://arxiv.org/pdf/2111.03110v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Emukpere2021SuccessorFN,\n author = {David Emukpere and Xavier Alameda-Pineda and Chris Reinke},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Successor Feature Neural Episodic Control},\n volume = {abs/2111.03110},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "emukpere2021"
  },
  {
    "id": "http://arxiv.org/abs/2407.16034v1",
    "title": "Efficient Replay Memory Architectures in Multi-Agent Reinforcement Learning for Traffic Congestion Control",
    "published": "2024-07-22T20:20:04Z",
    "updated": "2024-07-22T20:20:04Z",
    "authors": [
      "Mukul Chodhary",
      "Kevin Octavian",
      "SooJean Han"
    ],
    "summary": "Episodic control, inspired by the role of episodic memory in the human brain, has been shown to improve the sample inefficiency of model-free reinforcement learning by reusing high-return past experiences. However, the memory growth of episodic control is undesirable in large-scale multi-agent problems such as vehicle traffic management. This paper proposes a novel replay memory architecture called Dual-Memory Integrated Learning, to augment to multi-agent reinforcement learning methods for congestion control via adaptive light signal scheduling. Our dual-memory architecture mimics two core capabilities of human decision-making. First, it relies on diverse types of memory--semantic and episodic, short-term and long-term--in order to remember high-return states that occur often in the network and filter out states that don't. Second, it employs equivalence classes to group together similar state-action pairs and that can be controlled using the same action (i.e., light signal sequence). Theoretical analyses establish memory growth bounds, and simulation experiments on several intersection networks showcase improved congestion performance (e.g., vehicle throughput) from our method.",
    "pdf_url": "https://arxiv.org/pdf/2407.16034v1",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Full version of accepted paper to IEEE Intelligent Transportation Systems Conference (ITSC) 2024",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Chodhary2024EfficientRM,\n author = {Mukul Chodhary and Kevin Octavian and SooJean Han},\n booktitle = {2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)},\n journal = {2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)},\n pages = {1932-1937},\n title = {Efficient Replay Memory Architectures in Multi-Agent Reinforcement Learning for Traffic Congestion Control},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "chodhary2024"
  },
  {
    "id": "http://arxiv.org/abs/2106.08832v1",
    "title": "Solving Continuous Control with Episodic Memory",
    "published": "2021-06-16T14:51:39Z",
    "updated": "2021-06-16T14:51:39Z",
    "authors": [
      "Igor Kuznetsov",
      "Andrey Filchenkov"
    ],
    "summary": "Episodic memory lets reinforcement learning algorithms remember and exploit promising experience from the past to improve agent performance. Previous works on memory mechanisms show benefits of using episodic-based data structures for discrete action problems in terms of sample-efficiency. The application of episodic memory for continuous control with a large action space is not trivial. Our study aims to answer the question: can episodic memory be used to improve agent's performance in continuous control? Our proposed algorithm combines episodic memory with Actor-Critic architecture by modifying critic's objective. We further improve performance by introducing episodic-based replay buffer prioritization. We evaluate our algorithm on OpenAI gym domains and show greater sample-efficiency compared with the state-of-the art model-free off-policy algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2106.08832v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021)",
    "journal_ref": null,
    "citation_count": 21,
    "bibtex": "@Article{Kuznetsov2021SolvingCC,\n author = {Igor Kuznetsov and A. Filchenkov},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Solving Continuous Control with Episodic Memory},\n volume = {abs/2106.08832},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "kuznetsov2021"
  },
  {
    "id": "http://arxiv.org/abs/2402.15160v3",
    "title": "Spatially-Aware Transformer for Embodied Agents",
    "published": "2024-02-23T07:46:30Z",
    "updated": "2024-03-01T00:58:50Z",
    "authors": [
      "Junmo Cho",
      "Jaesik Yoon",
      "Sungjin Ahn"
    ],
    "summary": "Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning. The source code for our models and experiments will be available at https://github.com/junmokane/spatially-aware-transformer.",
    "pdf_url": "https://arxiv.org/pdf/2402.15160v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024 Spotlight. First two authors contributed equally",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Cho2024SpatiallyAwareTF,\n author = {Junmo Cho and Jaesik Yoon and Sungjin Ahn},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Spatially-Aware Transformer for Embodied Agents},\n volume = {abs/2402.15160},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "cho2024"
  },
  {
    "id": "http://arxiv.org/abs/2112.01853v1",
    "title": "Episodic Policy Gradient Training",
    "published": "2021-12-03T11:15:32Z",
    "updated": "2021-12-03T11:15:32Z",
    "authors": [
      "Hung Le",
      "Majid Abdolshah",
      "Thommen K. George",
      "Kien Do",
      "Dung Nguyen",
      "Svetha Venkatesh"
    ],
    "summary": "We introduce a novel training procedure for policy gradient methods wherein episodic memory is used to optimize the hyperparameters of reinforcement learning algorithms on-the-fly. Unlike other hyperparameter searches, we formulate hyperparameter scheduling as a standard Markov Decision Process and use episodic memory to store the outcome of used hyperparameters and their training contexts. At any policy update step, the policy learner refers to the stored experiences, and adaptively reconfigures its learning algorithm with the new hyperparameters determined by the memory. This mechanism, dubbed as Episodic Policy Gradient Training (EPGT), enables an episodic learning process, and jointly learns the policy and the learning algorithm's hyperparameters within a single run. Experimental results on both continuous and discrete environments demonstrate the advantage of using the proposed method in boosting the performance of various policy gradient algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2112.01853v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages",
    "journal_ref": null,
    "citation_count": 7,
    "bibtex": "@Article{Le2021EpisodicPG,\n author = {Hung Le and Majid Abdolshah and T. G. Karimpanal and Kien Do and D. Nguyen and S. Venkatesh},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Episodic Policy Gradient Training},\n volume = {abs/2112.01853},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "le2021"
  },
  {
    "id": "http://arxiv.org/abs/2012.13779v1",
    "title": "Towards sample-efficient episodic control with DAC-ML",
    "published": "2020-12-26T16:38:08Z",
    "updated": "2020-12-26T16:38:08Z",
    "authors": [
      "Ismael T. Freire",
      "Adrián F. Amil",
      "Vasiliki Vouloutsi",
      "Paul F. M. J. Verschure"
    ],
    "summary": "The sample-inefficiency problem in Artificial Intelligence refers to the inability of current Deep Reinforcement Learning models to optimize action policies within a small number of episodes. Recent studies have tried to overcome this limitation by adding memory systems and architectural biases to improve learning speed, such as in Episodic Reinforcement Learning. However, despite achieving incremental improvements, their performance is still not comparable to how humans learn behavioral policies. In this paper, we capitalize on the design principles of the Distributed Adaptive Control (DAC) theory of mind and brain to build a novel cognitive architecture (DAC-ML) that, by incorporating a hippocampus-inspired sequential memory system, can rapidly converge to effective action policies that maximize reward acquisition in a challenging foraging task.",
    "pdf_url": "https://arxiv.org/pdf/2012.13779v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "q-bio.NC",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Freire2020TowardsSP,\n author = {Ismael T. Freire and A. F. Amil and V. Vouloutsi and P. Verschure},\n booktitle = {BICA*AI},\n pages = {256-262},\n title = {Towards sample-efficient policy learning with DAC-ML},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "freire2020"
  },
  {
    "id": "http://arxiv.org/abs/2410.10132v1",
    "title": "Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning",
    "published": "2024-10-14T03:50:17Z",
    "updated": "2024-10-14T03:50:17Z",
    "authors": [
      "Hung Le",
      "Kien Do",
      "Dung Nguyen",
      "Sunil Gupta",
      "Svetha Venkatesh"
    ],
    "summary": "Effective decision-making in partially observable environments demands robust memory management. Despite their success in supervised learning, current deep-learning memory models struggle in reinforcement learning environments that are partially observable and long-term. They fail to efficiently capture relevant past information, adapt flexibly to changing observations, and maintain stable updates over long episodes. We theoretically analyze the limitations of existing memory models within a unified framework and introduce the Stable Hadamard Memory, a novel memory model for reinforcement learning agents. Our model dynamically adjusts memory by erasing no longer needed experiences and reinforcing crucial ones computationally efficiently. To this end, we leverage the Hadamard product for calibrating and updating memory, specifically designed to enhance memory capacity while mitigating numerical and learning challenges. Our approach significantly outperforms state-of-the-art memory-based methods on challenging partially observable benchmarks, such as meta-reinforcement learning, long-horizon credit assignment, and POPGym, demonstrating superior performance in handling long-term and evolving contexts.",
    "pdf_url": "https://arxiv.org/pdf/2410.10132v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint 18 pages",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Le2024StableHM,\n author = {Hung Le and Kien Do and D. Nguyen and Sunil Gupta and S. Venkatesh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning},\n volume = {abs/2410.10132},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "le2024"
  },
  {
    "id": "http://arxiv.org/abs/2403.01112v2",
    "title": "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
    "published": "2024-03-02T07:37:05Z",
    "updated": "2024-03-07T13:40:04Z",
    "authors": [
      "Hyungho Na",
      "Yunkyeong Seo",
      "Il-chul Moon"
    ],
    "summary": "In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2403.01112v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2024",
    "journal_ref": null,
    "citation_count": 9,
    "bibtex": "@Article{Na2024EfficientEM,\n author = {Hyungho Na and Yunkyeong Seo and Il-Chul Moon},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning},\n volume = {abs/2403.01112},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "na2024"
  },
  {
    "id": "http://arxiv.org/abs/1711.06677v2",
    "title": "Is prioritized sweeping the better episodic control?",
    "published": "2017-11-20T07:47:12Z",
    "updated": "2018-08-09T20:25:43Z",
    "authors": [
      "Johanni Brea"
    ],
    "summary": "Episodic control has been proposed as a third approach to reinforcement learning, besides model-free and model-based control, by analogy with the three types of human memory. i.e. episodic, procedural and semantic memory. But the theoretical properties of episodic control are not well investigated. Here I show that in deterministic tree Markov decision processes, episodic control is equivalent to a form of prioritized sweeping in terms of sample efficiency as well as memory and computation demands. For general deterministic and stochastic environments, prioritized sweeping performs better even when memory and computation demands are restricted to be equal to those of episodic control. These results suggest generalizations of prioritized sweeping to partially observable environments, its combined use with function approximation and the search for possible implementations of prioritized sweeping in brains.",
    "pdf_url": "https://arxiv.org/pdf/1711.06677v2",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Brea2017IsPS,\n author = {Johanni Brea},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Is prioritized sweeping the better episodic control?},\n volume = {abs/1711.06677},\n year = {2017}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "brea2017"
  },
  {
    "id": "http://arxiv.org/abs/2110.09817v1",
    "title": "State-based Episodic Memory for Multi-Agent Reinforcement Learning",
    "published": "2021-10-19T09:39:19Z",
    "updated": "2021-10-19T09:39:19Z",
    "authors": [
      "Xiao Ma",
      "Wu-Jun Li"
    ],
    "summary": "Multi-agent reinforcement learning (MARL) algorithms have made promising progress in recent years by leveraging the centralized training and decentralized execution (CTDE) paradigm. However, existing MARL algorithms still suffer from the sample inefficiency problem. In this paper, we propose a simple yet effective approach, called state-based episodic memory (SEM), to improve sample efficiency in MARL. SEM adopts episodic memory (EM) to supervise the centralized training procedure of CTDE in MARL. To the best of our knowledge, SEM is the first work to introduce EM into MARL. We can theoretically prove that, when using for MARL, SEM has lower space complexity and time complexity than state and action based EM (SAEM), which is originally proposed for single-agent reinforcement learning. Experimental results on StarCraft multi-agent challenge (SMAC) show that introducing episodic memory into MARL can improve sample efficiency and SEM can reduce storage cost and time cost compared with SAEM.",
    "pdf_url": "https://arxiv.org/pdf/2110.09817v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Ma2021StatebasedEM,\n author = {Xiao Ma and Wu-Jun Li},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {5163-5190},\n title = {State-based episodic memory for multi-agent reinforcement learning},\n volume = {112},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ma2021"
  },
  {
    "id": "http://arxiv.org/abs/2212.02098v4",
    "title": "A Machine with Short-Term, Episodic, and Semantic Memory Systems",
    "published": "2022-12-05T08:34:23Z",
    "updated": "2024-11-14T22:21:46Z",
    "authors": [
      "Taewoon Kim",
      "Michael Cochez",
      "Vincent François-Lavet",
      "Mark Neerincx",
      "Piek Vossen"
    ],
    "summary": "Inspired by the cognitive science theory of the explicit human memory systems, we have modeled an agent with short-term, episodic, and semantic memory systems, each of which is modeled with a knowledge graph. To evaluate this system and analyze the behavior of this agent, we designed and released our own reinforcement learning agent environment, \"the Room\", where an agent has to learn how to encode, store, and retrieve memories to maximize its return by answering questions. We show that our deep Q-learning based agent successfully learns whether a short-term memory should be forgotten, or rather be stored in the episodic or semantic memory systems. Our experiments indicate that an agent with human-like memory systems can outperform an agent without this memory structure in the environment.",
    "pdf_url": "https://arxiv.org/pdf/2212.02098v4",
    "doi": "10.1609/aaai.v37i1.25075",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": "Proceedings of the AAAI Conference on Artificial Intelligence (2023), 37(1), 48-56",
    "citation_count": 6,
    "bibtex": "@Article{Kim2022AMW,\n author = {Taewoon Kim and Michael Cochez and Vincent Franccois-Lavet and Mark Antonius Neerincx and Piek Vossen},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {A Machine with Short-Term, Episodic, and Semantic Memory Systems},\n volume = {abs/2212.02098},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "kim2022"
  },
  {
    "id": "http://arxiv.org/abs/2506.19686v2",
    "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers",
    "published": "2025-06-24T14:55:43Z",
    "updated": "2025-06-26T17:18:54Z",
    "authors": [
      "Ching Fang",
      "Kanaka Rajan"
    ],
    "summary": "Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.",
    "pdf_url": "https://arxiv.org/pdf/2506.19686v2",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Updates: added other funding sources; formatted title correctly",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Fang2025FromMT,\n author = {Ching Fang and Kanaka Rajan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers},\n volume = {abs/2506.19686},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "fang2025"
  },
  {
    "id": "http://arxiv.org/abs/1606.04460v1",
    "title": "Model-Free Episodic Control",
    "published": "2016-06-14T17:03:46Z",
    "updated": "2016-06-14T17:03:46Z",
    "authors": [
      "Charles Blundell",
      "Benigno Uria",
      "Alexander Pritzel",
      "Yazhe Li",
      "Avraham Ruderman",
      "Joel Z Leibo",
      "Jack Rae",
      "Daan Wierstra",
      "Demis Hassabis"
    ],
    "summary": "State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.",
    "pdf_url": "https://arxiv.org/pdf/1606.04460v1",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "stat.ML",
    "comment": null,
    "journal_ref": null,
    "citation_count": 261,
    "bibtex": "@Article{Blundell2016ModelFreeEC,\n author = {Charles Blundell and Benigno Uria and Alexander Pritzel and Yazhe Li and Avraham Ruderman and Joel Z. Leibo and Jack W. Rae and Daan Wierstra and Demis Hassabis},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Model-Free Episodic Control},\n volume = {abs/1606.04460},\n year = {2016}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "blundell2016"
  },
  {
    "id": "http://arxiv.org/abs/2212.06967v1",
    "title": "Explaining Agent's Decision-making in a Hierarchical Reinforcement Learning Scenario",
    "published": "2022-12-14T01:18:45Z",
    "updated": "2022-12-14T01:18:45Z",
    "authors": [
      "Hugo Muñoz",
      "Ernesto Portugal",
      "Angel Ayala",
      "Bruno Fernandes",
      "Francisco Cruz"
    ],
    "summary": "Reinforcement learning is a machine learning approach based on behavioral psychology. It is focused on learning agents that can acquire knowledge and learn to carry out new tasks by interacting with the environment. However, a problem occurs when reinforcement learning is used in critical contexts where the users of the system need to have more information and reliability for the actions executed by an agent. In this regard, explainable reinforcement learning seeks to provide to an agent in training with methods in order to explain its behavior in such a way that users with no experience in machine learning could understand the agent's behavior. One of these is the memory-based explainable reinforcement learning method that is used to compute probabilities of success for each state-action pair using an episodic memory. In this work, we propose to make use of the memory-based explainable reinforcement learning method in a hierarchical environment composed of sub-tasks that need to be first addressed to solve a more complex task. The end goal is to verify if it is possible to provide to the agent the ability to explain its actions in the global task as well as in the sub-tasks. The results obtained showed that it is possible to use the memory-based method in hierarchical environments with high-level tasks and compute the probabilities of success to be used as a basis for explaining the agent's behavior.",
    "pdf_url": "https://arxiv.org/pdf/2212.06967v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Munoz2022ExplainingAD,\n author = {Hugo Munoz and Ernesto Portugal and Angel Ayala and Bruno Fernandes and Francisco Cruz},\n booktitle = {International Conference of the Chilean Computer Science Society},\n journal = {2022 41st International Conference of the Chilean Computer Science Society (SCCC)},\n pages = {1-8},\n title = {Explaining Agent's Decision-making in a Hierarchical Reinforcement Learning Scenario},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "muoz2022"
  },
  {
    "id": "http://arxiv.org/abs/2506.01442v1",
    "title": "Agentic Episodic Control",
    "published": "2025-06-02T08:57:37Z",
    "updated": "2025-06-02T08:57:37Z",
    "authors": [
      "Xidong Yang",
      "Wenhao Li",
      "Junjie Sheng",
      "Chuyun Shen",
      "Yun Hua",
      "Xiangfeng Wang"
    ],
    "summary": "Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to scientific discovery and AI alignment. However, its broader applicability remains limited by challenges such as low data efficiency and poor generalizability. Recent advances suggest that large language models, with their rich world knowledge and reasoning capabilities, could complement RL by enabling semantic state modeling and task-agnostic planning. In this work, we propose the Agentic Episodic Control (AEC), a novel architecture that integrates RL with LLMs to enhance decision-making. The AEC can leverage a large language model (LLM) to map the observations into language-grounded embeddings, which further can be stored in an episodic memory for rapid retrieval of high-value experiences. Simultaneously, a World-Graph working memory module is utilized to capture structured environmental dynamics in order to enhance relational reasoning. Furthermore, a lightweight critical state detector dynamically arbitrates between the episodic memory recall and the world-model-guided exploration. On the whole, by combining the trial-and-error learning scheme with LLM-derived semantic priors, the proposed AEC can improve both data efficiency and generalizability in reinforcement learning. In experiments on BabyAI-Text benchmark tasks, AEC demonstrates substantial improvements over existing baselines, especially on complex and generalization tasks like FindObj, where it outperforms the best baseline by up to 76%. The proposed AEC framework bridges the strengths of numeric reinforcement learning and symbolic reasoning, which provides a pathway toward more adaptable and sample-efficient agents.",
    "pdf_url": "https://arxiv.org/pdf/2506.01442v1",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Yang2025AgenticEC,\n author = {Xidong Yang and Wenhao Li and Junjie Sheng and C. Shen and Yun Hua and Xiangfeng Wang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Agentic Episodic Control},\n volume = {abs/2506.01442},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yang2025"
  },
  {
    "id": "http://arxiv.org/abs/1810.08163v1",
    "title": "Fast deep reinforcement learning using online adjustments from the past",
    "published": "2018-10-18T17:00:20Z",
    "updated": "2018-10-18T17:00:20Z",
    "authors": [
      "Steven Hansen",
      "Pablo Sprechmann",
      "Alexander Pritzel",
      "André Barreto",
      "Charles Blundell"
    ],
    "summary": "We propose Ephemeral Value Adjusments (EVA): a means of allowing deep reinforcement learning agents to rapidly adapt to experience in their replay buffer. EVA shifts the value predicted by a neural network with an estimate of the value function found by planning over experience tuples from the replay buffer near the current state. EVA combines a number of recent ideas around combining episodic memory-like structures into reinforcement learning agents: slot-based storage, content-based retrieval, and memory-based planning. We show that EVAis performant on a demonstration task and Atari games.",
    "pdf_url": "https://arxiv.org/pdf/1810.08163v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NIPS 2018",
    "journal_ref": null,
    "citation_count": 45,
    "bibtex": "@Article{Hansen2018FastDR,\n author = {S. Hansen and P. Sprechmann and A. Pritzel and André Barreto and C. Blundell},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Fast deep reinforcement learning using online adjustments from the past},\n volume = {abs/1810.08163},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hansen2018"
  },
  {
    "id": "http://arxiv.org/abs/2506.17263v1",
    "title": "Memory Allocation in Resource-Constrained Reinforcement Learning",
    "published": "2025-06-09T21:15:37Z",
    "updated": "2025-06-09T21:15:37Z",
    "authors": [
      "Massimiliano Tamborski",
      "David Abel"
    ],
    "summary": "Resource constraints can fundamentally change both learning and decision-making. We explore how memory constraints influence an agent's performance when navigating unknown environments using standard reinforcement learning algorithms. Specifically, memory-constrained agents face a dilemma: how much of their limited memory should be allocated to each of the agent's internal processes, such as estimating a world model, as opposed to forming a plan using that model? We study this dilemma in MCTS- and DQN-based algorithms and examine how different allocations of memory impact performance in episodic and continual learning settings.",
    "pdf_url": "https://arxiv.org/pdf/2506.17263v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "RLDM 2025",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Tamborski2025MemoryAI,\n author = {Massimiliano Tamborski and David Abel},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Memory Allocation in Resource-Constrained Reinforcement Learning},\n volume = {abs/2506.17263},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "tamborski2025"
  },
  {
    "id": "http://arxiv.org/abs/1805.07603v1",
    "title": "Episodic Memory Deep Q-Networks",
    "published": "2018-05-19T14:33:00Z",
    "updated": "2018-05-19T14:33:00Z",
    "authors": [
      "Zichuan Lin",
      "Tianqi Zhao",
      "Guangwen Yang",
      "Lintao Zhang"
    ],
    "summary": "Reinforcement learning (RL) algorithms have made huge progress in recent years by leveraging the power of deep neural networks (DNN). Despite the success, deep RL algorithms are known to be sample inefficient, often requiring many rounds of interaction with the environments to obtain satisfactory performance. Recently, episodic memory based RL has attracted attention due to its ability to latch on good actions quickly. In this paper, we present a simple yet effective biologically inspired RL algorithm called Episodic Memory Deep Q-Networks (EMDQN), which leverages episodic memory to supervise an agent during training. Experiments show that our proposed method can lead to better sample efficiency and is more likely to find good policies. It only requires 1/5 of the interactions of DQN to achieve many state-of-the-art performances on Atari games, significantly outperforming regular DQN and other episodic memory based RL algorithms.",
    "pdf_url": "https://arxiv.org/pdf/1805.07603v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 2018",
    "journal_ref": null,
    "citation_count": 91,
    "bibtex": "@Article{Lin2018EpisodicMD,\n author = {Zichuan Lin and Tianqi Zhao and Guangwen Yang and Lintao Zhang},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {2433-2439},\n title = {Episodic Memory Deep Q-Networks},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "lin2018"
  },
  {
    "id": "http://arxiv.org/abs/2104.08492v1",
    "title": "A Self-Supervised Auxiliary Loss for Deep RL in Partially Observable Settings",
    "published": "2021-04-17T09:28:17Z",
    "updated": "2021-04-17T09:28:17Z",
    "authors": [
      "Eltayeb Ahmed",
      "Luisa Zintgraf",
      "Christian A. Schroeder de Witt",
      "Nicolas Usunier"
    ],
    "summary": "In this work we explore an auxiliary loss useful for reinforcement learning in environments where strong performing agents are required to be able to navigate a spatial environment. The auxiliary loss proposed is to minimize the classification error of a neural network classifier that predicts whether or not a pair of states sampled from the agents current episode trajectory are in order. The classifier takes as input a pair of states as well as the agent's memory. The motivation for this auxiliary loss is that there is a strong correlation with which of a pair of states is more recent in the agents episode trajectory and which of the two states is spatially closer to the agent. Our hypothesis is that learning features to answer this question encourages the agent to learn and internalize in memory representations of states that facilitate spatial reasoning. We tested this auxiliary loss on a navigation task in a gridworld and achieved 9.6% increase in accumulative episode reward compared to a strong baseline approach.",
    "pdf_url": "https://arxiv.org/pdf/2104.08492v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Ahmed2021ASA,\n author = {Eltayeb Ahmed and Luisa M. Zintgraf and C. S. D. Witt and Nicolas Usunier},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Self-Supervised Auxiliary Loss for Deep RL in Partially Observable Settings},\n volume = {abs/2104.08492},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ahmed2021"
  },
  {
    "id": "http://arxiv.org/abs/2103.06469v3",
    "title": "Generalizable Episodic Memory for Deep Reinforcement Learning",
    "published": "2021-03-11T05:31:21Z",
    "updated": "2021-06-11T14:27:01Z",
    "authors": [
      "Hao Hu",
      "Jianing Ye",
      "Guangxiang Zhu",
      "Zhizhou Ren",
      "Chongjie Zhang"
    ],
    "summary": "Episodic memory-based methods can rapidly latch onto past successful strategies by a non-parametric memory and improve sample efficiency of traditional reinforcement learning. However, little effort is put into the continuous domain, where a state is never visited twice, and previous episodic methods fail to efficiently aggregate experience across trajectories. To address this problem, we propose Generalizable Episodic Memory (GEM), which effectively organizes the state-action values of episodic memory in a generalizable manner and supports implicit planning on memorized trajectories. GEM utilizes a double estimator to reduce the overestimation bias induced by value propagation in the planning process. Empirical evaluation shows that our method significantly outperforms existing trajectory-based methods on various MuJoCo continuous control tasks. To further show the general applicability, we evaluate our method on Atari games with discrete action space, which also shows a significant improvement over baseline algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2103.06469v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 41,
    "bibtex": "@Article{Hu2021GeneralizableEM,\n author = {Haotian Hu and Jianing Ye and Zhizhou Ren and Guangxiang Zhu and Chongjie Zhang},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Generalizable Episodic Memory for Deep Reinforcement Learning},\n volume = {abs/2103.06469},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hu2021"
  },
  {
    "id": "http://arxiv.org/abs/2503.02303v3",
    "title": "Flexible Prefrontal Control over Hippocampal Episodic Memory for Goal-Directed Generalization",
    "published": "2025-03-04T06:04:54Z",
    "updated": "2025-08-11T19:07:30Z",
    "authors": [
      "Yicong Zheng",
      "Nora Wolf",
      "Charan Ranganath",
      "Randall C. O'Reilly",
      "Kevin L. McKee"
    ],
    "summary": "Many tasks require flexibly modifying perception and behavior based on current goals. Humans can retrieve episodic memories from days to years ago, using them to contextualize and generalize behaviors across novel but structurally related situations. The brain's ability to control episodic memories based on task demands is often attributed to interactions between the prefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement learning model that incorporates a PFC-HPC interaction mechanism for goal-directed generalization. In our model, the PFC learns to generate query-key representations to encode and retrieve goal-relevant episodic memories, modulating HPC memories top-down based on current task demands. Moreover, the PFC adapts its encoding and retrieval strategies dynamically when faced with multiple goals presented in a blocked, rather than interleaved, manner. Our results show that: (1) combining working memory with selectively retrieved episodic memory allows transfer of decisions among similar environments or situations, (2) top-down control from PFC over HPC improves learning of arbitrary structural associations between events for generalization to novel environments compared to a bottom-up sensory-driven approach, and (3) the PFC encodes generalizable representations during both encoding and retrieval of goal-relevant memories, whereas the HPC exhibits event-specific representations. Together, these findings highlight the importance of goal-directed prefrontal control over hippocampal episodic memory for decision-making in novel situations and suggest a computational mechanism by which PFC-HPC interactions enable flexible behavior.",
    "pdf_url": "https://arxiv.org/pdf/2503.02303v3",
    "doi": null,
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at the 2025 Conference on Cognitive Computational Neuroscience (CCN 2025). Preprint available at OpenReview: https://openreview.net/forum?id=7hhz5ToJnM",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Zheng2025FlexiblePC,\n author = {Yicong Zheng and Nora Wolf and Charan Ranganath and Randall C. O'Reilly and Kevin L. McKee},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Flexible Prefrontal Control over Hippocampal Episodic Memory for Goal-Directed Generalization},\n volume = {abs/2503.02303},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "zheng2025"
  },
  {
    "id": "http://arxiv.org/abs/1806.00540v1",
    "title": "Integrating Episodic Memory into a Reinforcement Learning Agent using Reservoir Sampling",
    "published": "2018-06-01T20:52:31Z",
    "updated": "2018-06-01T20:52:31Z",
    "authors": [
      "Kenny J. Young",
      "Richard S. Sutton",
      "Shuo Yang"
    ],
    "summary": "Episodic memory is a psychology term which refers to the ability to recall specific events from the past. We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful. Inspired by this idea, and the increasing popularity of external memory mechanisms to handle long-term dependencies in deep learning systems, we propose a novel algorithm which uses a reservoir sampling procedure to maintain an external memory consisting of a fixed number of past states. The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on. Critically this method allows for efficient online computation of gradient estimates with respect to the write process of the external memory. Thus unlike most prior mechanisms for external memory it is feasible to use in an online reinforcement learning setting.",
    "pdf_url": "https://arxiv.org/pdf/1806.00540v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Young2018IntegratingEM,\n author = {Kenny Young and R. Sutton and Shuo Yang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Integrating Episodic Memory into a Reinforcement Learning Agent using Reservoir Sampling},\n volume = {abs/1806.00540},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "young2018"
  },
  {
    "id": "http://arxiv.org/abs/2510.13220v1",
    "title": "EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems",
    "published": "2025-10-15T07:16:28Z",
    "updated": "2025-10-15T07:16:28Z",
    "authors": [
      "Yufei He",
      "Juncheng Liu",
      "Yue Liu",
      "Yibo Li",
      "Tri Cao",
      "Zhiyuan Hu",
      "Xinxing Xu",
      "Bryan Hooi"
    ],
    "summary": "A fundamental limitation of current AI agents is their inability to learn complex skills on the fly at test time, often behaving like \"clever but clueless interns\" in novel environments. This severely limits their practical utility. To systematically measure and drive progress on this challenge, we first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a new evaluation setup where an agent must play the same game for several consecutive episodes, attempting to improve its performance from one episode to the next. On J-TTL, we find that existing adaptation methods like reflection, memory, or reinforcement learning struggle. To address the challenges posed by our benchmark, we present EvoTest, an evolutionary test-time learning framework that improves an agent without any fine-tuning or gradients-by evolving the entire agentic system after every episode. EvoTest has two roles: the Actor Agent, which plays the game, and the Evolver Agent, which analyzes the episode transcript to propose a revised configuration for the next run. This configuration rewrites the prompt, updates memory by logging effective state-action choices, tunes hyperparameters, and learns the tool-use routines. On our J-TTL benchmark, EvoTest consistently increases performance, outperforming not only reflection and memory-only baselines but also more complex online fine-tuning methods. Notably, our method is the only one capable of winning two games (Detective and Library), while all baselines fail to win any.",
    "pdf_url": "https://arxiv.org/pdf/2510.13220v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{He2025EvoTestET,\n author = {Yufei He and Juncheng Liu and Yue Liu and Yibo Li and Tri Cao and Zhiyuan Hu and Xinxing Xu and Bryan Hooi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems},\n volume = {abs/2510.13220},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "he2025"
  },
  {
    "id": "http://arxiv.org/abs/1911.09560v1",
    "title": "Memory-Efficient Episodic Control Reinforcement Learning with Dynamic Online k-means",
    "published": "2019-11-21T15:54:49Z",
    "updated": "2019-11-21T15:54:49Z",
    "authors": [
      "Andrea Agostinelli",
      "Kai Arulkumaran",
      "Marta Sarrico",
      "Pierre Richemond",
      "Anil Anthony Bharath"
    ],
    "summary": "Recently, neuro-inspired episodic control (EC) methods have been developed to overcome the data-inefficiency of standard deep reinforcement learning approaches. Using non-/semi-parametric models to estimate the value function, they learn rapidly, retrieving cached values from similar past states. In realistic scenarios, with limited resources and noisy data, maintaining meaningful representations in memory is essential to speed up the learning and avoid catastrophic forgetting. Unfortunately, EC methods have a large space and time complexity. We investigate different solutions to these problems based on prioritising and ranking stored states, as well as online clustering techniques. We also propose a new dynamic online k-means algorithm that is both computationally-efficient and yields significantly better performance at smaller memory sizes; we validate this approach on classic reinforcement learning environments and Atari games.",
    "pdf_url": "https://arxiv.org/pdf/1911.09560v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Workshop on Biological and Artificial Reinforcement Learning, NeurIPS 2019",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Agostinelli2019MemoryEfficientEC,\n author = {A. Agostinelli and Kai Arulkumaran and Marta Sarrico and Pierre H. Richemond and A. Bharath},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Memory-Efficient Episodic Control Reinforcement Learning with Dynamic Online k-means},\n volume = {abs/1911.09560},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "agostinelli2019"
  },
  {
    "id": "http://arxiv.org/abs/2009.01719v4",
    "title": "Grounded Language Learning Fast and Slow",
    "published": "2020-09-03T14:52:03Z",
    "updated": "2020-10-14T14:38:58Z",
    "authors": [
      "Felix Hill",
      "Olivier Tieleman",
      "Tamara von Glehn",
      "Nathaniel Wong",
      "Hamza Merzic",
      "Stephen Clark"
    ],
    "summary": "Recent work has shown that large text-based neural language models, trained with conventional supervised learning objectives, acquire a surprising propensity for few- and one-shot learning. Here, we show that an embodied agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional reinforcement learning algorithms. After a single introduction to a novel object via continuous visual perception and a language prompt (\"This is a dax\"), the agent can re-identify the object and manipulate it as instructed (\"Put the dax on the bed\"). In doing so, it seamlessly integrates short-term, within-episode knowledge of the appropriate referent for the word \"dax\" with long-term lexical and motor knowledge acquired across episodes (i.e. \"bed\" and \"putting\"). We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful for later executing instructions. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for 'fast-mapping', a fundamental pillar of human cognitive development and a potentially transformative capacity for agents that interact with human users.",
    "pdf_url": "https://arxiv.org/pdf/2009.01719v4",
    "doi": null,
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": null,
    "journal_ref": null,
    "citation_count": 82,
    "bibtex": "@Article{Hill2020GroundedLL,\n author = {Felix Hill and O. Tieleman and Tamara von Glehn and Nathaniel Wong and Hamza Merzic and S. Clark},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Grounded Language Learning Fast and Slow},\n volume = {abs/2009.01719},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hill2020"
  },
  {
    "id": "http://arxiv.org/abs/2404.05353v1",
    "title": "A new family of locally $5$-arc transitive graphs of pushing up type with respect to the prime 3",
    "published": "2024-04-08T09:42:55Z",
    "updated": "2024-04-08T09:42:55Z",
    "authors": [
      "J. van Bon"
    ],
    "summary": "Let $q$ be a power of the prime 3. A locally 5-arc transitive $G$-graph of pushing up type is constructed for each value of $q$. For $q=3$, the $G$-graph constructed provides an example of a graph with a vertex stabilizer amalgam of shape ${\\cal E}_1$ in the sense of [1]. Whereas, for the other values of $q$, the vertex stabilizer amalgam of the $G$-graph is of a previously unknown shape. In particular, for $q \\neq 3$, these graphs are the first examples of locally 5-arc transitive graphs containing a vertex $z$ for which the group that fixes all 3-arcs originating at $z$ is non-trivial.",
    "pdf_url": "https://arxiv.org/pdf/2404.05353v1",
    "doi": null,
    "categories": [
      "math.CO",
      "math.GR"
    ],
    "primary_category": "math.CO",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Bon2024ANF,\n author = {J. V. Bon},\n title = {A new family of locally $5$-arc transitive graphs of pushing up type with respect to the prime 3},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "bon2024"
  },
  {
    "id": "http://arxiv.org/abs/2203.15845v3",
    "title": "Topological Experience Replay",
    "published": "2022-03-29T18:28:20Z",
    "updated": "2023-06-26T21:12:17Z",
    "authors": [
      "Zhang-Wei Hong",
      "Tao Chen",
      "Yen-Chen Lin",
      "Joni Pajarinen",
      "Pulkit Agrawal"
    ],
    "summary": "State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically show that our method is substantially more data-efficient than several baselines on a diverse range of goal-reaching tasks. Notably, the proposed method also outperforms baselines that consume more batches of training experience and operates from high-dimensional observational data such as images.",
    "pdf_url": "https://arxiv.org/pdf/2203.15845v3",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Published at ICLR 2022",
    "citation_count": 19,
    "bibtex": "@Article{Hong2022TopologicalER,\n author = {Zhang-Wei Hong and Tao Chen and Yen-Chen Lin and J. Pajarinen and Pulkit Agrawal},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Topological Experience Replay},\n volume = {abs/2203.15845},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hong2022"
  },
  {
    "id": "http://arxiv.org/abs/1507.01103v1",
    "title": "Oscillating hysteresis in the q-neighbor Ising model",
    "published": "2015-07-04T12:59:03Z",
    "updated": "2015-07-04T12:59:03Z",
    "authors": [
      "Arkadiusz Jȩdrzejewski",
      "Anna Chmiel",
      "Katarzyna Sznajd-Weron"
    ],
    "summary": "We modify the kinetic Ising model with Metropolis dynamics, allowing each spin to interact only with $q$ spins randomly chosen from the whole system, which corresponds to the topology of a complete graph. We show that the model with $q \\ge 3$ exhibits a phase transition between ferromagnetic and paramagnetic phases at temperature $T^*$, which linearly increases with $q$. Moreover, we show that for $q=3$ the phase transition is continuous and discontinuous for larger values of $q$. For $q>3$ the hysteresis exhibits oscillatory behavior -- expanding for even values of $q$ and shrinking for odd values of $q$. If only simulation results were taken into account, this phenomenon could be mistakenly interpreted as switching from discontinuous to continuous phase transitions or even as evidence of the so-called mixed phase transitions. Due to the mean-field like nature of the model we are able to calculate analytically not only the stationary value of the order parameter but also precisely determine the hysteresis and the effective potential showing stable, unstable and metastable steady states. The main message is that in case of non-equilibrium systems the hysteresis can behave in an odd way and computer simulations alone may mistakenly lead to incorrect conclusions.",
    "pdf_url": "https://arxiv.org/pdf/1507.01103v1",
    "doi": "10.1103/PhysRevE.92.052105",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": null,
    "journal_ref": "Phys. Rev. E 92, 052105 (2015)",
    "citation_count": 20,
    "bibtex": "@Article{Jędrzejewski2015OscillatingHI,\n author = {Arkadiusz Jędrzejewski and A. Chmiel and K. Sznajd-Weron},\n booktitle = {Physical review. E, Statistical, nonlinear, and soft matter physics},\n journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},\n pages = {\n          052105\n        },\n title = {Oscillating hysteresis in the q-neighbor Ising model.},\n volume = {92 5},\n year = {2015}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "jdrzejewski2015"
  },
  {
    "id": "http://arxiv.org/abs/2007.07582v1",
    "title": "Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning",
    "published": "2020-07-15T10:01:32Z",
    "updated": "2020-07-15T10:01:32Z",
    "authors": [
      "Sabrina Hoppe",
      "Marc Toussaint"
    ],
    "summary": "In state of the art model-free off-policy deep reinforcement learning, a replay memory is used to store past experience and derive all network updates. Even if both state and action spaces are continuous, the replay memory only holds a finite number of transitions. We represent these transitions in a data graph and link its structure to soft divergence. By selecting a subgraph with a favorable structure, we construct a simplified Markov Decision Process for which exact Q-values can be computed efficiently as more data comes in. The subgraph and its associated Q-values can be represented as a QGraph. We show that the Q-value for each transition in the simplified MDP is a lower bound of the Q-value for the same transition in the original continuous Q-learning problem. By using these lower bounds in temporal difference learning, our method QG-DDPG is less prone to soft divergence and exhibits increased sample efficiency while being more robust to hyperparameters. QGraphs also retain information from transitions that have already been overwritten in the replay memory, which can decrease the algorithm's sensitivity to the replay memory capacity.",
    "pdf_url": "https://arxiv.org/pdf/2007.07582v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 8 figures",
    "journal_ref": null,
    "citation_count": 7,
    "bibtex": "@Article{Hoppe2019QgraphboundedQS,\n author = {Sabrina Hoppe and Marc Toussaint},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Qgraph-bounded Q-learning: Stabilizing Model-Free Off-Policy Deep Reinforcement Learning},\n volume = {abs/2007.07582},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hoppe2020"
  },
  {
    "id": "http://arxiv.org/abs/math/0008191v2",
    "title": "Explicit isoperimetric constants and phase transitions in the random-cluster model",
    "published": "2000-08-24T22:18:34Z",
    "updated": "2001-04-17T20:51:57Z",
    "authors": [
      "Olle Haggstrom",
      "Johan Jonasson",
      "Russell Lyons"
    ],
    "summary": "The random-cluster model is a dependent percolation model that has applications in the study of Ising and Potts models. In this paper, several new results are obtained for the random-cluster model on nonamenable graphs with cluster parameter $q\\geq 1$. Among these, the main ones are the absence of percolation for the free random-cluster measure at the critical value, and examples of planar regular graphs with regular dual where $\\pc^\\f (q) > \\pu^\\w (q)$ for $q$ large enough. The latter follows from considerations of isoperimetric constants, and we give the first nontrivial explicit calculations of such constants. Such considerations are also used to prove non-robust phase transition for the Potts model on nonamenable regular graphs.",
    "pdf_url": "https://arxiv.org/pdf/math/0008191v2",
    "doi": null,
    "categories": [
      "math.PR",
      "math-ph"
    ],
    "primary_category": "math.PR",
    "comment": null,
    "journal_ref": "Ann. Probab. 30 (2002), 443--473",
    "citation_count": null,
    "bibtex": null,
    "markdown_text": null,
    "ranking": null,
    "citation_key": "haggstrom2000"
  },
  {
    "id": "http://arxiv.org/abs/1204.3151v4",
    "title": "Phase transitions in the q-voter model with two types of stochastic driving",
    "published": "2012-04-14T08:09:47Z",
    "updated": "2012-06-23T11:45:44Z",
    "authors": [
      "Piotr Nyczka",
      "Katarzyna Sznajd-Weron",
      "Jerzy Cislo"
    ],
    "summary": "In this paper we study nonlinear $q$-voter model with stochastic driving on a complete graph. We investigate two types of stochasticity that, using the language of social sciences, can be interpreted as different kinds of nonconformity. From a social point of view, it is very important to distinguish between two types nonconformity, so called anti-conformity and independence. A majority of works suggests that these social differences may be completely irrelevant in terms of microscopic modeling that uses tools of statistical physics and that both types of nonconformity play the role of so called 'social temperature'. In this paper we clarify the concept of 'social temperature' and show that different type of 'noise' may lead to qualitatively different emergent properties. In particularly, we show that in the model with anti-conformity the critical value of noise increases with parameter $q$, whereas in the model with independence the critical value of noise decreases with the $q$. Moreover, in the model with anti-conformity the phase transition is continuous for any value of $q$, whereas in the model with independence the transition is continuous for $q \\le 5$ and discontinuous for $q>5$.",
    "pdf_url": "https://arxiv.org/pdf/1204.3151v4",
    "doi": "10.1103/PhysRevE.86.011105",
    "categories": [
      "cond-mat.stat-mech",
      "physics.soc-ph"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": null,
    "journal_ref": null,
    "citation_count": 118,
    "bibtex": "@Article{Nyczka2012PhaseTI,\n author = {P. Nyczka and K. Sznajd-Weron and J. Cisło},\n booktitle = {Physical review. E, Statistical, nonlinear, and soft matter physics},\n journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},\n pages = {\n          011105\n        },\n title = {Phase transitions in the q-voter model with two types of stochastic driving.},\n volume = {86 1 Pt 1},\n year = {2012}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "nyczka2012"
  },
  {
    "id": "http://arxiv.org/abs/cond-mat/0701156v2",
    "title": "BKT-like transition in the Potts model on an inhomogeneous annealed network",
    "published": "2007-01-08T22:34:07Z",
    "updated": "2007-01-12T19:01:05Z",
    "authors": [
      "E. Khajeh",
      "S. N. Dorogovtsev",
      "J. F. F. Mendes"
    ],
    "summary": "We solve the ferromagnetic q-state Potts model on an inhomogeneous annealed network which mimics a random recursive graph. We find that this system has the inverted Berezinskii--Kosterlitz--Thouless (BKT) phase transition for any $q \\geq 1$, including the values $q \\geq 3$, where the Potts model normally shows a first order phase transition. We obtain the temperature dependences of the order parameter, specific heat, and susceptibility demonstrating features typical for the BKT transition. We show that in the entire normal phase, both the distribution of a linear response to an applied local field and the distribution of spin-spin correlations have a critical, i.e. power-law, form.",
    "pdf_url": "https://arxiv.org/pdf/cond-mat/0701156v2",
    "doi": "10.1103/PhysRevE.75.041112",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": "7 pages, 3 figures",
    "journal_ref": "Phys. Rev. E 75, 041112 (2007)",
    "citation_count": null,
    "bibtex": null,
    "markdown_text": null,
    "ranking": null,
    "citation_key": "khajeh2007"
  },
  {
    "id": "http://arxiv.org/abs/1912.09420v3",
    "title": "Erdös-Rényi phase transition in the Axelrod model on complete graphs",
    "published": "2019-12-19T17:44:53Z",
    "updated": "2020-03-31T23:04:26Z",
    "authors": [
      "Sebastián Pinto",
      "Pablo Balenzuela"
    ],
    "summary": "The Axelrod model has been widely studied since its proposal for social influence and cultural dissemination. In particular, the community of statistical physics focused on the presence of a phase transition as a function of its two main parameters, $F$ and $Q$. In this work, we show that the Axelrod model undergoes a second order phase transition in the limit of $F \\rightarrow \\infty $ on a complete graph. This transition is equivalent to the Erdös-Rényi phase transition in random networks when it is described in terms of the probability of interaction at the initial state, which depends on a scaling relation between $F$ and $Q$. We also found that this probability plays a key role in sparse topologies by collapsing the transition curves for different values of the parameter $F$.",
    "pdf_url": "https://arxiv.org/pdf/1912.09420v3",
    "doi": "10.1103/PhysRevE.101.052319",
    "categories": [
      "physics.soc-ph"
    ],
    "primary_category": "physics.soc-ph",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Pinto2019ErdósRényiPT,\n author = {Sebastián Pinto and P. Balenzuela},\n booktitle = {Physical Review E},\n journal = {Physical review. E},\n pages = {\n          052319\n        },\n title = {Erdós-Rényi phase transition in the Axelrod model on complete graphs.},\n volume = {101 5-1},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "pinto2019"
  },
  {
    "id": "http://arxiv.org/abs/2304.01162v2",
    "title": "Universality in prelimiting tail behavior for regular subgraph counts in the Poisson regime",
    "published": "2023-04-03T17:28:30Z",
    "updated": "2023-11-20T18:51:04Z",
    "authors": [
      "Mriganka Basu Roy Chowdhury"
    ],
    "summary": "Let $N$ be the number of copies of a small subgraph $H$ in an Erdős-Rényi graph $G \\sim \\mathcal{G}(n, p_n)$ where $p_n \\to 0$ is chosen so that $\\mathbb{E} N = c$, a constant. Results of Bollobás show that for regular graphs $H$, the count $N$ weakly converges to a Poisson random variable. For large but finite $n$, and for the specific case of the triangle, investigations of the upper tail $\\mathbb{P}(N \\geq k_n)$ by Ganguly, Hiesmayr and Nam (2022) revealed that there is a phase transition in the tail behavior and the associated mechanism. Smaller values of $k_n$ correspond to disjoint occurrences of $H$, leading to Poisson tails, with a different behavior emerging when $k_n$ is large, guided by the appearance of an almost clique. We show that a similar phase transition also occurs when $H$ is any regular graph, at the point where $k_n^{1 -2/q}\\log k_n = \\log n$ ($q$ is the number of vertices in $H$). This establishes universality of this transition, previously known only for the case of the triangle.",
    "pdf_url": "https://arxiv.org/pdf/2304.01162v2",
    "doi": null,
    "categories": [
      "math.PR",
      "math.CO"
    ],
    "primary_category": "math.PR",
    "comment": "26 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Chowdhury2023UniversalityIP,\n author = {Mriganka Basu Roy Chowdhury},\n booktitle = {Electronic Journal of Combinatorics},\n journal = {Electron. J. Comb.},\n title = {Universality in Prelimiting Tail Behavior for Regular Subgraph Counts in the Poisson Regime},\n volume = {32},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "chowdhury2023"
  },
  {
    "id": "http://arxiv.org/abs/2312.11857v2",
    "title": "Anderson transition and mobility edges on hyperbolic lattices with randomly connected boundaries",
    "published": "2023-12-19T04:56:59Z",
    "updated": "2024-11-19T09:54:52Z",
    "authors": [
      "Tianyu Li",
      "Yi Peng",
      "Yucheng Wang",
      "Haiping Hu"
    ],
    "summary": "Hyperbolic lattices, formed by tessellating the hyperbolic plane with regular polygons, exhibit a diverse range of exotic physical phenomena beyond conventional Euclidean lattices. Here, we investigate the impact of disorder on hyperbolic lattices and reveal that the Anderson localization occurs at strong disorder strength, accompanied by the presence of mobility edges. Taking the hyperbolic $\\{p,q\\}=\\{3,8\\}$ and $\\{p,q\\}=\\{4,8\\}$ lattices as examples, we employ finite-size scaling of both spectral statistics and the inverse participation ratio to pinpoint the transition point and critical exponents. Our findings indicate that the transition points tend to increase with larger values of $\\{p,q\\}$ or curvature. In the limiting case of $\\{\\infty, q\\}$, we further determine its Anderson transition using the cavity method, drawing parallels with the random regular graph. Our work lays the cornerstone for a comprehensive understanding of Anderson transition and mobility edges on hyperbolic lattices.",
    "pdf_url": "https://arxiv.org/pdf/2312.11857v2",
    "doi": "10.1038/s42005-024-01848-7",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "primary_category": "cond-mat.dis-nn",
    "comment": "10+6 pages, 5+5 figures",
    "journal_ref": "Commun Phys 7, 371 (2024)",
    "citation_count": 11,
    "bibtex": "@Article{Li2023AndersonTA,\n author = {Tianyu Li and Yi Peng and Yucheng Wang and Haiping Hu},\n booktitle = {Communications Physics},\n journal = {Communications Physics},\n title = {Anderson transition and mobility edges on hyperbolic lattices with randomly connected boundaries},\n volume = {7},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "li2023"
  },
  {
    "id": "http://arxiv.org/abs/1406.6625v3",
    "title": "Computational Lower Bounds for Community Detection on Random Graphs",
    "published": "2014-06-25T16:15:36Z",
    "updated": "2015-03-11T20:21:00Z",
    "authors": [
      "Bruce Hajek",
      "Yihong Wu",
      "Jiaming Xu"
    ],
    "summary": "This paper studies the problem of detecting the presence of a small dense community planted in a large Erdős-Rényi random graph $\\mathcal{G}(N,q)$, where the edge probability within the community exceeds $q$ by a constant factor. Assuming the hardness of the planted clique detection problem, we show that the computational complexity of detecting the community exhibits the following phase transition phenomenon: As the graph size $N$ grows and the graph becomes sparser according to $q=N^{-α}$, there exists a critical value of $α= \\frac{2}{3}$, below which there exists a computationally intensive procedure that can detect far smaller communities than any computationally efficient procedure, and above which a linear-time procedure is statistically optimal. The results also lead to the average-case hardness results for recovering the dense community and approximating the densest $K$-subgraph.",
    "pdf_url": "https://arxiv.org/pdf/1406.6625v3",
    "doi": null,
    "categories": [
      "math.ST",
      "cs.CC",
      "stat.ML"
    ],
    "primary_category": "math.ST",
    "comment": "28 pages",
    "journal_ref": null,
    "citation_count": 120,
    "bibtex": "@Article{Hajek2014ComputationalLB,\n author = {B. Hajek and Yihong Wu and Jiaming Xu},\n booktitle = {Annual Conference Computational Learning Theory},\n journal = {ArXiv},\n title = {Computational Lower Bounds for Community Detection on Random Graphs},\n volume = {abs/1406.6625},\n year = {2014}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hajek2014"
  },
  {
    "id": "http://arxiv.org/abs/1210.6248v2",
    "title": "Two-dimensional Potts antiferromagnets with a phase transition at arbitrarily large q",
    "published": "2012-10-23T14:32:18Z",
    "updated": "2013-02-06T12:42:48Z",
    "authors": [
      "Yuan Huang",
      "Kun Chen",
      "Youjin Deng",
      "Jesper Lykke Jacobsen",
      "Roman Kotecký",
      "Jesús Salas",
      "Alan D. Sokal",
      "Jan M. Swart"
    ],
    "summary": "We exhibit infinite families of two-dimensional lattices (some of which are triangulations or quadrangulations of the plane) on which the q-state Potts antiferromagnet has a finite-temperature phase transition at arbitrarily large values of q. This unexpected result is proven rigorously by using a Peierls argument to measure the entropic advantage of sublattice long-range order. Additional numerical data are obtained using transfer matrices, Monte Carlo simulation, and a high-precision graph-theoretic method.",
    "pdf_url": "https://arxiv.org/pdf/1210.6248v2",
    "doi": "10.1103/PhysRevE.87.012136",
    "categories": [
      "cond-mat.stat-mech",
      "math-ph"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": "RevTeX, 5 pages, includes 10 Postscript figures",
    "journal_ref": "Phys. Rev. E 87, 012136 (2013)",
    "citation_count": 16,
    "bibtex": "@Article{Huang2012TwodimensionalPA,\n author = {Yu’an Huang and Kun Chen and Youjin Deng and J. Jacobsen and R. Kotecký and J. Salas and A. Sokal and J. Swart},\n booktitle = {Physical review. E, Statistical, nonlinear, and soft matter physics},\n journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},\n pages = {\n          012136\n        },\n title = {Two-dimensional Potts antiferromagnets with a phase transition at arbitrarily large q.},\n volume = {87 1},\n year = {2012}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "huang2012"
  },
  {
    "id": "http://arxiv.org/abs/2205.05118v2",
    "title": "On the intersection density of the Kneser Graph $K(n,3)$",
    "published": "2022-05-10T18:46:19Z",
    "updated": "2023-11-24T23:01:59Z",
    "authors": [
      "Karen Meagher",
      "Andriaherimanana Sarobidy Razafimahatratra"
    ],
    "summary": "A set $\\mathcal{F} \\subset \\operatorname{Sym}(V)$ is \\textsl{intersecting} if any two of its elements agree on some element of $V$. Given a finite transitive permutation group $G\\leq \\operatorname{Sym}(V)$, the \\textsl{intersection density} $ρ(G)$ is the maximum ratio $\\frac{|\\mathcal{F}||V|}{|G|}$ where $\\mathcal{F}$ runs through all intersecting sets of $G$. The \\textsl{intersection density} $ρ(X)$ of a vertex-transitive graph $X = (V,E)$ is equal to $\\max \\left\\{ ρ(G) : G \\leq \\operatorname{Aut}(X), \\mbox{ $G$ transitive} \\right\\}$. In this paper, we study the intersection density of the Kneser graph $K(n,3)$, for $n\\geq 7$.\n  The intersection density of $K(n,3)$ is determined whenever its automorphism group contains $\\operatorname{PSL}_{2}(q)$, with some exceptional cases depending on the congruence of $q$. We also briefly consider the intersection density of $K(n,2)$ for values of $n$ where $\\operatorname{PSL}_{2}(q)$ is a subgroup of its automorphism group.",
    "pdf_url": "https://arxiv.org/pdf/2205.05118v2",
    "doi": null,
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO",
    "comment": "14 pages, final version, to appear in the European Journal of Combinatorics",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Inproceedings{Meagher2022OnTI,\n author = {Karen Meagher and A. S. Razafimahatratra},\n title = {On the intersection density of the Kneser Graph $K(n,3)$},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "meagher2022"
  },
  {
    "id": "http://arxiv.org/abs/2007.12168v1",
    "title": "A veritable zoology of successive phase transitions in the asymmetric $q$-voter model on multiplex networks",
    "published": "2020-07-23T17:58:02Z",
    "updated": "2020-07-23T17:58:02Z",
    "authors": [
      "Anna Chmiel",
      "Julian Sienkiewicz",
      "Agata Fronczak",
      "Piotr Fronczak"
    ],
    "summary": "We analyze a nonlinear $q$-voter model with stochastic noise, interpreted in the social context as independence, on a duplex network. The size of the lobby $q$ (i.e., the pressure group) is a crucial parameter that changes the behavior of the system. The $q$-voter model has been applied on multiplex networks in a previous work [Phys. Rev E. 92. 052812. (2015)], and it has been shown that the character of the phase transition depends on the number of levels in the multiplex network as well as the value of $q$. Here we study phase transition character in the case when on each level of the network the lobby size is different, resulting in two parameters $q_1$ and $q_2$. We find evidence of successive phase transitions when a continuous phase transition is followed by a discontinuous one or two consecutive discontinuous phases appear, depending on the parameter. When analyzing this system, we even encounter mixed-order (or hybrid) phase transition. We perform simulations and obtain supporting analytical solutions on a simple multiplex case - a duplex clique, which consists of two fully overlapped complete graphs (cliques).",
    "pdf_url": "https://arxiv.org/pdf/2007.12168v1",
    "doi": "10.3390/e22091018",
    "categories": [
      "cond-mat.stat-mech",
      "physics.soc-ph"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": "13 pages, 10 figures",
    "journal_ref": "Entropy 22(9), 1018 (2020)",
    "citation_count": 20,
    "bibtex": "@Article{Chmiel2020AVZ,\n author = {A. Chmiel and J. Sienkiewicz and A. Fronczak and P. Fronczak},\n booktitle = {Entropy},\n journal = {Entropy},\n title = {A Veritable Zoology of Successive Phase Transitions in the Asymmetric q-Voter Model on Multiplex Networks},\n volume = {22},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "chmiel2020"
  },
  {
    "id": "http://arxiv.org/abs/2205.15824v1",
    "title": "Graph Backup: Data Efficient Backup Exploiting Markovian Transitions",
    "published": "2022-05-31T14:26:00Z",
    "updated": "2022-05-31T14:26:00Z",
    "authors": [
      "Zhengyao Jiang",
      "Tianjun Zhang",
      "Robert Kirk",
      "Tim Rocktäschel",
      "Edward Grefenstette"
    ],
    "summary": "The successes of deep Reinforcement Learning (RL) are limited to settings where we have a large stream of online experiences, but applying RL in the data-efficient setting with limited access to online interactions is still challenging. A key to data-efficient RL is good value estimation, but current methods in this space fail to fully utilise the structure of the trajectory data gathered from the environment. In this paper, we treat the transition data of the MDP as a graph, and define a novel backup operator, Graph Backup, which exploits this graph structure for better value estimation. Compared to multi-step backup methods such as $n$-step $Q$-Learning and TD($λ$), Graph Backup can perform counterfactual credit assignment and gives stable value estimates for a state regardless of which trajectory the state is sampled from. Our method, when combined with popular value-based methods, provides improved performance over one-step and multi-step methods on a suite of data-efficient RL benchmarks including MiniGrid, Minatar and Atari100K. We further analyse the reasons for this performance boost through a novel visualisation of the transition graphs of Atari games.",
    "pdf_url": "https://arxiv.org/pdf/2205.15824v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Jiang2022GraphBD,\n author = {Zhengyao Jiang and Tianjun Zhang and Robert Kirk and Tim Rocktaschel and Edward Grefenstette},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Graph Backup: Data Efficient Backup Exploiting Markovian Transitions},\n volume = {abs/2205.15824},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "jiang2022"
  },
  {
    "id": "http://arxiv.org/abs/1110.0909v2",
    "title": "$\\ell^p$-distortion and $p$-spectral gap of finite regular graphs",
    "published": "2011-10-05T07:07:40Z",
    "updated": "2013-07-02T08:47:16Z",
    "authors": [
      "Pierre-Nicolas Jolissaint",
      "Alain Valette"
    ],
    "summary": "We give a lower bound for the $\\ell^p$-distortion $c_p(X)$ of finite graphs $X$, depending on the first eigenvalue $λ_1^{(p)}(X)$ of the $p$-Laplacian and the maximal displacement of permutations of vertices. For a $k$-regular vertex-transitive graph it takes the form $c_p(X)^{p}\\geq diam(X)^{p}λ_{1}^{(p)}(X)/2^{p-1}k$. This bound is optimal for expander families and, for $p=2$, it gives the exact value for cycles and hypercubes. As a new application we give a non-trivial lower bound for the $\\ell^2$-distortion of a family of Cayley graphs of $SL_n(q)$ ($q$ fixed, $n\\geq 2$) with respect to a standard two-element generating set.",
    "pdf_url": "https://arxiv.org/pdf/1110.0909v2",
    "doi": "10.1112/blms/bdt096",
    "categories": [
      "math.MG",
      "math.CO"
    ],
    "primary_category": "math.MG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 17,
    "bibtex": "@Article{Jolissaint2011LpdistortionAP,\n author = {P. Jolissaint and A. Valette},\n journal = {Bulletin of the London Mathematical Society},\n title = {Lp‐distortion and p‐spectral gap of finite graphs},\n volume = {46},\n year = {2011}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "jolissaint2011"
  },
  {
    "id": "http://arxiv.org/abs/cond-mat/0403725v2",
    "title": "Threshold values, stability analysis and high-q asymptotics for the coloring problem on random graphs",
    "published": "2004-03-30T14:47:25Z",
    "updated": "2004-07-28T22:32:07Z",
    "authors": [
      "Florent Krzakala",
      "Andrea Pagnani",
      "Martin Weigt"
    ],
    "summary": "We consider the problem of coloring Erdos-Renyi and regular random graphs of finite connectivity using q colors. It has been studied so far using the cavity approach within the so-called one-step replica symmetry breaking (1RSB) ansatz. We derive a general criterion for the validity of this ansatz and, applying it to the ground state, we provide evidence that the 1RSB solution gives exact threshold values c_q for the q-COL/UNCOL phase transition. We also study the asymptotic thresholds for q >> 1 finding c_q = 2qlog(q)-log(q)-1+o(1) in perfect agreement with rigorous mathematical bounds, as well as the nature of excited states, and give a global phase diagram of the problem.",
    "pdf_url": "https://arxiv.org/pdf/cond-mat/0403725v2",
    "doi": "10.1103/PhysRevE.70.046705",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.CC"
    ],
    "primary_category": "cond-mat.dis-nn",
    "comment": "23 pages, 10 figures. Replaced with accepted version",
    "journal_ref": "Phys. Rev. E 70, 046705 (2004)",
    "citation_count": null,
    "bibtex": null,
    "markdown_text": null,
    "ranking": null,
    "citation_key": "krzakala2004"
  },
  {
    "id": "http://arxiv.org/abs/cond-mat/0410583v1",
    "title": "Partition function of two- and three-dimensional Potts ferromagnets for arbitrary values of q>0",
    "published": "2004-10-22T16:50:44Z",
    "updated": "2004-10-22T16:50:44Z",
    "authors": [
      "A. K. Hartmann"
    ],
    "summary": "A new algorithm is presented, which allows to calculate numerically the partition function Z_q of the d-dimensional q-state Potts models for arbitrary real values q>0 at any given temperature T with high precision. The basic idea is to measure the distribution of the number of connected components in the corresponding Fortuin-Kasteleyn representation and to compare with the distribution of the case q=1 (graph percolation), where the exact result Z_1=1 is known.\n  As application, d=2 and d=3-dimensional ferromagnetic Potts models are studied, and the critical values q_c, where the transition changes from second to first order, are determined. Large systems of sizes N=1000^2 respectively N=100^3 are treated. The critical value q_c(d=2)=4 is confirmed and q_c(d=3)=2.35(5) is found.",
    "pdf_url": "https://arxiv.org/pdf/cond-mat/0410583v1",
    "doi": "10.1103/PhysRevLett.94.050601",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": "4 pages, 4 figures, RevTex",
    "journal_ref": "Phys. Rev. Lett. 94, 050601 (2005)",
    "citation_count": null,
    "bibtex": null,
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hartmann2004"
  },
  {
    "id": "http://arxiv.org/abs/2107.03338v2",
    "title": "Discontinuous phase transitions in the q-voter model with generalized anticonformity on random graphs",
    "published": "2021-07-07T16:26:57Z",
    "updated": "2021-08-16T10:30:53Z",
    "authors": [
      "Angelika Abramiuk-Szurlej",
      "Arkadiusz Lipiecki",
      "Jakub Pawłowski",
      "Katarzyna Sznajd-Weron"
    ],
    "summary": "We study the binary $q$-voter model with generalized anticonformity on random Erdős-Rényi graphs. In such a model, two types of social responses, conformity and anticonformity, occur with complementary probabilities and the size of the source of influence $q_c$ in case of conformity is independent from the size of the source of influence $q_a$ in case of anticonformity. For $q_c=q_a=q$ the model reduces to the original $q$-voter model with anticonformity. Previously, such a generalized model was studied only on the complete graph, which corresponds to the mean-field approach. It was shown that it can display discontinuous phase transitions for $q_c \\ge q_a + Δq$, where $Δq=4$ for $q_a \\le 3$ and $Δq=3$ for $q_a>3$. In this paper, we pose the question if discontinuous phase transitions survive on random graphs with an average node degree $\\langle k\\rangle \\le 150$ observed empirically in social networks. Using the pair approximation, as well as Monte Carlo simulations, we show that discontinuous phase transitions indeed can survive, even for relatively small values of $\\langle k\\rangle$. Moreover, we show that for $q_a < q_c - 1$ pair approximation results overlap the Monte Carlo ones. On the other hand, for $q_a \\ge q_c - 1$ pair approximation gives qualitatively wrong results indicating discontinuous phase transitions neither observed in the simulations nor within the mean-field approach. Finally, we report an intriguing result showing that the difference between the spinodals obtained within the pair approximation and the mean-field approach follows a power law with respect to $\\langle k\\rangle$, as long as the pair approximation indicates correctly the type of the phase transition.",
    "pdf_url": "https://arxiv.org/pdf/2107.03338v2",
    "doi": null,
    "categories": [
      "physics.soc-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "10 pages, 6 figures",
    "journal_ref": null,
    "citation_count": 14,
    "bibtex": "@Article{Abramiuk-Szurlej2021DiscontinuousPT,\n author = {Angelika Abramiuk-Szurlej and Arkadiusz Lipiecki and Jakub Pawłowski and K. Sznajd-Weron},\n booktitle = {Scientific Reports},\n journal = {Scientific Reports},\n title = {Discontinuous phase transitions in the q-voter model with generalized anticonformity on random graphs},\n volume = {11},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "abramiukszurlej2021"
  },
  {
    "id": "http://arxiv.org/abs/2307.14548v4",
    "title": "Nonlinear $q$-voter model involving nonconformity on networks",
    "published": "2023-07-27T00:10:48Z",
    "updated": "2025-01-08T02:58:40Z",
    "authors": [
      "Rinto Anugraha NQZ",
      "Roni Muslim",
      "Henokh Lugo Hariyanto",
      "Fahrudin Nugroho",
      "Idham Syah Alam",
      "Muhammad Ardhi Khalif"
    ],
    "summary": "The order-disorder phase transition is a fascinating phenomenon in opinion dynamics models within sociophysics. This transition emerges due to noise parameters, interpreted as social behaviors such as anticonformity and independence (nonconformity) in a social context. In this study, we examine the impact of nonconformist behaviors on the macroscopic states of the system. Both anticonformity and independence are parameterized by a probability \\( p \\), with the model implemented on a complete graph and a scale-free network. Furthermore, we introduce a skepticism parameter \\( s \\), which quantifies a voter's propensity for nonconformity. Our analytical and simulation results reveal that the model exhibits continuous and discontinuous phase transitions for nonzero values of \\( s \\) at specific values of \\( q \\). We estimate the critical exponents using finite-size scaling analysis to classify the model's universality. The findings suggest that the model on the complete graph and the scale-free network share the same universality class as the mean-field Ising model. Additionally, we explore the scaling behavior associated with variations in \\( s \\) and assess the influence of \\( p \\) and \\( s \\) on the system's opinion dynamics.",
    "pdf_url": "https://arxiv.org/pdf/2307.14548v4",
    "doi": "10.1016/j.physd.2024.134508",
    "categories": [
      "physics.soc-ph"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "14 pages and 15 figures",
    "journal_ref": "Volume 472, February 2025, 134508",
    "citation_count": 3,
    "bibtex": "@Article{Nqz2023NonlinearX,\n author = {Rinto Anugraha Nqz and R. Muslim and Henokh Lugo Hariyanto and Fahrudin Nugroho and Idham Syah Alam and Muhammad Ardhi Khalif},\n booktitle = {Physica A: Statistical Mechanics and its Applications},\n journal = {Physica D: Nonlinear Phenomena},\n title = {Nonlinear <mml:math xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\" id=\"d1e1829\" altimg=\"si72.svg\"><mml:mi>q</mml:mi></mml:math>-voter model involving nonconformity on networks},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "nqz2023"
  },
  {
    "id": "http://arxiv.org/abs/2005.01076v2",
    "title": "The complexity of approximating the complex-valued Potts model",
    "published": "2020-05-03T12:59:52Z",
    "updated": "2021-11-18T15:09:41Z",
    "authors": [
      "Andreas Galanis",
      "Leslie Ann Goldberg",
      "Andrés Herrera-Poyatos"
    ],
    "summary": "We study the complexity of approximating the partition function of the $q$-state Potts model and the closely related Tutte polynomial for complex values of the underlying parameters. Apart from the classical connections with quantum computing and phase transitions in statistical physics, recent work in approximate counting has shown that the behaviour in the complex plane, and more precisely the location of zeros, is strongly connected with the complexity of the approximation problem, even for positive real-valued parameters. Previous work in the complex plane by Goldberg and Guo focused on $q=2$, which corresponds to the case of the Ising model; for $q>2$, the behaviour in the complex plane is not as well understood and most work applies only to the real-valued Tutte plane.\n  Our main result is a complete classification of the complexity of the approximation problems for all non-real values of the parameters, by establishing \\#P-hardness results that apply even when restricted to planar graphs. Our techniques apply to all $q\\geq 2$ and further complement/refine previous results both for the Ising model and the Tutte plane, answering in particular a question raised by Bordewich, Freedman, Lovász and Welsh in the context of quantum computations.",
    "pdf_url": "https://arxiv.org/pdf/2005.01076v2",
    "doi": null,
    "categories": [
      "cs.CC",
      "cs.DM",
      "math.CO"
    ],
    "primary_category": "cs.CC",
    "comment": "58 pages. Changes on version 2: minor changes",
    "journal_ref": null,
    "citation_count": 7,
    "bibtex": "@Article{Galanis2020TheCO,\n author = {Andreas Galanis and L. A. Goldberg and Andrés Herrera-Poyatos},\n booktitle = {Computational Complexity},\n journal = {computational complexity},\n title = {The complexity of approximating the complex-valued Potts model},\n volume = {31},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "galanis2020"
  },
  {
    "id": "http://arxiv.org/abs/0709.3039v3",
    "title": "Random even graphs",
    "published": "2007-09-19T15:24:21Z",
    "updated": "2009-03-25T17:35:39Z",
    "authors": [
      "Geoffrey Grimmett",
      "Svante Janson"
    ],
    "summary": "We study a random even subgraph of a finite graph $G$ with a general edge-weight $p\\in(0,1)$. We demonstrate how it may be obtained from a certain random-cluster measure on $G$, and we propose a sampling algorithm based on coupling from the past. A random even subgraph of a planar lattice undergoes a phase transition at the parameter-value $\\frac 12 \\pc$, where $\\pc$ is the critical point of the $q=2$ random-cluster model on the dual lattice. The properties of such a graph are discussed, and are related to Schramm--Löwner evolutions (SLE).",
    "pdf_url": "https://arxiv.org/pdf/0709.3039v3",
    "doi": null,
    "categories": [
      "math.PR",
      "math-ph"
    ],
    "primary_category": "math.PR",
    "comment": "Version 2 includes material about random even graphs with general values of the edge-parameter p, together with a coupling-from-the-past algorithm for their simulation. Version 3 includes a treatment of infinite graphs, and is to appear in the Electronic Journal of Combinatorics",
    "journal_ref": null,
    "citation_count": 38,
    "bibtex": "@Article{Grimmett2007RandomEG,\n author = {G. Grimmett and S. Janson},\n booktitle = {Electronic Journal of Combinatorics},\n journal = {Electron. J. Comb.},\n title = {Random Even Graphs},\n volume = {16},\n year = {2007}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "grimmett2007"
  },
  {
    "id": "http://arxiv.org/abs/2109.13645v1",
    "title": "Optimization of the dynamic transition in the continuous coloring problem",
    "published": "2021-09-28T12:04:00Z",
    "updated": "2021-09-28T12:04:00Z",
    "authors": [
      "Angelo Giorgio Cavaliere",
      "Thibault Lesieur",
      "Federico Ricci-Tersenghi"
    ],
    "summary": "Random constraint satisfaction problems can exhibit a phase where the number of constraints per variable $α$ makes the system solvable in theory on the one hand, but also makes the search for a solution hard, meaning that common algorithms such as Monte-Carlo method fail to find a solution. The onset of this hardness is deeply linked to the appearance of a dynamical phase transition where the phase space of the problem breaks into an exponential number of clusters. The exact position of this dynamical phase transition is not universal with respect to the details of the Hamiltonian one chooses to represent a given problem. In this paper, we develop some theoretical tools in order to find a systematic way to build a Hamiltonian that maximizes the dynamic $α_{\\rm d}$ threshold. To illustrate our techniques, we will concentrate on the problem of continuous coloring, where one tries to set an angle $x_i \\in [0;2π]$ on each node of a network in such a way that no adjacent nodes are closer than some threshold angle $θ$, that is $\\cos(x_i - x_j) \\leq \\cosθ$. This problem can be both seen as a continuous version of the discrete graph coloring problem or as a one-dimensional version of the the Mari-Krzakala-Kurchan (MKK) model. The relevance of this model stems from the fact that continuous constraint satisfaction problems on sparse random graphs remain largely unexplored in statistical physics. We show that for sufficiently small angle $θ$ this model presents a random first order transition and compute the dynamical, condensation and Kesten-Stigum transitions; we also compare the analytical predictions with Monte Carlo simulations for values of $θ= 2π/q$, $q \\in \\mathbb{N}$. Choosing such values of $q$ allows us to easily compare our results with the renowned problem of discrete coloring.",
    "pdf_url": "https://arxiv.org/pdf/2109.13645v1",
    "doi": "10.1088/1742-5468/ac382e",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.dis-nn",
    "comment": null,
    "journal_ref": "J. Stat. Mech. 113302 (2021)",
    "citation_count": 6,
    "bibtex": "@Article{Cavaliere2021OptimizationOT,\n author = {A. Cavaliere and T. Lesieur and F. Ricci-Tersenghi},\n booktitle = {Journal of Statistical Mechanics: Theory and Experiment},\n journal = {Journal of Statistical Mechanics: Theory and Experiment},\n title = {Optimization of the dynamic transition in the continuous coloring problem},\n volume = {2021},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "cavaliere2021"
  },
  {
    "id": "http://arxiv.org/abs/2310.14359v1",
    "title": "The Fiedler connection to the parametrized modularity optimization for community detection",
    "published": "2023-10-22T17:00:26Z",
    "updated": "2023-10-22T17:00:26Z",
    "authors": [
      "Dimitris Floros",
      "Nikos Pitsianis",
      "Xiaobai Sun"
    ],
    "summary": "This paper presents a comprehensive analysis of the generalized spectral structure of the modularity matrix $B$, which is introduced by Newman as the kernel matrix for the quadratic-form expression of the modularity function $Q$ used for community detection. The analysis is then seamlessly extended to the resolution-parametrized modularity matrix $B(γ)$, where $γ$ denotes the resolution parameter. The modularity spectral analysis provides fresh and profound insights into the $γ$-dynamics within the framework of modularity maximization for community detection. It provides the first algebraic explanation of the resolution limit at any specific $γ$ value. Among the significant findings and implications, the analysis reveals that (1) the maxima of the quadratic function with $B(γ)$ as the kernel matrix always reside in the Fiedler space of the normalized graph Laplacian $L$ or the null space of $L$, or their combination, and (2) the Fiedler value of the graph Laplacian $L$ marks the critical $γ$ value in the transition of candidate community configuration states between graph division and aggregation. Additionally, this paper introduces and identifies the Fiedler pseudo-set (FPS) as the de facto critical region for the state transition. This work is expected to have an immediate and long-term impact on improvements in algorithms for modularity maximization and on model transformations.",
    "pdf_url": "https://arxiv.org/pdf/2310.14359v1",
    "doi": null,
    "categories": [
      "physics.soc-ph",
      "physics.data-an"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "11 pages, 3 figures, 1 table",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Inproceedings{Floros2023TheFC,\n author = {D. Floros and N. Pitsianis and Xiaobai Sun},\n title = {The Fiedler connection to the parametrized modularity optimization for community detection},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "floros2023"
  },
  {
    "id": "http://arxiv.org/abs/1705.03104v2",
    "title": "Sharp phase transition for the random-cluster and Potts models via decision trees",
    "published": "2017-05-08T22:05:54Z",
    "updated": "2018-12-23T14:14:04Z",
    "authors": [
      "Hugo Duminil-Copin",
      "Aran Raoufi",
      "Vincent Tassion"
    ],
    "summary": "We prove an inequality on decision trees on monotonic measures which generalizes the OSSS inequality on product spaces. As an application, we use this inequality to prove a number of new results on lattice spin models and their random-cluster representations. More precisely, we prove that\n  1. For the Potts model on transitive graphs, correlations decay exponentially fast for $β<β_c$.\n  2. For the random-cluster model with cluster weight $q\\geq1$ on transitive graphs, correlations decay exponentially fast in the subcritical regime and the cluster-density satisfies the mean-field lower bound in the supercritical regime.\n  3. For the random-cluster models with cluster weight $q\\geq1$ on planar quasi-transitive graphs $\\mathbb{G}$,\n  $$\\frac{p_c(\\mathbb{G})p_c(\\mathbb{G}^*)}{(1-p_c(\\mathbb{G}))(1-p_c(\\mathbb{G}^*))}~=~q.$$ As a special case, we obtain the value of the critical point for the square, triangular and hexagonal lattices (this provides a short proof of the result of Beffara and Duminil-Copin [Probability Theory and Related Fields, 153(3-4):511--542, 2012]).\n  These results have many applications for the understanding of the subcritical (respectively disordered) phase of all these models. The techniques developed in this paper have potential to be extended to a wide class of models including the Ashkin-Teller model, continuum percolation models such as Voronoi percolation and Boolean percolation, super-level sets of massive Gaussian Free Field, and random-cluster and Potts model with infinite range interactions.",
    "pdf_url": "https://arxiv.org/pdf/1705.03104v2",
    "doi": null,
    "categories": [
      "math.PR",
      "math-ph"
    ],
    "primary_category": "math.PR",
    "comment": "16 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 163,
    "bibtex": "@Article{Duminil-Copin2017SharpPT,\n author = {H. Duminil-Copin and Aran Raoufi and V. Tassion},\n booktitle = {Annals of Mathematics},\n journal = {Annals of Mathematics},\n title = {Sharp phase transition for the random-cluster and Potts models via decision trees},\n year = {2017}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "duminilcopin2017"
  },
  {
    "id": "http://arxiv.org/abs/1412.1004v2",
    "title": "On rigidity, orientability and cores of random graphs with sliders",
    "published": "2014-12-02T18:21:09Z",
    "updated": "2015-02-20T10:21:55Z",
    "authors": [
      "Julien Barré",
      "Marc Lelarge",
      "Dieter Mitsche"
    ],
    "summary": "Suppose that you add rigid bars between points in the plane, and suppose that a constant fraction $q$ of the points moves freely in the whole plane; the remaining fraction is constrained to move on fixed lines called sliders. When does a giant rigid cluster emerge? Under a genericity condition, the answer only depends on the graph formed by the points (vertices) and the bars (edges). We find for the random graph $G \\in \\mathcal{G}(n,c/n)$ the threshold value of $c$ for the appearance of a linear-sized rigid component as a function of $q$, generalizing results of Kasiviswanathan et al. We show that this appearance of a giant component undergoes a continuous transition for $q \\leq 1/2$ and a discontinuous transition for $q > 1/2$. In our proofs, we introduce a generalized notion of orientability interpolating between 1- and 2-orientability, of cores interpolating between 2-core and 3-core, and of extended cores interpolating between 2+1-core and 3+2-core; we find the precise expressions for the respective thresholds and the sizes of the different cores above the threshold. In particular, this proves a conjecture of Kasiviswanathan et al. about the size of the 3+2-core. We also derive some structural properties of rigidity with sliders (matroid and decomposition into components) which can be of independent interest.",
    "pdf_url": "https://arxiv.org/pdf/1412.1004v2",
    "doi": null,
    "categories": [
      "math.CO",
      "cond-mat.stat-mech",
      "math.PR"
    ],
    "primary_category": "math.CO",
    "comment": "32 pages, 1 figure",
    "journal_ref": null,
    "citation_count": 5,
    "bibtex": "@Article{Barr'e2014OnRO,\n author = {Julien Barr'e and M. Lelarge and D. Mitsche},\n booktitle = {Random Struct. Algorithms},\n journal = {Random Structures & Algorithms},\n pages = {419 - 453},\n title = {On rigidity, orientability, and cores of random graphs with sliders},\n volume = {52},\n year = {2014}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "barr2014"
  },
  {
    "id": "http://arxiv.org/abs/1901.00125v3",
    "title": "Kinked Entropy and Discontinuous Microcanonical Spontaneous Symmetry Breaking",
    "published": "2019-01-01T09:37:58Z",
    "updated": "2019-04-25T12:37:24Z",
    "authors": [
      "Hai-Jun Zhou"
    ],
    "summary": "Spontaneous symmetry breaking (SSB) in statistical physics is a macroscopic collective phenomenon. For the paradigmatic Q-state Potts model it means a transition from the disordered color-symmetric phase to an ordered phase in which one color dominates. Existing mean field theories imply that SSB in the microcanonical statistical ensemble (with energy being the control parameter) should be a continuous process. Here we study microcanonical SSB on the random-graph Potts model, and discover that the entropy is a kinked function of energy. This kink leads to a discontinuous phase transition at certain energy density value, characterized by a jump in the density of the dominant color and a jump in the microcanonical temperature. This discontinuous SSB in random graphs is confirmed by microcanonical Monte Carlo simulations, and it is also observed in bond-diluted finite-size lattice systems.",
    "pdf_url": "https://arxiv.org/pdf/1901.00125v3",
    "doi": "10.1103/PhysRevLett.122.160601",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": "16 pages, extensively revised and improved",
    "journal_ref": "Phys. Rev. Lett. 122, 160601 (2019)",
    "citation_count": 5,
    "bibtex": "@Article{Zhou2019KinkedEA,\n author = {Haijun Zhou},\n booktitle = {Physical Review Letters},\n journal = {Physical review letters},\n pages = {\n          160601\n        },\n title = {Kinked Entropy and Discontinuous Microcanonical Spontaneous Symmetry Breaking.},\n volume = {122 16},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "zhou2019"
  },
  {
    "id": "http://arxiv.org/abs/1904.11208v1",
    "title": "An ensemble of random graphs with identical degree distribution",
    "published": "2019-04-25T08:40:32Z",
    "updated": "2019-04-25T08:40:32Z",
    "authors": [
      "Fei Ma",
      "Xiaoming Wang",
      "Ping Wang"
    ],
    "summary": "Degree distribution, or equivalently called degree sequence, has been commonly used to be one of most significant measures for studying a large number of complex networks with which some well-known results have been obtained. By contrast, in this paper, we report a fact that two arbitrarily chosen networks with identical degree distribution can have completely different other topological structure, such as diameter, spanning trees number, pearson correlation coefficient, and so forth. Besides that, for a given degree distribution (as power-law distribution with exponent $γ=3$ discussed here), it is reasonable to ask how many network models with such a constraint we can have. To this end, we generate an ensemble of this kind of random graphs with $P(k)\\sim k^{-γ}$ ($γ=3$), denoted as graph space $\\mathcal{N}(p,q,t)$ where probability parameters $p$ and $q$ hold on $p+q=1$, and indirectly show the cardinality of $\\mathcal{N}(p,q,t)$ seems to be large enough in the thermodynamics limit, i.e., $N\\rightarrow\\infty$, by varying values of $p$ and $q$. From the theoretical point of view, given an ultrasmall constant $p_{c}$, perhaps only graph model $N(1,0,t)$ is small-world and other are not in terms of diameter. And then, we study spanning trees number on two deterministic graph models and obtain both upper bound and lower bound for other members. Meanwhile, for arbitrary $p(\\neq1)$, we prove that graph model $N(p,q,t)$ does go through two phase transitions over time, i.e., starting by non-assortative pattern and then suddenly going into disassortative region, and gradually converging to initial place (non-assortative point). Among of them, one \"null\" graph model is built.",
    "pdf_url": "https://arxiv.org/pdf/1904.11208v1",
    "doi": "10.1063/1.5105354",
    "categories": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": null,
    "journal_ref": null,
    "citation_count": 18,
    "bibtex": "@Article{Ma2019AnEO,\n author = {Fei Ma and Xiaomin Wang and Ping Wang},\n booktitle = {Chaos},\n journal = {Chaos},\n pages = {\n          013136\n        },\n title = {An ensemble of random graphs with identical degree distribution},\n volume = {30 1},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ma2019"
  },
  {
    "id": "http://arxiv.org/abs/2106.04249v3",
    "title": "Expansion, long cycles, and complete minors in supercritical random subgraphs of the hypercube",
    "published": "2021-06-08T10:55:09Z",
    "updated": "2021-12-01T10:57:53Z",
    "authors": [
      "Joshua Erde",
      "Mihyun Kang",
      "Michael Krivelevich"
    ],
    "summary": "Analogous to the case of the binomial random graph $G(d+1,p)$, it is known that the behaviour of a random subgraph of a $d$-dimensional hypercube, where we include each edge independently with probability $p$, which we denote by $Q^d_p$, undergoes a phase transition around the critical value of $p=\\frac{1}{d}$. More precisely, standard arguments show that significantly below this value of $p$, with probability tending to one as $d \\to \\infty$ (whp for short) all components of this graph have order $O(d)$, whereas Ajtai, Komlós and Szemerédi showed that significantly above this value, in the \\emph{supercritical regime}, whp there is a unique `giant' component of order $Θ\\left(2^d\\right)$. In $G(d+1,p)$ much more is known about the complex structure of the random graph which emerges in this supercritical regime. For example, it is known that in this regime whp $G(d+1,p)$ contains paths and cycles of length $Ω(d)$, as well as complete minors of order $Ω\\left(\\sqrt{d}\\right)$. In this paper we obtain analogous results in $Q^d_p$. In particular, we show that for supercritical $p$, i.e., when $p=\\frac{1+ε}{d}$ for a positive constant $ε$, whp $Q^d_p$ contains a cycle of length $Ω\\left(\\frac{2^d}{d^3(\\log d)^3} \\right)$ and a complete minor of order $Ω\\left(\\frac{2^{\\frac{d}{2}}}{d^3(\\log d)^3 }\\right)$. In order to prove these results, we show that whp the largest component of $Q^d_p$ has good edge-expansion properties, a result of independent interest. We also consider the genus of $Q^d_p$ and show that, in this regime of $p$, whp the genus is $Ω\\left(2^d\\right)$.",
    "pdf_url": "https://arxiv.org/pdf/2106.04249v3",
    "doi": null,
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO",
    "comment": "20 pages, the results of this paper are superseded by those in arXiv:2111.06752 and this paper will not be published",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Inproceedings{Erde2021ExpansionLC,\n author = {Joshua Erde and Mihyun Kang and Michael Krivelevich},\n title = {Expansion, long cycles, and complete minors in supercritical random subgraphs of the hypercube},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "erde2021"
  },
  {
    "id": "http://arxiv.org/abs/2305.06051v1",
    "title": "The external field effect on the opinion formation based on the majority rule and the $q$-voter models on the complete graph",
    "published": "2023-05-10T11:09:20Z",
    "updated": "2023-05-10T11:09:20Z",
    "authors": [
      "Azhari",
      "Roni Muslim"
    ],
    "summary": "We investigate the external field effect on opinion formation based on the majority rule and $q$-voter models on a complete graph. The external field can be considered as the mass media in the social system, with the probability $p$ agents following the mass media opinion. Based on our Monte Carlo simulation, the mass media effect is not strong enough to make the system reach a homogeneous state (complete consensus) with the magnetization $m = 1$ for all values of $p$, indicates that the existence of a usual phase transition for all values of $p$. In the $q$-voter model, the mass media eliminates the usual phase transition at $p \\approx 0.21$. We obtain the model's critical point and scaling parameters using the finite-size scaling analysis and obtain that both models have the same scaling parameters. The external field effect decreases both models' relaxation time and the relaxation time following the power-law relation such as $τ\\sim N^β$, where $N$ is the population size, and $β$ depends on the probability $p$. In the majority rule model, $β$ follows a linear relation, and in the $q$-voter model, $β$ follows a power-law relation.",
    "pdf_url": "https://arxiv.org/pdf/2305.06051v1",
    "doi": "10.1142/S0129183123500882",
    "categories": [
      "physics.soc-ph"
    ],
    "primary_category": "physics.soc-ph",
    "comment": null,
    "journal_ref": "International Journal of Modern Physics C, 2350088 (2022)",
    "citation_count": 12,
    "bibtex": "@Article{Azhari2022TheEF,\n author = {Azhari and R. Muslim},\n booktitle = {International Journal of Modern Physics C},\n journal = {International Journal of Modern Physics C},\n title = {The external field effect on the opinion formation based on the majority rule and the q-voter models on the complete graph},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "azhari2023"
  },
  {
    "id": "http://arxiv.org/abs/2310.02607v1",
    "title": "Convergence Analysis of Kernel Conjugate Gradient for Functional Linear Regression",
    "published": "2023-10-04T06:43:05Z",
    "updated": "2023-10-04T06:43:05Z",
    "authors": [
      "Naveen Gupta",
      "S. Sivananthan",
      "Bharath K. Sriperumbudur"
    ],
    "summary": "In this paper, we discuss the convergence analysis of the conjugate gradient-based algorithm for the functional linear model in the reproducing kernel Hilbert space framework, utilizing early stopping results in regularization against over-fitting. We establish the convergence rates depending on the regularity condition of the slope function and the decay rate of the eigenvalues of the operator composition of covariance and kernel operator. Our convergence rates match the minimax rate available from the literature.",
    "pdf_url": "https://arxiv.org/pdf/2310.02607v1",
    "doi": null,
    "categories": [
      "math.ST"
    ],
    "primary_category": "math.ST",
    "comment": null,
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Gupta2023ConvergenceAO,\n author = {Naveen Gupta and And S. SIVANANTHAN and Bharath K. Sriperumbudur},\n booktitle = {Journal of Applied and Numerical Analysis},\n journal = {Journal of Applied and Numerical Analysis},\n title = {Convergence analysis of kernel conjugate gradient for functional linear regression},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "gupta2023"
  },
  {
    "id": "http://arxiv.org/abs/2012.07167v8",
    "title": "Pseudo-likelihood-based $M$-estimation of random graphs with dependent edges and parameter vectors of increasing dimension",
    "published": "2020-12-13T22:07:32Z",
    "updated": "2025-05-19T15:37:31Z",
    "authors": [
      "Jonathan R. Stewart",
      "Michael Schweinberger"
    ],
    "summary": "An important question in statistical network analysis is how to estimate models of discrete and dependent network data with intractable likelihood functions, without sacrificing computational scalability and statistical guarantees. We demonstrate that scalable estimation of random graph models with dependent edges is possible, by establishing convergence rates of pseudo-likelihood-based $M$-estimators for discrete undirected graphical models with exponential parameterizations and parameter vectors of increasing dimension in single-observation scenarios. We highlight the impact of two complex phenomena on the convergence rate: phase transitions and model near-degeneracy. The main results have possible applications to discrete and dependent network, spatial, and temporal data. To showcase convergence rates, we introduce a novel class of generalized $β$-models with dependent edges and parameter vectors of increasing dimension, which leverage additional structure in the form of overlapping subpopulations to control dependence. We establish convergence rates of pseudo-likelihood-based $M$-estimators for generalized $β$-models in dense- and sparse-graph settings.",
    "pdf_url": "https://arxiv.org/pdf/2012.07167v8",
    "doi": null,
    "categories": [
      "math.ST"
    ],
    "primary_category": "math.ST",
    "comment": null,
    "journal_ref": null,
    "citation_count": 9,
    "bibtex": "@Article{Stewart2020PseudolikelihoodbasedO,\n author = {J. Stewart and M. Schweinberger},\n journal = {arXiv: Statistics Theory},\n title = {Pseudo-likelihood-based $M$-estimation of random graphs with dependent edges and parameter vectors of increasing dimension.},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "stewart2020"
  },
  {
    "id": "http://arxiv.org/abs/2104.02578v1",
    "title": "Neurons learn slower than they think",
    "published": "2021-04-02T09:09:52Z",
    "updated": "2021-04-02T09:09:52Z",
    "authors": [
      "Ilona Kulikovskikh"
    ],
    "summary": "Recent studies revealed complex convergence dynamics in gradient-based methods, which has been little understood so far. Changing the step size to balance between high convergence rate and small generalization error may not be sufficient: maximizing the test accuracy usually requires a larger learning rate than minimizing the training loss. To explore the dynamic bounds of convergence rate, this study introduces \\textit{differential capability} into an optimization process, which measures whether the test accuracy increases as fast as a model approaches the decision boundary in a classification problem. The convergence analysis showed that: 1) a higher convergence rate leads to slower capability growth; 2) a lower convergence rate results in faster capability growth and decay; 3) regulating a convergence rate in either direction reduces differential capability.",
    "pdf_url": "https://arxiv.org/pdf/2104.02578v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Kulikovskikh2021NeuronsLS,\n author = {I. Kulikovskikh},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Neurons learn slower than they think},\n volume = {abs/2104.02578},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "kulikovskikh2021"
  },
  {
    "id": "http://arxiv.org/abs/2510.12512v1",
    "title": "Temporal Variabilities Limit Convergence Rates in Gradient-Based Online Optimization",
    "published": "2025-10-14T13:41:32Z",
    "updated": "2025-10-14T13:41:32Z",
    "authors": [
      "Bryan Van Scoy",
      "Gianluca Bianchin"
    ],
    "summary": "This paper investigates the fundamental performance limits of gradient-based algorithms for time-varying optimization. Leveraging the internal model principle and root locus techniques, we show that temporal variabilities impose intrinsic limits on the achievable rate of convergence. For a problem with condition ratio $κ$ and time variation whose model has degree $n$, we show that the worst-case convergence rate of any minimal-order gradient-based algorithm is $ρ_\\text{TV} = (\\frac{κ-1}{κ+1})^{1/n}$. This bound reveals a fundamental tradeoff between problem conditioning, temporal complexity, and rate of convergence. We further construct explicit controllers that attain the bound for low-degree models of time variation.",
    "pdf_url": "https://arxiv.org/pdf/2510.12512v1",
    "doi": null,
    "categories": [
      "math.OC",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Scoy2025TemporalVL,\n author = {Bryan Van Scoy and G. Bianchin},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Temporal Variabilities Limit Convergence Rates in Gradient-Based Online Optimization},\n volume = {abs/2510.12512},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "scoy2025"
  },
  {
    "id": "http://arxiv.org/abs/1901.00150v3",
    "title": "Accelerated MM Algorithms for Ranking Scores Inference from Comparison Data",
    "published": "2019-01-01T13:08:53Z",
    "updated": "2020-12-26T12:49:42Z",
    "authors": [
      "Milan Vojnovic",
      "Seyoung Yun",
      "Kaifang Zhou"
    ],
    "summary": "In this paper, we study a popular method for inference of the Bradley-Terry model parameters, namely the MM algorithm, for maximum likelihood estimation and maximum a posteriori probability estimation. This class of models includes the Bradley-Terry model of paired comparisons, the Rao-Kupper model of paired comparisons allowing for tie outcomes, the Luce choice model, and the Plackett-Luce ranking model. We establish tight characterizations of the convergence rate for the MM algorithm, and show that it is essentially equivalent to that of a gradient descent algorithm. For the maximum likelihood estimation, the convergence is shown to be linear with the rate crucially determined by the algebraic connectivity of the matrix of item pair co-occurrences in observed comparison data. For the Bayesian inference, the convergence rate is also shown to be linear, with the rate determined by a parameter of the prior distribution in a way that can make the convergence arbitrarily slow for small values of this parameter. We propose a simple modification of the classical MM algorithm that avoids the observed slow convergence issue and accelerates the convergence. The key component of the accelerated MM algorithm is a parameter rescaling performed at each iteration step that is carefully chosen based on theoretical analysis and characterisation of the convergence rate.\n  Our experimental results, performed on both synthetic and real-world data, demonstrate the identified slow convergence issue of the classic MM algorithm, and show that significant efficiency gains can be obtained by our new proposed method.",
    "pdf_url": "https://arxiv.org/pdf/1901.00150v3",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "stat.ML",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Vojnović2019ConvergenceRO,\n author = {M. Vojnović and Seyoung Yun and Kaifang Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Convergence Rates of Gradient Descent and MM Algorithms for Generalized Bradley-Terry Models},\n volume = {abs/1901.00150},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "vojnovic2019"
  },
  {
    "id": "http://arxiv.org/abs/2209.12834v2",
    "title": "On convergence rate bounds for a class of nonlinear Markov chains",
    "published": "2022-09-26T16:39:12Z",
    "updated": "2022-09-28T21:28:49Z",
    "authors": [
      "Alexander Shchegolev",
      "Alexander Veretennikov"
    ],
    "summary": "A new approach is developed for evaluating the convergence rate for nonlinear Markov chains (MC) based on the recently developed spectral radius technique of markovian coupling for linear MC and the idea of small nonlinear perturbations of linear MC. The method further enhances recent advances in the problem of convergence for such models. The new convergence rate may be used, in particular, for the justification of $D$-condition in the Extreme Values theory.",
    "pdf_url": "https://arxiv.org/pdf/2209.12834v2",
    "doi": "10.61102/1024-2953-mprf.2023.29.5.001",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR",
    "comment": "15 pages, 12 references",
    "journal_ref": "Markov processes and related fields, 2023, v.29, Issue 5, 619-639",
    "citation_count": 2,
    "bibtex": "@Inproceedings{Shchegolev2022OnCR,\n author = {A. Shchegolev and A. Veretennikov},\n title = {On convergence rate bounds for a class of nonlinear Markov chains},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "shchegolev2022"
  },
  {
    "id": "http://arxiv.org/abs/2310.15387v1",
    "title": "Error analysis of generative adversarial network",
    "published": "2023-10-23T22:39:28Z",
    "updated": "2023-10-23T22:39:28Z",
    "authors": [
      "Mahmud Hasan",
      "Hailin Sang"
    ],
    "summary": "The generative adversarial network (GAN) is an important model developed for high-dimensional distribution learning in recent years. However, there is a pressing need for a comprehensive method to understand its error convergence rate. In this research, we focus on studying the error convergence rate of the GAN model that is based on a class of functions encompassing the discriminator and generator neural networks. These functions are VC type with bounded envelope function under our assumptions, enabling the application of the Talagrand inequality. By employing the Talagrand inequality and Borel-Cantelli lemma, we establish a tight convergence rate for the error of GAN. This method can also be applied on existing error estimations of GAN and yields improved convergence rates. In particular, the error defined with the neural network distance is a special case error in our definition.",
    "pdf_url": "https://arxiv.org/pdf/2310.15387v1",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "16 pages",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Hasan2023ErrorAO,\n author = {Mahmud Hasan and Hailin Sang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Error analysis of generative adversarial network},\n volume = {abs/2310.15387},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hasan2023"
  },
  {
    "id": "http://arxiv.org/abs/2306.15760v1",
    "title": "xAI-CycleGAN, a Cycle-Consistent Generative Assistive Network",
    "published": "2023-06-27T19:26:28Z",
    "updated": "2023-06-27T19:26:28Z",
    "authors": [
      "Tibor Sloboda",
      "Lukáš Hudec",
      "Wanda Benešová"
    ],
    "summary": "In the domain of unsupervised image-to-image transformation using generative transformative models, CycleGAN has become the architecture of choice. One of the primary downsides of this architecture is its relatively slow rate of convergence. In this work, we use discriminator-driven explainability to speed up the convergence rate of the generative model by using saliency maps from the discriminator that mask the gradients of the generator during backpropagation, based on the work of Nagisetty et al., and also introducing the saliency map on input, added onto a Gaussian noise mask, by using an interpretable latent variable based on Wang M.'s Mask CycleGAN. This allows for an explainability fusion in both directions, and utilizing the noise-added saliency map on input as evidence-based counterfactual filtering. This new architecture has much higher rate of convergence than a baseline CycleGAN architecture while preserving the image quality.",
    "pdf_url": "https://arxiv.org/pdf/2306.15760v1",
    "doi": "10.1007/978-3-031-44137-0_33",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 4 figures, ICVS TU Wien 2023",
    "journal_ref": "Computer Vision Systems. ICVS 2023. Lecture Notes in Computer Science, vol 14253. Springer",
    "citation_count": 2,
    "bibtex": "@Article{Sloboda2023xAICycleGANAC,\n author = {Tibor Sloboda and L. Hudec and Wanda Benesova},\n booktitle = {International Conference on Virtual Storytelling},\n pages = {403-411},\n title = {xAI-CycleGAN, a Cycle-Consistent Generative Assistive Network},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "sloboda2023"
  },
  {
    "id": "http://arxiv.org/abs/2102.02396v2",
    "title": "Exact Linear Convergence Rate Analysis for Low-Rank Symmetric Matrix Completion via Gradient Descent",
    "published": "2021-02-04T03:41:54Z",
    "updated": "2021-02-06T03:55:14Z",
    "authors": [
      "Trung Vu",
      "Raviv Raich"
    ],
    "summary": "Factorization-based gradient descent is a scalable and efficient algorithm for solving low-rank matrix completion. Recent progress in structured non-convex optimization has offered global convergence guarantees for gradient descent under certain statistical assumptions on the low-rank matrix and the sampling set. However, while the theory suggests gradient descent enjoys fast linear convergence to a global solution of the problem, the universal nature of the bounding technique prevents it from obtaining an accurate estimate of the rate of convergence. In this paper, we perform a local analysis of the exact linear convergence rate of gradient descent for factorization-based matrix completion for symmetric matrices. Without any additional assumptions on the underlying model, we identify the deterministic condition for local convergence of gradient descent, which only depends on the solution matrix and the sampling set. More crucially, our analysis provides a closed-form expression of the asymptotic rate of convergence that matches exactly with the linear convergence observed in practice. To the best of our knowledge, our result is the first one that offers the exact rate of convergence of gradient descent for matrix factorization in Euclidean space for matrix completion.",
    "pdf_url": "https://arxiv.org/pdf/2102.02396v2",
    "doi": null,
    "categories": [
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "comment": "Full version",
    "journal_ref": null,
    "citation_count": 10,
    "bibtex": "@Article{Vu2021ExactLC,\n author = {Trung Vu and R. Raich},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {3240-3244},\n title = {Exact Linear Convergence Rate Analysis for Low-Rank Symmetric Matrix Completion via Gradient Descent},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "vu2021"
  },
  {
    "id": "http://arxiv.org/abs/1812.02409v1",
    "title": "Goodness-of-fit testing the error distribution in multivariate indirect regression",
    "published": "2018-12-06T09:11:32Z",
    "updated": "2018-12-06T09:11:32Z",
    "authors": [
      "Justin Chown",
      "Nicolai Bissantz",
      "Holger Dette"
    ],
    "summary": "We propose a goodness-of-fit test for the distribution of errors from a multivariate indirect regression model. The test statistic is based on the Khmaladze transformation of the empirical process of standardized residuals. This goodness-of-fit test is consistent at the root-n rate of convergence, and the test can maintain power against local alternatives converging to the null at a root-n rate.",
    "pdf_url": "https://arxiv.org/pdf/1812.02409v1",
    "doi": null,
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME",
    "comment": "23 pages, 4 figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Chown2018GoodnessoffitTT,\n author = {J. Chown and N. Bissantz and H. Dette},\n booktitle = {Electronic Journal of Statistics},\n journal = {Electronic Journal of Statistics},\n title = {Goodness-of-fit testing the error distribution in multivariate indirect regression},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "chown2018"
  },
  {
    "id": "http://arxiv.org/abs/1310.4221v1",
    "title": "Flow of Navier-Stokes Fluids in Converging-Diverging Distensible Tubes",
    "published": "2013-10-15T22:42:48Z",
    "updated": "2013-10-15T22:42:48Z",
    "authors": [
      "Taha Sochi"
    ],
    "summary": "We use a method based on the lubrication approximation in conjunction with a residual-based mass-continuity iterative solution scheme to compute the flow rate and pressure field in distensible converging-diverging tubes for Navier-Stokes fluids. We employ an analytical formula derived from a one-dimensional version of the Navier-Stokes equations to describe the underlying flow model that provides the residual function. This formula correlates the flow rate to the boundary pressures in straight cylindrical elastic tubes with constant-radius. We validate our findings by the convergence toward a final solution with fine discretization as well as by comparison to the Poiseuille-type flow in its convergence toward analytic solutions found earlier in rigid converging-diverging tubes. We also tested the method on limiting special cases of cylindrical elastic tubes with constant-radius where the numerical solutions converged to the expected analytical solutions. The distensible model has also been endorsed by its convergence toward the rigid Poiseuille-type model with increasing the tube wall stiffness. Lubrication-based one-dimensional finite element method was also used for verification. In this investigation five converging-diverging geometries are used for demonstration, validation and as prototypes for modeling converging-diverging geometries in general.",
    "pdf_url": "https://arxiv.org/pdf/1310.4221v1",
    "doi": "10.1016/j.aej.2015.03.028",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "31 pages, 9 figures, 2 tables",
    "journal_ref": null,
    "citation_count": 10,
    "bibtex": "@Article{Sochi2013NavierStokesFI,\n author = {T. Sochi},\n journal = {alexandria engineering journal},\n pages = {713-723},\n title = {Navier-Stokes Flow in Converging-Diverging Distensible Tubes},\n volume = {54},\n year = {2013}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "sochi2013"
  },
  {
    "id": "http://arxiv.org/abs/2001.03443v5",
    "title": "Accelerated and nonaccelerated stochastic gradient descent with model conception",
    "published": "2020-01-10T13:47:53Z",
    "updated": "2020-07-13T09:51:34Z",
    "authors": [
      "Darina Dvinskikh",
      "Alexander Tyurin",
      "Alexander Gasnikov",
      "Sergey Omelchenko"
    ],
    "summary": "In this paper, we describe a new way to get convergence rates for optimal methods in smooth (strongly) convex optimization tasks. Our approach is based on results for tasks where gradients have nonrandom small noises. Unlike previous results, we obtain convergence rates with model conception.",
    "pdf_url": "https://arxiv.org/pdf/2001.03443v5",
    "doi": null,
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC",
    "comment": "in Russian",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Dvinskikh2020AcceleratedAN,\n author = {D. Dvinskikh and A. Tyurin and A. Gasnikov},\n journal = {arXiv: Optimization and Control},\n title = {Accelerated and nonaccelerated stochastic gradient descent with model conception},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "dvinskikh2020"
  },
  {
    "id": "http://arxiv.org/abs/1807.05855v1",
    "title": "A Fast-Converged Acoustic Modeling for Korean Speech Recognition: A Preliminary Study on Time Delay Neural Network",
    "published": "2018-07-11T05:34:09Z",
    "updated": "2018-07-11T05:34:09Z",
    "authors": [
      "Hosung Park",
      "Donghyun Lee",
      "Minkyu Lim",
      "Yoseb Kang",
      "Juneseok Oh",
      "Ji-Hwan Kim"
    ],
    "summary": "In this paper, a time delay neural network (TDNN) based acoustic model is proposed to implement a fast-converged acoustic modeling for Korean speech recognition. The TDNN has an advantage in fast-convergence where the amount of training data is limited, due to subsampling which excludes duplicated weights. The TDNN showed an absolute improvement of 2.12% in terms of character error rate compared to feed forward neural network (FFNN) based modelling for Korean speech corpora. The proposed model converged 1.67 times faster than a FFNN-based model did.",
    "pdf_url": "https://arxiv.org/pdf/1807.05855v1",
    "doi": null,
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 2 figures",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Park2018AFA,\n author = {Hosung Park and Donghyun Lee and Minkyu Lim and Yoseb Kang and Juneseok Oh and Ji-Hwan Kim},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Fast-Converged Acoustic Modeling for Korean Speech Recognition: A Preliminary Study on Time Delay Neural Network},\n volume = {abs/1807.05855},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "park2018"
  },
  {
    "id": "http://arxiv.org/abs/2011.02379v2",
    "title": "Asynchrony and Acceleration in Gossip Algorithms",
    "published": "2020-11-04T16:15:32Z",
    "updated": "2021-02-07T11:26:03Z",
    "authors": [
      "Mathieu Even",
      "Hadrien Hendrikx",
      "Laurent Massoulié"
    ],
    "summary": "This paper considers the minimization of a sum of smooth and strongly convex functions dispatched over the nodes of a communication network. Previous works on the subject either focus on synchronous algorithms, which can be heavily slowed down by a few slow nodes (the straggler problem), or consider a model of asynchronous operation (Boyd et al., 2006) in which adjacent nodes communicate at the instants of Poisson point processes. We have two main contributions. 1) We propose CACDM (a Continuously Accelerated Coordinate Dual Method), and for the Poisson model of asynchronous operation, we prove CACDM to converge to optimality at an accelerated convergence rate in the sense of Nesterov et Stich, 2017. In contrast, previously proposed asynchronous algorithms have not been proven to achieve such accelerated rate. While CACDM is based on discrete updates, the proof of its convergence crucially depends on a continuous time analysis. 2) We introduce a new communication scheme based on Loss-Networks, that is programmable in a fully asynchronous and decentralized way, unlike the Poisson model of asynchronous operation that does not capture essential aspects of asynchrony such as non-instantaneous communications and computations. Under this Loss-Network model of asynchrony, we establish for CDM (a Coordinate Dual Method) a rate of convergence in terms of the eigengap of the Laplacian of the graph weighted by local effective delays. We believe this eigengap to be a fundamental bottleneck for convergence rates of asynchronous optimization. Finally, we verify empirically that CACDM enjoys an accelerated convergence rate in the Loss-Network model of asynchrony.",
    "pdf_url": "https://arxiv.org/pdf/2011.02379v2",
    "doi": null,
    "categories": [
      "cs.DC",
      "cs.MA",
      "math.OC"
    ],
    "primary_category": "cs.DC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Even2020AsynchronyAA,\n author = {Mathieu Even and Hadrien Hendrikx and L. Massouli'e},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Asynchrony and Acceleration in Gossip Algorithms},\n volume = {abs/2011.02379},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "even2020"
  },
  {
    "id": "http://arxiv.org/abs/1701.08407v3",
    "title": "Subband adaptive filter trained by differential evolution for channel estimation",
    "published": "2017-01-29T17:30:53Z",
    "updated": "2017-03-17T02:04:06Z",
    "authors": [
      "Lu Lu",
      "Haiquan Zhao"
    ],
    "summary": "The normalized subband adaptive filter (NSAF) is widely accepted as a preeminent adaptive filtering algorithm because of its efficiency under the colored excitation. However, the convergence rate of NSAF is slow. To address this drawback, in this paper, a variant of the NSAF, called the differential evolution (DE)-NSAF (DE-NSAF), is proposed for channel estimation based on DE strategy. It is worth noticing that there are several papers concerning designing DE strategies for adaptive filter. But their signal models are still the single adaptive filter model rather than the fullband adaptive filter model considered in this paper. Thus, the problem considered in our work is quite different from those. The proposed DE-NSAF algorithm is based on real-valued manipulations and has fast convergence rate for searching the global solution of optimized weight vector. Moreover, a design step of new algorithm is given in detail. Simulation results demonstrate the improved performance of the proposed DE-NSAF algorithm in terms of the convergence rate.",
    "pdf_url": "https://arxiv.org/pdf/1701.08407v3",
    "doi": null,
    "categories": [
      "cs.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "7 pages, 4 figures",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Lu2017SteadystatePA,\n author = {Lu Lu and Haiquan Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Steady-state performance analysis of the recursive maximum correntropy algorithm and its application in adaptive beamforming with alpha-stable noise},\n volume = {abs/1701.08407},\n year = {2017}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "lu2017"
  },
  {
    "id": "http://arxiv.org/abs/2109.14474v3",
    "title": "Asymptotic Properties of the Maximum Smoothed Partial Likelihood Estimator in the Change-Plane Cox Model",
    "published": "2021-09-29T15:05:25Z",
    "updated": "2023-02-12T19:20:42Z",
    "authors": [
      "Shota Takeishi"
    ],
    "summary": "The change-plane Cox model is a popular tool for the subgroup analysis of survival data. Despite the rich literature on this model, there has been limited investigation into the asymptotic properties of the estimators of the finite-dimensional parameter. Particularly, the convergence rate, not to mention the asymptotic distribution, has not been fully characterized for the general model where classification is based on multiple covariates. To bridge this theoretical gap, this study proposes a maximum smoothed partial likelihood estimator and establishes the following asymptotic properties. First, it shows that the convergence rate for the classification parameter can be arbitrarily close to 1/n up to a logarithmic factor under a certain condition on covariates and the choice of tuning parameter. Given this convergence rate result, it also establishes the asymptotic normality for the regression parameter.",
    "pdf_url": "https://arxiv.org/pdf/2109.14474v3",
    "doi": null,
    "categories": [
      "math.ST",
      "stat.ME"
    ],
    "primary_category": "math.ST",
    "comment": "33 pages; accepted to Scandinavian Journal of Statistics",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Takeishi2021AsymptoticPO,\n author = {Shota Takeishi},\n booktitle = {Scandinavian Journal of Statistics},\n journal = {Scandinavian Journal of Statistics},\n pages = {1503 - 1531},\n title = {Asymptotic properties of the maximum smoothed partial likelihood estimator in the change‐plane Cox model},\n volume = {50},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "takeishi2021"
  },
  {
    "id": "http://arxiv.org/abs/0811.0503v1",
    "title": "Trimming and likelihood: Robust location and dispersion estimation in the elliptical model",
    "published": "2008-11-04T14:04:33Z",
    "updated": "2008-11-04T14:04:33Z",
    "authors": [
      "Juan A. Cuesta-Albertos",
      "Carlos Matrán",
      "Agustín Mayo-Iscar"
    ],
    "summary": "Robust estimators of location and dispersion are often used in the elliptical model to obtain an uncontaminated and highly representative subsample by trimming the data outside an ellipsoid based in the associated Mahalanobis distance. Here we analyze some one (or $k$)-step Maximum Likelihood Estimators computed on a subsample obtained with such a procedure. We introduce different models which arise naturally from the ways in which the discarded data can be treated, leading to truncated or censored likelihoods, as well as to a likelihood based on an only outliers gross errors model. Results on existence, uniqueness, robustness and asymptotic properties of the proposed estimators are included. A remarkable fact is that the proposed estimators generally keep the breakdown point of the initial (robust) estimators, but they could improve the rate of convergence of the initial estimator because our estimators always converge at rate $n^{1/2}$, independently of the rate of convergence of the initial estimator.",
    "pdf_url": "https://arxiv.org/pdf/0811.0503v1",
    "doi": "10.1214/07-AOS541",
    "categories": [
      "math.ST"
    ],
    "primary_category": "math.ST",
    "comment": "Published in at http://dx.doi.org/10.1214/07-AOS541 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org)",
    "journal_ref": "Annals of Statistics 2008, Vol. 36, No. 5, 2284-2318",
    "citation_count": 29,
    "bibtex": "@Article{Cuesta-Albertos2008TrimmingAL,\n author = {J. A. Cuesta-Albertos and C. Matr'an and A. Mayo‐Iscar},\n journal = {Annals of Statistics},\n pages = {2284-2318},\n title = {Trimming and likelihood: Robust location and dispersion estimation in the elliptical model},\n volume = {36},\n year = {2008}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "cuestaalbertos2008"
  },
  {
    "id": "http://arxiv.org/abs/2408.11629v1",
    "title": "A Markovian Model for Learning-to-Optimize",
    "published": "2024-08-21T14:00:22Z",
    "updated": "2024-08-21T14:00:22Z",
    "authors": [
      "Michael Sucker",
      "Peter Ochs"
    ],
    "summary": "We present a probabilistic model for stochastic iterative algorithms with the use case of optimization algorithms in mind. Based on this model, we present PAC-Bayesian generalization bounds for functions that are defined on the trajectory of the learned algorithm, for example, the expected (non-asymptotic) convergence rate and the expected time to reach the stopping criterion. Thus, not only does this model allow for learning stochastic algorithms based on their empirical performance, it also yields results about their actual convergence rate and their actual convergence time. We stress that, since the model is valid in a more general setting than learning-to-optimize, it is of interest for other fields of application, too. Finally, we conduct five practically relevant experiments, showing the validity of our claims.",
    "pdf_url": "https://arxiv.org/pdf/2408.11629v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Sucker2024AMM,\n author = {Michael Sucker and Peter Ochs},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Markovian Model for Learning-to-Optimize},\n volume = {abs/2408.11629},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "sucker2024"
  },
  {
    "id": "http://arxiv.org/abs/2106.13878v1",
    "title": "Convergence Analysis and Numerical Studies for Linearly Elastic Peridynamics with Dirichlet-Type Boundary Conditions",
    "published": "2021-06-25T20:40:42Z",
    "updated": "2021-06-25T20:40:42Z",
    "authors": [
      "Mikil Foss",
      "Petronela Radu",
      "Yue Yu"
    ],
    "summary": "The nonlocal models of peridynamics have successfully predicted fractures and deformations for a variety of materials. In contrast to local mechanics, peridynamic boundary conditions must be defined on a finite volume region outside the body. Therefore, theoretical and numerical challenges arise in order to properly formulate Dirichlet-type nonlocal boundary conditions, while connecting them to the local counterparts. While a careless imposition of local boundary conditions leads to a smaller effective material stiffness close to the boundary and an artificial softening of the material, several strategies were proposed to avoid this unphysical surface effect.\n  In this work, we study convergence of solutions to nonlocal state-based linear elastic model to their local counterparts as the interaction horizon vanishes, under different formulations and smoothness assumptions for nonlocal Dirichlet-type boundary conditions. Our results provide explicit rates of convergence that are sensitive to the compatibility of the nonlocal boundary data and the extension of the solution for the local model. In particular, under appropriate assumptions, constant extensions yield $\\frac{1}{2}$ order convergence rates and linear extensions yield $\\frac{3}{2}$ order convergence rates. With smooth extensions, these rates are improved to quadratic convergence. We illustrate the theory for any dimension $d\\geq 2$ and numerically verify the convergence rates with a number of two dimensional benchmarks, including linear patch tests, manufactured solutions, and domains with curvilinear surfaces. Numerical results show a first order convergence for constant extensions and second order convergence for linear extensions, which suggests a possible room of improvement in the future convergence analysis.",
    "pdf_url": "https://arxiv.org/pdf/2106.13878v1",
    "doi": null,
    "categories": [
      "math.AP",
      "cs.CE",
      "math.NA"
    ],
    "primary_category": "math.AP",
    "comment": null,
    "journal_ref": null,
    "citation_count": 28,
    "bibtex": "@Article{Foss2021ConvergenceAA,\n author = {M. Foss and P. Radu and Yue Yu},\n booktitle = {Journal of Peridynamics and Nonlocal Modeling},\n journal = {Journal of Peridynamics and Nonlocal Modeling},\n pages = {275-310},\n title = {Convergence Analysis and Numerical Studies for Linearly Elastic Peridynamics with Dirichlet-Type Boundary Conditions},\n volume = {5},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "foss2021"
  },
  {
    "id": "http://arxiv.org/abs/2509.15353v1",
    "title": "On the convergence rate in the central limit theorem for linearly extended negative quadrant dependent random variables and its applications",
    "published": "2025-09-18T18:46:30Z",
    "updated": "2025-09-18T18:46:30Z",
    "authors": [
      "Mohamed Kaber El Alem",
      "Zohra Guessoum",
      "Abdelkader Tatachak",
      "Ourida Sadki"
    ],
    "summary": "In this paper, we establish the convergence rate in central limit theorem (CLT) for linearly extended negative quadrant dependent (LENQD) random variables (rv's). Under some weak conditions, the rate of normal approximation is shown as $O(n^{-1/9})$. As an application, the convergence rate in CLT of the wavelet estimator for the nonparametric regression model with LENQD errors is presented as $O(n^{-1/9})$. The performance of the main results is illustrated through a simulation study based on a real dataset.",
    "pdf_url": "https://arxiv.org/pdf/2509.15353v1",
    "doi": null,
    "categories": [
      "math.ST"
    ],
    "primary_category": "math.ST",
    "comment": "This manuscript has been submitted to the Electronic Journal of Probability",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Alem2025OnTC,\n author = {Mohamed Kaber El Alem and Z. Guessoum and Abdelkader Tatachak and Ourida Sadki},\n title = {On the convergence rate in the central limit theorem for linearly extended negative quadrant dependent random variables and its applications},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "alem2025"
  },
  {
    "id": "http://arxiv.org/abs/2312.11788v1",
    "title": "Faster Convergence with Multiway Preferences",
    "published": "2023-12-19T01:52:13Z",
    "updated": "2023-12-19T01:52:13Z",
    "authors": [
      "Aadirupa Saha",
      "Vitaly Feldman",
      "Tomer Koren",
      "Yishay Mansour"
    ],
    "summary": "We address the problem of convex optimization with preference feedback, where the goal is to minimize a convex function given a weaker form of comparison queries. Each query consists of two points and the dueling feedback returns a (noisy) single-bit binary comparison of the function values of the two queried points. Here we consider the sign-function-based comparison feedback model and analyze the convergence rates with batched and multiway (argmin of a set queried points) comparisons. Our main goal is to understand the improved convergence rates owing to parallelization in sign-feedback-based optimization problems. Our work is the first to study the problem of convex optimization with multiway preferences and analyze the optimal convergence rates. Our first contribution lies in designing efficient algorithms with a convergence rate of $\\smash{\\widetilde O}(\\frac{d}{\\min\\{m,d\\} ε})$ for $m$-batched preference feedback where the learner can query $m$-pairs in parallel. We next study a $m$-multiway comparison (`battling') feedback, where the learner can get to see the argmin feedback of $m$-subset of queried points and show a convergence rate of $\\smash{\\widetilde O}(\\frac{d}{ \\min\\{\\log m,d\\}ε})$. We show further improved convergence rates with an additional assumption of strong convexity. Finally, we also study the convergence lower bounds for batched preferences and multiway feedback optimization showing the optimality of our convergence rates w.r.t. $m$.",
    "pdf_url": "https://arxiv.org/pdf/2312.11788v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Saha2023FasterCW,\n author = {Aadirupa Saha and Vitaly Feldman and Tomer Koren and Y. Mansour},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n journal = {ArXiv},\n title = {Faster Convergence with Multiway Preferences},\n volume = {abs/2312.11788},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "saha2023"
  },
  {
    "id": "http://arxiv.org/abs/1003.1535v1",
    "title": "Kink estimation in stochastic regression with dependent errors and predictors",
    "published": "2010-03-08T00:38:36Z",
    "updated": "2010-03-08T00:38:36Z",
    "authors": [
      "Justin Wishart",
      "Rafal Kulik"
    ],
    "summary": "In this article we study the estimation of the location of jump points in the first derivative (referred to as kinks) of a regression function μin two random design models with different long-range dependent (LRD) structures. The method is based on the zero-crossing technique and makes use of high-order kernels. The rate of convergence of the estimator   is contingent on the level of dependence and the smoothness of the regression function μ. In one of the models, the convergence rate is the same as the minimax rate for kink estimation in the fixed design scenario with i.i.d. errors which suggests that the method is optimal in the minimax sense.",
    "pdf_url": "https://arxiv.org/pdf/1003.1535v1",
    "doi": null,
    "categories": [
      "math.ST"
    ],
    "primary_category": "math.ST",
    "comment": "35 pages",
    "journal_ref": null,
    "citation_count": 9,
    "bibtex": "@Article{Wishart2010KinkEI,\n author = {J. Wishart and Rafal Kulik},\n journal = {Electronic Journal of Statistics},\n pages = {875-913},\n title = {Kink estimation in stochastic regression with dependent errors and predictors},\n volume = {4},\n year = {2010}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wishart2010"
  },
  {
    "id": "http://arxiv.org/abs/2306.16077v2",
    "title": "Secure and Fast Asynchronous Vertical Federated Learning via Cascaded Hybrid Optimization",
    "published": "2023-06-28T10:18:08Z",
    "updated": "2023-06-29T14:42:05Z",
    "authors": [
      "Ganyu Wang",
      "Qingsong Zhang",
      "Li Xiang",
      "Boyu Wang",
      "Bin Gu",
      "Charles Ling"
    ],
    "summary": "Vertical Federated Learning (VFL) attracts increasing attention because it empowers multiple parties to jointly train a privacy-preserving model over vertically partitioned data. Recent research has shown that applying zeroth-order optimization (ZOO) has many advantages in building a practical VFL algorithm. However, a vital problem with the ZOO-based VFL is its slow convergence rate, which limits its application in handling modern large models. To address this problem, we propose a cascaded hybrid optimization method in VFL. In this method, the downstream models (clients) are trained with ZOO to protect privacy and ensure that no internal information is shared. Meanwhile, the upstream model (server) is updated with first-order optimization (FOO) locally, which significantly improves the convergence rate, making it feasible to train the large models without compromising privacy and security. We theoretically prove that our VFL framework converges faster than the ZOO-based VFL, as the convergence of our framework is not limited by the size of the server model, making it effective for training large models with the major part on the server. Extensive experiments demonstrate that our method achieves faster convergence than the ZOO-based VFL framework, while maintaining an equivalent level of privacy protection. Moreover, we show that the convergence of our VFL is comparable to the unsafe FOO-based VFL baseline. Additionally, we demonstrate that our method makes the training of a large model feasible.",
    "pdf_url": "https://arxiv.org/pdf/2306.16077v2",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Article{Wang2023SecureAF,\n author = {Ganyu Wang and Qingsong Zhang and Li Xiang and Boyu Wang and Bin Gu and C. Ling},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n pages = {6413 - 6451},\n title = {Secure and fast asynchronous Vertical Federated Learning via cascaded hybrid optimization},\n volume = {113},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wang2023"
  },
  {
    "id": "http://arxiv.org/abs/1611.01280v2",
    "title": "Optimal portfolio selection under vanishing fixed transaction costs",
    "published": "2016-11-04T07:43:09Z",
    "updated": "2017-07-06T08:22:09Z",
    "authors": [
      "Sören Christensen",
      "Albrecht Irle",
      "Andreas Ludwig"
    ],
    "summary": "In this paper, asymptotic results in a long-term growth rate portfolio optimization model under both fixed and proportional transaction costs are obtained. More precisely, the convergence of the model when the fixed costs tend to zero is investigated. A suitable limit model with purely proportional costs is introduced and an optimal strategy is shown to consist of keeping the risky fraction process in a unique interval $[A,B]\\subseteq\\,]0,1[$ with minimal effort. Furthermore, the convergence of optimal boundaries, asymptotic growth rates, and optimal risky fraction processes is rigorously proved. The results are based on an in-depth analysis of the convergence of the solutions to the corresponding HJB-equations.",
    "pdf_url": "https://arxiv.org/pdf/1611.01280v2",
    "doi": null,
    "categories": [
      "q-fin.PM"
    ],
    "primary_category": "q-fin.PM",
    "comment": null,
    "journal_ref": null,
    "citation_count": 7,
    "bibtex": "@Article{Christensen2016OptimalPS,\n author = {S. Christensen and A. Irle and A. Ludwig},\n booktitle = {Advances in Applied Probability},\n journal = {Advances in Applied Probability},\n pages = {1116 - 1143},\n title = {Optimal portfolio selection under vanishing fixed transaction costs},\n volume = {49},\n year = {2016}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "christensen2016"
  },
  {
    "id": "http://arxiv.org/abs/2502.04849v1",
    "title": "Advancing Wasserstein Convergence Analysis of Score-Based Models: Insights from Discretization and Second-Order Acceleration",
    "published": "2025-02-07T11:37:51Z",
    "updated": "2025-02-07T11:37:51Z",
    "authors": [
      "Yifeng Yu",
      "Lu Yu"
    ],
    "summary": "Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\varepsilon}\\right)$ significantly improving upon the standard rate $\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\varepsilon^2}\\right)$ of vanilla diffusion models, where $\\varepsilon$ denotes the target accuracy.",
    "pdf_url": "https://arxiv.org/pdf/2502.04849v1",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "stat.ML",
    "comment": null,
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Article{Yu2025AdvancingWC,\n author = {Yifeng Yu and Lu Yu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Advancing Wasserstein Convergence Analysis of Score-Based Models: Insights from Discretization and Second-Order Acceleration},\n volume = {abs/2502.04849},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yu2025"
  },
  {
    "id": "http://arxiv.org/abs/2505.17288v1",
    "title": "Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation",
    "published": "2025-05-22T21:05:04Z",
    "updated": "2025-05-22T21:05:04Z",
    "authors": [
      "Seamus Somerstep",
      "Vinod Raman",
      "Unique Subedi",
      "Yuekai Sun"
    ],
    "summary": "Using the bit string generation problem as a case study, we theoretically compare two standard methods for adapting large language models to new tasks. The first, referred to as supervised fine-tuning, involves training a new next token predictor on good generations. The second method, Best-of-N, trains a reward model to select good responses from a collection generated by an unaltered base model. If the learning setting is realizable, we find that supervised fine-tuning outperforms BoN through a better dependence on the response length in its rate of convergence. If realizability fails, then depending on the failure mode, BoN can enjoy a better rate of convergence in either n or a rate of convergence with better dependence on the response length.",
    "pdf_url": "https://arxiv.org/pdf/2505.17288v1",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Somerstep2025LearningTC,\n author = {Seamus Somerstep and Vinod Raman and Unique Subedi and Yuekai Sun},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning to Choose or Choosing to Learn: Best-of-N vs. Supervised Fine-Tuning for Bit String Generation},\n volume = {abs/2505.17288},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "somerstep2025"
  },
  {
    "id": "http://arxiv.org/abs/1510.00761v1",
    "title": "On the Rate of Convergence of Mean-Field Models: Stein's Method Meets the Perturbation Theory",
    "published": "2015-10-03T00:18:57Z",
    "updated": "2015-10-03T00:18:57Z",
    "authors": [
      "Lei Ying"
    ],
    "summary": "This paper studies the rate of convergence of a family of continuous-time Markov chains (CTMC) to a mean-field model. When the mean-field model is a finite-dimensional dynamical system with a unique equilibrium point, an analysis based on Stein's method and the perturbation theory shows that under some mild conditions, the stationary distributions of CTMCs converge (in the mean-square sense) to the equilibrium point of the mean-field model if the mean-field model is globally asymptotically stable and locally exponentially stable. In particular, the mean square difference between the $M$th CTMC in the steady state and the equilibrium point of the mean-field system is $O(1/M),$ where $M$ is the size of the $M$th CTMC. This approach based on Stein's method provides a new framework for studying the convergence of CTMCs to their mean-field limit by mainly looking into the stability of the mean-field model, which is a deterministic system and is often easier to analyze than the CTMCs. More importantly, this approach quantifies the rate of convergence, which reveals the approximation error of using mean-field models for approximating finite-size systems.",
    "pdf_url": "https://arxiv.org/pdf/1510.00761v1",
    "doi": null,
    "categories": [
      "cs.PF",
      "eess.SY",
      "math.PR"
    ],
    "primary_category": "cs.PF",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Ying2015OnTR,\n author = {Lei Ying},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {On the Rate of Convergence of Mean-Field Models: Stein's Method Meets the Perturbation Theory},\n volume = {abs/1510.00761},\n year = {2015}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ying2015"
  },
  {
    "id": "http://arxiv.org/abs/1304.4848v1",
    "title": "Sequential robust efficient estimation for nonparametric autoregressive models",
    "published": "2013-04-17T15:06:41Z",
    "updated": "2013-04-17T15:06:41Z",
    "authors": [
      "Ouerdia Arkoun",
      "Serguei Pergamenchtchikov"
    ],
    "summary": "We construct efficient robust truncated sequential estimators for the pointwise estimation problem in nonparametric autoregression models with smooth coefficients. For Gaussian models we propose an adaptive procedure based on the constructed sequential estimators. The minimax nonadaptive and adaptive convergence rates are established. It turns out that in this case these rates are the same as for regression models.",
    "pdf_url": "https://arxiv.org/pdf/1304.4848v1",
    "doi": null,
    "categories": [
      "math.ST"
    ],
    "primary_category": "math.ST",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Arkoun2013SequentialRE,\n author = {Ouerdia Arkoun and S. Pergamenchtchikov},\n journal = {arXiv: Statistics Theory},\n title = {Sequential robust efficient estimation for nonparametric autoregressive models},\n year = {2013}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "arkoun2013"
  },
  {
    "id": "http://arxiv.org/abs/2107.08377v1",
    "title": "A New Adaptive Gradient Method with Gradient Decomposition",
    "published": "2021-07-18T06:37:28Z",
    "updated": "2021-07-18T06:37:28Z",
    "authors": [
      "Zhou Shao",
      "Tong Lin"
    ],
    "summary": "Adaptive gradient methods, especially Adam-type methods (such as Adam, AMSGrad, and AdaBound), have been proposed to speed up the training process with an element-wise scaling term on learning rates. However, they often generalize poorly compared with stochastic gradient descent (SGD) and its accelerated schemes such as SGD with momentum (SGDM). In this paper, we propose a new adaptive method called DecGD, which simultaneously achieves good generalization like SGDM and obtain rapid convergence like Adam-type methods. In particular, DecGD decomposes the current gradient into the product of two terms including a surrogate gradient and a loss based vector. Our method adjusts the learning rates adaptively according to the current loss based vector instead of the squared gradients used in Adam-type methods. The intuition for adaptive learning rates of DecGD is that a good optimizer, in general cases, needs to decrease the learning rates as the loss decreases, which is similar to the learning rates decay scheduling technique. Therefore, DecGD gets a rapid convergence in the early phases of training and controls the effective learning rates according to the loss based vectors which help lead to a better generalization. Convergence analysis is discussed in both convex and non-convex situations. Finally, empirical results on widely-used tasks and models demonstrate that DecGD shows better generalization performance than SGDM and rapid convergence like Adam-type methods.",
    "pdf_url": "https://arxiv.org/pdf/2107.08377v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Shao2021ANA,\n author = {Zhou Shao and Hang Zhou and Tong Lin},\n booktitle = {Machine-mediated learning},\n journal = {Machine Learning},\n title = {A new adaptive gradient method with gradient decomposition},\n volume = {114},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "shao2021"
  },
  {
    "id": "http://arxiv.org/abs/2406.08185v2",
    "title": "Non-stationary Gaussian random fields on hypersurfaces: Sampling and strong error analysis",
    "published": "2024-06-12T13:16:01Z",
    "updated": "2024-11-29T13:09:24Z",
    "authors": [
      "Erik Jansson",
      "Annika Lang",
      "Mike Pereira"
    ],
    "summary": "A flexible model for non-stationary Gaussian random fields on hypersurfaces is introduced.The class of random fields on curves and surfaces is characterized by an amplitude spectral density of a second order elliptic differential operator.Sampling is done by a Galerkin--Chebyshev approximation based on the surface finite element method and Chebyshev polynomials. Strong error bounds are shown with convergence rates depending on the smoothness of the approximated random field. Numerical experiments that confirm the convergence rates are presented.",
    "pdf_url": "https://arxiv.org/pdf/2406.08185v2",
    "doi": null,
    "categories": [
      "math.NA",
      "math.PR"
    ],
    "primary_category": "math.NA",
    "comment": "V1: 32 pages, 4 figures. V2: Added improved convergence rate with proof, and numerical experiment. 39 pages, 6 figures",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Jansson2024NonstationaryGR,\n author = {Erik Jansson and Annika Lang and Mike Pereira},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Non-stationary Gaussian random fields on hypersurfaces: Sampling and strong error analysis},\n volume = {abs/2406.08185},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "jansson2024"
  },
  {
    "id": "http://arxiv.org/abs/2102.05710v1",
    "title": "Derivative-Free Reinforcement Learning: A Review",
    "published": "2021-02-10T19:29:22Z",
    "updated": "2021-02-10T19:29:22Z",
    "authors": [
      "Hong Qian",
      "Yang Yu"
    ],
    "summary": "Reinforcement learning is about learning agent models that make the best sequential decisions in unknown environments. In an unknown environment, the agent needs to explore the environment while exploiting the collected information, which usually forms a sophisticated problem to solve. Derivative-free optimization, meanwhile, is capable of solving sophisticated problems. It commonly uses a sampling-and-updating framework to iteratively improve the solution, where exploration and exploitation are also needed to be well balanced. Therefore, derivative-free optimization deals with a similar core issue as reinforcement learning, and has been introduced in reinforcement learning approaches, under the names of learning classifier systems and neuroevolution/evolutionary reinforcement learning. Although such methods have been developed for decades, recently, derivative-free reinforcement learning exhibits attracting increasing attention. However, recent survey on this topic is still lacking. In this article, we summarize methods of derivative-free reinforcement learning to date, and organize the methods in aspects including parameter updating, model selection, exploration, and parallel/distributed methods. Moreover, we discuss some current limitations and possible future directions, hoping that this article could bring more attentions to this topic and serve as a catalyst for developing novel and efficient approaches.",
    "pdf_url": "https://arxiv.org/pdf/2102.05710v1",
    "doi": "10.1007/s11704-020-0241-4",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This article has been accepted by Frontiers of Computer Science in 2020",
    "journal_ref": null,
    "citation_count": 46,
    "bibtex": "@Article{Qian2021DerivativefreeRL,\n author = {Hong Qian and Yang Yu},\n booktitle = {Frontiers of Computer Science},\n journal = {Frontiers of Computer Science},\n title = {Derivative-free reinforcement learning: a review},\n volume = {15},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "qian2021"
  },
  {
    "id": "http://arxiv.org/abs/2009.10396v1",
    "title": "Is Q-Learning Provably Efficient? An Extended Analysis",
    "published": "2020-09-22T09:00:25Z",
    "updated": "2020-09-22T09:00:25Z",
    "authors": [
      "Kushagra Rastogi",
      "Jonathan Lee",
      "Fabrice Harel-Canada",
      "Aditya Joglekar"
    ],
    "summary": "This work extends the analysis of the theoretical results presented within the paper Is Q-Learning Provably Efficient? by Jin et al. We include a survey of related research to contextualize the need for strengthening the theoretical guarantees related to perhaps the most important threads of model-free reinforcement learning. We also expound upon the reasoning used in the proofs to highlight the critical steps leading to the main result showing that Q-learning with UCB exploration achieves a sample efficiency that matches the optimal regret that can be achieved by any model-based approach.",
    "pdf_url": "https://arxiv.org/pdf/2009.10396v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Rastogi2020IsQP,\n author = {Kushagra Rastogi and Jonathan Lee and Fabrice Harel-Canada and Aditya Sunil Joglekar},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Is Q-Learning Provably Efficient? An Extended Analysis},\n volume = {abs/2009.10396},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "rastogi2020"
  },
  {
    "id": "http://arxiv.org/abs/2107.03603v1",
    "title": "CLAIM: Curriculum Learning Policy for Influence Maximization in Unknown Social Networks",
    "published": "2021-07-08T04:52:50Z",
    "updated": "2021-07-08T04:52:50Z",
    "authors": [
      "Dexun Li",
      "Meghna Lowalekar",
      "Pradeep Varakantham"
    ],
    "summary": "Influence maximization is the problem of finding a small subset of nodes in a network that can maximize the diffusion of information. Recently, it has also found application in HIV prevention, substance abuse prevention, micro-finance adoption, etc., where the goal is to identify the set of peer leaders in a real-world physical social network who can disseminate information to a large group of people. Unlike online social networks, real-world networks are not completely known, and collecting information about the network is costly as it involves surveying multiple people. In this paper, we focus on this problem of network discovery for influence maximization. The existing work in this direction proposes a reinforcement learning framework. As the environment interactions in real-world settings are costly, so it is important for the reinforcement learning algorithms to have minimum possible environment interactions, i.e, to be sample efficient. In this work, we propose CLAIM - Curriculum LeArning Policy for Influence Maximization to improve the sample efficiency of RL methods. We conduct experiments on real-world datasets and show that our approach can outperform the current best approach.",
    "pdf_url": "https://arxiv.org/pdf/2107.03603v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 10,
    "bibtex": "@Article{Li2021CLAIMCL,\n author = {Dexun Li and Meghna Lowalekar and Pradeep Varakantham},\n booktitle = {Conference on Uncertainty in Artificial Intelligence},\n pages = {1455-1465},\n title = {CLAIM: Curriculum Learning Policy for Influence Maximization in Unknown Social Networks},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "li2021"
  },
  {
    "id": "http://arxiv.org/abs/2206.09328v1",
    "title": "A Survey on Model-based Reinforcement Learning",
    "published": "2022-06-19T05:28:03Z",
    "updated": "2022-06-19T05:28:03Z",
    "authors": [
      "Fan-Ming Luo",
      "Tian Xu",
      "Hang Lai",
      "Xiong-Hui Chen",
      "Weinan Zhang",
      "Yang Yu"
    ],
    "summary": "Reinforcement learning (RL) solves sequential decision-making problems via a trial-and-error process interacting with the environment. While RL achieves outstanding success in playing complex video games that allow huge trial-and-error, making errors is always undesired in the real world. To improve the sample efficiency and thus reduce the errors, model-based reinforcement learning (MBRL) is believed to be a promising direction, which builds environment models in which the trial-and-errors can take place without real costs. In this survey, we take a review of MBRL with a focus on the recent progress in deep RL. For non-tabular environments, there is always a generalization error between the learned environment model and the real environment. As such, it is of great importance to analyze the discrepancy between policy training in the environment model and that in the real environment, which in turn guides the algorithm design for better model learning, model usage, and policy training. Besides, we also discuss the recent advances of model-based techniques in other forms of RL, including offline RL, goal-conditioned RL, multi-agent RL, and meta-RL. Moreover, we discuss the applicability and advantages of MBRL in real-world tasks. Finally, we end this survey by discussing the promising prospects for the future development of MBRL. We think that MBRL has great potential and advantages in real-world applications that were overlooked, and we hope this survey could attract more research on MBRL.",
    "pdf_url": "https://arxiv.org/pdf/2206.09328v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 137,
    "bibtex": "@Article{Luo2022ASO,\n author = {Fan Luo and Tian Xu and Hang Lai and Xiong-Hui Chen and Weinan Zhang and Yang Yu},\n booktitle = {Science China Information Sciences},\n journal = {Science China Information Sciences},\n title = {A survey on model-based reinforcement learning},\n volume = {67},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "luo2022"
  },
  {
    "id": "http://arxiv.org/abs/2506.17518v1",
    "title": "A Survey of State Representation Learning for Deep Reinforcement Learning",
    "published": "2025-06-20T23:47:04Z",
    "updated": "2025-06-20T23:47:04Z",
    "authors": [
      "Ayoub Echchahed",
      "Pablo Samuel Castro"
    ],
    "summary": "Representation learning methods are an important tool for addressing the challenges posed by complex observations spaces in sequential decision making problems. Recently, many methods have used a wide variety of types of approaches for learning meaningful state representations in reinforcement learning, allowing better sample efficiency, generalization, and performance. This survey aims to provide a broad categorization of these methods within a model-free online setting, exploring how they tackle the learning of state representations differently. We categorize the methods into six main classes, detailing their mechanisms, benefits, and limitations. Through this taxonomy, our aim is to enhance the understanding of this field and provide a guide for new researchers. We also discuss techniques for assessing the quality of representations, and detail relevant future directions.",
    "pdf_url": "https://arxiv.org/pdf/2506.17518v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Echchahed2025ASO,\n author = {Ayoub Echchahed and Pablo Samuel Castro},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {A Survey of State Representation Learning for Deep Reinforcement Learning},\n volume = {2025},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "echchahed2025"
  },
  {
    "id": "http://arxiv.org/abs/2412.07177v1",
    "title": "Effective Reward Specification in Deep Reinforcement Learning",
    "published": "2024-12-10T04:22:11Z",
    "updated": "2024-12-10T04:22:11Z",
    "authors": [
      "Julien Roy"
    ],
    "summary": "In the last decade, Deep Reinforcement Learning has evolved into a powerful tool for complex sequential decision-making problems. It combines deep learning's proficiency in processing rich input signals with reinforcement learning's adaptability across diverse control tasks. At its core, an RL agent seeks to maximize its cumulative reward, enabling AI algorithms to uncover novel solutions previously unknown to experts. However, this focus on reward maximization also introduces a significant difficulty: improper reward specification can result in unexpected, misaligned agent behavior and inefficient learning. The complexity of accurately specifying the reward function is further amplified by the sequential nature of the task, the sparsity of learning signals, and the multifaceted aspects of the desired behavior.\n  In this thesis, we survey the literature on effective reward specification strategies, identify core challenges relating to each of these approaches, and propose original contributions addressing the issue of sample efficiency and alignment in deep reinforcement learning. Reward specification represents one of the most challenging aspects of applying reinforcement learning in real-world domains. Our work underscores the absence of a universal solution to this complex and nuanced challenge; solving it requires selecting the most appropriate tools for the specific requirements of each unique application.",
    "pdf_url": "https://arxiv.org/pdf/2412.07177v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Roy2024EffectiveRS,\n author = {Julien Roy},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Effective Reward Specification in Deep Reinforcement Learning},\n volume = {abs/2412.07177},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "roy2024"
  },
  {
    "id": "http://arxiv.org/abs/1803.04706v5",
    "title": "Policy Search in Continuous Action Domains: an Overview",
    "published": "2018-03-13T09:57:42Z",
    "updated": "2019-06-13T11:39:06Z",
    "authors": [
      "Olivier Sigaud",
      "Freek Stulp"
    ],
    "summary": "Continuous action policy search is currently the focus of intensive research, driven both by the recent success of deep reinforcement learning algorithms and the emergence of competitors based on evolutionary algorithms. In this paper, we present a broad survey of policy search methods, providing a unified perspective on very different approaches, including also Bayesian Optimization and directed exploration methods. The main message of this overview is in the relationship between the families of methods, but we also outline some factors underlying sample efficiency properties of the various approaches.",
    "pdf_url": "https://arxiv.org/pdf/1803.04706v5",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in the Neural Networks Journal (Volume 113, May 2019)",
    "journal_ref": null,
    "citation_count": 76,
    "bibtex": "@Article{Sigaud2018PolicySI,\n author = {Olivier Sigaud and F. Stulp},\n booktitle = {Neural Networks},\n journal = {Neural networks : the official journal of the International Neural Network Society},\n pages = {\n          28-40\n        },\n title = {Policy Search in Continuous Action Domains: an Overview},\n volume = {113},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "sigaud2018"
  },
  {
    "id": "http://arxiv.org/abs/2109.06668v6",
    "title": "Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain",
    "published": "2021-09-14T13:16:33Z",
    "updated": "2023-02-02T00:11:58Z",
    "authors": [
      "Jianye Hao",
      "Tianpei Yang",
      "Hongyao Tang",
      "Chenjia Bai",
      "Jinyi Liu",
      "Zhaopeng Meng",
      "Peng Liu",
      "Zhen Wang"
    ],
    "summary": "Deep Reinforcement Learning (DRL) and Deep Multi-agent Reinforcement Learning (MARL) have achieved significant successes across a wide range of domains, including game AI, autonomous vehicles, robotics, and so on. However, DRL and deep MARL agents are widely known to be sample inefficient that millions of interactions are usually needed even for relatively simple problem settings, thus preventing the wide application and deployment in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how efficiently exploring the environment and collecting informative experiences that could benefit policy learning towards the optimal ones. This problem becomes more challenging in complex environments with sparse rewards, noisy distractions, long horizons, and non-stationary co-learners. In this paper, we conduct a comprehensive survey on existing exploration methods for both single-agent and multi-agent RL. We start the survey by identifying several key challenges to efficient exploration. Beyond the above two main branches, we also include other notable exploration methods with different ideas and techniques. In addition to algorithmic analysis, we provide a comprehensive and unified empirical comparison of different exploration methods for DRL on a set of commonly used benchmarks. According to our algorithmic and empirical investigation, we finally summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.",
    "pdf_url": "https://arxiv.org/pdf/2109.06668v6",
    "doi": "10.1109/TNNLS.2023.3236361",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by IEEE Transactions on Neural Networks and Learning Systems (TNNLS)",
    "journal_ref": null,
    "citation_count": 140,
    "bibtex": "@Article{Yang2021ExplorationID,\n author = {Tianpei Yang and Hongyao Tang and Chenjia Bai and Jinyi Liu and Jianye Hao and Zhaopeng Meng and Peng Liu and Zhen Wang},\n booktitle = {IEEE Transactions on Neural Networks and Learning Systems},\n journal = {IEEE Transactions on Neural Networks and Learning Systems},\n pages = {8762-8782},\n title = {Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain},\n volume = {35},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hao2021"
  },
  {
    "id": "http://arxiv.org/abs/2411.18892v2",
    "title": "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges",
    "published": "2024-11-28T03:53:14Z",
    "updated": "2025-02-01T23:49:26Z",
    "authors": [
      "Majid Ghasemi",
      "Amir Hossein Moosavi",
      "Dariush Ebrahimi"
    ],
    "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.",
    "pdf_url": "https://arxiv.org/pdf/2411.18892v2",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "79 pages",
    "journal_ref": null,
    "citation_count": 12,
    "bibtex": "@Inproceedings{Ghasemi2024ACS,\n author = {Majid Ghasemi and Amir Hossein Moosavi and Dariush Ebrahimi},\n title = {A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ghasemi2024"
  },
  {
    "id": "http://arxiv.org/abs/2503.09829v3",
    "title": "SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey",
    "published": "2025-03-12T20:47:40Z",
    "updated": "2025-04-23T20:00:44Z",
    "authors": [
      "Joohwan Seo",
      "Soochul Yoo",
      "Junwoo Chang",
      "Hyunseok An",
      "Hyunwoo Ryu",
      "Soomi Lee",
      "Arvind Kruthiventy",
      "Jongeun Choi",
      "Roberto Horowitz"
    ],
    "summary": "Recent advances in deep learning and Transformers have driven major breakthroughs in robotics by employing techniques such as imitation learning, reinforcement learning, and LLM-based multimodal perception and decision-making. However, conventional deep learning and Transformer models often struggle to process data with inherent symmetries and invariances, typically relying on large datasets or extensive data augmentation. Equivariant neural networks overcome these limitations by explicitly integrating symmetry and invariance into their architectures, leading to improved efficiency and generalization. This tutorial survey reviews a wide range of equivariant deep learning and control methods for robotics, from classic to state-of-the-art, with a focus on SE(3)-equivariant models that leverage the natural 3D rotational and translational symmetries in visual robotic manipulation and control design. Using unified mathematical notation, we begin by reviewing key concepts from group theory, along with matrix Lie groups and Lie algebras. We then introduce foundational group-equivariant neural network design and show how the group-equivariance can be obtained through their structure. Next, we discuss the applications of SE(3)-equivariant neural networks in robotics in terms of imitation learning and reinforcement learning. The SE(3)-equivariant control design is also reviewed from the perspective of geometric control. Finally, we highlight the challenges and future directions of equivariant methods in developing more robust, sample-efficient, and multi-modal real-world robotic systems.",
    "pdf_url": "https://arxiv.org/pdf/2503.09829v3",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to International Journcal of Control, Automation and Systems (IJCAS)",
    "journal_ref": null,
    "citation_count": 5,
    "bibtex": "@Article{Seo2025SE3equivariantRL,\n author = {Joohwan Seo and Soochul Yoo and Junwoo Chang and Hyunseok An and Hyunwoo Ryu and Soomi Lee and Arvind Kruthiventy and Jongeun Choi and R. Horowitz},\n booktitle = {International Journal of Control, Automation and Systems},\n journal = {International Journal of Control, Automation and Systems},\n pages = {1271 - 1306},\n title = {SE(3)-equivariant Robot Learning and Control: A Tutorial Survey},\n volume = {23},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "seo2025"
  },
  {
    "id": "http://arxiv.org/abs/2310.06253v2",
    "title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning",
    "published": "2023-10-10T01:58:38Z",
    "updated": "2024-04-06T20:56:20Z",
    "authors": [
      "Ran Wei",
      "Nathan Lambert",
      "Anthony McDonald",
      "Alfredo Garcia",
      "Roberto Calandra"
    ],
    "summary": "Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the objective mismatch between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.",
    "pdf_url": "https://arxiv.org/pdf/2310.06253v2",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Wei2023AUV,\n author = {Ran Wei and Nathan Lambert and Anthony D. McDonald and Alfredo Garcia and Roberto Calandra},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {Trans. Mach. Learn. Res.},\n title = {A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning},\n volume = {2024},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wei2023"
  },
  {
    "id": "http://arxiv.org/abs/2210.04561v4",
    "title": "A Comprehensive Survey of Data Augmentation in Visual Reinforcement Learning",
    "published": "2022-10-10T11:01:57Z",
    "updated": "2024-10-21T14:11:42Z",
    "authors": [
      "Guozheng Ma",
      "Zhen Wang",
      "Zhecheng Yuan",
      "Xueqian Wang",
      "Bo Yuan",
      "Dacheng Tao"
    ],
    "summary": "Visual reinforcement learning (RL), which makes decisions directly from high-dimensional visual inputs, has demonstrated significant potential in various domains. However, deploying visual RL techniques in the real world remains challenging due to their low sample efficiency and large generalization gaps. To tackle these obstacles, data augmentation (DA) has become a widely used technique in visual RL for acquiring sample-efficient and generalizable policies by diversifying the training data. This survey aims to provide a timely and essential review of DA techniques in visual RL in recognition of the thriving development in this field. In particular, we propose a unified framework for analyzing visual RL and understanding the role of DA in it. We then present a principled taxonomy of the existing augmentation techniques used in visual RL and conduct an in-depth discussion on how to better leverage augmented data in different scenarios. Moreover, we report a systematic empirical evaluation of DA-based techniques in visual RL and conclude by highlighting the directions for future research. As the first comprehensive survey of DA in visual RL, this work is expected to offer valuable guidance to this emerging field.",
    "pdf_url": "https://arxiv.org/pdf/2210.04561v4",
    "doi": null,
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "A well-classified paper list that will be continuously updated can be found at https://github.com/Guozheng-Ma/DA-in-visualRL",
    "journal_ref": null,
    "citation_count": 40,
    "bibtex": "@Article{Ma2022ACS,\n author = {Guozheng Ma and Zhen Wang and Zhecheng Yuan and Xueqian Wang and Bo Yuan and Dacheng Tao},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {7368 - 7405},\n title = {A Comprehensive Survey of Data Augmentation in Visual Reinforcement Learning},\n volume = {133},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ma2022"
  },
  {
    "id": "http://arxiv.org/abs/2304.00732v1",
    "title": "Leveraging Predictive Models for Adaptive Sampling of Spatiotemporal Fluid Processes",
    "published": "2023-04-03T05:55:27Z",
    "updated": "2023-04-03T05:55:27Z",
    "authors": [
      "Sandeep Manjanna",
      "Tom Z. Jiahao",
      "M. Ani Hsieh"
    ],
    "summary": "Persistent monitoring of a spatiotemporal fluid process requires data sampling and predictive modeling of the process being monitored. In this paper we present PASST algorithm: Predictive-model based Adaptive Sampling of a Spatio-Temporal process. PASST is an adaptive robotic sampling algorithm that leverages predictive models to efficiently and persistently monitor a fluid process in a given region of interest. Our algorithm makes use of the predictions from a learned prediction model to plan a path for an autonomous vehicle to adaptively and efficiently survey the region of interest. In turn, the sampled data is used to obtain better predictions by giving an updated initial state to the predictive model. For predictive model, we use Knowledged-based Neural Ordinary Differential Equations to train models of fluid processes. These models are orders of magnitude smaller in size and run much faster than fluid data obtained from direct numerical simulations of the partial differential equations that describe the fluid processes or other comparable computational fluids models. For path planning, we use reinforcement learning based planning algorithms that use the field predictions as reward functions. We evaluate our adaptive sampling path planning algorithm on both numerically simulated fluid data and real-world nowcast ocean flow data to show that we can sample the spatiotemporal field in the given region of interest for long time horizons. We also evaluate PASST algorithm's generalization ability to sample from fluid processes that are not in the training repertoire of the learned models.",
    "pdf_url": "https://arxiv.org/pdf/2304.00732v1",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Manjanna2023LeveragingPM,\n author = {Sandeep Manjanna and Tom Z. Jiahao and M. A. Hsieh},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Leveraging Predictive Models for Adaptive Sampling of Spatiotemporal Fluid Processes},\n volume = {abs/2304.00732},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "manjanna2023"
  },
  {
    "id": "http://arxiv.org/abs/2508.08189v2",
    "title": "Reinforcement Learning in Vision: A Survey",
    "published": "2025-08-11T17:08:55Z",
    "updated": "2025-08-14T14:32:17Z",
    "authors": [
      "Weijia Wu",
      "Chen Gao",
      "Joya Chen",
      "Kevin Qinghong Lin",
      "Qingwei Meng",
      "Yiming Zhang",
      "Yuke Qiu",
      "Hong Zhou",
      "Mike Zheng Shou"
    ],
    "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
    "pdf_url": "https://arxiv.org/pdf/2508.08189v2",
    "doi": null,
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Wu2025ReinforcementLI,\n author = {Weijia Wu and Chen Gao and Joya Chen and Kevin Qinghong Lin and Qingwei Meng and Yiming Zhang and Yuke Qiu and Hong Zhou and Mike Zheng Shou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement Learning in Vision: A Survey},\n volume = {abs/2508.08189},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wu2025"
  },
  {
    "id": "http://arxiv.org/abs/2502.09417v1",
    "title": "A Survey of Reinforcement Learning for Optimization in Automation",
    "published": "2025-02-13T15:40:39Z",
    "updated": "2025-02-13T15:40:39Z",
    "authors": [
      "Ahmad Farooq",
      "Kamran Iqbal"
    ],
    "summary": "Reinforcement Learning (RL) has become a critical tool for optimization challenges within automation, leading to significant advancements in several areas. This review article examines the current landscape of RL within automation, with a particular focus on its roles in manufacturing, energy systems, and robotics. It discusses state-of-the-art methods, major challenges, and upcoming avenues of research within each sector, highlighting RL's capacity to solve intricate optimization challenges. The paper reviews the advantages and constraints of RL-driven optimization methods in automation. It points out prevalent challenges encountered in RL optimization, including issues related to sample efficiency and scalability; safety and robustness; interpretability and trustworthiness; transfer learning and meta-learning; and real-world deployment and integration. It further explores prospective strategies and future research pathways to navigate these challenges. Additionally, the survey includes a comprehensive list of relevant research papers, making it an indispensable guide for scholars and practitioners keen on exploring this domain.",
    "pdf_url": "https://arxiv.org/pdf/2502.09417v1",
    "doi": "10.1109/CASE59546.2024.10711718",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International Conference on Automation Science and Engineering (CASE) 2024",
    "journal_ref": "2024 IEEE 20th International Conference on Automation Science and Engineering (CASE), Bari, Italy, 2024, pp. 2487-2494",
    "citation_count": 12,
    "bibtex": "@Article{Farooq2024ASO,\n author = {Ahmad Farooq and K. Iqbal},\n booktitle = {2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)},\n journal = {2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)},\n pages = {2487-2494},\n title = {A Survey of Reinforcement Learning for Optimization in Automation},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "farooq2025"
  },
  {
    "id": "http://arxiv.org/abs/2506.00098v2",
    "title": "Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey",
    "published": "2025-05-30T12:19:32Z",
    "updated": "2025-08-11T09:13:37Z",
    "authors": [
      "Edgar Welte",
      "Rania Rayyes"
    ],
    "summary": "Dexterous manipulation is a crucial yet highly complex challenge in humanoid robotics, demanding precise, adaptable, and sample-efficient learning methods. As humanoid robots are usually designed to operate in human-centric environments and interact with everyday objects, mastering dexterous manipulation is critical for real-world deployment. Traditional approaches, such as reinforcement learning and imitation learning, have made significant strides, but they often struggle due to the unique challenges of real-world dexterous manipulation, including high-dimensional control, limited training data, and covariate shift. This survey provides a comprehensive overview of these challenges and reviews existing learning-based methods for real-world dexterous manipulation, spanning imitation learning, reinforcement learning, and hybrid approaches. A promising yet underexplored direction is interactive imitation learning, where human feedback actively refines a robots behavior during training. While interactive imitation learning has shown success in various robotic tasks, its application to dexterous manipulation remains limited. To address this gap, we examine current interactive imitation learning techniques applied to other robotic tasks and discuss how these methods can be adapted to enhance dexterous manipulation. By synthesizing state-of-the-art research, this paper highlights key challenges, identifies gaps in current methodologies, and outlines potential directions for leveraging interactive imitation learning to improve dexterous robotic skills.",
    "pdf_url": "https://arxiv.org/pdf/2506.00098v2",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "27 pages, 4 figures, 3 tables",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Welte2025InteractiveIL,\n author = {Edgar Welte and Rania Rayyes},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives - A Survey},\n volume = {abs/2506.00098},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "welte2025"
  },
  {
    "id": "http://arxiv.org/abs/2201.08300v1",
    "title": "From Psychological Curiosity to Artificial Curiosity: Curiosity-Driven Learning in Artificial Intelligence Tasks",
    "published": "2022-01-20T17:07:03Z",
    "updated": "2022-01-20T17:07:03Z",
    "authors": [
      "Chenyu Sun",
      "Hangwei Qian",
      "Chunyan Miao"
    ],
    "summary": "Psychological curiosity plays a significant role in human intelligence to enhance learning through exploration and information acquisition. In the Artificial Intelligence (AI) community, artificial curiosity provides a natural intrinsic motivation for efficient learning as inspired by human cognitive development; meanwhile, it can bridge the existing gap between AI research and practical application scenarios, such as overfitting, poor generalization, limited training samples, high computational cost, etc. As a result, curiosity-driven learning (CDL) has become increasingly popular, where agents are self-motivated to learn novel knowledge. In this paper, we first present a comprehensive review on the psychological study of curiosity and summarize a unified framework for quantifying curiosity as well as its arousal mechanism. Based on the psychological principle, we further survey the literature of existing CDL methods in the fields of Reinforcement Learning, Recommendation, and Classification, where both advantages and disadvantages as well as future work are discussed. As a result, this work provides fruitful insights for future CDL research and yield possible directions for further improvement.",
    "pdf_url": "https://arxiv.org/pdf/2201.08300v1",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages, 2 figures, to be submitted",
    "journal_ref": null,
    "citation_count": 13,
    "bibtex": "@Article{Sun2022FromPC,\n author = {Chenyu Sun and Hangwei Qian and C. Miao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {From Psychological Curiosity to Artificial Curiosity: Curiosity-Driven Learning in Artificial Intelligence Tasks},\n volume = {abs/2201.08300},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "sun2022"
  },
  {
    "id": "http://arxiv.org/abs/2009.13303v2",
    "title": "Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey",
    "published": "2020-09-24T21:05:46Z",
    "updated": "2021-07-08T09:00:05Z",
    "authors": [
      "Wenshuai Zhao",
      "Jorge Peña Queralta",
      "Tomi Westerlund"
    ],
    "summary": "Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-to-real gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.",
    "pdf_url": "https://arxiv.org/pdf/2009.13303v2",
    "doi": "10.1109/SSCI47803.2020.9308468.",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 2020 IEEE Symposium Series on Computational Intelligence",
    "journal_ref": "2020 IEEE Symposium Series on Computational Intelligence (SSCI), 2020, pp. 737-744",
    "citation_count": 856,
    "bibtex": "@Article{Zhao2020SimtoRealTI,\n author = {Wenshuai Zhao and J. P. Queralta and Tomi Westerlund},\n booktitle = {IEEE Symposium Series on Computational Intelligence},\n journal = {2020 IEEE Symposium Series on Computational Intelligence (SSCI)},\n pages = {737-744},\n title = {Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "zhao2020"
  },
  {
    "id": "http://arxiv.org/abs/2011.00901v1",
    "title": "Sampling Algorithms, from Survey Sampling to Monte Carlo Methods: Tutorial and Literature Review",
    "published": "2020-11-02T11:27:23Z",
    "updated": "2020-11-02T11:27:23Z",
    "authors": [
      "Benyamin Ghojogh",
      "Hadi Nekoei",
      "Aydin Ghojogh",
      "Fakhri Karray",
      "Mark Crowley"
    ],
    "summary": "This paper is a tutorial and literature review on sampling algorithms. We have two main types of sampling in statistics. The first type is survey sampling which draws samples from a set or population. The second type is sampling from probability distribution where we have a probability density or mass function. In this paper, we cover both types of sampling. First, we review some required background on mean squared error, variance, bias, maximum likelihood estimation, Bernoulli, Binomial, and Hypergeometric distributions, the Horvitz-Thompson estimator, and the Markov property. Then, we explain the theory of simple random sampling, bootstrapping, stratified sampling, and cluster sampling. We also briefly introduce multistage sampling, network sampling, and snowball sampling. Afterwards, we switch to sampling from distribution. We explain sampling from cumulative distribution function, Monte Carlo approximation, simple Monte Carlo methods, and Markov Chain Monte Carlo (MCMC) methods. For simple Monte Carlo methods, whose iterations are independent, we cover importance sampling and rejection sampling. For MCMC methods, we cover Metropolis algorithm, Metropolis-Hastings algorithm, Gibbs sampling, and slice sampling. Then, we explain the random walk behaviour of Monte Carlo methods and more efficient Monte Carlo methods, including Hamiltonian (or hybrid) Monte Carlo, Adler's overrelaxation, and ordered overrelaxation. Finally, we summarize the characteristics, pros, and cons of sampling methods compared to each other. This paper can be useful for different fields of statistics, machine learning, reinforcement learning, and computational physics.",
    "pdf_url": "https://arxiv.org/pdf/2011.00901v1",
    "doi": null,
    "categories": [
      "stat.ME",
      "physics.comp-ph",
      "physics.data-an",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "comment": "The first three authors contributed equally to this work",
    "journal_ref": null,
    "citation_count": 17,
    "bibtex": "@Article{Ghojogh2020SamplingAF,\n author = {Benyamin Ghojogh and Hadi Nekoei and Aydin Ghojogh and F. Karray and Mark Crowley},\n journal = {arXiv: Methodology},\n title = {Sampling Algorithms, from Survey Sampling to Monte Carlo Methods: Tutorial and Literature Review.},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ghojogh2020"
  },
  {
    "id": "http://arxiv.org/abs/2409.12799v3",
    "title": "The Central Role of the Loss Function in Reinforcement Learning",
    "published": "2024-09-19T14:10:38Z",
    "updated": "2025-04-04T15:09:19Z",
    "authors": [
      "Kaiwen Wang",
      "Nathan Kallus",
      "Wen Sun"
    ],
    "summary": "This paper illustrates the central role of loss functions in data-driven decision making, providing a comprehensive survey on their influence in cost-sensitive classification (CSC) and reinforcement learning (RL). We demonstrate how different regression loss functions affect the sample efficiency and adaptivity of value-based decision making algorithms. Across multiple settings, we prove that algorithms using the binary cross-entropy loss achieve first-order bounds scaling with the optimal policy's cost and are much more efficient than the commonly used squared loss. Moreover, we prove that distributional algorithms using the maximum likelihood loss achieve second-order bounds scaling with the policy variance and are even sharper than first-order bounds. This in particular proves the benefits of distributional RL. We hope that this paper serves as a guide analyzing decision making algorithms with varying loss functions, and can inspire the reader to seek out better loss functions to improve any decision making algorithm.",
    "pdf_url": "https://arxiv.org/pdf/2409.12799v3",
    "doi": null,
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "stat.ML",
    "comment": "Accepted to Statistical Science",
    "journal_ref": null,
    "citation_count": 10,
    "bibtex": "@Article{Wang2024TheCR,\n author = {Kaiwen Wang and Nathan Kallus and Wen Sun},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {The Central Role of the Loss Function in Reinforcement Learning},\n volume = {abs/2409.12799},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wang2024"
  },
  {
    "id": "http://arxiv.org/abs/2411.04832v2",
    "title": "Plasticity Loss in Deep Reinforcement Learning: A Survey",
    "published": "2024-11-07T16:13:54Z",
    "updated": "2024-11-08T10:19:15Z",
    "authors": [
      "Timo Klein",
      "Lukas Miklautz",
      "Kevin Sidak",
      "Claudia Plant",
      "Sebastian Tschiatschek"
    ],
    "summary": "Akin to neuroplasticity in human brains, the plasticity of deep neural networks enables their quick adaption to new data. This makes plasticity particularly crucial for deep Reinforcement Learning (RL) agents: Once plasticity is lost, an agent's performance will inevitably plateau because it cannot improve its policy to account for changes in the data distribution, which are a necessary consequence of its learning process. Thus, developing well-performing and sample-efficient agents hinges on their ability to remain plastic during training. Furthermore, the loss of plasticity can be connected to many other issues plaguing deep RL, such as training instabilities, scaling failures, overestimation bias, and insufficient exploration. With this survey, we aim to provide an overview of the emerging research on plasticity loss for academics and practitioners of deep reinforcement learning. First, we propose a unified definition of plasticity loss based on recent works, relate it to definitions from the literature, and discuss metrics for measuring plasticity loss. Then, we categorize and discuss numerous possible causes of plasticity loss before reviewing currently employed mitigation strategies. Our taxonomy is the first systematic overview of the current state of the field. Lastly, we discuss prevalent issues within the literature, such as a necessity for broader evaluation, and provide recommendations for future research, like gaining a better understanding of an agent's neural activity and behavior.",
    "pdf_url": "https://arxiv.org/pdf/2411.04832v2",
    "doi": null,
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Klein2024PlasticityLI,\n author = {Timo Klein and Lukas Miklautz and Kevin Sidak and Claudia Plant and Sebastian Tschiatschek},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Plasticity Loss in Deep Reinforcement Learning: A Survey},\n volume = {abs/2411.04832},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "klein2024"
  },
  {
    "id": "http://arxiv.org/abs/2304.12090v2",
    "title": "Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey",
    "published": "2023-04-24T13:35:11Z",
    "updated": "2025-02-23T06:55:10Z",
    "authors": [
      "Chao Yu",
      "Shicheng Ye",
      "Hankz Hankui Zhuo"
    ],
    "summary": "Reinforcement Learning (RL) has achieved tremendous development in recent years, but still faces significant obstacles in addressing complex real-life problems due to the issues of poor system generalization, low sample efficiency as well as safety and interpretability concerns. The core reason underlying such dilemmas can be attributed to the fact that most of the work has focused on the computational aspect of value functions or policies using a representational model to describe atomic components of rewards, states and actions etc, thus neglecting the rich high-level declarative domain knowledge of facts, relations and rules that can be either provided a priori or acquired through reasoning over time. Recently, there has been a rapidly growing interest in the use of Knowledge Representation and Reasoning (KRR) methods, usually using logical languages, to enable more abstract representation and efficient learning in RL. In this survey, we provide a preliminary overview on these endeavors that leverage the strengths of KRR to help solving various problems in RL, and discuss the challenging open problems and possible directions for future work in this area.",
    "pdf_url": "https://arxiv.org/pdf/2304.12090v2",
    "doi": null,
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": null,
    "journal_ref": null,
    "citation_count": 10,
    "bibtex": "@Article{Yu2023ReinforcementLW,\n author = {Chao Yu and Xuejing Zheng and H. Zhuo and Hai Wan and Weilin Luo},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey},\n volume = {abs/2304.12090},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "yu2023"
  },
  {
    "id": "http://arxiv.org/abs/2103.04706v1",
    "title": "A Taxonomy of Similarity Metrics for Markov Decision Processes",
    "published": "2021-03-08T12:36:42Z",
    "updated": "2021-03-08T12:36:42Z",
    "authors": [
      "Álvaro Visús",
      "Javier García",
      "Fernando Fernández"
    ],
    "summary": "Although the notion of task similarity is potentially interesting in a wide range of areas such as curriculum learning or automated planning, it has mostly been tied to transfer learning. Transfer is based on the idea of reusing the knowledge acquired in the learning of a set of source tasks to a new learning process in a target task, assuming that the target and source tasks are close enough. In recent years, transfer learning has succeeded in making Reinforcement Learning (RL) algorithms more efficient (e.g., by reducing the number of samples needed to achieve the (near-)optimal performance). Transfer in RL is based on the core concept of similarity: whenever the tasks are similar, the transferred knowledge can be reused to solve the target task and significantly improve the learning performance. Therefore, the selection of good metrics to measure these similarities is a critical aspect when building transfer RL algorithms, especially when this knowledge is transferred from simulation to the real world. In the literature, there are many metrics to measure the similarity between MDPs, hence, many definitions of similarity or its complement distance have been considered. In this paper, we propose a categorization of these metrics and analyze the definitions of similarity proposed so far, taking into account such categorization. We also follow this taxonomy to survey the existing literature, as well as suggesting future directions for the construction of new metrics.",
    "pdf_url": "https://arxiv.org/pdf/2103.04706v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, submitted to IJCAI",
    "journal_ref": null,
    "citation_count": 12,
    "bibtex": "@Article{Visús2021ATO,\n author = {Á. Visús and Javier García and F. Fernández},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Taxonomy of Similarity Metrics for Markov Decision Processes},\n volume = {abs/2103.04706},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "viss2021"
  },
  {
    "id": "http://arxiv.org/abs/2407.13734v1",
    "title": "Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review",
    "published": "2024-07-18T17:35:32Z",
    "updated": "2024-07-18T17:35:32Z",
    "authors": [
      "Masatoshi Uehara",
      "Yulai Zhao",
      "Tommaso Biancalani",
      "Sergey Levine"
    ],
    "summary": "This tutorial provides a comprehensive survey of methods for fine-tuning diffusion models to optimize downstream reward functions. While diffusion models are widely known to provide excellent generative modeling capability, practical applications in domains such as biology require generating samples that maximize some desired metric (e.g., translation efficiency in RNA, docking score in molecules, stability in protein). In these cases, the diffusion model can be optimized not only to generate realistic samples but also to explicitly maximize the measure of interest. Such methods are based on concepts from reinforcement learning (RL). We explain the application of various RL algorithms, including PPO, differentiable optimization, reward-weighted MLE, value-weighted sampling, and path consistency learning, tailored specifically for fine-tuning diffusion models. We aim to explore fundamental aspects such as the strengths and limitations of different RL-based fine-tuning algorithms across various scenarios, the benefits of RL-based fine-tuning compared to non-RL-based approaches, and the formal objectives of RL-based fine-tuning (target distributions). Additionally, we aim to examine their connections with related topics such as classifier guidance, Gflownets, flow-based diffusion models, path integral control theory, and sampling from unnormalized distributions such as MCMC. The code of this tutorial is available at https://github.com/masa-ue/RLfinetuning_Diffusion_Bioseq",
    "pdf_url": "https://arxiv.org/pdf/2407.13734v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "We plan to add more content/codes. Please let us know if there are any comments",
    "journal_ref": null,
    "citation_count": 49,
    "bibtex": "@Article{Uehara2024UnderstandingRL,\n author = {Masatoshi Uehara and Yulai Zhao and Tommaso Biancalani and Sergey Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review},\n volume = {abs/2407.13734},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "uehara2024"
  },
  {
    "id": "http://arxiv.org/abs/2303.11191v1",
    "title": "A Survey of Demonstration Learning",
    "published": "2023-03-20T15:22:10Z",
    "updated": "2023-03-20T15:22:10Z",
    "authors": [
      "André Correia",
      "Luís A. Alexandre"
    ],
    "summary": "With the fast improvement of machine learning, reinforcement learning (RL) has been used to automate human tasks in different areas. However, training such agents is difficult and restricted to expert users. Moreover, it is mostly limited to simulation environments due to the high cost and safety concerns of interactions in the real world. Demonstration Learning is a paradigm in which an agent learns to perform a task by imitating the behavior of an expert shown in demonstrations. It is a relatively recent area in machine learning, but it is gaining significant traction due to having tremendous potential for learning complex behaviors from demonstrations. Learning from demonstration accelerates the learning process by improving sample efficiency, while also reducing the effort of the programmer. Due to learning without interacting with the environment, demonstration learning would allow the automation of a wide range of real world applications such as robotics and healthcare. This paper provides a survey of demonstration learning, where we formally introduce the demonstration problem along with its main challenges and provide a comprehensive overview of the process of learning from demonstrations from the creation of the demonstration data set, to learning methods from demonstrations, and optimization by combining demonstration learning with different machine learning methods. We also review the existing benchmarks and identify their strengths and limitations. Additionally, we discuss the advantages and disadvantages of the paradigm as well as its main applications. Lastly, we discuss our perspective on open problems and research directions for this rapidly growing field.",
    "pdf_url": "https://arxiv.org/pdf/2303.11191v1",
    "doi": null,
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 9 figures",
    "journal_ref": null,
    "citation_count": 26,
    "bibtex": "@Article{Correia2023ASO,\n author = {André Rosa de Sousa Porfírio Correia and Lu'is A. Alexandre},\n booktitle = {Robotics Auton. Syst.},\n journal = {ArXiv},\n title = {A Survey of Demonstration Learning},\n volume = {abs/2303.11191},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "correia2023"
  },
  {
    "id": "http://arxiv.org/abs/2506.09368v1",
    "title": "Anomaly Detection and Generation with Diffusion Models: A Survey",
    "published": "2025-06-11T03:29:18Z",
    "updated": "2025-06-11T03:29:18Z",
    "authors": [
      "Yang Liu",
      "Jing Liu",
      "Chengfang Li",
      "Rui Xi",
      "Wenchao Li",
      "Liang Cao",
      "Jin Wang",
      "Laurence T. Yang",
      "Junsong Yuan",
      "Wei Zhou"
    ],
    "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including cybersecurity, finance, healthcare, and industrial manufacturing, by identifying unexpected patterns that deviate from established norms in real-world data. Recent advancements in deep learning, specifically diffusion models (DMs), have sparked significant interest due to their ability to learn complex data distributions and generate high-fidelity samples, offering a robust framework for unsupervised AD. In this survey, we comprehensively review anomaly detection and generation with diffusion models (ADGDM), presenting a tutorial-style analysis of the theoretical foundations and practical implementations and spanning images, videos, time series, tabular, and multimodal data. Crucially, unlike existing surveys that often treat anomaly detection and generation as separate problems, we highlight their inherent synergistic relationship. We reveal how DMs enable a reinforcing cycle where generation techniques directly address the fundamental challenge of anomaly data scarcity, while detection methods provide critical feedback to improve generation fidelity and relevance, advancing both capabilities beyond their individual potential. A detailed taxonomy categorizes ADGDM methods based on anomaly scoring mechanisms, conditioning strategies, and architectural designs, analyzing their strengths and limitations. We final discuss key challenges including scalability and computational efficiency, and outline promising future directions such as efficient architectures, conditioning strategies, and integration with foundation models (e.g., visual-language models and large language models). By synthesizing recent advances and outlining open research questions, this survey aims to guide researchers and practitioners in leveraging DMs for innovative AD solutions across diverse applications.",
    "pdf_url": "https://arxiv.org/pdf/2506.09368v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 11 figures, 13 tables",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Article{Liu2025AnomalyDA,\n author = {Yang Liu and Jing Liu and Chengfang Li and Rui Xi and Wenchao Li and Liang Cao and Jin Wang and Laurence T. Yang and Junsong Yuan and Wei Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Anomaly Detection and Generation with Diffusion Models: A Survey},\n volume = {abs/2506.09368},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "liu2025"
  },
  {
    "id": "http://arxiv.org/abs/2408.03539v3",
    "title": "Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes",
    "published": "2024-08-07T04:35:38Z",
    "updated": "2024-09-16T15:41:05Z",
    "authors": [
      "Chen Tang",
      "Ben Abbatematteo",
      "Jiaheng Hu",
      "Rohan Chandra",
      "Roberto Martín-Martín",
      "Peter Stone"
    ],
    "summary": "Reinforcement learning (RL), particularly its combination with deep neural networks referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms, holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks, and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL's power to create generally capable real-world robotic systems.",
    "pdf_url": "https://arxiv.org/pdf/2408.03539v3",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "The first three authors contributed equally. Accepted to Annual Review of Control, Robotics, and Autonomous Systems",
    "journal_ref": null,
    "citation_count": 164,
    "bibtex": "@Article{Tang2024DeepRL,\n author = {Chen Tang and Ben Abbatematteo and Jiaheng Hu and Rohan Chandra and Roberto Mart'in-Mart'in and Peter Stone},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {28698-28699},\n title = {Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "tang2024"
  },
  {
    "id": "http://arxiv.org/abs/2510.14315v1",
    "title": "Active Measuring in Reinforcement Learning With Delayed Negative Effects",
    "published": "2025-10-16T05:21:36Z",
    "updated": "2025-10-16T05:21:36Z",
    "authors": [
      "Daiqi Gao",
      "Ziping Xu",
      "Aseel Rawashdeh",
      "Predrag Klasnja",
      "Susan A. Murphy"
    ],
    "summary": "Measuring states in reinforcement learning (RL) can be costly in real-world settings and may negatively influence future outcomes. We introduce the Actively Observable Markov Decision Process (AOMDP), where an agent not only selects control actions but also decides whether to measure the latent state. The measurement action reveals the true latent state but may have a negative delayed effect on the environment. We show that this reduced uncertainty may provably improve sample efficiency and increase the value of the optimal policy despite these costs. We formulate an AOMDP as a periodic partially observable MDP and propose an online RL algorithm based on belief states. To approximate the belief states, we further propose a sequential Monte Carlo method to jointly approximate the posterior of unknown static environment parameters and unobserved latent states. We evaluate the proposed algorithm in a digital health application, where the agent decides when to deliver digital interventions and when to assess users' health status through surveys.",
    "pdf_url": "https://arxiv.org/pdf/2510.14315v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Gao2025ActiveMI,\n author = {Daiqi Gao and Ziping Xu and Aseel Rawashdeh and P. Klasnja and Susan A. Murphy},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Active Measuring in Reinforcement Learning With Delayed Negative Effects},\n volume = {abs/2510.14315},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "gao2025"
  },
  {
    "id": "http://arxiv.org/abs/2005.02979v3",
    "title": "A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical Systems",
    "published": "2020-05-06T17:31:51Z",
    "updated": "2021-10-14T16:40:00Z",
    "authors": [
      "Anthony Corso",
      "Robert J. Moss",
      "Mark Koren",
      "Ritchie Lee",
      "Mykel J. Kochenderfer"
    ],
    "summary": "Autonomous cyber-physical systems (CPS) can improve safety and efficiency for safety-critical applications, but require rigorous testing before deployment. The complexity of these systems often precludes the use of formal verification and real-world testing can be too dangerous during development. Therefore, simulation-based techniques have been developed that treat the system under test as a black box operating in a simulated environment. Safety validation tasks include finding disturbances in the environment that cause the system to fail (falsification), finding the most-likely failure, and estimating the probability that the system fails. Motivated by the prevalence of safety-critical artificial intelligence, this work provides a survey of state-of-the-art safety validation techniques for CPS with a focus on applied algorithms and their modifications for the safety validation problem. We present and discuss algorithms in the domains of optimization, path planning, reinforcement learning, and importance sampling. Problem decomposition techniques are presented to help scale algorithms to large state spaces, which are common for CPS. A brief overview of safety-critical applications is given, including autonomous vehicles and aircraft collision avoidance systems. Finally, we present a survey of existing academic and commercially available safety validation tools.",
    "pdf_url": "https://arxiv.org/pdf/2005.02979v3",
    "doi": "10.1613/jair.1.12716",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": "Journal of Artificial Intelligence Research, vol. 72, p. 377-428, 2021",
    "citation_count": 187,
    "bibtex": "@Article{Corso2020ASO,\n author = {Anthony Corso and Robert J. Moss and Mark Koren and Ritchie Lee and Mykel J. Kochenderfer},\n booktitle = {Journal of Artificial Intelligence Research},\n journal = {ArXiv},\n title = {A Survey of Algorithms for Black-Box Safety Validation},\n volume = {abs/2005.02979},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "corso2020"
  },
  {
    "id": "http://arxiv.org/abs/1912.10944v2",
    "title": "A Survey of Deep Reinforcement Learning in Video Games",
    "published": "2019-12-23T16:04:40Z",
    "updated": "2019-12-26T14:47:34Z",
    "authors": [
      "Kun Shao",
      "Zhentao Tang",
      "Yuanheng Zhu",
      "Nannan Li",
      "Dongbin Zhao"
    ],
    "summary": "Deep reinforcement learning (DRL) has made great achievements since proposed. Generally, DRL agents receive high-dimensional inputs at each step, and make actions according to deep-neural-network-based policies. This learning mechanism updates the policy to maximize the return with an end-to-end method. In this paper, we survey the progress of DRL methods, including value-based, policy gradient, and model-based algorithms, and compare their main techniques and properties. Besides, DRL plays an important role in game artificial intelligence (AI). We also take a review of the achievements of DRL in various video games, including classical Arcade games, first-person perspective games and multi-agent real-time strategy games, from 2D to 3D, and from single-agent to multi-agent. A large number of video game AIs with DRL have achieved super-human performance, while there are still some challenges in this domain. Therefore, we also discuss some key points when applying DRL methods to this field, including exploration-exploitation, sample efficiency, generalization and transfer, multi-agent learning, imperfect information, and delayed spare rewards, as well as some research directions.",
    "pdf_url": "https://arxiv.org/pdf/1912.10944v2",
    "doi": null,
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "13 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 218,
    "bibtex": "@Article{Shao2019ASO,\n author = {Kun Shao and Zhentao Tang and Yuanheng Zhu and Nannan Li and Dongbin Zhao},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey of Deep Reinforcement Learning in Video Games},\n volume = {abs/1912.10944},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "shao2019"
  },
  {
    "id": "http://arxiv.org/abs/1210.3956v1",
    "title": "Free energy calculations for atomic solids through the Einstein crystal/molecule methodology using GROMACS and LAMMPS",
    "published": "2012-10-15T09:42:04Z",
    "updated": "2012-10-15T09:42:04Z",
    "authors": [
      "J. L. Aragones",
      "C. Valeriani",
      "C. Vega"
    ],
    "summary": "In this work the free energy of solid phases is computed for the Lennard-Jones potential and for a model of NaCl. The free energy is evaluated through the Einstein crystal/molecule methodologies using the Molecular Dynamics programs: GROMACS and LAMMPS. The obtained results are compared with the results obtained from Monte Carlo. Good agreement between the different programs and methodologies was found. The procedure to perform the free energy calculations for the solid phase in the Molecular Dynamic programs is described. Since these programs allow to study any continuous intermolecular potential (when given in a tabulated form) this work shows that for isotropic potentials (describing for instance atomic solids or colloidal particles) free energy calculations can be performed on a routinely basis using GROMACS and/or LAMMPS.",
    "pdf_url": "https://arxiv.org/pdf/1210.3956v1",
    "doi": "10.1063/1.4758700",
    "categories": [
      "cond-mat.soft",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.soft",
    "comment": null,
    "journal_ref": "J. Chem. Phys., 137, 146101 (2012)",
    "citation_count": 40,
    "bibtex": "@Article{Aragones2012NoteFE,\n author = {J. L. Aragones and C. Valeriani and C. Vega},\n booktitle = {Journal of Chemical Physics},\n journal = {The Journal of chemical physics},\n pages = {\n          146101\n        },\n title = {Note: Free energy calculations for atomic solids through the Einstein crystal/molecule methodology using GROMACS and LAMMPS.},\n volume = {137 14},\n year = {2012}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "aragones2012"
  },
  {
    "id": "http://arxiv.org/abs/1611.09067v3",
    "title": "Dynamic Choreographies: Theory And Implementation",
    "published": "2016-11-28T11:04:47Z",
    "updated": "2017-04-06T11:37:43Z",
    "authors": [
      "Mila Dalla Preda",
      "Maurizio Gabbrielli",
      "Saverio Giallorenzo",
      "Ivan Lanese",
      "Jacopo Mauro"
    ],
    "summary": "Programming distributed applications free from communication deadlocks and race conditions is complex. Preserving these properties when applications are updated at runtime is even harder. We present a choreographic approach for programming updatable, distributed applications. We define a choreography language, called Dynamic Interaction-Oriented Choreography (AIOC), that allows the programmer to specify, from a global viewpoint, which parts of the application can be updated. At runtime, these parts may be replaced by new AIOC fragments from outside the application. AIOC programs are compiled, generating code for each participant in a process-level language called Dynamic Process-Oriented Choreographies (APOC). We prove that APOC distributed applications generated from AIOC specifications are deadlock free and race free and that these properties hold also after any runtime update. We instantiate the theoretical model above into a programming framework called Adaptable Interaction-Oriented Choreographies in Jolie (AIOCJ) that comprises an integrated development environment, a compiler from an extension of AIOCs to distributed Jolie programs, and a runtime environment to support their execution.",
    "pdf_url": "https://arxiv.org/pdf/1611.09067v3",
    "doi": "10.23638/LMCS-13(2:1)2017",
    "categories": [
      "cs.PL",
      "cs.LO"
    ],
    "primary_category": "cs.PL",
    "comment": "arXiv admin note: text overlap with arXiv:1407.0970",
    "journal_ref": "Logical Methods in Computer Science, Volume 13, Issue 2 (April 10, 2017) lmcs:3263",
    "citation_count": 57,
    "bibtex": "@Article{Preda2016DynamicCT,\n author = {M. Preda and M. Gabbrielli and Saverio Giallorenzo and Ivan Lanese and J. Mauro},\n booktitle = {Log. Methods Comput. Sci.},\n journal = {Log. Methods Comput. Sci.},\n title = {Dynamic Choreographies: Theory And Implementation},\n volume = {13},\n year = {2016}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "preda2016"
  },
  {
    "id": "http://arxiv.org/abs/1311.2889v1",
    "title": "Reinforcement Learning for Matrix Computations: PageRank as an Example",
    "published": "2013-11-01T14:24:32Z",
    "updated": "2013-11-01T14:24:32Z",
    "authors": [
      "Vivek S. Borkar",
      "Adwaitvedant S. Mathkar"
    ],
    "summary": "Reinforcement learning has gained wide popularity as a technique for simulation-driven approximate dynamic programming. A less known aspect is that the very reasons that make it effective in dynamic programming can also be leveraged for using it for distributed schemes for certain matrix computations involving non-negative matrices. In this spirit, we propose a reinforcement learning algorithm for PageRank computation that is fashioned after analogous schemes for approximate dynamic programming. The algorithm has the advantage of ease of distributed implementation and more importantly, of being model-free, i.e., not dependent on any specific assumptions about the transition probabilities in the random web-surfer model. We analyze its convergence and finite time behavior and present some supporting numerical experiments.",
    "pdf_url": "https://arxiv.org/pdf/1311.2889v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 6 figures, invited lecture at ICDIT (International Conference on Distributed Computing and Internet Technologies), 2014, will be published in Lecture notes in Computer Science along with the conference proceedings",
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Borkar2013ReinforcementLF,\n author = {V. Borkar and Adwaitvedant S. Mathkar},\n booktitle = {International Conference on Distributed Computing and Internet Technology},\n pages = {14-24},\n title = {Reinforcement Learning for Matrix Computations: PageRank as an Example},\n year = {2013}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "borkar2013"
  },
  {
    "id": "http://arxiv.org/abs/1912.13007v1",
    "title": "World Programs for Model-Based Learning and Planning in Compositional State and Action Spaces",
    "published": "2019-12-30T17:03:16Z",
    "updated": "2019-12-30T17:03:16Z",
    "authors": [
      "Marwin H. S. Segler"
    ],
    "summary": "Some of the most important tasks take place in environments which lack cheap and perfect simulators, thus hampering the application of model-free reinforcement learning (RL). While model-based RL aims to learn a dynamics model, in a more general case the learner does not know a priori what the action space is. Here we propose a formalism where the learner induces a world program by learning a dynamics model and the actions in graph-based compositional environments by observing state-state transition examples. Then, the learner can perform RL with the world program as the simulator for complex planning tasks. We highlight a recent application, and propose a challenge for the community to assess world program-based planning.",
    "pdf_url": "https://arxiv.org/pdf/1912.13007v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Generative Modeling and Model-Based Reasoning for Robotics and AI workshop at ICML 2019. Presented on June 14th 2019. See https://sites.google.com/view/mbrl-icml2019",
    "journal_ref": "https://sites.google.com/view/mbrl-icml2019",
    "citation_count": 4,
    "bibtex": "@Article{Segler2019WorldPF,\n author = {Marwin H. S. Segler},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {World Programs for Model-Based Learning and Planning in Compositional State and Action Spaces},\n volume = {abs/1912.13007},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "segler2019"
  },
  {
    "id": "http://arxiv.org/abs/2012.14654v1",
    "title": "The Adaptive Dynamic Programming Toolbox",
    "published": "2020-12-29T08:28:31Z",
    "updated": "2020-12-29T08:28:31Z",
    "authors": [
      "Xiaowei Xing",
      "Dong Eui Chang"
    ],
    "summary": "The paper develops the Adaptive Dynamic Programming Toolbox (ADPT), which solves optimal control problems for continuous-time nonlinear systems. Based on the adaptive dynamic programming technique, the ADPT computes optimal feedback controls from the system dynamics in the model-based working mode, or from measurements of trajectories of the system in the model-free working mode without the requirement of knowledge of the system model. Multiple options are provided such that the ADPT can accommodate various customized circumstances. Compared to other popular software toolboxes for optimal control, the ADPT enjoys its computational precision and speed, which is illustrated with its applications to a satellite attitude control problem.",
    "pdf_url": "https://arxiv.org/pdf/2012.14654v1",
    "doi": null,
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": null,
    "journal_ref": null,
    "citation_count": 8,
    "bibtex": "@Article{Xing2020TheAD,\n author = {Xiaowei Xing and D. Chang},\n booktitle = {Italian National Conference on Sensors},\n journal = {Sensors (Basel, Switzerland)},\n title = {The Adaptive Dynamic Programming Toolbox},\n volume = {21},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "xing2020"
  },
  {
    "id": "http://arxiv.org/abs/2302.14170v1",
    "title": "Generic framework for data-race-free many-particle simulations on shared memory hardware",
    "published": "2023-02-27T22:06:35Z",
    "updated": "2023-02-27T22:06:35Z",
    "authors": [
      "Julian Jeggle",
      "Raphael Wittkowski"
    ],
    "summary": "Recently, there has been much progress in the formulation and implementation of methods for generic many-particle simulations. These models, however, typically either do not utilize shared memory hardware or do not guarantee data-race freedom for arbitrary particle dynamics. Here, we present both a abstract formal model of particle dynamics and a corresponding domain-specific programming language that can guarantee data-race freedom. The design of both the model and the language are heavily inspired by the Rust programming language that enables data-race-free general-purpose parallel computation. We also present a method of preventing deadlocks within our model by a suitable graph representation of a particle simulation. Finally, we demonstrate the practicability of our model on a number of common numerical primitives from molecular dynamics.",
    "pdf_url": "https://arxiv.org/pdf/2302.14170v1",
    "doi": null,
    "categories": [
      "physics.comp-ph",
      "cond-mat.mtrl-sci",
      "cond-mat.soft",
      "cond-mat.stat-mech",
      "physics.bio-ph"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "21 pages, 2 figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Inproceedings{Jeggle2023GenericFF,\n author = {Julian Jeggle and R. Wittkowski},\n title = {Generic framework for data-race-free many-particle simulations on shared memory hardware},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "jeggle2023"
  },
  {
    "id": "http://arxiv.org/abs/cond-mat/0307201v1",
    "title": "Punctuated Equilibrium in Software Evolution",
    "published": "2003-07-09T13:26:32Z",
    "updated": "2003-07-09T13:26:32Z",
    "authors": [
      "A. A. Gorshenev",
      "Yu. M. Pis'mak"
    ],
    "summary": "The approach based on paradigm of self-organized criticality proposed for experimental investigation and theoretical modelling of software evolution. The dynamics of modifications studied for three free, open source programs Mozilla, Free-BSD and Emacs using the data from version control systems. Scaling laws typical for the self-organization criticality found. The model of software evolution presenting the natural selection principle is proposed. The results of numerical and analytical investigation of the model are presented. They are in a good agreement with the data collected for the real-world software.",
    "pdf_url": "https://arxiv.org/pdf/cond-mat/0307201v1",
    "doi": "10.1103/PhysRevE.70.067103",
    "categories": [
      "cond-mat.stat-mech",
      "cs.SE"
    ],
    "primary_category": "cond-mat.stat-mech",
    "comment": "4 pages, LaTeX, 2 Postscript figures",
    "journal_ref": null,
    "citation_count": null,
    "bibtex": null,
    "markdown_text": null,
    "ranking": null,
    "citation_key": "gorshenev2003"
  },
  {
    "id": "http://arxiv.org/abs/1409.3717v1",
    "title": "Probabilistic Selection in AgentSpeak(L)",
    "published": "2014-09-12T12:27:44Z",
    "updated": "2014-09-12T12:27:44Z",
    "authors": [
      "Francisco Coelho",
      "Vitor Nogueira"
    ],
    "summary": "Agent programming is mostly a symbolic discipline and, as such, draws little benefits from probabilistic areas as machine learning and graphical models. However, the greatest objective of agent research is the achievement of autonomy in dynamical and complex environments --- a goal that implies embracing uncertainty and therefore the entailed representations, algorithms and techniques. This paper proposes an innovative and conflict free two layer approach to agent programming that uses already established methods and tools from both symbolic and probabilistic artificial intelligence. Moreover, this framework is illustrated by means of a widely used agent programming example, GoldMiners.",
    "pdf_url": "https://arxiv.org/pdf/1409.3717v1",
    "doi": "10.1007/978-3-319-25524-8_44",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "8 pages, 3 figures",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Coelho2014ProbabilisticPR,\n author = {F. Coelho and V. Nogueira},\n booktitle = {Prima},\n pages = {613-621},\n title = {Probabilistic Perception Revision in AgentSpeak(L)},\n year = {2014}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "coelho2014"
  },
  {
    "id": "http://arxiv.org/abs/2402.04561v1",
    "title": "Spacecraft Rendezvous Guidance via Factorization-Free Sequential Convex Programming using a First-Order Method",
    "published": "2024-02-07T03:42:09Z",
    "updated": "2024-02-07T03:42:09Z",
    "authors": [
      "Govind M. Chari",
      "Behçet Açıkmeşe"
    ],
    "summary": "We implement a fully factorization-free algorithm for nonconvex, free-final-time trajectory optimization. This algorithm is based on sequential convex programming and utilizes an inverse-free, exact discretization procedure to ensure dynamic feasibility of the converged trajectory and PIPG, a fast, first-order conic optimization algorithm as the subproblem solver. Although PIPG requires the tuning of a hyperparameter to achieve fastest convergence, we show that PIPG can be tuned to a nominal trajectory optimization problem and it is robust to variations in initial condition. We demonstrate this with a monte carlo simulation of the free-final-time rendezvous problem, using Clohessy-Wiltshire dynamics, an impulsive thrust model, and various state and control constraints including a spherical keepout zone.",
    "pdf_url": "https://arxiv.org/pdf/2402.04561v1",
    "doi": null,
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC",
    "comment": "AAS Rocky Mountain Guidance, Navigation and Control Conference, 2024",
    "journal_ref": null,
    "citation_count": 3,
    "bibtex": "@Inproceedings{Chari2024SpacecraftRG,\n author = {Govind M. Chari and Behccet Accikmecse},\n title = {Spacecraft Rendezvous Guidance via Factorization-Free Sequential Convex Programming using a First-Order Method},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "chari2024"
  },
  {
    "id": "http://arxiv.org/abs/0709.3689v1",
    "title": "Static Deadlock Detection in MPI Synchronization Communication",
    "published": "2007-09-24T05:33:29Z",
    "updated": "2007-09-24T05:33:29Z",
    "authors": [
      "Liao Ming-Xue",
      "He Xiao-Xin",
      "Fan Zhi-Hua"
    ],
    "summary": "It is very common to use dynamic methods to detect deadlocks in MPI programs for the reason that static methods have some restrictions. To guarantee high reliability of some important MPI-based application software, a model of MPI synchronization communication is abstracted and a type of static method is devised to examine deadlocks in such modes. The model has three forms with different complexity: sequential model, single-loop model and nested-loop model. Sequential model is a base for all models. Single-loop model must be treated with a special type of equation group and nested-loop model extends the methods for the other two models. A standard Java-based software framework originated from these methods is constructed for determining whether MPI programs are free from synchronization communication deadlocks. Our practice shows the software framework is better than those tools using dynamic methods because it can dig out all synchronization communication deadlocks before an MPI-based program goes into running.",
    "pdf_url": "https://arxiv.org/pdf/0709.3689v1",
    "doi": null,
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC",
    "comment": "accepted by HPC Asia 2007",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Liao2007StaticDD,\n author = {Ming-xue Liao and Xiao-Xin He and Z. Fan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Static Deadlock Detection in MPI Synchronization Communication},\n volume = {abs/0709.3689},\n year = {2007}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "mingxue2007"
  },
  {
    "id": "http://arxiv.org/abs/2106.05535v1",
    "title": "Differentiable Robust LQR Layers",
    "published": "2021-06-10T06:52:31Z",
    "updated": "2021-06-10T06:52:31Z",
    "authors": [
      "Ngo Anh Vien",
      "Gerhard Neumann"
    ],
    "summary": "This paper proposes a differentiable robust LQR layer for reinforcement learning and imitation learning under model uncertainty and stochastic dynamics. The robust LQR layer can exploit the advantages of robust optimal control and model-free learning. It provides a new type of inductive bias for stochasticity and uncertainty modeling in control systems. In particular, we propose an efficient way to differentiate through a robust LQR optimization program by rewriting it as a convex program (i.e. semi-definite program) of the worst-case cost. Based on recent work on using convex optimization inside neural network layers, we develop a fully differentiable layer for optimizing this worst-case cost, i.e. we compute the derivative of a performance measure w.r.t the model's unknown parameters, model uncertainty and stochasticity parameters. We demonstrate the proposed method on imitation learning and approximate dynamic programming on stochastic and uncertain domains. The experiment results show that the proposed method can optimize robust policies under uncertain situations, and are able to achieve a significantly better performance than existing methods that do not model uncertainty directly.",
    "pdf_url": "https://arxiv.org/pdf/2106.05535v1",
    "doi": null,
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Vien2021DifferentiableRL,\n author = {Ngo Anh Vien and G. Neumann},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Differentiable Robust LQR Layers},\n volume = {abs/2106.05535},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "vien2021"
  },
  {
    "id": "http://arxiv.org/abs/2411.18762v1",
    "title": "Kernelized offset-free data-driven predictive control for nonlinear systems",
    "published": "2024-11-27T21:34:27Z",
    "updated": "2024-11-27T21:34:27Z",
    "authors": [
      "Thomas Oliver de Jong",
      "Mircea Lazar"
    ],
    "summary": "This paper presents a kernelized offset-free data-driven predictive control scheme for nonlinear systems. Traditional model-based and data-driven predictive controllers often struggle with inaccurate predictors or persistent disturbances, especially in the case of nonlinear dynamics, leading to tracking offsets and stability issues. To overcome these limitations, we employ kernel methods to parameterize the nonlinear terms of a velocity model, preserving its structure and efficiently learning unknown parameters through a least squares approach. This results in a offset-free data-driven predictive control scheme formulated as a nonlinear program, but solvable via sequential quadratic programming. We provide a framework for analyzing recursive feasibility and stability of the developed method and we demonstrate its effectiveness through simulations on a nonlinear benchmark example.",
    "pdf_url": "https://arxiv.org/pdf/2411.18762v1",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Jong2024KernelizedOD,\n author = {Thomas O. de Jong and Mircea Lazar},\n booktitle = {IEEE Control Systems Letters},\n journal = {IEEE Control Systems Letters},\n pages = {2877-2882},\n title = {Kernelized Offset-Free Data-Driven Predictive Control for Nonlinear Systems},\n volume = {8},\n year = {2024}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "jong2024"
  },
  {
    "id": "http://arxiv.org/abs/1509.03017v1",
    "title": "Weak Completeness of Coalgebraic Dynamic Logics",
    "published": "2015-09-10T05:32:04Z",
    "updated": "2015-09-10T05:32:04Z",
    "authors": [
      "Helle Hvid Hansen",
      "Clemens Kupke"
    ],
    "summary": "We present a coalgebraic generalisation of Fischer and Ladner's Propositional Dynamic Logic (PDL) and Parikh's Game Logic (GL). In earlier work, we proved a generic strong completeness result for coalgebraic dynamic logics without iteration. The coalgebraic semantics of such programs is given by a monad T, and modalities are interpreted via a predicate lifting Î whose transpose is a monad morphism from T to the neighbourhood monad. In this paper, we show that if the monad T carries a complete semilattice structure, then we can define an iteration construct, and suitable notions of diamond-likeness and box-likeness of predicate-liftings which allows for the definition of an axiomatisation parametric in T, Î and a chosen set of pointwise program operations. As our main result, we show that if the pointwise operations are \"negation-free\" and Kleisli composition left-distributes over the induced join on Kleisli arrows, then this axiomatisation is weakly complete with respect to the class of standard models. As special instances, we recover the weak completeness of PDL and of dual-free Game Logic. As a modest new result we obtain completeness for dual-free GL extended with intersection (demonic choice) of games.",
    "pdf_url": "https://arxiv.org/pdf/1509.03017v1",
    "doi": "10.4204/EPTCS.191.9",
    "categories": [
      "cs.LO"
    ],
    "primary_category": "cs.LO",
    "comment": "In Proceedings FICS 2015, arXiv:1509.02826",
    "journal_ref": "EPTCS 191, 2015, pp. 90-104",
    "citation_count": 19,
    "bibtex": "@Article{Hansen2015WeakCO,\n author = {H. Hansen and C. Kupke},\n booktitle = {Fixed Points in Computer Science},\n pages = {90-104},\n title = {Weak Completeness of Coalgebraic Dynamic Logics},\n year = {2015}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hansen2015"
  },
  {
    "id": "http://arxiv.org/abs/2201.11433v2",
    "title": "ETAP: Energy-aware Timing Analysis of Intermittent Programs",
    "published": "2022-01-27T10:40:18Z",
    "updated": "2022-02-03T20:15:28Z",
    "authors": [
      "Ferhat Erata",
      "Arda Goknil",
      "Eren Yıldız",
      "Kasım Sinan Yıldırım",
      "Ruzica Piskac",
      "Jakub Szefer",
      "Gökçin Sezgin"
    ],
    "summary": "Energy harvesting battery-free embedded devices rely only on ambient energy harvesting that enables stand-alone and sustainable IoT applications. These devices execute programs when the harvested ambient energy in their energy reservoir is sufficient to operate and stop execution abruptly (and start charging) otherwise. These intermittent programs have varying timing behavior under different energy conditions, hardware configurations, and program structures. This paper presents Energy-aware Timing Analysis of intermittent Programs (ETAP), a probabilistic symbolic execution approach that analyzes the timing and energy behavior of intermittent programs at compile time. ETAP symbolically executes the given program while taking time and energy cost models for ambient energy and dynamic energy consumption into account. We evaluated ETAP on several intermittent programs and compared the compile-time analysis results with executions on real hardware. The results show that ETAP's normalized prediction accuracy is 99.5%, and it speeds up the timing analysis by at least two orders of magnitude compared to manual testing.",
    "pdf_url": "https://arxiv.org/pdf/2201.11433v2",
    "doi": "10.1145/3563216",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE",
    "comment": "Corrected typos in the previous submission",
    "journal_ref": null,
    "citation_count": 11,
    "bibtex": "@Article{Erata2022ETAPET,\n author = {Ferhat Erata and Eren Yildiz and Arda Goknil and K. Yıldırım and Jakub Szefer and R. Piskac and Gokcin Sezgin},\n booktitle = {ACM Transactions on Embedded Computing Systems},\n journal = {ACM Transactions on Embedded Computing Systems},\n pages = {1 - 31},\n title = {ETAP: Energy-aware Timing Analysis of Intermittent Programs},\n volume = {22},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "erata2022"
  },
  {
    "id": "http://arxiv.org/abs/1609.04093v1",
    "title": "A Canonical Model Construction for Iteration-Free PDL with Intersection",
    "published": "2016-09-14T00:58:48Z",
    "updated": "2016-09-14T00:58:48Z",
    "authors": [
      "Florian Bruse",
      "Daniel Kernberger",
      "Martin Lange"
    ],
    "summary": "We study the axiomatisability of the iteration-free fragment of Propositional Dynamic Logic with Intersection and Tests. The combination of program composition, intersection and tests makes its proof-theory rather difficult. We develop a normal form for formulae which minimises the interaction between these operators, as well as a refined canonical model construction. From these we derive an axiom system and a proof of its strong completeness.",
    "pdf_url": "https://arxiv.org/pdf/1609.04093v1",
    "doi": "10.4204/EPTCS.226.9",
    "categories": [
      "cs.LO"
    ],
    "primary_category": "cs.LO",
    "comment": "In Proceedings GandALF 2016, arXiv:1609.03648",
    "journal_ref": "EPTCS 226, 2016, pp. 120-134",
    "citation_count": 0,
    "bibtex": "@Article{Bruse2016ACM,\n author = {Florian Bruse and Daniel Kernberger and M. Lange},\n booktitle = {International Symposium on Games, Automata, Logics and Formal Verification},\n pages = {120-134},\n title = {A Canonical Model Construction for Iteration-Free PDL with Intersection},\n year = {2016}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "bruse2016"
  },
  {
    "id": "http://arxiv.org/abs/2011.02596v1",
    "title": "Rule-based Strategies for Dynamic Life Cycle Investment",
    "published": "2020-11-05T00:27:42Z",
    "updated": "2020-11-05T00:27:42Z",
    "authors": [
      "T. R. B. den Haan",
      "K. W. Chau",
      "M. van der Schans",
      "C. W. Oosterlee"
    ],
    "summary": "In this work, we consider rule-based investment strategies for managing a defined contribution saving scheme under the Dutch pension fund testing model. We found that dynamic rule-based investment can outperform traditional static strategies, by which we mean that the pensioner can achieve the target retirement income with higher probability and limit the shortfall when target is not met. In comparison with the popular dynamic programming technique, the rule-based strategy has a more stable asset allocation throughout time and avoid excessive transactions, which may be hard to explain to the investor. We also study a combined strategy of rule based target and dynamic programming in this work. Another key feature of this work is that there is no risk-free asset under our setting, instead, a matching portfolio is introduced for the investor to avoid unnecessary risk.",
    "pdf_url": "https://arxiv.org/pdf/2011.02596v1",
    "doi": "10.1007/s13385-021-00283-0",
    "categories": [
      "q-fin.PM"
    ],
    "primary_category": "q-fin.PM",
    "comment": null,
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Haan2020RulebasedSF,\n author = {T.R.B. den Haan and K. Chau and M. V. D. Schans and C. Oosterlee},\n booktitle = {European Actuarial Journal},\n journal = {European Actuarial Journal},\n pages = {189-213},\n title = {Rule-based strategies for dynamic life cycle investment},\n volume = {12},\n year = {2020}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "haan2020"
  },
  {
    "id": "http://arxiv.org/abs/2311.15471v1",
    "title": "PLQ-Sim: A Computational Tool for Simulating Photoluminescence Quenching Dynamics in Organic Donor/Acceptor Blends",
    "published": "2023-11-27T01:06:07Z",
    "updated": "2023-11-27T01:06:07Z",
    "authors": [
      "Leandro Benatto",
      "Omar Mesquita",
      "Lucimara S. Roman",
      "Rodrigo B. Capaz",
      "Graziâni Candiotto",
      "Marlus Koehler"
    ],
    "summary": "Photoluminescence Quenching Simulator (PLQ-Sim) is a user-friendly software to study the photoexcited state dynamics at the interface between two organic semiconductors forming a blend: an electron donor (D), and an electron acceptor (A). Its main function is to provide substantial information on the photophysical processes relevant to organic photovoltaic and photothermal devices, such as charge transfer state formation and subsequent free charge generation or exciton recombination. From input parameters provided by the user, the program calculates the transfer rates of the D/A blend and employs a kinetic model that provides the photoluminescence quenching efficiency for initial excitation in the donor or acceptor. When calculating the rates, the user can choose to use disorder parameters to better describe the system. In addition, the program was developed to address energy transfer phenomena that are commonly present in organic blends. The time evolution of state populations is also calculated providing relevant information for the user. In this article, we present the theory behind the kinetic model, along with suggestions for methods to obtain the input parameters. A detailed demonstration of the program, its applicability, and an analysis of the outputs are also presented. PLQ-Sim is license free software that can be run via dedicated webserver nanocalc.org or downloading the program executables (for Unix, Windows, and macOS) from the PLQ-Sim repository on GitHub.",
    "pdf_url": "https://arxiv.org/pdf/2311.15471v1",
    "doi": "10.1016/j.cpc.2023.109015",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "13 pages, 11 figures",
    "journal_ref": "Comput. Phys. Commun., 296, 2024, 109015",
    "citation_count": 3,
    "bibtex": "@Article{Benatto2023PLQsimAC,\n author = {L. Benatto and Omar Mesquita and L. Roman and Rodrigo B. Capaz and G. Candiotto and M. Koehler},\n booktitle = {Computer Physics Communications},\n journal = {Comput. Phys. Commun.},\n pages = {109015},\n title = {PLQ-sim: A computational tool for simulating photoluminescence quenching dynamics in organic donor/acceptor blends},\n volume = {296},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "benatto2023"
  },
  {
    "id": "http://arxiv.org/abs/1904.08361v1",
    "title": "Decoupled Data Based Approach for Learning to Control Nonlinear Dynamical Systems",
    "published": "2019-04-17T16:58:18Z",
    "updated": "2019-04-17T16:58:18Z",
    "authors": [
      "Ran Wang",
      "Karthikeya Parunandi",
      "Dan Yu",
      "Dileep Kalathil",
      "Suman Chakravorty"
    ],
    "summary": "This paper addresses the problem of learning the optimal control policy for a nonlinear stochastic dynamical system with continuous state space, continuous action space and unknown dynamics. This class of problems are typically addressed in stochastic adaptive control and reinforcement learning literature using model-based and model-free approaches respectively. Both methods rely on solving a dynamic programming problem, either directly or indirectly, for finding the optimal closed loop control policy. The inherent `curse of dimensionality' associated with dynamic programming method makes these approaches also computationally difficult.\n  This paper proposes a novel decoupled data-based control (D2C) algorithm that addresses this problem using a decoupled, `open loop - closed loop', approach. First, an open-loop deterministic trajectory optimization problem is solved using a black-box simulation model of the dynamical system. Then, a closed loop control is developed around this open loop trajectory by linearization of the dynamics about this nominal trajectory. By virtue of linearization, a linear quadratic regulator based algorithm can be used for this closed loop control. We show that the performance of D2C algorithm is approximately optimal. Moreover, simulation performance suggests significant reduction in training time compared to other state of the art algorithms.",
    "pdf_url": "https://arxiv.org/pdf/1904.08361v1",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.RO",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 14,
    "bibtex": "@Article{Wang2019DecoupledDA,\n author = {Ran Wang and Karthikeya S. Parunandi and Dan Yu and D. Kalathil and S. Chakravorty},\n booktitle = {IEEE Transactions on Automatic Control},\n journal = {IEEE Transactions on Automatic Control},\n pages = {3582-3589},\n title = {Decoupled Data-Based Approach for Learning to Control Nonlinear Dynamical Systems},\n volume = {67},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "wang2019"
  },
  {
    "id": "http://arxiv.org/abs/1908.05845v1",
    "title": "Memory-Efficient Object-Oriented Programming on GPUs",
    "published": "2019-08-16T04:50:29Z",
    "updated": "2019-08-16T04:50:29Z",
    "authors": [
      "Matthias Springer"
    ],
    "summary": "Object-oriented programming is often regarded as too inefficient for high-performance computing (HPC), despite the fact that many important HPC problems have an inherent object structure. Our goal is to bring efficient, object-oriented programming to massively parallel SIMD architectures, especially GPUs.\n  In this thesis, we develop various techniques for optimizing object-oriented GPU code. Most notably, we identify the object-oriented Single-Method Multiple-Objects (SMMO) programming model. We first develop an embedded C++ Structure of Arrays (SOA) data layout DSL for SMMO applications. We then design a lock-free, dynamic memory allocator that stores allocations in SOA layout. Finally, we show how to further optimize the memory access of SMMO applications with memory defragmentation.",
    "pdf_url": "https://arxiv.org/pdf/1908.05845v1",
    "doi": null,
    "categories": [
      "cs.PL"
    ],
    "primary_category": "cs.PL",
    "comment": "Ph.D. Thesis",
    "journal_ref": null,
    "citation_count": 1,
    "bibtex": "@Article{Springer2019MemoryEfficientOP,\n author = {M. Springer},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Memory-Efficient Object-Oriented Programming on GPUs},\n volume = {abs/1908.05845},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "springer2019"
  },
  {
    "id": "http://arxiv.org/abs/1911.00507v2",
    "title": "SpecuSym: Speculative Symbolic Execution for Cache Timing Leak Detection",
    "published": "2019-11-04T15:59:22Z",
    "updated": "2020-02-14T21:19:33Z",
    "authors": [
      "Shengjian Guo",
      "Yueqi Chen",
      "Peng Li",
      "Yueqiang Cheng",
      "Huibo Wang",
      "Meng Wu",
      "Zhiqiang Zuo"
    ],
    "summary": "CPU cache is a limited but crucial storage component in modern processors, whereas the cache timing side-channel may inadvertently leak information through the physically measurable timing variance. Speculative execution, an essential processor optimization, and a source of such variances, can cause severe detriment on deliberate branch mispredictions. Despite static analysis could qualitatively verify the timing-leakage-free property under speculative execution, it is incapable of producing endorsements including inputs and speculated flows to diagnose leaks in depth. This work proposes a new symbolic execution based method, SpecuSym, for precisely detecting cache timing leaks introduced by speculative execution. Given a program (leakage-free in non-speculative execution), SpecuSymsystematically explores the program state space, models speculative behavior at conditional branches, and accumulates the cache side effects along with subsequent path explorations. During the dynamic execution, SpecuSymconstructs leak predicates for memory visits according to the specified cache model and conducts a constraint-solving based cache behavior analysis to inspect the new cache behaviors. We have implementedSpecuSymatop KLEE and evaluated it against 15 open-source benchmarks. Experimental results show thatSpecuSymsuccessfully detected from 2 to 61 leaks in 6 programs under 3 different cache settings and identified false positives in 2 programs reported by recent work.",
    "pdf_url": "https://arxiv.org/pdf/1911.00507v2",
    "doi": null,
    "categories": [
      "cs.CR",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": null,
    "journal_ref": null,
    "citation_count": 51,
    "bibtex": "@Article{Guo2019SPECUSYMSS,\n author = {Shengjian Guo and Yueqi Chen and Peng Li and Yueqiang Cheng and Huibo Wang and Meng Wu and Zhiqiang Zuo},\n booktitle = {International Conference on Software Engineering},\n journal = {2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)},\n pages = {1235-1247},\n title = {SPECUSYM: Speculative Symbolic Execution for Cache Timing Leak Detection},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "guo2019"
  },
  {
    "id": "http://arxiv.org/abs/2205.00156v1",
    "title": "A Sequential MPC Approach to Reactive Planning for Bipedal Robots",
    "published": "2022-04-30T04:17:37Z",
    "updated": "2022-04-30T04:17:37Z",
    "authors": [
      "Kunal Sanjay Narkhede",
      "Abhijeet Mangesh Kulkarni",
      "Dhruv Ashwinkumar Thanki",
      "Ioannis Poulakakis"
    ],
    "summary": "This paper presents a sequential Model Predictive Control (MPC) approach to reactive motion planning for bipedal robots in dynamic environments. The approach relies on a sequential polytopic decomposition of the free space, which provides an ordered collection of mutually intersecting obstacle free polytopes and waypoints. These are subsequently used to define a corresponding sequence of MPC programs that drive the system to a goal location avoiding static and moving obstacles. This way, the planner focuses on the free space in the vicinity of the robot, thus alleviating the need to consider all the obstacles simultaneously and reducing computational time. We verify the efficacy of our approach in high-fidelity simulations with the bipedal robot Digit, demonstrating robust reactive planning in the presence of static and moving obstacles.",
    "pdf_url": "https://arxiv.org/pdf/2205.00156v1",
    "doi": null,
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "comment": "10 pages, 7 figures",
    "journal_ref": null,
    "citation_count": 36,
    "bibtex": "@Article{Narkhede2022ASM,\n author = {K. S. Narkhede and Abhijeet M. Kulkarni and D. Thanki and I. Poulakakis},\n booktitle = {IEEE Robotics and Automation Letters},\n journal = {IEEE Robotics and Automation Letters},\n pages = {11831-11838},\n title = {A Sequential MPC Approach to Reactive Planning for Bipedal Robots Using Safe Corridors in Highly Cluttered Environments},\n volume = {7},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "narkhede2022"
  },
  {
    "id": "http://arxiv.org/abs/1802.03013v1",
    "title": "D2.4 Report on the final prototype of programming abstractions for energy-efficient inter-process communication",
    "published": "2018-02-08T11:10:38Z",
    "updated": "2018-02-08T11:10:38Z",
    "authors": [
      "Phuong Hoai Ha",
      "Vi Ngoc-Nha Tran",
      "Ibrahim Umar",
      "Aras Atalar",
      "Anders Gidenstam",
      "Paul Renaud-Goud",
      "Philippas Tsigas",
      "Ivan Walulya"
    ],
    "summary": "Work package 2 (WP2) aims to develop libraries for energy-efficient inter-process communication and data sharing on the EXCESS platforms. The Deliverable D2.4 reports on the final prototype of programming abstractions for energy-efficient inter- process communication. Section 1 is the updated overview of the prototype of programming abstraction and devised power/energy models. The Section 2-6 contain the latest results of the four studies: i) GreenBST, a energy-efficient and concurrent search tree (cf. Section 2) ii) Customization methodology for implementation of streaming aggregation in embedded systems (cf. Section 3) iii) Energy Model on CPU for Lock-free Data-structures in Dynamic Environments (cf. Section 4.10) iv) A General and Validated Energy Complexity Model for Multithreaded Algorithms (cf. Section 5)",
    "pdf_url": "https://arxiv.org/pdf/1802.03013v1",
    "doi": null,
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC",
    "comment": "146 pages. arXiv admin note: text overlap with arXiv:1611.05793, arXiv:1605.08222",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Ha2018D24RO,\n author = {P. Ha and V. Tran and I. Umar and A. Atalar and Anders Gidenstam and Paul Renaud-Goud and P. Tsigas and Ivan Walulya},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {D2.4 Report on the final prototype of programming abstractions for energy-efficient inter-process communication},\n volume = {abs/1802.03013},\n year = {2018}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "ha2018"
  },
  {
    "id": "http://arxiv.org/abs/2309.05105v1",
    "title": "Convex Q Learning in a Stochastic Environment: Extended Version",
    "published": "2023-09-10T18:24:43Z",
    "updated": "2023-09-10T18:24:43Z",
    "authors": [
      "Fan Lu",
      "Sean Meyn"
    ],
    "summary": "The paper introduces the first formulation of convex Q-learning for Markov decision processes with function approximation. The algorithms and theory rest on a relaxation of a dual of Manne's celebrated linear programming characterization of optimal control. The main contributions firstly concern properties of the relaxation, described as a deterministic convex program: we identify conditions for a bounded solution, and a significant relationship between the solution to the new convex program, and the solution to standard Q-learning. The second set of contributions concern algorithm design and analysis: (i) A direct model-free method for approximating the convex program for Q-learning shares properties with its ideal. In particular, a bounded solution is ensured subject to a simple property of the basis functions; (ii) The proposed algorithms are convergent and new techniques are introduced to obtain the rate of convergence in a mean-square sense; (iii) The approach can be generalized to a range of performance criteria, and it is found that variance can be reduced by considering ``relative'' dynamic programming equations; (iv) The theory is illustrated with an application to a classical inventory control problem.",
    "pdf_url": "https://arxiv.org/pdf/2309.05105v1",
    "doi": null,
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "Extended version of \"Convex Q-learning in a stochastic environment\", IEEE Conference on Decision and Control, 2023 (to appear)",
    "journal_ref": null,
    "citation_count": 5,
    "bibtex": "@Article{Lu2023ConvexQL,\n author = {F. Lu and Sean P. Meyn},\n booktitle = {IEEE Conference on Decision and Control},\n journal = {2023 62nd IEEE Conference on Decision and Control (CDC)},\n pages = {776-781},\n title = {Convex Q Learning in a Stochastic Environment},\n year = {2023}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "lu2023"
  },
  {
    "id": "http://arxiv.org/abs/2105.10205v1",
    "title": "Rational Dynamic Price Model for Demand Response Programs in Modern Distribution Systems",
    "published": "2021-05-21T08:41:19Z",
    "updated": "2021-05-21T08:41:19Z",
    "authors": [
      "Rayees A. Thokar",
      "Nikhil Gupta",
      "K. R. Niazi",
      "Anil Swarnkar",
      "Nand K. Meena"
    ],
    "summary": "Demand response (DR) refers to change in electricity consumption pattern of customers during on-peak hours in lieu of financial gains to reduce stress on distribution systems. Existing dynamic price models have not provided adequate success to price-based demand response (PBDR) programs. It happened as these models have raised typical socio-economic problems pertaining to cross-subsidy, free-riders, social inequity, assured profit of utilities, financial gains and comfort of customers, etc. This paper presents a new dynamic price model for PBDR in distribution systems which aims to overcome some of the above mentioned problems of the existing price models. The main aim of the developed price model is to overcome the problems of cross-subsidy and free-riders of the existing price models for widespread acceptance, deployment and efficient utilization of PBDR programs in contemporary distribution systems. Proposed price model generates demand-linked price signal that imposes different price signals to different customers during on-peak hours and remains static otherwise. This makes proposed model a class apart from other existing models. The novelty of the proposed model lies in the fact that the financial benefits and penalties pertaining to DR are self-adjusted among customers while preserving social equity and profit of the utility. Such an ideology has not been yet addressed in the literature. Detailed investigation of application results on a standard test bench reveals that the proposed model equally cares regarding the interests of both customers and utility. For economic assessment, a comparison of the proposed price model with the existing pricing models is also performed.",
    "pdf_url": "https://arxiv.org/pdf/2105.10205v1",
    "doi": null,
    "categories": [
      "eess.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "10 pages, 9 figures",
    "journal_ref": null,
    "citation_count": 0,
    "bibtex": "@Article{Thokar2021RationalDP,\n author = {R. A. Thokar and N. Gupta and K. R. Niazi and A. Swarnkar and N. Meena},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Rational Dynamic Price Model for Demand Response Programs in Modern Distribution Systems},\n volume = {abs/2105.10205},\n year = {2021}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "thokar2021"
  },
  {
    "id": "http://arxiv.org/abs/1903.07441v1",
    "title": "Path Planning in Dynamic Environments Using Time-Warped Grids and a Parallel Implementation",
    "published": "2019-03-18T13:48:17Z",
    "updated": "2019-03-18T13:48:17Z",
    "authors": [
      "Siavash Farzan",
      "Guilherme N. DeSouza"
    ],
    "summary": "This paper proposes a solution to the problem of smooth path planning for mobile robots in dynamic and unknown environments. A novel concept of Time-Warped Grid is introduced to predict the pose of obstacles in the environment and avoid collisions. The algorithm is implemented using C/C++ and the CUDA programming environment, and combines stochastic estimation (Kalman filter), harmonic potential fields and a rubber band model, and it translates naturally into the parallel paradigm of GPU programming. In simple terms, time-warped grids are progressively wider orbits around the mobile robot. Those orbits represent the variable time intervals estimated by the robot to reach detected obstacles. The proposed method was tested using several simulation scenarios for the Pioneer P3-DX robot, which demonstrated the robustness of the algorithm by finding the optimum path in terms of smoothness, distance, and collision-free, in both static or dynamic environments, and with large number of obstacles.",
    "pdf_url": "https://arxiv.org/pdf/1903.07441v1",
    "doi": null,
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO",
    "comment": "18 pages, 10 figures",
    "journal_ref": null,
    "citation_count": 4,
    "bibtex": "@Article{Farzan2019PathPI,\n author = {Siavash Farzan and G. DeSouza},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Path Planning in Dynamic Environments Using Time-Warped Grids and a Parallel Implementation},\n volume = {abs/1903.07441},\n year = {2019}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "farzan2019"
  },
  {
    "id": "http://arxiv.org/abs/1204.0305v2",
    "title": "Shadow prices and well-posedness in the problem of optimal investment and consumption with transaction costs",
    "published": "2012-04-02T03:12:21Z",
    "updated": "2012-06-15T19:32:13Z",
    "authors": [
      "Jin Hyuk Choi",
      "Mihai Sirbu",
      "Gordan Zitkovic"
    ],
    "summary": "We revisit the optimal investment and consumption model of Davis and Norman (1990) and Shreve and Soner (1994), following a shadow-price approach similar to that of Kallsen and Muhle-Karbe (2010). Making use of the completeness of the model without transaction costs, we reformulate and reduce the Hamilton-Jacobi-Bellman equation for this singular stochastic control problem to a non-standard free-boundary problem for a first-order ODE with an integral constraint. Having shown that the free boundary problem has a smooth solution, we use it to construct the solution of the original optimal investment/consumption problem in a self-contained manner and without any recourse to the dynamic programming principle. Furthermore, we provide an explicit characterization of model parameters for which the value function is finite.",
    "pdf_url": "https://arxiv.org/pdf/1204.0305v2",
    "doi": null,
    "categories": [
      "q-fin.PM",
      "math.OC"
    ],
    "primary_category": "q-fin.PM",
    "comment": "31 pages, 20 figures",
    "journal_ref": null,
    "citation_count": 43,
    "bibtex": "@Article{Choi2012ShadowPA,\n author = {J. Choi and M. Sîrbu and Gordan Zitkovic},\n booktitle = {SIAM Journal of Control and Optimization},\n journal = {SIAM J. Control. Optim.},\n pages = {4414-4449},\n title = {Shadow Prices and Well-Posedness in the Problem of Optimal Investment and Consumption with Transaction Costs},\n volume = {51},\n year = {2012}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "choi2012"
  },
  {
    "id": "http://arxiv.org/abs/2206.02902v5",
    "title": "Goal-Space Planning with Subgoal Models",
    "published": "2022-06-06T20:59:07Z",
    "updated": "2024-02-27T06:15:53Z",
    "authors": [
      "Chunlok Lo",
      "Kevin Roice",
      "Parham Mohammad Panahi",
      "Scott Jordan",
      "Adam White",
      "Gabor Mihucz",
      "Farzane Aminmansour",
      "Martha White"
    ],
    "summary": "This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely. We show that our GSP algorithm can propagate value from an abstract space in a manner that helps a variety of base learners learn significantly faster in different domains.",
    "pdf_url": "https://arxiv.org/pdf/2206.02902v5",
    "doi": null,
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "citation_count": 6,
    "bibtex": "@Article{Lo2022GoalSpacePW,\n author = {Chun-Ping Lo and Emma Jordan and Gábor Mihucz and Adam White and Farzane Aminmansour and Martha White},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Goal-Space Planning with Subgoal Models},\n volume = {abs/2206.02902},\n year = {2022}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "lo2022"
  },
  {
    "id": "http://arxiv.org/abs/2508.11425v1",
    "title": "Tapas are free! Training-Free Adaptation of Programmatic Agents via LLM-Guided Program Synthesis in Dynamic Environments",
    "published": "2025-08-15T12:02:46Z",
    "updated": "2025-08-15T12:02:46Z",
    "authors": [
      "Jinwei Hu",
      "Yi Dong",
      "Youcheng Sun",
      "Xiaowei Huang"
    ],
    "summary": "Autonomous agents in safety-critical applications must continuously adapt to dynamic conditions without compromising performance and reliability. This work introduces TAPA (Training-free Adaptation of Programmatic Agents), a novel framework that positions large language models (LLMs) as intelligent moderators of the symbolic action space. Unlike prior programmatic agents that typically generate a monolithic policy program or rely on fixed symbolic action sets, TAPA synthesizes and adapts modular programs for individual high-level actions, referred to as logical primitives. By decoupling strategic intent from execution, TAPA enables meta-agents to operate over an abstract, interpretable action space while the LLM dynamically generates, composes, and refines symbolic programs tailored to each primitive. Extensive experiments across cybersecurity and swarm intelligence domains validate TAPA's effectiveness. In autonomous DDoS defense scenarios, TAPA achieves 77.7% network uptime while maintaining near-perfect detection accuracy in unknown dynamic environments. In swarm intelligence formation control under environmental and adversarial disturbances, TAPA consistently preserves consensus at runtime where baseline methods fail completely. This work promotes a paradigm shift for autonomous system design in evolving environments, from policy adaptation to dynamic action adaptation.",
    "pdf_url": "https://arxiv.org/pdf/2508.11425v1",
    "doi": null,
    "categories": [
      "cs.MA"
    ],
    "primary_category": "cs.MA",
    "comment": "Under Review",
    "journal_ref": null,
    "citation_count": 2,
    "bibtex": "@Article{Hu2025TapasAF,\n author = {Jinwei Hu and Yi Dong and Youcheng Sun and Xiaowei Huang},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Tapas are free! Training-Free Adaptation of Programmatic Agents via LLM-Guided Program Synthesis in Dynamic Environments},\n volume = {abs/2508.11425},\n year = {2025}\n}\n",
    "markdown_text": null,
    "ranking": null,
    "citation_key": "hu2025"
  }
]