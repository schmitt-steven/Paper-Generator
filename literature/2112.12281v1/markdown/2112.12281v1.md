## Improving the Efficiency of Off-Policy Reinforcement Learning by Accounting for Past Decisions



Brett Daley
Khoury College of Computer Sciences
Northeastern University
Boston, MA 02115, USA
daley.br@northeastern.edu



Christopher Amato
Khoury College of Computer Sciences
Northeastern University
Boston, MA 02115, USA

c.amato@northeastern.edu



Abstract


Off-policy learning from multistep returns is crucial for sample-efficient reinforcement learning, particularly in the experience replay setting now commonly used with deep neural networks. Classically,
off-policy estimation bias is corrected in a per-decision manner: past temporal-difference errors are reweighted by the instantaneous Importance Sampling (IS) ratio (via eligibility traces) after each action.
Many important off-policy algorithms such as Tree Backup and Retrace rely on this mechanism along
with differing protocols for truncating (“cutting”) the ratios (“traces”) to counteract the excessive variance of the IS estimator. Unfortunately, cutting traces on a per-decision basis is not necessarily efficient;
once a trace has been cut according to local information, the effect cannot be reversed later, potentially
resulting in the premature truncation of estimated returns and slower learning. In the interest of motivating efficient off-policy algorithms, we propose a multistep operator that permits arbitrary past-dependent
traces. We prove that our operator is convergent for policy evaluation, and for optimal control when
targeting greedy-in-the-limit policies. Our theorems establish the first convergence guarantees for many
existing algorithms including Truncated IS, Non-Markov Retrace, and history-dependent TD(λ). Our
theoretical results also provide guidance for the development of new algorithms that jointly consider
multiple past decisions for better credit assignment and faster learning.

### 1 Introduction


Reinforcement learning concerns an agent interacting with its environment through trial and error to maximize its expected cumulative reward [23]. One of the great challenges of reinforcement learning is the
temporal credit assignment problem [21]: upon the receipt of a reward, which past action(s) should be held
responsible and, hence, be reinforced? Basic temporal-difference (TD) methods [e.g. 29, 18] assign credit
to the immediately taken action, bootstrapping from previous experience to learn long-term dependencies.
This process requires a large number of repetitions to generate effective behaviors from rewards, motivating research into multistep return estimation in which credit is distributed among multiple (even infinitely
many) past actions according to some eligibility rule [e.g. 22, 29, 23]. This can lead to significantly faster
learning [23].


One challenge of multistep estimators is that they generally have higher variance than 1-step estimators [7].
This is exacerbated in the off-policy setting, where environment interaction is conducted according to a
behavior policy that differs from the target policy for which returns are being estimated. The discrepancy
between the two policies manifests mathematically as bias in the return estimation, which can be detrimental
to learning if left unaddressed. Despite these challenges, off-policy learning is important for exploration and
sample efficiency, especially when combined with experience replay [10], which has gained popularity due
to deep reinforcement learning [e.g. 13]. The canonical bias-correction technique is Importance Sampling
(IS) wherein the bias due to the differing policies is eliminated by the product of their probability ratios.


1




Although IS theoretically resolves the off-policy bias, it can suffer from extreme variance that makes the
algorithm largely untenable in practice.


Directly managing the variance of the IS estimator has been a fruitful avenue for developing efficient offpolicy algorithms. Past work has focused on modifying the individual IS ratios to reduce the variance of
the full update: e.g. Tree Backup [16], Q(λ) with Off-Policy Corrections [5], Retrace [14], ABQ [12], and
C-trace [17]. All of these methods can be implemented online with per-decision rules [16] that determine
how much to decay the IS ratio according to the current state-action pair—a process commonly called “trace
cutting” in reference to the first method to utilize this technique, Watkins’ Q(λ) [29, 23, 14]. The re-weighted
TD error is then broadcast to previous experiences using eligibility traces [8, 1, 21, 23]. These algorithms
are Markov in the sense that each iterative off-policy correction depends on only the current state-action
pair. One issue with this strategy is that it may lead to suboptimal decisions, since cutting a trace cannot
be reversed later, even as new information is ascertained. In contrast, a non-Markov method could examine
an entire trajectory of past state-action pairs to make globally better decisions regarding credit assignment.


Indeed, some existing off-policy methods already conduct offline bias correction in a non-Markov manner.
Perhaps the simplest example is Truncated IS where the IS ratio products are pre-calculated offline and then
clipped to some finite value (see Section 2.2). More recently, [14] suggested a recursive variant of Retrace
that automatically relaxes the clipping bound when its historical trace magnitude becomes small; the authors
conjecture that this could lead to more efficient learning. To our knowledge, no theoretical analysis has been
conducted on non-Markov algorithms such as these, meaning that their convergence properties are unknown,
and the space of possible algorithms has not been fully explored.


To better understand the behavior of these non-Markov off-policy algorithms, and to support new discoveries
of efficient algorithms, we propose a multistep operator M that accounts for arbitrary dependencies on past
decisions. Our operator is a significant generalization of the R operator proposed by [14] in that it allows
an arbitrary, time-dependent weight for each TD error that may generally depend on the history of the
Markov Decision Process (MDP). We prove that our operator converges for policy evaluation and for control
provided that the chosen weights never exceed the true IS ratio product. We also remove the assumptions
of increasingly greedy policies and pessimistic initialization used by [14] in the control setting. Finally,
we discuss practical considerations when implementing algorithms described by our operator. Our results
presented here provide new insights into off-policy credit assignment and open avenues for efficient empirical
methods—with particularly interesting opportunities for deep reinforcement learning and experience replay.

### 2 Preliminaries


We consider MDPs of the form (S, A, P, R, γ). S and A are finite sets of states and actions, respectively.
Letting ∆X denote the set of distributions over a set X, then P : S × A → ∆S is the transition function,
R : S × A → R is the reward function, and γ ∈ [0, 1] is the discount factor.


A policy π : S → ∆A determines the agent’s probability of selecting a given action in each state. A Q-function
Q : S × A → R represents the agent’s estimate of the expected return achievable from each state-action pair.
For a policy π, we define the operator P π as



(P π Q)(s, a) := �

s [′] ∈S



� P (s [′] |s, a)π(a [′] |s [′] )Q(s [′], a [′] ).

a [′] ∈A



As a shorthand notation, it is useful to represent Q-functions and the reward function as vectors in R [n] where
n = |S × A|. We define e to be the vector whose components are equal to 1. Linear operators such as P π
can hence be interpreted as n × n square matrices that multiply these vectors, with repeated application
corresponding to exponentiation: e.g. P π [t] [Q][ =][ P] [π] [(][P] π [ t][−][1] Q). All inequalities in our work should be interpreted
element wise when involving vectors or matrices. Finally, we let ∥A∥ := ∥A∥ ∞ for a matrix or vector A.


In the policy evaluation setting, we seek to estimate the expected discounted return for policy π, given by


Q [π] := � γ [t] P π [t] [R.]

t≥0


2




Q [π] is the unique fixed point of the Bellman operator T π Q := R + γP π Q [2]. In the control setting, we seek to
estimate the expected return under the optimal policy π [∗], denoted by Q [∗] . Letting Π be the set of all possible
policies, then Q [∗] is the unique fixed point of the Bellman optimality operator T Q := R + γ max
π [′] ∈Π [P] [π] [′] [Q][.]


2.1 Multistep Off-Policy Operators


In our work, we are particularly interested in the off-policy learning case, where trajectories of the form
((s, a), (s 1, a 1 ), (s 2, a 2 ), . . . ), (s k, a k ) ∈S × A, are generated by interacting with the MDP using a behavior
policy µ ∈ Π, µ ̸= π. Let F t denote the first t + 1 terms of this sequence. We define the TD error for policy
π at timestep t as
δ t [π] [:=][ r] [t] [+][ γ] � π(a [′] |s t+1 )Q(s t+1, a [′] ) − Q(s t, a t ).

a [′] ∈A


Let ρ k := [π] µ( [(] a [a] k [k] [|] | [s] s [k] k [)] ) [for brevity. [14] introduced the following off-policy operator:]



t
γ [t] [�] �
t≥0 k=1



(RQ)(s, a) := Q(s, a) + E µ
��



� c(s k, a k )�δ t [π] �, (1)

k=1



where c(s k, a k ) ∈ [0, ρ k ] are arbitrary nonnegative coefficients, or traces. When c(s k, a k ) < ρ k, we say that
the trace has been (partially) cut. Note that the traces are Markov in the sense that they depend only on
the state-action pair (s k, a k ) and are otherwise independent of F k . In other words, the traces for R can be
calculated per decision [16], thereby permitting an efficient online implementation with eligibility traces.


While per-decision traces are convenient from a computational perspective, they require making choices
about how much to cut a trace without direct consideration of the past. This can lead to locally optimal
decisions; for example, if the trace is cut by setting c(s k, a k ) = 0 at some timestep, then the effect cannot
be reversed later. Regardless of whatever new experiences are encountered by the agent, experiences before
timestep k will be ineligible for credit assignment, resulting in an opportunity cost. In fact, this exact
phenomenon is why Watkins’ Q(λ) [29] often learns more slowly than Peng’s Q(λ) [15] even though the
former avoids off-policy bias [23, 4, 9]. The same effect (but to a lesser extent) impacts Tree Backup and
Retrace where c(s k, a k ) ≤ 1 always, implying that the eligibilities for past experiences can never increase.


For this reason, it may be desirable to expend some additional computation in exchange for better decisions
regarding credit assignment. Our principal contribution is the proposal and analysis of the following off-policy
operator that encompasses this possibility:


(MQ)(s, a) := Q(s, a) + E µ γ [t] β(F t )δ t [π], (2)
�� �

t≥0


where each β(F t ) is an arbitrary nonnegative coefficient that generally depends on the history F t . By analogy
to TD(0) [22], we define β(F 0 ) := 1. The principal goal of Section 3 is to characterize the values of β(F t )
for t ≥ 1 that lead to convergence in the policy evaluation and control settings.


The major analytical challenge of our operator, and its main novelty, is the complex dependence on the
sequence F t . Our operator is therefore inherently non-Markov, in spite of the fact that the MDP dynamics
are Markov by definition. Mathematically, this makes the M operator difficult to analyze, as the terms in
the series 1 + β(F 1 )+ β(F 2 )+ · · · generally share no common factors that would allow a per-decision update
for eligibility traces. Nevertheless, removing the Markov assumption is necessary to understand important
existing algorithms (see Section 2.2) while also paving the way for new credit assignment algorithms.

A special case arises when β(F t ) does factor into traces: that is, β(F t ) = [�] [t] k=1 [c][(][s] [k] [, a] [k] [) for every history]
F t . Equation (2) therefore reduces to equation (1), taking us back to the Markov setting studied by [14]. As
such, our M operator subsumes the R operator as a specific case, and our later theoretical developments
have implications for Retrace and related off-policy algorithms. We discuss these implications in Section 4.


3




2.2 Example Algorithms


By weighting each TD error with an arbitrary history-dependent coefficient β(F t ), the M operator is remarkably general, but this also makes it abstract. To help concretize it, we describe several existing off-policy
algorithms below and how they can be described in the notation of our operator. This should aid with
understanding and illuminate cases where our operator is necessary to explain convergence.


Importance Sampling (IS): β(F t ) = [�] [t] k=1 [ρ] [k] [.]
The standard approach for correcting off-policy experience. Although it is the only unbiased estimator in this
list, it suffers from high variance when µ(a k |s k ) ≈ 0 that makes it difficult to utilize effectively in practice.

Q(λ) with Off-Policy Corrections [5]: β(F t ) = [�] [t] k=1 [λ][.]
A straightforward algorithm that decays the TD errors by a fixed constant λ ∈ [0, 1]. In the on-policy
(µ = π) policy evaluation case, this is equivalent to the TD(λ) extension [22] of Expected Sarsa [23]. The
algorithm does not require explicit knowledge of the behavior policy µ, which is desirable; however, it is not
convergent when π and µ differ too much, and this criterion is restrictive in practice.

Tree Backup (TB) [16]: β(F t ) = [�] [t] k=1 [λπ][(][a] [k] [|][s] [k] [)][.]
A method that automatically cuts traces according to the product of probabilities under the target policy π,
which forms a conservative lower bound on the IS ratio product. As a result, TB converges for any behavior
policy µ, but it is not efficient since traces are cut excessively—especially in the on-policy case.

Retrace [14]: β(F t ) = [�] [t] k=1 [λ][ min(1][, ρ] [k] [)][.]
A convergent algorithm for arbitrary policies π and µ that remains efficient in the on-policy case because it
does not cut traces (if λ = 1); however, the fact that β(F t ) is monotone non-increasing can cause the trace
products to decay too quickly in practice [12, 17].


- Non-Markov Retrace [14]: β(F t ) = λ min(1, β(F t−1 )ρ t ).
A modification to Retrace proposed by [14] and conjectured to lead to faster learning. It permits trace
values larger than 1 by relaxing the clipping bound when the historical trace product β(F t−1 ) is small (see
Section 4.3). The algorithm is recursive, which makes it difficult to analyze, and its convergence for control
is an open question.


- Truncated Importance Sampling [6]: β(F t ) = min(d t, [�] [t] k=1 [ρ] [k] [)][.]
A simple but effective method to combat the variance of the IS estimator. For any value of d t ≥ 0, the
obtained variance is finite. Variations of this algorithm have been applied in the reinforcement learning
literature [e.g. 27, 31, 30, 28], but to our knowledge its convergence in an MDP setting has not been studied.

### 3 Analysis


In this section, we study the convergence properties of the M operator for policy evaluation and control. It
will be convenient to re-express equation (2) in operator notation in our following analysis. First, we can
define an operator B t such that


(B t Q)(s, a) := β(F t−1 ∪ (s, a))Q(s, a).


Note that B t can be interpreted as a diagonal matrix where each element along the main diagonal is equal to
β(F t ) after hypothetically selecting the corresponding state-action pair (s, a) in history F t−1 . Furthermore,
by our earlier definition of β(F 0 ) = 1, we must have B 0 = I, the identity matrix. With this, we can write
the M operator as
MQ = Q + � γ [t] P µ [t] [B] [t] [(][T] [π] [Q][ −] [Q][)][.] (3)

t≥0


  - Denotes a non-Markov algorithm that must be modeled by our M operator defined in equation (2).


4




3.1 Policy Evaluation


In this setting, we seek to estimate Q [π] for a fixed target policy π ∈ Π from interactions conducted according
to a fixed behavior policy µ ∈ Π. Specifically, our goal is to prove that repeated application of the M
operator to an arbitrarily initialized Q converges to Q [π] . We start by proving the following lemma:


Lemma 1. Let M be the operator defined in (3). Q [π] is a fixed point of M; the difference between MQ and
Q [π] is given by
MQ − Q [π] = � γ [t] P µ [t][−][1] (B t−1 P π − P µ B t )(Q − Q [π] ). (4)

t≥1


We present the proof in Appendix A.1. With this, we can introduce our main theorem for policy evaluation:


Theorem 1. Suppose that B t ≤ [�] [t] k=1 [ρ] [k] [ for any history][ F] [t][−][1] [ ∪] [(][s] [t] [, a] [t] [)][. The operator][ M][ defined in (3) is]
a contraction mapping with Q [π] as its unique fixed point. That is,


∥MQ − Q [π] ∥≤ γ∥Q − Q [π] ∥,


and consequently lim
k→∞ [M] [k] [Q][ =][ Q] [π] [ for any][ Q][.]


Proof. Lemma 1 already established that Q [π] is a fixed point of M. We will show that M is a contraction
mapping, which will guarantee that Q [π] is its unique fixed point.


Let A := [�] t≥1 [γ] [t] [P] µ [ t][−][1] (B t−1 P π − P µ B t ) and rewrite (4) as MQ − Q [π] = A(Q − Q [π] ). By our assumption
that B t is diagonal and B t ≤ [�] [t] k=1 [ρ] [k] [, the matrix][ A][ has nonnegative elements (since][ B] [t][−][1] [P] [π] [ −] [P] [µ] [B] [t] [ ≥] [0).]
To complete the proof, we can show that the row sums of A are never greater than γ. Equivalently, we will
show that Ae ≤ γe:


Ae = � γ [t] P µ [t][−][1] (B t−1 P π − P µ B t )e

t≥1

= � γ [t] P µ [t][−][1] (B t−1 e − P µ B t e)

t≥1



= γ �



� γ [t] P µ [t] [B] [t] [e]

t≥1



� γ [t] P µ [t] [B] [t] [e][ −] �

t≥0 t≥1



= γ(e + S) − S


= γe − (1 − γ)S,


where we have let S := [�] t≥1 [γ] [t] [P] µ [ t] [B] [t] [e][. Because][ B] [t] [≥] [0, we have that][ S][ ≥] [0 and hence][ Ae][ ≤] [γe][; thus,]

A(Q − Q [π] ) is a vector whose components each comprise a nonnegative-weighted combination of the components of Q − Q [π] where the weights add up to at most γ. This implies that


∥MQ − Q [π] ∥≤ γ∥Q − Q [π] ∥


and the operator M is a contraction mapping. Its fixed point Q [π] established by Lemma 1 must therefore
be unique, which implies that lim
k→∞ [M] [k] [Q][ =][ Q] [π] [ for any][ Q][ when][ γ <][ 1.]


We conclude with some remarks about the interpretation of our result. Note that when β(F t ) = 0 for t ≥ 1,
we have the slowest contraction rate of γ, which corresponds to TD(0). When β(F t ) = [�] [t] k=1 [ρ] [k] [, we get an]
optimal contraction rate of 0, which corresponds to the standard IS estimator. (Of course, this is only in
expectation.) Between these two extremes lies a vast spectrum of convergent algorithms, which includes the
examples mentioned in Section 2.2 as well as an infinitude of other possibilities. In Section 4.2, we discuss
practical considerations for choosing β(F t ) and how to efficiently implement history-dependent algorithms.


5




3.2 Control


We now consider the more-difficult setting of control. Given a sequence of target policies (π k ) k≥0, π k ∈ Π,
and a sequence of behavior policies (µ k ) k≥0, µ k ∈ Π, we aim to show that the sequence of Q-functions (Q k )
given by Q k+1 := M k Q k converges to Q [∗] . (Here, M k is the M operator defined for π k and µ k .)


Unlike [14], we do not assume that the target policies are increasingly greedy with respect to (Q k ). Instead,
we require only that the policies become greedy in the limit. We say that the sequence of policies (π k ) is
greedy in the limit if and only if T π k Q k → T Q k as k →∞. Intuitively, this means that each policy need
not be greedier than its predecessors, so long as the sequence of policies eventually does become greedy. We
discuss the significance of this assumption more in Section 4.1.



Let C := [�]



t≥0 [γ] [t] [P] µ [ t] [B] [t] [. We first rewrite the][ M][ operator in (3) concisely as]


MQ = Q + C(T π Q − Q). (5)



The following lemma will be useful to show that M remains a contraction mapping in this new setting:


Lemma 2. For any policy π ∈ Π,
∥I − C(I − γP π )∥≤ γ. (6)


Once again, we present the proof in Appendix A.2. We now arrive at our main theoretical result for control:


Theorem 2. Consider a sequence of target policies (π k ) and a sequence of arbitrary behavior policies (µ k ).
Let Q 0 be an arbitrarily initialized Q-function and define the sequence Q k+1 := M k Q k where M k is the
operator defined in (3) for π k and µ k . Assume that (π k ) is greedy in the limit and let ǫ k ∈ [0, 1] be the
smallest constant such that T π k Q k ≥ T Q k − ǫ k ∥Q k ∥e. Then,


ǫ k
∥Q k+1 − Q [∗] ∥≤ γ∥Q k − Q [∗] ∥ +
1 − γ [∥][Q] [k] [∥][,]


and consequently lim
k→∞ [Q] [k] [ =][ Q] [∗] [.]


Proof. Part 1: Upper bound on Q k+1 − Q [∗] . First, note the following inequality:


T π k Q k − T Q [∗] = γP π k Q k − γ max π [′] ∈Π [P] [π] [′] [Q] [k] [ ≤] [γP] [π] [k] [(][Q] [k] [ −] [Q] [∗] [)][.]


Then, from (5), we can deduce that


Q k+1 − Q [∗] = Q k − Q [∗] + C(T π k Q k − Q k )

= (I − C)(Q k − Q [∗] ) + C(T π k Q k − Q [∗] ) (7)
= (I − C)(Q k − Q [∗] ) + C(T π k Q k − T Q [∗] )

≤ (I − C)(Q k − Q [∗] ) + γCP π k (Q k − Q [∗] )
= (I − C(I − γP π k ))(Q k − Q [∗] ). (8)


Part 2: Lower bound on Q k+1 − Q [∗] . Note the following inequality:


T Q k − T Q [∗] ≥ T π [∗] Q k − T Q [∗] = γP π [∗] (Q k − Q [∗] ).


Additionally, for each policy π k, there exists some ǫ k ∈ [0, 1] such that T π k Q k ≥ T Q k − ǫ k ∥Q k ∥e. (The
inequality is vacuously true when ǫ k = 1, but recall that Theorem 2 defines ǫ k to be as small as possible.)
Starting again from equation (7), and noting that the elements of C are nonnegative,


Q k+1 − Q [∗] = (I − C)(Q k − Q [∗] ) + C(T π Q − Q [∗] )


≥ (I − C)(Q k − Q [∗] ) + C(T Q k − Q [∗] ) − ǫ k ∥Q k ∥Ce
= (I − C)(Q k − Q [∗] ) + C(T Q k − T Q [∗] ) − ǫ k ∥Q k ∥Ce


≥ (I − C)(Q k − Q [∗] ) + γCP π [∗] (Q k − Q [∗] ) − ǫ k ∥Q k ∥Ce
= (I − C(I − γP π ∗ ))(Q k − Q [∗] ) − ǫ k ∥Q k ∥Ce (9)


6




Part 3: Conclusion. When Q k+1 − Q [∗] ≥ 0, inequality (8) and Lemma 2 imply


∥Q k+1 − Q [∗] ∥≤∥I − C(I − γP π k )∥∥Q k − Q [∗] ∥≤ γ∥Q k − Q [∗] ∥. (10)


Additionally, when Q k+1 − Q [∗] ≤ 0, inequality (9) and Lemma 2 imply


∥Q k+1 − Q [∗] ∥≤∥I − C(I − γP π [∗] )∥∥Q k − Q [∗] ∥ + ǫ k ∥Q k ∥∥C∥


ǫ k
≤ γ∥Q k − Q [∗] ∥ + (11)
1 − γ [∥][Q] [k] [∥][,]


−1
because ∥C∥≤ [�] t≥0 [γ] [t] [��][P] µ [ t] [B] [t] �� ≤ (1 − γ) . Since (11) is more conservative than (10), its bound holds
regardless of the sign of Q k+1 − Q [∗] . It remains to be shown that this bound implies convergence to Q [∗] .
Observe that


ǫ k ǫ k
γ∥Q k − Q [∗] ∥ +
1 − γ [∥][Q] [k] [∥≤] [γ][∥][Q] [k] [ −] [Q] [∗] [∥] [+] 1 − γ [(][∥][Q] [k] [ −] [Q] [∗] [∥] [+][ ∥][Q] [∗] [∥][)]



ǫ k
= γ +
� 1 − γ



ǫ k
∥Q k − Q [∗] ∥ +
� 1 − γ [∥][Q] [∗] [∥][.]



Our assumption of greedy-in-the-limit policies tells us that ǫ k → 0 as k →∞; there must exist some iteration
k [′] such that ǫ k ≤ [1] 2 [(1][ −] [γ][)] [2] [ for all][ k][ ≥] [k] [′] [. Therefore, for][ k][ ≥] [k] [′] [,]



ǫ k

∥Q k+1 − Q [∗] ∥≤ [1 +][ γ] ∥Q k − Q [∗] ∥ +

2 1 − γ [∥][Q] [∗] [∥][.]



If γ < 1, then [1] 2 [(1 +][ γ][)][ <][ 1. Since][ ∥][Q] [∗] [∥] [is finite, we conclude that][ Q] [k] [ →] [Q] [∗] [as][ ǫ] [k] [ →] [0.]


We see that the convergence criterion of β(F t ) ≤ [�] [t] k=1 [ρ] [k] [ in the control setting is the same as that for]
the policy evaluation. In fact, the only additional assumption we needed is the greedy-in-the-limit target
policies. Crucially, the proof allows arbitrary behavior policies and arbitrary initialization of the Q-function,
which we discuss further in the next section.

### 4 Discussion


Despite weighting TD errors by general coefficients β(F t ) ∈ [0, [�] [t] k=1 [ρ] [k] [] that could be chosen by any arbitrary]
selection criteria, the M operator converges for both policy evaluation and control with few assumptions.
In this section, we summarize our theoretical contributions and discuss the choice of coefficients, practical
implementations, and the convergence of specific algorithms from Section 2.2.


4.1 Theoretical Contributions


Removal of Markov assumption. As we stated in the introduction, removing the Markov assumption
of the R operator [14] was our primary theoretical goal. With Markov traces, the operator B t is independent of t, allowing the sum C = [�] t≥0 [γ] [t] [P] µ [ t] [B] [t] [to be reduced to][ �] t≥0 [γ] [t] [P] cµ [ t] [for the linear operator]
P cµ := [�] (s [′],a [′] )∈S×A [P] [(][s] [′] [|][s, a][)][µ][(][a] [′] [|][s] [′] [)][c][(][s] [′] [, a] [′] [)][Q][(][s] [′] [, a] [′] [). The resulting geometric series can then be evaluated]
analytically: (I − γP cµ ) [−][1] . This technique was used by [14]. In our proofs, we avoided the Markov assumption by directly analyzing the infinite summation C, which generally does not have a closed-form expression.
We believe our work is the first to do this, and it should have far-reaching consequences for existing on- and
off-policy algorithms (see Sections 4.3 and 5) as well as yet-undiscovered algorithms.


Arbitrary initialization of Q-function. Our proof of Theorem 2 permits any initialization of Q 0 in the
control setting. In contrast, [14] made the assumption that T π 0 Q 0 − Q 0 ≥ 0 in order to produce a lower
bound on RQ k+1 − Q [∗] . This is accomplished in practice by a pessimistic initialization of the Q-function:
Q 0 (s, a) = −∥R∥ / (1 − γ), ∀(s, a) ∈S × A. Since R is a special case of our operator M where each β(F t )


7




Algorithm 1 Non-Markov Eligibility Trace

Initialize Q(s, a) arbitrarily
Select learning rate α ∈ (0, 1]
for each episode do
Initialize environment state s 0
F ←{}
repeat for t = 0, 1, 2, . . .
Take action a t ∼ µ(s t ), observe reward r t and next state s t+1
F ←F ∪ (s t, a t )
δ t [π] [←] [r] [t] [+][ γ][ �] a [′] ∈A [Q][(][s] [t][+1] [, a] [′] [)][ −] [Q][(][s] [t] [, a] [t] [)]
For k = 0, . . ., t: Q(s k, a k ) ← Q(s k, a k ) + αγ [t][−][k] β(F k:t )δ t [π] ⊲ Let F k:t := F t \ F k−1, β(F t:t ) := 1
until s t+1 is terminal
end for


factors into Markov traces, we deduce as a corollary that Retrace and all other algorithms described by R
do not require this pessimistic initialization for convergence.


Greedy-in-the-limit policies. Our requirement of greedy-in-the-limit target policies for the control setting
is significantly less restrictive than the increasingly greedy policies proposed by [14]. We need only that
lim k→∞ T π k Q k = T Q k, and we do not force the sequence of target policies to satisfy P π k+1 Q k+1 ≥ P π k Q k+1 .
This implies that the agent may target non-greedy policies for an arbitrarily long period of time, as long as
the policies do eventually become arbitrarily close to the greedy policy. This could lead to faster learning
since targeting greedy policies causes excessive trace cutting (e.g. Watkins’ Q(λ)). Our greedy-in-the-limit
policies are closely related to the Greedy-in-the-Limit with Infinite Exploration (GLIE) assumption of [19];
however, we are able to remove the need for infinite exploration by allowing an arbitrary sequence of behavior
policies, as did [14]. Once again, our theory implies as a corollary that neither increasingly greedy policies
nor the GLIE assumption are strictly necessary for the convergence of Retrace and other Markov algorithms.


4.2 Practical Concerns


Implementation with eligibility traces. The fact that the coefficient β(F t ) does not generally factor
into traces c(s 1, a 1 ), . . ., c(s t, a t ) means that eligibility traces in the traditional sense are not possible. The
problem occurs when the same state-action pair appears more than once in F t ; afterwards, there does
not exist a constant factor by which to decay the eligibility for that state-action pair that would produce
the correct updates according to (3). We can rectify this by modifying the eligibility trace to remember
each individual occurrence of the state-action pairs: a Non-Markov Eligibility Trace (see Algorithm 1). By
tracking a separate eligibility for each repeated visitation, the algorithm is able to generate the correct
coefficients β(F t ) even though they do not factor into traces. In terms of computation, our algorithm is
efficient when episodes are not extremely long, and is reminiscent of the practical linked-list implementation
of the eligibility trace described by [23].


Online convergence. Algorithm 1 is presented as an online eligibility-trace algorithm. In practice, we
expect that the updates will be calculated and accumulated offline, akin to the offline λ-return algorithm

[23], for the sake of efficiency and compatibility with deep function approximation [4]. As such, the analysis
of the online variant is beyond the scope of our work and we leave its convergence as an open question.
Nevertheless, the fact that the expected M operator is convergent would make it surprising (although
not impossible) that the online version diverges in the presence of zero-mean, finite-variance noise with
appropriately annealed stepsizes [3].

Choice of coefficients β(F t ). Our theorems guarantee convergence provided that β(F t ) ≤ [�] [t] k=1 [ρ] [k] [ for]
every F t . Of course, not all choices of β will perform well in practice. At one extreme, when β(F t ) = [�] [t] k=1 [ρ] [k]
for every F t, we recover the standard IS estimator, which we know suffers from high variance and is often
ineffectual. At the other extreme, if β(F t ) < [�] [t] k=1 [π][(][a] [k] [|][s] [k] [) for every][ F] [t] [, then we have a method that cuts]


8




traces more than Tree Backup does; it seems unlikely that such a method would learn faster than Tree
Backup, making it difficult to justify the extra complexity of the non-Markov coefficients in this case.


We therefore see that while it is important to control the overall variance of the coefficients β(F t ), it is also
important to maintain some minimum efficiency by avoiding unnecessary trace cuts, and to leverage the
non-Markov capabilities of the operator. Effective algorithms will likely compute the full IS ratio product
� tk=1 [ρ] [k] [ first and then apply some nonlinear transformation (e.g. clipping) to control the variance. This]
ensures that the coefficients are cut only when necessary.


One final yet pertinent consideration is the discounted sum of coefficients [�] t≥0 [γ] [t] [β][(][F] [t] [). Roughly speaking,]
this sum represents the potential magnitude of an update to a state-action pair. If its value is extremely
large along some trajectories with nonzero probability of occurrence, then the algorithm may not be stable.
This suggests that a constant bound on each β(F t ), or a recency heuristic that guarantees β(F t ) ≥ β(F t+1 ),
may be desirable to control the update magnitude.


4.3 Convergence for Specific Algorithms


In Section 2.2, we mentioned some algorithms that cannot be modeled by the R operator can, in fact,
be modeled by the M operator. It remains to show that their coefficients always satisfy our condition of
β(F t ) ≤ [�] [t] k=1 [ρ] [k] [ to prove that they converge according to Theorems 1 and 2.]


Truncated IS. Clearly, min(d t, [�] [t] k=1 [ρ] [k] [)][ ≤] [�] [t] k=1 [ρ] [k] [ for any choice of][ d] [t] [, hence the algorithm converges.]


Non-Markov Retrace. [14] define the traces for the algorithm as c(s t, a t ) = λ min(1 / β(F t−1 ), ρ t ), so
β(F t−1 ) = [�] [t] k [−] =1 [1] [c][(][s] [k] [, a] [k] [). This means that]



1
β(F t ) = β(F t−1 )c(s t, a t ) = λ min
� β(F t−1 ) [, ρ] [t]



β(F t−1 ) = λ min (1, β(F t−1 )ρ t ) .
�



We can now prove by induction that the required bound always holds. Assume that β(F t−1 ) ≤ [�] [t] k [−] =1 [1] [ρ] [k] [.]
Our base case is β(F 0 ) = 1 by definition, which clearly holds. Then, by hypothesis,



�



β(F t ) = λ min (1, β(F t−1 )ρ t ) ≤ λ min


and the non-Markov variant of Retrace therefore converges.

### 5 Conclusion



�



1,



t
� ρ k


k=1



≤



t
� ρ k,


k=1



Although per-decision traces are convenient from a computational perspective, they are not expressive enough
to implement arbitrary off-policy corrections, and their inability to reverse previously cut traces can lead to
inefficient learning. By removing the assumption of Markov (i.e. per-decision) traces, our M operator enables
off-policy algorithms that jointly consider all of the agent’s past experiences when assigning credit. This
provides myriad opportunities for efficient off-policy algorithms and offers the first convergence guarantees
for many previous algorithms.


The M operator is convergent for both policy evaluation and control. In the latter setting, our proof
removes the assumptions of increasingly greedy policies and pessimistic initialization. This has significant
implications for Retrace, the other algorithms discussed in Section 2.2, Watkins’ Q(λ), and more.


The generality of the M operator means that it provides convergence guarantees for a number of TD methods
that we were not able to discuss in the main text. These include methods with variable discount factors or
λ-values [e.g. 29, 23, 11, 25, 12, 24], and even history-dependent λ-values [e.g. 20, 32]. All of these can be
expressed in a common form: β(F t ) = [�] [t] k=1 [γ][(][F] [k] [)][λ][(][F] [k] [)][ρ] [k] [ where][ γ][(][F] [k] [)][ ∈] [[0][,][ 1] and][ λ][(][F] [k] [)][ ∈] [[0][,][ 1]. Clearly,]
β(F t ) ≤ [�] [t] k=1 [ρ] [k] [ and we determine that these methods converge.]


9




An interesting variant of our theory is one where the coefficient-determining function β is not fixed but
instead varies with time. In this case, the operator M becomes non-stationary; each diagonal matrix B t in
equation (3) depends not only on the number of timesteps t since the action was taken, but also the time t [′]

when the action took place. Such instances may arise, for example, when coefficients depend on information
external to the trajectory F t, such as the number of previous visitations to the state-action pair [e.g. 26].
Provided that the same conditions of B t ≤ [�] [t] k=1 [ρ] [k] [ hold for all][ t] [′] [, then the proofs of Theorems 1 and 2]
should go through with no modifications.


Important open questions remain. Namely, do the convergence guarantees established here hold for online
algorithms? And, more practically, what non-Markov algorithms exist that lead to reliably efficient and
stable learning? Interesting possibilities with regard to both of these questions are recursive methods like
Non-Markov Retrace, which are easier to implement and to analyze according to our theoretical framework.
In conjunction, our earlier considerations for choosing coefficients (Section 4.2) are pertinent. These should
be interesting starting points for future developments in off-policy learning.

### References


[1] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve
difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, pages 834–846, 1983.


[2] Richard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.


[3] Dimitri P Bertsekas and John N Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.


[4] Brett Daley and Christopher Amato. Reconciling λ-returns with experience replay. In Advances in Neural
Information Processing Systems, pages 1133–1142, 2019.


[5] Anna Harutyunyan, Marc G Bellemare, Tom Stepleton, and R´emi Munos. Q(λ) with off-policy corrections. In
International Conference on Algorithmic Learning Theory, pages 305–320, 2016.


[6] Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics,
17(2):295–311, 2008.


[7] Michael J Kearns and Satinder P Singh. Bias-variance error bounds for temporal difference updates. In COLT,
pages 142–147, 2000.


[8] A Harry Klopf. Brain function and adaptive systems: A heterostatic theory. Technical report, Air Force
Cambridge Research Labs, Hanscom AFB, MA, 1972.


[9] Tadashi Kozuno, Yunhao Tang, Mark Rowland, R´emi Munos, Steven Kapturowski, Will Dabney, Michal Valko,
and David Abel. Revisiting Peng’s Q(λ) for modern reinforcement learning. arXiv:2103.00107, 2021.


[10] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and reaching. Machine
Learning, 8(3-4):293–321, 1992.


[11] Hamid Reza Maei and Richard S Sutton. GQ(λ): A general gradient algorithm for temporal-difference prediction
learning with eligibility traces. In Proceedings of the Third Conference on Artificial General Intelligence, pages
91–96, 2010.


[12] Ashique Rupam Mahmood, Huizhen Yu, and Richard S Sutton. Multi-step off-policy learning without importance
sampling ratios. arXiv:1702.03006, 2017.


[13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep
reinforcement learning. Nature, 518(7540):529–533, 2015.


[14] R´emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G Bellemare. Safe and efficient off-policy reinforcement learning. arXiv:1606.02647, 2016.


[15] Jing Peng and Ronald J Williams. Incremental multi-step Q-Learning. Machine Learning, pages 226–232, 1994.


10




[16] Doina Precup, Richard S Sutton, and Satinder Singh. Eligibility traces for off-policy policy evaluation. In
Proceedings of the Seventeenth International Conference on Machine Learning, page 759–766, 2000.


[17] Mark Rowland, Will Dabney, and R´emi Munos. Adaptive trade-offs in off-policy learning. In International
Conference on Artificial Intelligence and Statistics, pages 34–44, 2020.


[18] Gavin A Rummery and Mahesan Niranjan. On-line Q-Learning using connectionist systems. Technical report,
Cambridge University, 1994.


[19] Satinder Singh, Tommi Jaakkola, Michael L Littman, and Csaba Szepesv´ari. Convergence results for single-step
on-policy reinforcement-learning algorithms. Machine learning, 38(3):287–308, 2000.


[20] Satinder P Singh and Richard S Sutton. Reinforcement learning with replacing eligibility traces. Machine
learning, 22(1):123–158, 1996.


[21] Richard S Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of Massachusetts Amherst, 1984.


[22] Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9–44,
1988.


[23] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, 1st edition, 1998.


[24] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT Press, 2nd edition,
2018.


[25] Richard S Sutton, Ashique Rupam Mahmood, Doina Precup, and Hado Hasselt. A new Q(λ) with interim
forward view and Monte Carlo equivalence. In International Conference on Machine Learning, pages 568–576,
2014.


[26] Richard S Sutton and Satinder P Singh. On step-size and bias in temporal-difference learning. In Proceedings
of the Eighth Yale Workshop on Adaptive and Learning Systems, pages 91–96, 1994.


[27] Eiji Uchibe and Kenji Doya. Competitive-cooperative-concurrent reinforcement learning with importance sampling. In Proceedings of International Conference on Simulation of Adaptive Behavior: From Animals and
Animats, pages 287–296, 2004.


[28] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando
de Freitas. Sample efficient actor-critic with experience replay. arXiv:1611.01224, 2016.


[29] Christopher John Cornish Hellaby Watkins. Learning from Delayed Rewards. PhD thesis, King’s College,
Cambridge, 1989.


[30] Pawe�l Wawrzy´nski. Real-time reinforcement learning by sequential actor-critics and experience replay. Neural
Networks, 22(10):1484–1497, 2009.


[31] Pawel Wawrzynski and Andrzej Pacut. Truncated importance sampling for reinforcement learning with experience replay. Proc. CSIT Int. Multiconf, pages 305–315, 2007.


[32] Huizhen Yu, A Rupam Mahmood, and Richard S Sutton. On generalized Bellman equations and temporaldifference learning. The Journal of Machine Learning Research, 19(1):1864–1912, 2018.


11




### A Proofs

A.1 Proof of Lemma 1


Proof. We begin by rewriting the operator M in (3):


MQ = Q + � γ [t] P µ [t] [B] [t] [(][T] [π] [Q][ −] [Q][)]

t≥0



� γ [t] P µ [t] [B] [t] [(][T] [π] [Q][ −] [R][) +] �

t≥0 t≥0

� γ [t] P µ [t] [B] [t] [(][T] [π] [Q][ −] [R][) +] �

t≥0 t≥1



= Q + �



= R +
�



� γ [t] P µ [t] [B] [t] [(][R][ −] [Q][)]

t≥0

� γ [t] P µ [t] [B] [t] [(][R][ −] [Q][)][.]

t≥1



It is evident from (3) that Q [π] is the fixed point of M because Q = Q [π] =⇒ T π Q − Q = 0. Therefore,


MQ − Q [π] = MQ −MQ [π]



� γ [t] P µ [t] [B] [t] [(][T] [π] [Q][ −] [T] [π] [Q] [π] [)][ −] �

t≥0 t≥1

� γ [t][+1] P µ [t] [B] [t] [P] [π] [(][Q][ −] [Q] [π] [)][ −] �

t≥0 t≥1



=
�



=
�



� γ [t] P µ [t] [B] [t] [(][Q][ −] [Q] [π] [)]

t≥1

� γ [t] P µ [t] [B] [t] [(][Q][ −] [Q] [π] [)]

t≥1



� γ [t] P µ [t][−][1] B t−1 P π (Q − Q [π] ) − �

t≥1 t≥1



=
�



� γ [t] P µ [t] [B] [t] [(][Q][ −] [Q] [π] [)]

t≥1



= � γ [t] P µ [t][−][1] (B t−1 P π − P µ B t )(Q − Q [π] ),

t≥1



which is the desired quantity.


A.2 Proof of Lemma 2


Proof. We start with the negative of the original matrix expression: C(I −γP π )−I = (C −I)−γCP π . This expression
is affine in C; we can check its two extremal values to determine the norm.


Recall that C = [�] t≥0 [γ] [t] [P] µ [ t] [B] [t] [, a nonnegative matrix. One extreme occurs when][ B] [t] [= 0 for][ t][ ≥] [1 and hence][ C][ =][ I][;]

substituting this into the expression, we obtain −γP π and deduce that ∥C(I − γP π ) − I∥ = γ when C = I.


The other extreme occurs when each B t is maximized. By our criteria for B t from Theorem 1, we must have
P µ [t] [B] [t] [≤] [P] π [ t] [and therefore][ C][ ≤] [�] t≥0 [γ] [t] [P] π [ t] [= (][I][ −] [γP] [π] [)] [−][1] [. We can also deduce in this case that]


C − I ≤ (γP π + γ [2] P π + · · · ) = γ(I + γP π + · · · )P π = γ(I − γP π ) [−][1] P π .


Substituting these into the expression, we obtain γ(I − γP π ) [−][1] P π − γ(I − γP π ) [−][1] P π = 0 when C = (I − γP π ) [−][1] .


Combining these two cases, we conclude that ∥I − C(I − γP π )∥ = ∥C(I − γP π ) − I∥≤ γ.


12


