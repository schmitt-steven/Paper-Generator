## A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation

Jalaj Bhandari, Daniel Russo and Raghav Singal


Columbia University


Abstract


Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used
algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its
statistical efficiency are available. In this work, we provide a simple and explicit finite time analysis of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors
standard techniques for analyzing stochastic gradient descent algorithms, and therefore inherits the simplicity
and elegance of that literature. Final sections of the paper show how all of our main results extend to the study
of TD learning with eligibility traces, known as TD(λ), and to Q-learning applied in high-dimensional optimal
stopping problems.
Keywords: Reinforcement learning, temporal difference learning, finite time analysis, stochastic gradient
descent.

## 1 Introduction


Originally proposed by Sutton [1988], temporal difference learning (TD) is one of the most widely used
reinforcement learning algorithms and a foundational idea on which more complex methods are built. The
algorithm operates on a stream of data generated by applying some policy to a poorly understood Markov
decision process. The goal is to learn an approximate value function, which can then be used to track the
net present value of future rewards as a function of the system’s evolving state. TD maintains a parametric
approximation to the value function, making a simple incremental update to the estimated parameter vector
each time a state transition occurs.

While easy to implement, theoretical analysis of TD is subtle. Reinforcement learning researchers in
the 1990s gathered both limited convergence guarantees [Jaakkola et al., 1994] and examples of divergence

[Baird, 1995]. Many issues were then clarified in the work of Tsitsiklis and Van Roy [1997], which establishes precise conditions for the asymptotic convergence of TD with linear function approximation and gives
examples of divergent behavior when key conditions are violated. With guarantees of asymptotic convergence
in place, a natural next step is to understand the algorithm’s statistical efficiency. How much data is required
to guarantee a given level of accuracy? Can one give uniform bounds on this, or could data requirements
explode depending on the problem instance? Twenty years after the work of Tsitsiklis and Van Roy [1997],
such questions remain largely unsettled.


1.1 Contributions


This paper develops a simple and explicit non-asymptotic analysis of TD with linear function approximation.
The resulting guarantees provide assurances of robustness. They explicitly bound the worst-case dependence
on problem features like the discount factor, the conditioning of the feature covariance matrix, and the mixing
time of the underlying Markov chain. Our analysis reveals rigorous connections between TD and stochastic gradient descent algorithms, provides a template for finite time analysis of incremental algorithms with
Markovian noise, and applies without modification to analyzing a class of high-dimensional optimal stopping
problems. We elaborate on these contributions below.


1




  - Links with gradient descent: Despite a cosmetic connection to stochastic gradient descent (SGD), incremental updates of TD are not (stochastic) gradient steps with respect to any fixed loss function. It
is therefore difficult to show that it makes consistent, quantifiable, progress toward its asymptotic limit
point. Nevertheless, Section 6 shows that expected TD updates obey crucial properties mirroring those
of gradient descent on a particular quadratic loss function. In a model where the observations are corrupted by i.i.d. noise, these gradient-like properties of TD allow us to give state-of-the-art convergence
bounds by essentially mirroring standard analyses of stochastic gradient descent (SGD). This approach
may be of broader interest as SGD analyses are commonly taught in machine learning courses and serve
as a launching point for a much broader literature on first-order optimization. Rigorous connections
with the optimization literature can facilitate research on principled improvements to TD.


  - Non-asymptotic treatment with Markovian noise: TD is usually applied online to a single Markovian
data stream. However, to our knowledge, there has been no successful [1] non-asymptotic analysis in the
setting with Markovian observation noise. Instead, many papers have studied such algorithms under the
simpler i.i.d noise model mentioned earlier [Sutton et al., 2009b,a, Liu et al., 2015, Touati et al., 2018,
Dalal et al., 2018b, Lakshminarayanan and Szepesv´ari, 2018]. One reason is that the dependent nature
of the data introduces a substantial technical challenge: the algorithm’s updates are not only noisy,
but can be severely biased. We use information theoretic techniques to control the magnitude of bias,
yielding bounds that are essentially scaled by a factor of the mixing time of the underlying Markov
process relative to those attained for i.i.d. model. Our analysis in this setting applies only to a variant
of TD that projects the iterates onto a norm ball. This projection step imposes a uniform bound on the
noise of gradient updates, which is needed for tractability. For similar reasons, projection operators are
widely used throughout the stochastic approximation literature [Kushner, 2010, Section 2].


  - An extendable approach: Much of the paper focuses on analyzing the most basic temporal difference
learning algorithm, known as TD(0). We also extend this analysis to other algorithms. First, we establish convergence bounds for temporal difference learning with eligibility traces, known as TD(λ).
This is known to often outperform TD(0) [Sutton and Barto, 1998], but a finite time analysis is more
involved. Our analysis also applies without modification to Q-learning for a class of high-dimensional
optimal stopping problems. Such problems have been widely studied due to applications in the pricing
of financial derivatives [Tsitsiklis and Van Roy, 1999, Andersen and Broadie, 2004, Haugh and Kogan,
2004, Desai et al., 2012, Goldberg and Chen, 2018]. For our purposes, this example illustrates more
clearly the link between value prediction and decision-making. It also shows our techniques extend
seamlessly to analyzing an instance of non-linear stochastic approximation. To our knowledge, no
prior work has provided non-asymptotic guarantees for either TD(λ) or Q-learning with function approximation.


1.2 Related Literature


Non-asymptotic analysis of TD(0): There has been very little non-asymptotic analysis of TD(0). To our
knowledge, Korda and La [2015] provided the first finite time analysis. However, several serious errors in
their proofs were pointed out by Lakshminarayanan and Szepesv´ari [2017]. A very recent work by Dalal et al.

[2018a] studies TD(0) with linear function approximation in an i.i.d.observation model, which assumes sequential observations used by the algorithm are drawn independently from their steady-state distribution.
They focus on analysis with problem independent step-sizes of the form 1/T [σ] for a fixed σ ∈ (0, 1) and
establish that mean-squared error convergences at a rate [2] of O(1/T [σ] ). Unfortunately, while the analysis is
technically non-asymptotic, the constant factors in the bound display a complex dependence on the problem
instance and even scale exponentially with the eigenvalues of certain matrices. Dalal et al. [2018a] also give
a high-probability bound, a nice feature that we do not address in this work.


1 This was previously attempted by Korda and La [2015], but critical errors were shown by Lakshminarayanan and Szepesv´ari [2017].
2 In personal communication, the authors have told us their analysis also yields a O(1/T ) rate of convergence for problem dependent
step-sizes, though we have not been able to easily verify this.


2




This paper was accepted at the 2018 Conference on Learning Theory (COLT) and published in the
proceedings as a two-page extended abstract. While the paper was under review, an interesting paper by
Lakshminarayanan and Szepesv´ari [2018] appeared. They study linear stochastic approximation algorithms
under i.i.d noise, including TD(0), with constant step-sizes and iterate averaging. This line of work dates
back to Gy¨orfi and Walk [1996], who show that the iterates of a constant step-size linear stochastic approximation algorithm form an ergodic Markov chain and, in the case of i.i.d.observation noise, their expectation
in steady-state is equal to the true solution of the linear system. By a central limit theorem for ergodic sequences, the average iterate converges to the true solution, with mean-squared error decaying at rate O(1/T ).
Bach and Moulines [2013] give a sophisticated non-asymptotic analysis of the least-mean-squares algorithm
with constant step-size and iterate-averaging. Lakshminarayanan and Szepesv´ari [2018] aim to understand
whether such guarantees extend to linear stochastic approximation algorithms more broadly. In the process,
their work provides O(1/T ) bounds for iterate-averaged TD(0) with constant step-size. A remarkable feature
of their approach is that the choice of step-size is independent of the conditioning of the features (although
the bounds themselves do degrade if features become ill-conditioned). It is worth noting that these results
rely critically on the assumption that noise is i.i.d. In fact, Gy¨orfi and Walk [1996] provide a very simple
example of failure under correlated noise. In this example, under a linear stochastic approximation algorithm
applied with any constant step-size, the averaged-iterate will converge to the wrong limit.
The recent works of Dalal et al. [2018a] and Lakshminarayanan and Szepesv´ari [2018] give bounds for
TD(0) only under i.i.d. observation noise. Therefore their results are most comparable to what is presented in
Section 7. For the i.i.d.noise model, the main argument in favor of our approach is that it allows for extremely
simple proofs, interpretable constant terms, and illuminating connections with SGD. Moreover, it is worth
emphasizing that our approach gracefully extends to more complex settings, including more realistic models
with Markovian noise, the analysis of TD with eligibility traces, and the analysis of Q-learning for optimal
stopping problems as shown in Sections 8, 9 and 10.
While not directly comparable to our results, we point the readers to the excellent work of Schapire and Warmuth

[1996]. To facilitate theoretical analysis, they consider a slightly modified version of the TD(λ). The authors
provide a finite time analysis for this algorithm in an adversarial model where the goal is to predict the discounted sum of future rewards from each state. Performance is measured relative to the best fixed linear
predictor in hindsight. The analysis is creative, but results depend on a several unknown constants and on the
specific sequence of states and rewards on which the algorithm is applied. Schapire and Warmuth [1996] also
apply their techniques to study value function approximation in a Markov decision process. In that case, the
bounds are much weaker than what is established here. Their bound scales with the size of the state space–
which is enormous in most practical problems–and applies only to TD(1)–a somewhat degenerate special
case of TD(λ) in which it is equivalent to Monte Carlo policy evaluation [Sutton and Barto, 1998].


Asymptotic analysis of stochastic approximation: There is a well developed asymptotic theory of stochastic approximation, a field that studies noisy recursive algorithms like TD [Kushner and Yin, 2003, Borkar,
2009, Benveniste et al., 2012]. Most asymptotic convergence proofs in reinforcement learning use a technique known as the ODE method [Borkar and Meyn, 2000]. Under some technical conditions and appropriate
decaying step-sizes, this method ensures the almost-sure convergence of stochastic approximation algorithms
to the invariant set of a certain ‘mean’ differential equation. The technique greatly simplifies asymptotic convergence arguments, since it completely circumvents issues with noise in the system and issues of step-size
selection. But this also makes it a somewhat coarse tool, unable to generate insight into an algorithm’s sensitivity to noise, ill-conditioning, or step-size choices. A more refined set of techniques begin to address
these issues. Under fairly broad conditions, a central limit theorem for stochastic approximation algorithms
characterizes their limiting variance. Such a central limit theorem has been specifically provided for TD by
Konda [2002] and Devraj and Meyn [2017].
Despite the availability of such asymptotic techniques, the modern literature on first-order stochastic
optimization focuses heavily on non-asymptotic analysis [Bottou et al., 2018, Bubeck, 2015, Jain and Kar,
2017]. One reason is that such asymptotic analysis necessarily focuses on a regime where step-sizes are
arbitrarily small relative to problem features and the iterates have already converged to a small neighborhood


3




of the optimum. However, the use of a first-order method in the first place signals that a practitioner is mostly
interesting in cheaply reaching a reasonably accurate solution, rather than the rate of convergence in the
neighborhood of the optimum. In practice, it is common to use constant step-sizes, so iterates never truly
converge to the optimum. A non-asymptotic analysis requires grappling with the algorithm’s behavior in
practically relevant regimes where step-sizes are still relatively large and iterates are not yet close to the true
solution.


Analysis of related algorithms: A number of papers analyze algorithms related to and inspired by the
classic TD algorithm. First, among others, Antos et al. [2008], Lazaric et al. [2010], Ghavamzadeh et al.

[2010], Pires and Szepesv´ari [2012], Prashanth et al. [2013] and Tu and Recht [2018] analyze least-squares
temporal difference learning (LSTD). Yu and Bertsekas [2009] study the related least-squares policy iteration
algorithm. The asymptotic limit point of TD is a minimizer of a certain population loss, known as the
mean-squared projected Bellman error. LSTD solves a least-squares problem, essentially computing the
exact minimizer of this loss on the empirical data. It is easy to derive a central limit theorem for LSTD.
Finite time bounds follow from establishing uniform convergence rates of the empirical loss to the population
loss. Unfortunately, such techniques appear to be quite distinct from those needed to understand the online
TD algorithms studied in this paper. Online TD has seen much wider use due to significant computational
advantages [Sutton and Barto, 1998].
Gradient TD methods are another related class of algorithms. These were derived by Sutton et al. [2009b,a]
to address the issue that TD can diverge in so-called off-policy settings, where data is collected from a policy
different from the one for which we want to estimate the value function. Unlike the classic TD(0) algorithm,
gradient TD methods are designed to mimic gradient descent with respect to the mean squared projected
Bellman error. Sutton et al. [2009b,a] propose asymptotically convergent two-time scale stochastic approximation schemes based on this and more recently Dalal et al. [2018b] give a finite time analysis of two time
scale stochastic approximation algorithms, including several variants of gradient TD algorithms. A creative
paper by Liu et al. [2015] reformulates the original optimization as a primal-dual saddle point problem and
leverages convergence analysis form that literature to give a non-asymptotic analysis. This work was later
revisited by Touati et al. [2018], who established a faster rate of convergence. The works of Dalal et al.

[2018b], Liu et al. [2015] and Touati et al. [2018] all consider only i.i.d. observation noise. One interesting
open question is whether our techniques for treating the Markovian observation model will also apply to these
analyses. Finally, it is worth highlighting that, to the best of our knowledge, substantial new techniques are
needed to analyze the widely used TD(0), TD(λ) and the Q-learning algorithm studied in this paper. Unlike
gradient TD methods, they do not mimic noisy gradient steps with respect to any fixed objective [3] .

## 2 Problem formulation


Markov reward process. We consider the problem of evaluating the value function V µ of a given policy µ
in a Markov decision process (MDP). We work in the on policy setting, where data is generated by applying
the policy µ in the MDP. Because the policy µ is applied automatically to select actions, such problems
are most naturally formulated as value function estimation in a Markov reward process (MRP). A MRP [4]

comprises of (S, P, R, γ) [Sutton and Barto, 1998] where S is the set of states, P is the Markovian transition
kernel, R is a reward function, and γ < 1 is the discount factor. For a discrete state-space S, P(s [′] |s) specifies
the probability of transitioning from a state s to another state s [′] . The reward function R(s, s [′] ) associates a
reward with each state transition. We denote by R(s) = [�] s [′] ∈S [P][(][s] [′] [|][s][)][R][(][s, s] [′] [)][ the expected instantaneous]
reward generated from an initial state s.


3 This can be formally verified for TD(0) with linear function approximation. If the TD step were a gradient with respect to a fixed
objective, differentiating it should give the Hessian and hence a symmetric matrix. Instead, the matrix one attains is typically not a
symmetric one.
4 We avoid µ from notation for simplicity.


4




The value function associated with this MRP, V µ, specifies the expected cumulative discounted future
reward as a function of the state of the system. In particular,



�



V µ (s) = E



∞
� γ [t] R(s t ) | s 0 = s
� t=0



,



where the expectation is over sequences of states generated according to the transition kernel P. This value
function obeys the Bellman equation T µ V µ = V µ, where the Bellman operator T µ associates a value function
V : S → R with another value function T µ V satisfying


(T µ V )(s) = R(s) + γ � P(s [′] |s)V (s [′] ) ∀ s ∈S.

s [′] ∈S


We assume rewards are bounded uniformly such that


|R(s, s [′] )| ≤ r max ∀ s, s [′] ∈S.


Under this assumption, value functions are assured to exist and are the unique solution to Bellman’s equation

[Bertsekas, 2012]. We also assume that the Markov reward process induced by following the policy µ is
ergodic with a unique stationary distribution π. For any two states s, s [′] : π(s [′] ) = lim t→∞ P(s t = s [′] |s 0 = s).
Following common references [Bertsekas, 2012, Dann et al., 2014, De Farias and Van Roy, 2003], we
will simplify the presentation by assuming the state space S is a finite set of size n = |S|. We elaborate on
this choice in the remark below.


Remark 1. Working with a finite state space allows for the use of compact matrix notation, which is the
convention in work on linear value function approximation. It also avoids measure theoretic notation for
conditional probability distributions. Our proofs extend in an obvious way to problems with countably infinite
state-spaces. For problems with general state-space, even the core results in dynamic programming hold only
under suitable technical conditions [Bertsekas and Shreve, 1978].


Value function approximation. Given a fixed policy µ, the problem is to efficiently estimate the corresponding value function V µ using only the observed rewards and state transitions. Unfortunately, due to
the curse of dimensionality, most modern applications have intractably large state spaces, rendering exact
value function learning hopeless. Instead, researchers resort to parametric approximations of the value function, for example by using a linear function approximator [Sutton and Barto, 1998] or a non-linear function
approximation such as a neural network [Mnih et al., 2015]. In this work, we consider a linear function
approximation architecture where the true value-to-go V µ (s) is approximated as


V µ (s) ≈ V θ (s) = φ(s) [⊤] θ,


where φ(s) ∈ R [d] is a fixed feature vector for state s and θ ∈ R [d] is a parameter vector that is shared across
states. When the state space is the finite set S = {s 1, . . ., s n }, V θ ∈ R [n] can be expressed compactly as





 θ =













V θ =



φ(s 1 ) [⊤]

...

φ(s n ) [⊤]



φ 1 (s 1 ) φ k (s 1 ) φ d (s 1 )

... ... ...
φ 1 (s n ) φ k (s n ) φ d (s n )



 θ = Φθ,



where Φ ∈ R [n][×][d] and θ ∈ R [d] . We assume throughout that the d features vectors {φ k } [d] k=1 [, forming the]
columns of Φ are linearly independent.


5




Norms in value function and parameter space. For a symmetric positive definite matrix A, define the
inner product ⟨x, y⟩ A = x [⊤] Ay and the associated norm ∥x∥ A = √x [⊤] Ax. If A is positive semi-definite

rather than positive definite then ∥·∥ A is called a semi-norm. Let D = diag(π(s 1 ), . . ., π(s n )) ∈ R [n][×][n]

denote the diagonal matrix whose elements are given by the entries of the stationary distribution π(·). Then,
for two value functions V and V [′],


∥V − V [′] ∥ D = π(s) (V (s) − V [′] (s)) [2],


s∈S

��


measures the mean-square difference between the value predictions under V and V [′], in steady-state. This
suggests a natural norm on the space of parameter vectors. In particular, for any θ, θ [′] ∈ R [d],


∥V θ − V θ ′ ∥ D = π(s) (φ(s) [⊤] (θ − θ [′] )) [2] = ∥θ − θ [′] ∥ Σ


s∈S

��


where
Σ := Φ [⊤] DΦ = � π(s)φ(s)φ(s) [⊤]

s∈S


is the steady-state feature covariance matrix.


Feature regularity. We assume that any entirely redundant or irrelevant features have been removed, so Σ
has full rank. Additionally, we also assume that ∥φ(s)∥ 2 [2] [≤] [1][ for all][ s][ ∈S][, which can be ensured through]
feature normalization. This also ensures that Σ exists. Let ω > 0 be the minimum eigenvalue of Σ. From
our bound on the feature vectors, the maximum eigenvalue of Σ is less than 1 [5], so 1/ω bounds the condition number of the feature covariance matrix. The following lemma is an immediate consequence of our
assumptions.


Lemma 1 (Norm equivalence). For all θ ∈ R [d], [√] ω∥θ∥ 2 ≤∥V θ ∥ D ≤∥θ∥ 2 .


While we assume it has full rank, we will establish some finite time bounds that are independent of the
conditioning of the feature covariance matrix.

## 3 Temporal difference learning


We consider the classic temporal difference learning algorithm [Sutton, 1988]. The algorithm starts with an
initial parameter estimate θ 0 and at every time step t, it observes one data tuple O t = (s t, r t = R(s t, s [′] t [)][, s] [′] t [)]
consisting of the current state, the current reward and the next state reached by playing policy µ in the current
state. This tuple is used to define a loss function, which is taken to be the squared sample Bellman error.
It then proceeds to compute the next iterate θ t+1 by taking a gradient step. Some of our bounds guarantee
accuracy of the average iterate, denoted by θ [¯] t = t [−][1] [ �] [t] i=0 [−][1] [θ] [i] [. The version of TD presented in Algorithm][ 1]
also makes online updates to the averaged iterate.
We present in Algorithm 1 the simplest variant of TD, which is known as TD(0). It is also worth
highlighting that here we study online temporal difference learning, which makes incremental gradient-like
updates to the parameter estimate based on the most recent data observations only. Such algorithms are
widely used in practice, but harder to analyze than so-called batch TD methods like the LSTD algorithm of
Bradtke and Barto [1996].
At time t, TD takes a step in the direction of the negative gradient g t (θ t ) evaluated at the current parameter.
As a general function of θ and the tuple O t = (s t, r t, s [′] t [)][, the negative gradient can be written as]


g t (θ) = r t + γφ(s [′] t [)] [⊤] [θ][ −] [φ][(][s] [t] [)] [⊤] [θ] φ(s t ). (1)
� �


5 Let λ max (A) = max ∥x∥ 2 =1 x ⊤ Ax denote the maximum eigenvalue of a symmetric positive-semidefinite matrix. Since this is a
convex function, λ max (Σ) ≤ [�] s∈S [π][(][s][)][λ] [max] [(][φ][(][s][)][φ][(][s][)] [⊤] [)][ ≤] [�] s∈S [π][(][s][) = 1][.]


6




Algorithm 1: TD(0) with linear function approximation

Input : initial guess θ 0, step-size sequence {α t } t∈N .
Initialize: θ [¯] 0 ← θ 0 .
for t = 0, 1, . . . do



Observe tuple: O t = (s t, r t = R(s t, s [′] t [)][, s] [′] t [)]
Define target: y t = R(s t, s [′] t [) +][ γV] [θ] t [(][s] [′] t [)] /* sample Bellman operator */
Define loss function: [1] [(][y] [t] [ −] [V] [θ] [(][s] [t] [))] [2] /* sample Bellman error squared */



Define loss function: 2 [1] [(][y] [t] [ −] [V] [θ] [(][s] [t] [))] [2] /* sample Bellman error squared */

1

Compute negative gradient: g t (θ t ) = − ∂θ [∂] 2 [(][y] [t] [ −] [V] [θ] [(][s] [t] [))] [2] [|] θ=θ



1

Compute negative gradient: g t (θ t ) = − ∂θ [∂] 2 [(][y] [t] [ −] [V] [θ] [(][s] [t] [))] [2] [|] θ=θ t

Take a gradient step: θ t+1 = θ t + α t g t (θ t ) /* α t :step-size */



Update averaged iterate: θ [¯] t+1 ← � t+1t � θ¯ t + � t+11 � θ t+1 /* θ [¯] t+1 = t+11 � tℓ=0 [θ] [ℓ] [*/]

end


The long-run dynamics of TD are closely linked to the expected negative gradient step when the tuple O t =
(s t, r t, s [′] t [)][ follows its][ steady-state][ behavior:]


¯
g(θ) := � π(s)P(s [′] |s) �R(s, s [′] ) + γφ(s [′] ) [⊤] θ − φ(s) [⊤] θ� φ(s) ∀ θ ∈ R [d] .

s,s [′] ∈S


This can be rewritten more compactly in several useful ways. One such way is,


¯
g(θ) = E [φr] + E �φ(γφ [′] − φ) [⊤] [�] θ, (2)


where φ = φ(s) is the feature vector of a random initial state s ∼ π, φ [′] = φ(s [′] ) is the feature vector of a random next state drawn according to s [′] ∼P(· | s), and r = R(s, s [′] ). In addition, since
� s [′] ∈S [P][(][s] [′] [|][s][)] �R(s, s [′] ) + γφ(s [′] ) [⊤] θ� = (T µ Φθ)(s), we can recognize that


g¯(θ) = Φ [⊤] D(T µ Φθ − Φθ). (3)


See Tsitsiklis and Van Roy [1997] for a derivation of this fact.

## 4 Asymptotic convergence of temporal difference learning


The main challenge in analyzing TD is that the gradient steps g t (θ) are not true stochastic gradients with respect to any fixed objective. The gradient step taken at time t pulls the value prediction V θ t+1 (s t ) closer to y t,
but y t itself depends on V θ t . So does this circular process converge? The key insight of Tsitsiklis and Van Roy

[1997] was to interpret this as a stochastic approximation scheme for solving a fixed point equation known as
the projected Bellman equation. Contraction properties together with general results from stochastic approximation theory can then be used to show convergence.
Should TD converge at all, it should be to a stationary point. Because the feature covariance matrix Σ is
full rank there is a unique [6] vector θ [∗] with ¯g(θ [∗] ) = 0. We briefly review results that offer insight into θ [∗] and
proofs of the asymptotic convergence of TD.


Understanding the TD limit point. Tsitsiklis and Van Roy [1997] give an interesting characterization of
the limit point θ [∗] . They show it is the unique solution to the projected Bellman equation


Φθ = Π D T µ Φθ, (4)


6 This follows formally as a consequence of Lemma 3 in this paper.


7




where Π D (·) is the projection operator onto the subspace {Φx | x ∈ R [d] } spanned by these features in the
inner product ⟨·, ·⟩ D . To see why this is the case, note that by using ¯g(θ [∗] ) = 0 along with Equation (3),


¯
0 = x [⊤] g(θ [∗] ) = ⟨Φx, T µ Φθ [∗] − Φθ [∗] ⟩ D ∀ x ∈ R [d] .


That is, the Bellman error at θ [∗], given by (T µ Φθ [∗] − Φθ [∗] ), is orthogonal to the space spanned by the features
in the inner product ⟨·, ·⟩ D . By definition, this means Π D (T µ Φθ [∗] − Φθ [∗] ) = 0 and hence θ [∗] must satisfy the
projected Bellman equation.
The following lemma shows the projected Bellman operator, Π D T µ (·) is a contraction, and so in principle,
one could converge to the approximate value function Φθ [∗] by repeatedly applying it. TD appears to serve a
simple stochastic approximation scheme for solving the projected-Bellman fixed point equation.


Lemma 2. [Tsitsiklis and Van Roy [1997]] Π D T µ (·) is a contraction with respect to ∥·∥ D with modulus γ,
that is,


∥Π D T µ V θ − Π D T µ V θ ′ ∥ D ≤ γ∥V θ − V θ ′ ∥ D ∀ θ, θ [′] ∈ R [d] .


Finally, the limit of convergence comes with some competitive guarantees. From Lemma 2, a short argument
shows
1
∥V θ ∗ − V µ ∥ D ≤ (5)
�1 − γ [2] [ ∥][Π] [D] [V] [µ] [ −] [V] [µ] [∥] [D] [.]


See Chapter 6 of Bertsekas [2012] for a proof. The left hand side of Equation (5) measures the root-meansquared deviation between the value predictions of the limiting TD value function and the true value function.
On the right hand side, the projected value function Π D V µ minimizes root-mean-squared prediction errors
among all value functions in the span of Φ. If V µ actually falls within the span of the features, there is no
approximation error at all and TD converges to the true value function.


Asymptotic convergence via the ODE method. Like many analyses in reinforcement learning, the convergence proof of Tsitsiklis and Van Roy [1997] appeals to a powerful technique from the stochastic approximation literature known as the “ODE method”. Under appropriate conditions, and assuming a decaying
step-size sequence satisfying the Robbins-Monro conditions, this method establishes the asymptotic convergence of the stochastic recursion θ t+1 = θ t + α t g t (θ t ) as a consequence of the global asymptotic stability
of the deterministic ODE: θ [˙] t = ¯g(θ t ). The critical step in the proof of Tsitsiklis and Van Roy [1997] is to
use the contraction properties of the Bellman operator to establish this ODE is globally asymptotically stable
with the equilibrium point θ [∗] .
The ODE method vastly simplifies convergence proofs. First, because the continuous dynamics can be
easier to analyze than discretized ones, and more importantly, because it avoids dealing with stochastic noise
in the problem. At the same time, by side-stepping these issues, the method offers little insight into the critical
effect of step-size sequences, problem conditioning, and mixing time issues on algorithm performance.

## 5 Outline of analysis


The remainder of the paper focuses on a finite time analysis of TD. Broadly, we establish two types of finite
time bounds. We first derive bounds that depend on the condition number of the feature covariance matrix. In
that case, we state explicit bounds on the expected distance E �∥θ T − θ [∗] ∥ 2 [2] � of the iterate from the TD fixedpoint, θ [∗] . These mirror what one might expect from the literature on stochastic optimization of strongly
convex functions: results showing that TD with constant step-sizes converges to within a radius of θ [∗] at an
exponential rate, and O(1/T ) convergence rates with appropriate decaying step-sizes. Note that by Lemma
1, ∥V θ T − V θ ∗ ∥ [2] D [≤∥][θ] [T] [ −] [θ] [∗] [∥] 2 [2] [, so bounds on the distance of the iterate to the TD fixed point also imply]
bounds on the distance between value predictions.
These results establish fast rates of convergence, but only if the problem is well conditioned. The
choice of step-sizes is also very sensitive to problem conditioning. Work on robust stochastic approximation


8




[Nemirovski et al., 2009] argues instead for the use of comparatively large step-sizes together with iterate
averaging. Following the spirit of this work, we also give explicit bounds on E �∥V θ¯ T − V θ ∗ ∥ [2] D �, which measures the mean-squared gap between the predictions under the averaged-iterate θ [¯] T and under the TD limit
point θ [∗] . These yield slower O(1/√T ) convergence rates, but both the bounds and step-sizes are completely

independent of problem conditioning.
Our approach is to start by developing insights from simple, stylized settings, and then incrementally
extend the analysis to more complex settings. The analysis is outlined below.


Noiseless case: Drawing inspiration from the ODE method discussed above, we start by analyzing the Euler

¯
discretization of the ODE θ [˙] t = ¯g(θ t ), which is the deterministic recursion θ t+1 = θ t + αg(θ t ). We
call this method “mean-path TD”. As motivation, the section first considers a fictitious gradient descent
algorithm designed to converge to the TD fixed point. We then develop striking analogues for meanpath TD of the key properties underlying the convergence of gradient descent. Easy proofs then yield
two bounds mirroring those given for gradient descent.


Independent noise: Section 7 studies TD under an i.i.d. observation model, where the data-tuples used by
TD are drawn i.i.d. from the stationary distribution. The techniques used to analyze mean-path TD(0)
extend easily to this setting, and the resulting bounds mirror standard guarantees for stochastic gradient
descent.


Markov noise: In Section 8, we analyze TD in the more realistic setting where the data is collected from a
single sample path of an ergodic Markov chain. This setting introduces significant challenges due to
the highly dependent nature of the data. For tractability, we assume the Markov chain satisfies a certain
uniform bound on the rate at which it mixes, and study a variant of TD that uses a projection step to
ensure uniform boundedness of the iterates. In this case, our results essentially scale by a factor of the
mixing time relative to the i.i.d. case.


Extension to TD(λ): In Section 9, we extend the analysis under the Markov noise to TD with eligibility
traces, popularly known as TD(λ). Eligibility traces are known to often provide performance gains in
practice, but theoretical analysis is more complex. Such analysis offers some insight into the subtle
tradeoffs in the selection of the parameter λ ∈ [0, 1].


Approximate optimal stopping: A final section extends our results to a class of high dimensional optimal
stopping problems. We analyze Q-learning with linear function approximation. Building on observations of Tsitsiklis and Van Roy [1999], we show the key properties used in our analysis of TD continue
to hold for Q-learning in this setting. The convergence bounds shown in Sections 7 and 8 therefore
apply without any modification.

## 6 Analysis of mean-path TD


All practical applications of TD involve observation noise. However, a great deal of insight can be gained by
investigating a natural deterministic analogue of the algorithm. Here we study the recursion


θ t+1 = θ t + αg¯(θ t ) t ∈ N 0 = {0, 1, 2, . . .},


which is the Euler discretization of the ODE described in Section 4. We will refer to this iterative algorithm
as mean-path TD. In this section, we develop key insights into the dynamics of mean-path TD that allow for
a remarkably simple finite time analysis of its convergence. Later sections of the paper show how these ideas
extend gracefully to analyses with observation noise.
The key to our approach is to develop properties of mean-path TD that closely mirror those of gradient
descent on a particular quadratic loss function. To this end, in the next subsection, we review a simple analysis
of gradient descent. In Subsection 6.2, we establish key properties of mean-path TD mirroring those used to
analyze this gradient descent algorithm. Finally, Subsection 6.3 gives convergence rates of mean-path TD,


9




with proofs and rates mirroring those given for gradient descent except for a constant that depends on the
discount factor, γ.


6.1 Gradient descent on a value function loss



Consider the cost function
f (θ) = [1]



2 [∥][θ] [∗] [−] [θ][∥] Σ [2] [,]




[1] D [= 1]

2 [∥][V] [θ] [∗] [−] [V] [θ] [∥] [2] 2



which measures the mean-squared gap between the value predictions under θ and those under the stationary
point of TD, θ [∗] . Consider as well a hypothetical algorithm that performs gradient descent on f, iterating
θ t+1 = θ t − α∇f (θ t ) for all t ∈ N 0 . Of course, this algorithm is not implementable, as one does not know
the limit point θ [∗] of TD. However, reviewing an analysis of such an algorithm will offer great insights into
our eventual analysis of TD.
To start, a standard decomposition characterizes the evolution of the error at iterate θ t :


∥θ [∗] − θ t+1 ∥ 2 [2] [=][ ∥][θ] [∗] [−] [θ] [t] [∥] 2 [2] [+2][α][∇][f] [(][θ] [t] [)] [⊤] [(][θ] [∗] [−] [θ] [t] [) +][ α] [2] [∥][∇][f] [(][θ] [t] [)][∥] 2 [2] [.]


To use this decomposition, we need two things. First, some understanding of ∇f (θ t ) [⊤] (θ [∗] − θ t ), capturing
whether the gradient points in the direction of (θ [∗] − θ t ). And second, we need an upper bound on the norm
of the gradient ∥∇f (θ t )∥ 2 [2] [. In this case,][ ∇][f] [(][θ][) = Σ(][θ][ −] [θ] [∗] [)][, from which we conclude]


∇f (θ) [⊤] (θ [∗] − θ) = −∥θ [∗] − θ∥ Σ [2] [=][ −∥][V] [θ] [∗] [−] [V] [θ] [∥] [2] D [.] (6)


In addition, one can show [7]

∥∇f (θ)∥ 2 ≤∥V θ ∗ − V θ ∥ D . (7)


Now, using (6) and (7), we have that for step-size α = 1,


∥θ [∗] − θ t+1 ∥ 2 [2] [≤∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 [−∥][V] [θ] [∗] [−] [V] [θ] t [∥] D [2] [.] (8)


The distance to θ [∗] decreases in every step, and does so more rapidly if there is a large gap between the value
predictions under θ and θ [∗] . Combining this with Lemma 1 gives


∥θ [∗] − θ t+1 ∥ [2] 2 [≤] [(1][ −] [ω][)][∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 [≤] [. . .][ ≤] [(1][ −] [ω][)] [t][+1] [∥][θ] [∗] [−] [θ] [0] [∥] [2] 2 [.] (9)


Recall that ω denotes the minimum eigenvalue of Σ. This shows that error converges at a fast geometric rate.
However the rate of convergence degrades if the minimum eigenvalue ω is close to zero. Such a convergence
rate is therefore only meaningful if the feature covariance matrix is well conditioned.
By working in the space of value functions and performing iterate averaging, one can also give a guarantee
that is independent of ω. Recall the notation θ [¯] T = T [−][1] [ �] [T] t=0 [ −][1] [θ] [t] [ for the averaged iterate. A simple proof]
from (8) shows



∥V θ ∗ − V θ¯ T ∥ D [2] [≤] [1]

T



T −1
�∥V θ ∗ − V θ t ∥ D [2] [≤∥][θ] [∗] [−] T [θ] [0] [∥] 2 [2] . (10)

t=0



6.2 Key properties of mean-path TD


This subsection establishes analogues for mean-path TD of the key properties (6) and (7) used to analyze gradient descent. First, to characterize the gradient update, our analysis builds on Lemma 7 of Tsitsiklis and Van Roy

[1997], which uses the contraction properties of the projected Bellman operator to conclude


¯
g(θ) [⊤] (θ [∗] − θ) > 0 ∀ θ ̸= θ [∗] . (11)


7 This can be seen from the fact that for any vector u with ∥u∥ 2 ≤ 1,

u [⊤] ∇f (θ) = ⟨u, θ − θ [∗] ⟩ Σ ≤∥u∥ Σ ∥θ [∗] − θ∥ Σ ≤∥θ [∗] − θ∥ Σ = ∥V θ ∗ − V θ ∥ D .


10




That is, the expected update of TD always forms a positive angle with (θ [∗] − θ). Though only Equation (11)
was stated in their lemma, Tsitsiklis and Van Roy [1997] actually reach a much stronger conclusion in their
proof itself. This result, given in Lemma 3 below, establishes that the expected updates of TD point in a
descent direction of ∥θ [∗] − θ∥ [2] 2 [, and do so more strongly when the gap between value functions under][ θ][ and]
θ [∗] is large. We will show that this more quantitative form of (11) allows for elegant finite time-bounds on the
performance of TD.
Note that this lemma mirrors the property in Equation (6), but with a smaller constant of (1 − γ). This
reflects that expected TD must converge to θ [∗] by bootstrapping [Sutton, 1988] and may follow a less direct
path to θ [∗] than the fictitious gradient descent method considered in the previous subsection. Recall that the
limit point θ [∗] solves ¯g(θ [∗] ) = 0.


Lemma 3. For any θ ∈ R [d],

¯
(θ [∗] − θ) [⊤] g(θ) ≥ (1 − γ)∥V θ ∗ − V θ ∥ [2] D [.]


Proof. We use the notation described in Equation (2) of Section 3. Consider a stationary sequence of states
with random initial state s ∼ π and subsequent state s [′], which, conditioned on s, is drawn from P(·|s).
Set φ = φ(s), φ [′] = φ(s [′] ) and r = R(s, s [′] ). Define ξ = V θ ∗ (s) − V θ (s) = (θ [∗] − θ) [⊤] φ and ξ [′] =
V θ [∗] (s [′] ) − V θ (s [′] ) = (θ [∗] − θ) [⊤] φ [′] . By stationarity, ξ and ξ [′] are two correlated random variables with the same
same marginal distribution. By definition, π, E[ξ [2] ] = ∥V θ ∗ − V θ ∥ [2] D [since][ s][ is drawn from][ π][.]
Using the expression for ¯g(θ) in Equation (2),


¯ ¯
g(θ) = ¯g(θ) − g(θ [∗] ) = E[φ(γφ [′] − φ) [⊤] (θ − θ [∗] )] = E[φ(ξ − γξ [′] )]. (12)


Therefore


¯
(θ [∗] − θ) [⊤] g(θ) = E [ξ(ξ − γξ [′] )] = E[ξ [2] ] − γE[ξ [′] ξ] ≥ (1 − γ)E[ξ [2] ] = (1 − γ)∥V θ [∗] − V θ ∥ [2] D [.]


The inequality above uses Cauchy-Schwartz inequality together with the fact that ξ and ξ [′] have the same
marginal distribution to conclude E[ξξ [′] ] ≤ �E[ξ [2] ]�E[(ξ [′] ) [2] ] = E[ξ [2] ].


Lemma 4 is the other key ingredient to our results. It upper bounds the norm of the expected negative
gradient, providing an analogue of Equation (7).


¯
Lemma 4. ∥g(θ)∥ 2 ≤ 2∥V θ − V θ ∗ ∥ D ∀ θ ∈ R [d] .


Proof. Beginning from (12) in the Proof of Lemma 3, we have



E[ξ [2] ]�



E[(ξ [′] ) [2] ] = E[ξ [2] ].



∥g¯(θ)∥ 2 = ∥E[φ(ξ − γξ [′] )]∥ 2 ≤ �



E [(ξ − γξ [′] ) [2] ] ≤ �



E[(ξ [′] ) [2] ] = (1 + γ)�



E [∥φ∥ [2] 2 []] �



E[ξ [2] ] + γ�



E[ξ [2] ],



where the second inequality uses the assumption that ∥φ∥ 2 ≤ 1 and the final equality uses that ξ and ξ [′] have
the same marginal distribution. We conclude by recalling that E[ξ [2] ] = ∥V θ ∗ − V θ ∥ [2] D [and][ 1 +][ γ][ ≤] [2][.]


Lemmas 3 and 4 are quite powerful when used in conjunction. As in the analysis of gradient descent
reviewed in the previous subsection, our analysis starts with a recursion for the error term, ∥θ t − θ [∗] ∥ [2] . See
Equation (13) in Theorem 1 below. Lemma 3 shows the first order term in this recursion reduces the error at
each time step, while using the two lemmas in conjunction shows the first order term dominates a constant
times the second order term. Precisely,


¯ ¯
g(θ) [⊤] (θ [∗] − θ) ≥ (1 − γ)∥V θ ∗ − V θ ∥ [2] D [≥] [(1][ −] [γ][)] ∥g(θ)∥ 2 [2] [.]

4


This leads immediately to conclusions like Equation (14), from which finite time convergence bounds follow.
It is also worth pointing out that as TD(0) is an instance of linear stochastic approximation, these two
lemmas can be interpreted as statements about the eigenvalues of the matrix driving its behavior [8] .

3 shows that 8 Recall from Section A ⪯−(1 − 3γ that)Σ, i.e. that ¯g(θ) is an affine function. That is, it can be written as A +(1 − γ)Σ is negative definite. It is easy to show that Aθ − b for some ∥g¯(θ)∥ A [2] 2 [= (] ∈ R [θ][ −] d× [θ] d [∗] and [)] [⊤] [(] b [A] ∈ [⊤] [A] R d [)(] . Lemma [θ][ −] [θ] [∗] [)][,]
so Lemma 4 shows that A [⊤] A ⪯ Σ. Taking this perspective, the important part of these lemmas is that they allows us to understand TD
in terms of feature covariance matrix Σ and the discount factor γ rather than the more mysterious matrix A.


11




6.3 Finite time analysis of mean-path TD


We now combine the insights of the previous subsection to establish convergence rates for mean-path TD.
These mirror the bounds for gradient descent given in Equations (9) and (10), except for an additional dependence on the discount factor. The first result bounds the distance between the value function under an
averaged iterate and under the TD stationary point. This gives a comparatively slow O(1/T ) convergence
rate, but does not depend at all on the conditioning of the feature covariance matrix. When this matrix is well
conditioned, so the minimum eigenvalue ω of Σ is not too small, the geometric convergence rate given in the
second part of the theorem dominates. Note that by Lemma 1, bounds on ∥θ t − θ [∗] ∥ 2 always imply bounds
on ∥V θ t − V θ ∗ ∥ D .


Theorem 1. Consider a sequence of parameters (θ 0, θ 1, . . .) obeying the recursion


θ t+1 = θ t + αg¯(θ t ) t ∈ N 0 = {0, 1, 2, . . .},


where α = (1 − γ)/4. Then,

∥V θ ∗ − V θ¯ T ∥ D [2] [≤] [4][∥][θ] [∗] [−] [θ] [0] [∥] 2 [2]
T (1 − γ) [2]


and



2
∥θ [∗] − θ T ∥ [2] 2 [≤] [exp] − (1 − γ) ω
� � 4


Proof. With probability 1, for every t ∈ N 0, we have



T ∥θ [∗] − θ 0 ∥ [2] 2 [.]
� �



∥θ [∗] − θ t+1 ∥ 2 [2] [=][ ∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 [−][2][α][(][θ] [∗] [−] [θ] [t] [)] [⊤] [g][¯][(][θ] [t] [) +][ α] [2] [∥][g][¯][(][θ] [t] [)][∥] [2] 2 [.] (13)


Applying Lemmas 3 and 4 and using a constant step-size of α = (1 − γ)/4, we get


∥θ [∗] − θ t+1 ∥ [2] 2 ≤ ∥θ [∗] − θ t ∥ [2] 2 [−] �2α(1 − γ) − 4α [2] [�] ∥V θ ∗ − V θ t ∥ [2] D



2
= ∥θ [∗] − θ t ∥ [2] 2 [−] � (1 −4 γ) � ∥V θ ∗ − V θ t ∥ [2] D [.] (14)



Then,

2
(1 − γ)
� 4



T −1
�∥V θ ∗ − V θ t ∥ D [2] [≤]
� t=0



T −1
�


t=0



�∥θ [∗] − θ t ∥ 2 [2] [−∥][θ] [∗] [−] [θ] [t][+1] [∥] [2] 2 � ≤∥θ [∗] − θ 0 ∥ 2 [2] [.]



Applying Jensen’s inequality gives the first result:



∥V θ ∗ − V θ¯ T ∥ D [2] [≤] [1]

T



T −1
� t=0 ∥V θ ∗ − V θ t ∥ D [2] [≤] [4] (1 [∥][θ] − [∗] [−] γ) [θ] [2][0] T [∥] 2 [2] [.]



Now, returning to (14), and applying Lemma 1 implies



∥θ [∗] − θ t ∥ 2 [2]
�



2
∥θ [∗] − θ t+1 ∥ 2 [2] [≤∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 [−] (1 − γ)
� 4



ω∥θ [∗] − θ t ∥ 2 [2] = 1 − [ω][(1][ −] [γ][)] [2]
� � 4



− [ω][(1][ −] [γ][)] [2]
≤ exp
� 4



∥θ [∗] − θ t ∥ 2 [2] [,]
�




[−] 4 [γ][)] [2] ≤ e −ω(14−γ) [2]

�



where the final inequality uses that 1 − [ω][(1][−] 4 [γ][)] [2]
�



where the final inequality uses that 1 − [−] 4 ≤ e 4 . Repeating this inductively gives the desired


result.



12




## 7 Analysis for the i.i.d. observation model

This section studies TD under an i.i.d.observation model, and establishes three explicit guarantees that mirror
standard finite time bounds available for SGD. Specifically, we study a model where the random tuples
observed by the TD algorithm are sampled i.i.d. from the stationary distribution of the Markov reward process.
This means that for all states s and s [′],


P [(s t, r t, s [′] t [) = (][s,][ R][(][s, s] [′] [)][, s] [′] [)] =][ π][(][s][)][P][(][s] [′] [|][s][)][,] (15)


and the tuples {(s t, r t, s [′] t [)][}] [t][∈][N] [ are drawn independently across time. Note that the probabilities in Equation]
(15) correspond to a setting where the first state s t is drawn from the stationary distribution, and then s [′] t [is]
drawn from P(·|s t ). This model is widely used for analyzing RL algorithms. See for example Sutton et al.

[2009b], Sutton et al. [2009a], Korda and La [2015], and Dalal et al. [2018a].
Theorem 2 follows from a unified analysis that combines the techniques of the previous section with
typical arguments used in the SGD literature. All bounds depend on σ [2] = E[∥g t (θ [∗] )∥ [2] 2 [] =][ E][[][∥][g] [t] [(][θ] [∗] [)][ −]
g¯(θ [∗] )∥ [2] 2 []][, which roughly captures the variance of TD updates at the stationary point][ θ] [∗] [. The bound in part (a)]
follows the spirit of work on so-called robust stochastic approximation [Nemirovski et al., 2009]. It applies
to TD with iterate averaging and relatively large step-sizes. The result is a simple bound on the mean-squared
gap between the value predictions under the averaged iterate and the TD fixed point. The main strength of
this result is that the step-sizes and the bound do not depend at all on the condition number of the feature
covariance matrix. Note that the requirement that √T ≥ 8/(1 − γ) is not critical; one can carry out analysis

using the step-size α 0 = min{(1 − γ)/8, √T }, but the bounds we attain only become meaningful in the case

where T is sufficiently large, so we chose to simplify the exposition.
Parts (b) and (c) provide faster convergence rates in the case where the feature covariance matrix is well
conditioned. Part (b) studies TD applied with a constant step-size, which is common in practice. In this
case, the iterate θ t will never converge to the TD fixed point, but our results show the expected distance to θ [∗]

converges at an exponential rate below some level that depends on the choice of step-size. This is sometimes
referred to as the rate at which the initial point θ 0 is “forgotten”. Bounds like this justify the common practice
of starting with large step-sizes, and sometimes dividing the step-sizes in half once it appears error is nolonger decreasing. Part (c) attains an O(1/T ) convergence rate for a carefully chosen decaying step-size
sequence. This step-size sequence requires knowledge of the minimum eigenvalue of the feature covariance
matrix Σ, which plays a role similar to a strong convexity parameter in the optimization literature. In practice,
this would need to be estimated, possibly by constructing a sample average approximation to the feature
covariance matrix. The proof of part (c) closely follows an inductive argument presented in Bottou et al.

[2018]. Recall that θ [¯] T = T [−][1] [ �] [T] t=0 [ −][1] [θ] [t] [ denotes the averaged iterate.]


Theorem 2. Suppose TD is applied under the i.i.d. observation model and set σ [2] = E �∥g t (θ [∗] )∥ 2 [2] �.


(a) For any T ≥ (8/(1 − γ)) [2] and a constant step-size sequence α 0 = · · · = α T = √1T [,]


E �∥V θ ∗ − V θ¯ T ∥ D [2] � ≤ [∥][θ] [∗] [−] [θ] [0] [∥] 2 [2] [+2][σ] [2] .
√T (1 − γ)


(b) For any constant step-size sequence α 0 = · · · = α T ≤ ω(1 − γ)/8,



.
�



E �∥θ [∗] − θ T ∥ 2 [2] � ≤ �e [−][α] [0] [(1][−][γ][)][ωT] [�] ∥θ [∗] − θ 0 ∥ 2 [2] [+][ α] [0]



2σ [2]
� (1 − γ)ω



β 2 16
(c) For a decaying step-size sequence α t = λ+t [with][ β][ =] (1−γ)ω [and][ λ][ =] (1−γ) [2] ω [,]


E �∥θ [∗] − θ T ∥ 2 [2] � ≤ ν where ν = max 8σ [2] 2
λ + T � (1 − γ) [2] ω [2] [,][ 16] (1 [∥][θ] − [∗] [−] γ) [θ] [2][0] ω [∥] [2]


13



.
�




Our proof is able to directly leverage Lemma 3, but the analysis requires the following extension of
Lemma 4 which gives an upper bound to the expected norm of the stochastic gradient.

Lemma 5. For any fixed θ ∈ R [d], E �∥g t (θ)∥ [2] 2 � ≤ 2σ [2] + 8∥V θ − V θ ∗ ∥ [2] D [where][ σ] [2] [ =][ E] �∥g t (θ [∗] )∥ [2] 2 �.

Proof. For brevity of notation, set φ = φ(s t ) and φ [′] = φ(s [′] t [)][. Define][ ξ][ = (][θ] [∗] [−] [θ][)] [⊤] [φ][ and][ ξ] [′] [ = (][θ] [∗] [−] [θ][)] [⊤] [φ] [′] [.]
By stationarity, ξ and ξ [′] have the same marginal distribution and E[ξ [2] ] = ∥V θ ∗ − V θ ∥ [2] D [, following the same]
argument as in Lemma 3. Using the formula for g t (θ) in Equation (1), we have

E �∥g t (θ)∥ [2] 2 � ≤ E �(∥g t (θ [∗] )∥ 2 +∥g t (θ) − g t (θ [∗] )∥ 2 ) [2] [�]

≤ 2E �∥g t (θ [∗] )∥ [2] 2 � + 2E �∥g t (θ) − g t (θ [∗] )∥ [2] 2 �



2
= 2σ [2] + 2E φ (φ − γφ ′ ) ⊤ (θ ∗ − θ)
���� ��� 2

= 2σ [2] + 2E ∥φ(ξ − γξ [′] )∥ [2] 2
� �



�



≤ 2σ [2] + 2E �|ξ − γξ [′] | [2] [�]

≤ 2σ [2] + 4 �E �|ξ| [2] [�] + γ [2] E �|ξ [′] | [2] [��]

≤ 2σ [2] + 8∥V θ [∗] − V θ ∥ D [2] [,]


where we used the assumption that ∥φ∥ [2] 2 [≤] [1][. The second inequality uses the basic algebraic identity][ (][x][ +]
y) [2] ≤ 2 max{x, y} [2] ≤ 2x [2] + 2y [2], along with the monotonicity of expectation operators.


Using this we give a proof of Theorem 2 below. Let us remark here on a consequence of the i.i.d noise
model that considerably simplifies the proof. Until now, we have often developed properties of the TD updates
g t (θ) applied to an arbitrary, but fixed, vector θ ∈ R [d] . For example, we have given an expression for ¯g(θ) :=
E[g t (θ)], where this expectation integrates over the random tuple O t = (s t, r t, s [′] t [)][ influencing the TD update.]
In the i.i.d noise model, the current iterate, θ t, is independent of the tuple O t, and so E[g t (θ t )|θ t ] = ¯g(θ t ). In
a similar manner, after conditioning on θ t, we can seamlessly apply Lemmas 3 and 5, as is done in inequality
(16) of the proof below.


Proof. The TD algorithm updates the parameters as: θ t+1 = θ t + α t g t (θ t ). Thus, for each t ∈ N 0, we have,

∥θ [∗] − θ t+1 ∥ [2] 2 [=][ ∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 [−][2][α] [t] [g] [t] [(][θ] [t] [)] [⊤] [(][θ] [∗] [−] [θ] [t] [) +][ α] [2] t [∥][g] [t] [(][θ] [t] [)][∥] 2 [2] [.]


Under the hypotheses of (a), (b) and (c), we have that α t ≤ (1 − γ)/8. Taking expectations and applying
Lemma 3 and Lemma 5 implies,

E �∥θ [∗] − θ t+1 ∥ 2 [2] � = E �∥θ [∗] − θ t ∥ 2 [2] � − 2α t E �g t (θ t ) [⊤] (θ [∗] − θ t )� + α [2] t [E] �∥g t (θ t )∥ 2 [2] �

= E �∥θ [∗] − θ t ∥ [2] 2 � − 2α t E �E �g t (θ t ) [⊤] (θ [∗] − θ t ) | θ t �� + α [2] t [E] �E �∥g t (θ t )∥ [2] 2 [|][ θ] [t] ��

≤ E �∥θ [∗] − θ t ∥ [2] 2 � − �2α t (1 − γ) − 8α [2] t � E �∥V θ ∗ − V θ t ∥ [2] D � + 2α [2] t [σ] [2] (16)

≤ E �∥θ [∗] − θ t ∥ [2] 2 � − α t (1 − γ)E �∥V θ ∗ − V θ t ∥ [2] D � + 2α [2] t [σ] [2] [.] (17)


The inequality (16) follows from Lemmas 3 and 5. The application of these lemmas uses that the random
tuple O t = (s t, r t, s [′] t [)][ influencing][ g] [t] [(][·][)][ is independent of the iterate,][ θ] [t] [.]


Part (a). Consider a constant step-size of α T = · · · = α 0 = 1/√T . Starting with Equation (17) and

summing over t gives



�



√



T ∥θ [∗] − θ 0 ∥ 2 [2] √
+ [2]
(1 − γ) (1



(1 − γ) [.]



E



T −1
�∥V θ ∗ − V θ t ∥ D [2]
� t=0



≤ [∥][θ] [∗] [−] [θ] [0] [∥] 2 [2]
α 0 (1 − γ) [+ 2] (1 [α] − [0] [T σ] γ) [2] [=]



Tσ [2]



We find



�



E �∥V θ ∗ − V θ¯ T ∥ D [2] � ≤ [1]

T [E]



T −1
�∥V θ ∗ − V θ t ∥ [2] D
� t=0



T −1
�
� t=0



2 [+2][σ] [2]
≤ [∥][θ] [∗] [−] [θ] [0] [∥] [2] .
√T (1 − γ)



2 [+2][σ] [2]
≤ [∥][θ] [∗] [−] [θ] [0] [∥] [2]
√T (1 − γ)



14




Part (b). Consider a constant step-size of α 0 ≤ ω(1 − γ)/8. Applying Lemma 1 to Equation (17) implies


E �∥θ [∗] − θ t+1 ∥ 2 [2] � ≤ (1 − α 0 (1 − γ)ω) E �∥θ [∗] − θ t ∥ 2 [2] � + 2α [2] 0 [σ] [2] [.] (18)


Iterating this inequality establishes that for any T ∈ N 0,


∞
E �∥θ [∗] − θ T ∥ 2 [2] � ≤ (1 − α 0 (1 − γ)ω) [T] E �∥θ [∗] − θ 0 ∥ 2 [2] � + 2α [2] 0 [σ] [2] � (1 − α 0 (1 − γ)ω) [t] .


t=0


The result follows by solving the geometric series and using that (1 − α 0 (1 − γ)ω) ≤ e [−][α] [0] [(1][−][γ][)][ω] .


Part (c). Note that by the definitions of ν, λ and β, we have


ν = max{2β [2] σ [2], λ∥θ [∗] − θ 0 ∥ 2 [2] [}][.]


We then have ∥θ [∗] − θ 0 ∥ [2] 2 [≤] λ [ν] [by the definition of][ ν][. Proceeding by induction, suppose][ E] �∥θ [∗] − θ t ∥ [2] 2 � ≤

ν
λ+t [. Then,]


E �∥θ [∗] − θ t+1 ∥ 2 [2] � ≤ (1 − α t (1 − γ)ω) E �∥θ [∗] − θ t ∥ 2 [2] � + 2α [2] t [σ] [2]



≤ 1 − [(1][ −] ˆ [γ][)][ωβ]
� t


ˆ
= � t − (1 −tˆ [2] γ)ωβ



ν + [2][β] ˆ [2] [σ] [2]
� t [2]



ν


ˆ ˆ
t [+ 2][β] t [2][2] [σ] [2]



ν
� t



ˆ [where t [ˆ] ≡ λ + t]
t [2]



ˆ
t − 1
= � tˆ [2]


ˆ
t − 1
= � tˆ [2]


ν
≤ ˆ
t + 1 [,]



ν + [2][β] [2] [σ] [2] [ −] [((1][ −] ˆ [γ][)][ωβ][ −] [1)][ ν]
� t [2]


2

ν + [2][β] [2] [σ] ˆ [2] [ −] [ν] [using β =
� t [2] (1 − γ)ω []]



where the final inequality uses thatˆ 2β [2] σ [2] − ν ≤ 0, which holds by the definition of ν and the fact that
t [2] ≥ (ˆt − 1)(ˆt + 1).

## 8 Analysis for the Markov chain observation model: Projected TD algorithm


In Section 7, we developed a method for analyzing TD under an i.i.d.sampling model in which tuples are
drawn independently from the stationary distribution of the underlying MDP. But a more realistic setting is
one in which the observed tuples used by TD are gathered from a single trajectory of the Markov chain.
In particular, if for a given sample path the Markov chain visits states (s 0, s 1, . . . s t, . . .), then these are
processed into tuples O t = (s t, r t = R(s t, s t+1 ), s t+1 ) that are fed into the TD algorithm. Mathematical
analysis is difficult since the tuples used by the algorithm can be highly correlated with each other. We outline
the main challenges below.


Challenges in the Markov chain noise model. In the i.i.d. observation setting, our analysis relied heavily
on a Martingale property of the noise sequence. This no longer holds in the Markov chain model due to
strong dependencies between the noisy observations. To understand this, recall the expression of the negative
gradient,
g t (θ) = r t + γφ(s t+1 ) [⊤] θ − φ(s t ) [⊤] θ φ(s t ). (19)
� �


15




To make the statistical dependencies more transparent, we can overload notation to write this as g(θ, O t ) ≡
g t (θ), where O t = (s t, r t, s t+1 ). Assuming the sequence of states is stationary, we have defined the function
g¯ : R [d] → R [d] by ¯g(θ) = E[g(θ, O t )], where, since θ is non-random, this expectation integrates over the
marginal distribution of the tuple O t . However, E[g(θ t, O t ) | θ t = θ] ̸= ¯g(θ) because θ t is a function of
past tuples {O 1, . . ., O t−1 }, potentially introducing strong dependencies between θ t and O t . Similarly, in
general E[g(θ t, O t ) − g¯(θ t )] ̸= 0, indicating bias in the algorithm’s gradient evaluation. A related challenge
arises in trying to control the norm of the gradient step, E[∥g t (θ t )∥ [2] 2 []][. Lemma][ 5][ does not yield a bound due]
to coupling between the iterate θ t and the observation O t .
Our analysis uses an information-theoretictechnique to control for this coupling and explicitly account for
the gradient basis. This technique may be of broader use in analyzing reinforcement learning and stochastic
approximation algorithms. However, our analysis also requires some strong regularity conditions, as outlined
below.


Projected TD algorithm. Our technique for controlling the gradient bias relies critically on a condition
that, when step-sizes are small, the iterates (θ t ) t∈N 0 do not change too rapidly. This is the case as long as
norms of the gradient steps do not explode. For tractability, we modify the TD algorithm itself by adding a
projection step that ensures gradient norms are uniformly bounded across time. In particular, starting with an
initial guess of θ 0 such that ∥θ 0 ∥ 2 ≤ R, we consider the Projected TD algorithm, which iterates


θ t+1 = Π 2,R (θ t + α t g t (θ t )) ∀ t ∈ N 0, (20)


where


Π 2,R (θ) = arg min ∥θ − θ [′] ∥ 2
θ [′] :∥θ [′] ∥ 2 ≤R


is the projection operator onto a norm ball of radius R < ∞. The subscript 2 on the operator indicates that the
projection is with respect the unweighted Euclidean norm. This should not be confused with the projection
operator Π D used earlier, which projects onto the subspace of approximate value functions with respect to a
weighted norm.
One may wonder whether this projection step is practical. We note that, from a computational perspective,
it only involves rescaling of the iterates, as Π 2,R (θ) = Rθ/∥θ∥ if ∥θ∥ 2 - R and is simply θ otherwise. In
addition, Subsection 8.2 suggests that by using aprori bounds on the value function, it should be possible to
estimate a projection radius containing the TD fixed point. However, at this stage, we view this mainly as a
tool that enables clean finite time analysis, rather than a practical algorithmic proposal.
It is worth mentioning that projection steps have a long history in the stochastic approximation literature,
and many of the standard analyses for stochastic gradient descent rely on projections steps to control the norm
of the gradient [Kushner, 2010, Lacoste-Julien et al., 2012, Bubeck, 2015, Nemirovski et al., 2009].


Structural assumptions on the Markov reward process. To control the statistical bias in the gradient
updates–which is the main challenge under the Markov observation model–we assume that the Markov chain
mixes at a uniform geometric rate, as stated below.


Assumption 1. There are constants m > 0 and ρ ∈ (0, 1) such that


sup d TV (P(s t ∈·|s 0 = s), π) ≤ mρ [t] ∀ t ∈ N 0,
s∈S


where d TV (P, Q) denotes the total-variation distance between probability measures P and Q. In addition,
the initial distribution of s 0 is the steady-state distribution π, so (s 0, s 1, . . .) is a stationary sequence.


This uniform mixing assumption always holds for irreducible and aperiodic Markov chains [Levin and Peres,
2017]. We emphasize that the assumption that the chain begins in steady-state is not essential: given the uniform mixing assumption, we can always apply our analysis after the Markov chain has approximately reached


16




its steady-state. However, adding this assumption allows us to simplify many mathematical expressions. Another useful quantity for our analysis is the mixing time which we define as


τ [mix] (ǫ) = min{t ∈ N 0 | mρ [t] ≤ ǫ}. (21)


For interpreting the bounds, note that from Assumption 1,


τ [mix] (ǫ) ∼ [log(1][/ǫ][)] as ǫ → 0.

log(1/ρ)


We can therefore evaluate the mixing time at very small thresholds like ǫ = 1/T while only contributing a
logarithmic factor to the bounds.


A bound on the norm of the gradient: Before proceeding, we also state a bound on the euclidean norm of
the gradient under TD(0) that follows from the uniform bound on rewards, along with feature normalization [9]

and boundedness of the iterates through the projection step. Under projected TD(0) with projection radius
R, this lemma implies that ∥g t (θ t )∥ 2 ≤ (r max + 2R). This gradient bound plays an important role in our
convergence bounds.


Lemma 6. For all θ ∈ R [d], ∥g t (θ)∥ 2 ≤ r max + 2∥θ∥ 2 with probability 1.


Proof. Using the expression of g t (θ) in Equation (19), we have


∥g t (θ)∥ 2 ≤|r t + (γφ(s [′] t [)][ −] [φ][(][s] [t] [))] [⊤] [θ][| ∥][φ][(][s] [t] [)][∥] ≤ r max + ∥γφ(s [′] t [)][ −] [φ][(][s] [t] [)][∥] [2] [∥][θ][∥] [2]
≤ r max + 2∥θ∥.


8.1 Finite time bounds


Following Section 7, we state several finite time bounds on the performance of the Projected TD algorithm.
As before, in the spirit of robust stochastic approximation [Nemirovski et al., 2009], the bound in part (a)
gives a comparatively slow convergence rate of O [˜] (1/√T ), but where the bound and step-size sequence are

independent of the conditioning of the feature covariance matrix Σ. The bound in part (c) gives a faster
convergence rate in terms of the number of samples T, but the bound and as well as the step-size sequence
depend on the minimum eigenvalue ω of Σ. Part (b) confirms that for sufficiently small step-sizes, the iterates
converge at an exponential rate to within some radius of the TD fixed-point, θ [∗] .
It is also instructive to compare the bounds for the Markov model vis-a-vis the i.i.d. model. One can see
that in the case of part(b) for the Markov chain setting, a O �G [2] τ [mix] (α 0 )� term controls the limiting error due
to gradient noise. This scaling by the mixing time is intuitive, reflecting that roughly every cycle of τ [mix] (·)
observations provides as much information as a single independent sample from the stationary distribution.
We can also imagine specializing the results to the case of Projected TD under the i.i.d. model, thereby
eliminating all terms depending on the mixing time. We would attain bounds that mirror those in Theorem
2, except that the gradient noise term σ [2] there would be replaced by G [2] . This is a consequence using G as a
uniform upper bound on the gradient norm in the proof, which is possible because of the projection step.


Theorem 3. Suppose the Projected TD algorithm is applied with parameter R ≥∥θ [∗] ∥ 2 under the Markov
chain observation model with Assumption 1. Set G = (r max + 2R). Then the following claims hold.


(a) With a constant step-size sequence α 0 = · · · = α T = 1/√T,



E ���V θ ∗ − V θ¯ T �� 2D



∥θ [∗] − θ 0 ∥ [2] 2 [+][ G] [2] [ �] 9 + 12τ [mix] (1/√
≤
�



2√



T (1 − γ) .



T )
�



9 Recall that we assumed ∥φ(s)∥ 2 ≤ 1 for all s ∈S and |R(s, s ′ )| ≤ r max for all s, s ′ ∈S


17




(b) With a constant step-size sequence α 0 = · · · = α T < 1/(2ω(1 − γ)),



�



E ∥θ [∗] − θ T ∥ [2] 2 ≤ e [−][2][α] [0] [(1][−][γ][)][ωT] [�] ∥θ [∗] − θ 0 ∥ [2] 2 [+][ α] [0]
� � �



G [2] [ �] 9 + 12τ [mix] (α 0 )�

2(1 − γ)ω

�



.



(c) With a decaying step-size sequence α t = 1/(ω(t + 1)(1 − γ)) for all t ∈ N 0,



E ���V θ ∗ − V θ¯ T �� 2D



9 + 24τ [mix] (α T )�
≤ [G] [2] [ �] (1 + log T ),
� T (1 − γ) [2] ω



Remark 1: The proof of part (c) also implies an O [˜] (1/T ) convergence rate for the iterate θ T itself; similar
to the O(1/T ) convergence shown for the i.i.d. case, in part (c) of Theorem 2.


Remark 2: It is likely possible to eliminate the log T term in the numerator of part (c) to get a O(1/T ) convergence rate. One approach is to use a different weighting of the iterates when averaging, as in Lacoste-Julien et al.

[2012]. For brevity and simplicity, we do not pursue this direction.


8.2 Choice of the projection radius


We briefly comment on the choice of the projection radius, R. Note that Theorem 3 assumes that ∥θ [∗] ∥ 2 ≤ R,
so the TD limit point lies within the projected ball. How do we choose such an R when θ [∗] is unknown? It
turns out we can use Lemma 2, which relates the value function at the limit of convergence V θ [∗] to the true
value function, to give a conservative upper bound. This is shown in the proof of the following lemma.


Lemma 7. ∥θ [∗] ∥ Σ ≤ (12−rγ max ) [3][/][2] [ and hence][ ∥][θ] [∗] [∥] [2] [≤] √ω2(1r− max γ) 3/2 .


Proof. Because rewards are uniformly bounded, |V µ (s)|≤ r max /(1 − γ) for all s ∈S. Recall that V µ denotes
the true value function of the Markov reward process. This implies that


∥V µ ∥ D ≤∥V µ ∥ ∞ ≤ r max
(1 − γ) [.]


Lemma 2 along with simple matrix inequalities enable a simple upper bound on ∥θ [∗] ∥ 2 . We have



1
∥V θ ∗ − V µ ∥ D ≤
�1



1 1

1 − γ [2] [ ∥][V] [µ] [ −] [Π] [D] [V] [µ] [∥] [D] [ ≤] �1



1 1

1 − γ [2] [ ∥][V] [µ] [∥] [D] [ ≤] √1 − γ ∥V µ ∥ D,



where the penultimate inequality holds by the Pythagorean theorem. By the reverse triangle inequality we
have ��∥V θ ∗ ∥ D −∥V µ ∥ D �� ≤∥V θ ∗ − V µ ∥ D . Thus,


∥V θ ∗ ∥ D ≤ ∥V θ ∗ − V µ ∥ D + ∥V µ ∥ D ≤ 2 V µ ∥ D ≤ 2 r max
√1 − γ ∥ √1 − γ (1 − γ) [.]


Recall from Section 2 we have, ∥V θ [∗] ∥ D = ∥θ [∗] ∥ Σ which establishes first part of the claim. The second claim
uses that ∥θ [∗] ∥ Σ ≥ ω∥θ [∗] ∥ 2 which follows by Lemma 1.


It is important to remark here that this bound is problem dependent as it depends on the minimum eigenvalue ω of the steady-state feature covariance matrix Σ. We believe that estimating ω online would make the
projection step practical to implement.


18




8.3 Analysis


We now present the key analysis used to establish Theorem 3. Throughout, we assume the conditions of the
theorem hold: we consider the Markov chain observation model with Assumption 1 and study the Projected
TD algorithm applied with parameter R ≥∥θ [∗] ∥ 2 and some step-size sequence (α 0, · · ·, α T ).
We fix some notation throughout the scope of this subsection. Define the set Θ R = {θ ∈ R [d] : ∥θ∥ 2 ≤ R},
so θ t ∈ Θ R for each t because of the algorithm’s projection step. Set G = (r max + 2R), so ∥g t (θ)∥ 2 ≤ G for
all θ ∈ Θ R by Lemma 6. Finally, we set


ζ t (θ) ≡ (g t (θ) − g¯(θ)) [⊤] (θ − θ [∗] ) ∀ θ ∈ Θ R,


which can be thought of as the error in the evaluation of gradient-update under parameter θ at time t.
Referring back to the analysis of the i.i.d. observation model, one can see that an error decomposition
given in Equation (17) is the crucial component of the proof. The main objective in this section is to establish
two key lemmas that yield a similar decomposition in the Markov chain observation model. The result can
be stated cleanly in the case of a constant step-size. If α 0 = · · · = α T = α, we show


E �∥θ [∗] − θ t+1 ∥ 2 [2] � ≤ E �∥θ [∗] − θ t ∥ 2 [2] � − 2α(1 − γ)E �∥V θ ∗ − V θ t ∥ D [2] � + E[αζ t (θ t )] + α [2] G [2]

≤ E �∥θ [∗] − θ t ∥ [2] 2 � − 2α(1 − γ)E �∥V θ ∗ − V θ t ∥ [2] D � + α [2] [ �] 5 + 6τ [mix] (α)� G [2] . (22)


The first inequality follows from Lemma 8. The second follows from Lemma 11, which in the case of a
constant step-size α shows E[αζ t (θ t )] ≤ G [2] (4 + 6τ [mix] (α))α [2] . Notice that bias in the gradient enters into
the analysis as if by scaling the magnitude of the noise in gradient evaluations by a factor of the mixing time.
From this decomposition, parts (a) and (b) of Theorem 3 follow by essentially copying the proof of Theorem
2. Similar, but messier, inequalities hold for any decaying step-size sequence, which allows us to establish
part (c).


8.3.1 Error decomposition under Projected TD


The next lemma establishes a recursion for the error under projected TD(0) that hold for each sample path.


Lemma 8. With probability 1, for every t ∈ N 0,


∥θ [∗] − θ t+1 ∥ [2] 2 [≤∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 [−][2][α] [t] [(1][ −] [γ][)][∥][V] [θ] [∗] [−] [V] [θ] t [∥] [2] D [+2][α] [t] [ζ] [t] [(][θ] [t] [) +][ α] [2] t [G] [2] [.]


Proof. From the projected TD(0) recursion in Equation (20), for any t ∈ N 0,


∥θ [∗] − θ t+1 ∥ [2] 2 = ∥θ [∗] − Π 2,R (θ t + α t g t (θ t ))∥ [2] 2
= ∥Π 2,R (θ [∗] ) − Π 2,R (θ t + α t g t (θ t ))∥ 2
≤ ∥θ [∗] − θ t − α t g t (θ t )∥ [2] 2
= ∥θ [∗] − θ t ∥ 2 [2] [−][2][α] [t] [g] [t] [(][θ] [t] [)] [⊤] [(][θ] [∗] [−] [θ] [t] [) +][ α] [2] t [∥][g] [t] [(][θ] [t] [)][∥] [2] 2
≤ ∥θ [∗] − θ t ∥ [2] 2 [−][2][α] [t] [g] [t] [(][θ] [t] [)] [⊤] [(][θ] [∗] [−] [θ] [t] [) +][ α] [2] t [G] [2] [.]

= ∥θ [∗] − θ t ∥ 2 [2] [−][2][α] [t] [g][¯][(][θ] [t] [)] [⊤] [(][θ] [∗] [−] [θ] [t] [) + 2][α] [t] [ζ] [t] [(][θ] [t] [) +][ α] [2] t [G] [2] [.]
≤ ∥θ [∗] − θ t ∥ [2] 2 [−][2][α] [t] [(1][ −] [γ][)][∥][V] [θ] [∗] [−] [V] [θ] t [∥] [2] D [+2][α] [t] [ζ] [t] [(][θ] [t] [) +][ α] [2] t [G] [2] [.]


The first inequality used that orthogonal projection operators onto a convex set are non-expansive [10], the
second used Lemma 6 together with the fact that ∥θ t ∥ 2 ≤ R due to projection, and the third used Lemma
3.


By taking expectation of both sides, this inequality could be used to produce bounds in the same manner
as in the previous section, except that in general E[ζ t (θ t )] ̸= 0 due to bias in the gradient evaluations.


10 Let P C (x) = arg min x ′ ∈C ∥x ′ − x∥ denote the projection operator onto a closed, non-empty, convex set C ⊂ R d . Then ∥P C (x)−
P C (y)∥≤∥x − y∥ for all vectors x and y.


19




8.3.2 Information–theoretic techniques for controlling the gradient bias


The uniform mixing condition in Assumption 1 can be used in conjunction with some information theoretic
inequalities to control the magitude of the gradient bias. This section presents a general lemma, which is the
key to this analysis. We start by reviewing some important properties of information-measures.


Information theory background. The total-variation distance between two probability measures is a special case of the more general f -divergence defined as



dP
d f (P ||Q) = f
� � dQ



dQ,
�



where f is a convex function such that f (1) = 0. By choosing f (x) = |x − 1|/2, one recovers the totalvariation distance. A choice of f (x) = x log(x) yields the Kullback-Leibler divergence. This yields a generalization of the mutual information between two random variables X and Y . The f -information between X
and Y is the f -divergence between their joint distribution and the product of their marginals:


I f (X, Y ) = d f (P(X = ·, Y = ·), P(X = ·) ⊗ P(Y = ·)).


This measure satisfies several nice properties. By definition it is symmetric, so I f (X, Y ) = I f (Y, X). It can
be expressed in terms of the expected divergence between conditional distributions:


I f (X, Y ) = � P(X = x)d f (P(Y = ·|X = x), P(Y = ·)). (23)


x


Finally, it satisfies the following data-processing inequality. If X → Y → Z forms a Markov chain, then


I f (X, Z) ≤ I f (X, Y ).


Here, we use the notation X → Y → Z, which is standard in information theory and the study of graphical
models, to indicate that the random variables Z and X are independent conditioned on Y . Note that by
symmetry we also have I f (X, Z) ≤ I f (Y, Z). To use these results in conjunction with Assumption 1, we
can specialize to total-variation distance (d TV ) and total-variation mutual information (I TV ) using f (x) =
|x − 1|/2. The total-variation is especially useful for our purposes because of the following variational
representation.



d TV (P, Q) = sup
v:∥v∥ ∞ ≤ [1] 2



vdP − vdQ . (24)

����� � ����



In particular, if P and Q are close in total-variation distance, then the expected value of any bounded function
under P will be close to that under Q.


Information theoretic control of coupling. With this background in place, we are ready to establish a
general lemma, which is central to our analysis. We use ∥f ∥ ∞ = sup x∈X |f (x)| to denote the supremum
norm of a function f : X → R.


Lemma 9 (Control of couping). Consider two random variables X and Y such that


X → s t → s t+τ → Y


for some fixed t ∈{0, 1, 2, . . .} and τ > 0. Assume the Markov chain mixes uniformly, as stated in Assumption 1. Let X [′] and Y [′] denote independent copies drawn from the marginal distributions of X and Y, so
P(X [′] = ·, Y [′] = ·) = P(X = ·) ⊗ P(Y = ·). Then, for any bounded function v,


|E [v(X, Y )] − E [v(X [′], Y [′] )]| ≤ 2∥v∥ ∞ (mρ [τ] ).


20




Proof. Let P = P(X ∈·, Y ∈·) denote the joint distribution of X and Y and Q = P(X ∈·) ⊗ P(Y ∈·)

v
denote the product of the marginal distributions. Let h = 2∥v∥ ∞ [, which is the function][ v][ rescaled to take]
values in [−1/2, 1/2]. Then, by Equation (24)


E[h(X, Y )] − E[h(X [′], Y [′] )] = hdP − hdQ ≤ d TV (P, Q) = I TV (X, Y ),
� �


where the last equality uses the definition of the total-variation mutual information, I T V . Then,


I TV (X, Y ) ≤ I TV (s t, s t+τ ) = � P(s t = s)d TV (P(s t+τ = · | s t = s)), P(s t+τ = ·))

s∈S


≤ sup d TV (P(s t+τ = · | s t = s), π)
s∈S


≤ mρ [τ],


where the three steps follow, respectively, from the data-processing inequality, the property in Equation (23),
the stationarity of the Markov chain, and the the uniform mixing condition in Assumption 1. Combining
these steps gives


|E [v(X, Y )] − E [v(X [′], Y [′] )]| ≤ 2∥v∥ ∞ I TV (X, Y ) ≤ 2∥v∥ ∞ mρ [τ] .


8.3.3 Bounding the gradient bias.


We are now ready to bound the expected gradient error E[ζ t (θ t )]. First, we establish some basic regularity
properties of the function ζ t (·).


Lemma 10 (Gradient error is bounded and Lipschitz). With probability 1,


|ζ t (θ)|≤ 2G [2] for all θ ∈ Θ R


and

′
|ζ t (θ) − ζ t (θ [′] )| ≤ 6G (θ − θ ) for all θ, θ [′] ∈ Θ R .
��� ��� 2


Proof. The result follows from a straightforward application of the bounds ∥g t (θ)∥ 2 ≤ G and ∥θ∥ 2 ≤ R ≤
G/2, which hold for each θ ∈ Θ R . A full derivation is given in Appendix A.3.


We now use Lemmas 9 and 10 to establish a bound on the expected gradient error.


Lemma 11 (Bound on gradient bias). Consider a non-increasing step-size sequence, α 0 ≥ α 1 . . . ≥ α T . Fix
any t < T, and set t [∗] ≡ max{0, t − τ [mix] (α T )}. Then,


E [ζ t (θ t )] ≤ G [2] [ �] 4 + 6τ [mix] (α T )� α t ∗ .


The following bound also holds:

t−1
E [ζ t (θ t )] ≤ 6G [2] � α i .


i=0


Proof. We break the proof down into three steps.


Step 1: Relate ζ t (θ t ) and ζ t (θ t−τ ).

Note that for any i ∈ N 0,


∥θ i+1 − θ i ∥ 2 = ∥Π 2,R (θ i + α i g i (θ i )) − Π 2,R (θ i )∥ 2 ≤∥θ i + α i g i (θ i ) − θ i ∥ 2 = α i ∥g i (θ i )∥ 2 ≤ α i G.


21




Therefore,


∥θ t − θ t−τ ∥ 2 ≤


Applying Lemma 10, we conclude



t−1
� ∥θ i+1 − θ i ∥ 2 ≤ G


i=t−τ



t−1
� α i .


i=t−τ



t−1
ζ t (θ t ) ≤ ζ t (θ t−τ ) + 6G [2] � α i for all τ ∈{0, · · ·, t}. (25)


i=t−τ



Step 2: Bound E[ζ t (θ t−τ )] using Lemma 9.
Recall that the gradient g t (θ) depends implicitly on the observed tuple O t = (s t, R(s t, s t+1 ), s t+1 ). Let us
overload notation to make this statistical dependency more transparent. Put


g(θ, O t ) := g t (θ) = r t + γφ(s t+1 ) [⊤] θ − φ(s t ) [⊤] θ φ(s t ) θ ∈ Θ R
� �


and

¯
ζ(θ, O t ) := ζ t (θ) = (g(θ, O t ) − g(θ)) [⊤] (θ − θ [∗] ) θ ∈ Θ R .


We have defined ¯g : Θ R → R [d] as ¯g(θ) = E[g(θ, O t )] for all θ ∈ Θ R, where this expectation integrates over
the marginal distribution of O t . Then, by definition, for any fixed (non-random) θ ∈ Θ R,


E[ζ(θ, O t )] = (E[g(θ, O t )] − g¯(θ)) [⊤] (θ − θ [∗] ) = 0.


Since θ 0 ∈ Θ R is non-random, it follows immediately that


E[ζ(θ 0, O t )] = 0. (26)


We use Lemma 10 to bound E[ζ t (θ t−τ, O t )]. First, consider random variables θ t [′] −τ [and][ O] t [′] [drawn indepen-]
dently from the marginal distributions of θ t−τ and O t, so P(θ t [′] −τ [=][ ·][, O] t [′] [=][ ·][) =][ P][(][θ] [t][−][τ] [ =][ ·][)][ ⊗] [P][(][O] [t] [ =][ ·][)][.]
Then E[ζ(θ t [′] −τ [, O] t [′] [)] =][ E][[][E][[][ζ][(][θ] t [′] −τ [, O] t [′] [)][ |][ θ] t [′] −τ []] = 0][. Since][ |][ζ][(][θ, O] [t] [)][|≤] [2][G] [2] [ for all][ θ][ ∈] [Θ] [R] [ by Lemma][ 11]
and θ t−τ → s t−τ → s t → O t forms a Markov chain, applying Lemma 10 gives


E[ζ(θ t−τ, O t )] ≤ 2(2G [2] )(mρ [τ] ) = 4G [2] mρ [τ] . (27)


Step 3: Combine terms.

The second claim follows immediately from Equation (25) together with Equation (26). We focus on establishing the first claim. Taking the expectation of Equation (25) implies


E[ζ t (θ t )] ≤ E[ζ t (θ t−τ )] + 6G [2] τα t−τ ∀τ ∈{0, · · ·, t}.


For t ≤ τ [mix] (α T ), choosing τ = t gives



E[ζ t (θ t )] ≤ E[ζ t (θ 0 )] +6G [2] tα 0 ≤ 6G [2] τ [mix] (α T )α 0 .
� �� �
=0



For t > τ [mix] (α T ), choosing τ = τ 0 ≡ τ [mix] (α T ) gives


E[ζ t (θ t )] ≤ 4G [2] mρ [τ] [0] + 6G [2] τ 0 α t−τ 0 ≤ 4G [2] α T + 6G [2] τ 0 α t−τ ≤ G [2] (4 + 6τ 0 ) α t−τ 0 .


where the second inequality used that mρ [τ] [0] ≤ α T by the definition of the mixing time τ 0 ≡ τ [mix] (α T ) and
the second inequality uses that step-sizes are non-increasing.


22




8.3.4 Completing the proof of Theorem 3


Combining Lemmas 8 and 10 gives the error decomposition in Equation 22 for the case of a constant stepsize. As noted at the beginning of this subsection, from this decomposition, parts (a) and (b) of Theorem 3 can
be established by essentially copying the proof of Theorem 2. For completeness, this is included in Appendix
A. For part (c), we closely follow analysis of SGD with decaying step-sizes presented in Lacoste-Julien et al.

[2012]. However, some headache is introduced because Lemma 11 includes terms of the form α t−τ mix (α T )
instead of the typical α t terms present in analyses of SGD. A complete proof of part (c) is given in Appendix
A as well.

## 9 Extension to TD with eligibility traces


This section extends our analysis to provide finite time guarantees for temporal difference learning with eligibility traces. We study a class of algorithms, denoted by TD(λ) and parameterized by λ ∈ [0, 1], that
contains as a special case the TD(0) algorithm studied in previous sections [11] . For λ > 0, the algorithm
maintains an eligibility trace vector, which is a geometric weighted average of the negative gradients at all
previously visited states, and makes parameter updates in the direction of the eligibility vector rather than
the negative gradient. Eligibility traces sometimes provide substantial performance improvements in practice [Sutton and Barto, 1998]. Unfortunately, they also introduce subtle dependency issues that complicate
theoretical analysis; to our knowledge, this section provides the first non-asymptotic analysis TD(λ).
Our analysis focuses on the Markov chain observation model studied in the previous section and we
mirror the technical assumptions used there. In particular, we assume that the Markov chain is stationary and
mixes at a uniform geometric rate (Assumption 1). As before, for tractability, we study a projected variant of
TD(λ).


9.1 Projected TD(λ) algorithm


TD(λ) makes a simple, but highly consequential, modification to TD(0). Pseudo-code for the algorithm is presented below in Algorithm 2. As with TD(0), at each time-step t it observes a tuple (s t, r t = R(s t, s t+1 ), s t+1 )
and computes the TD error δ t (θ t ) = r t + γV θ t (s t+1 ) − V θ t (s t ). However, while TD(0) makes an update
θ t+1 = θ t + α t δ t (θ t )φ(s t ) in the direction of the feature vector at the current state, TD(λ) makes the update
θ t+1 = θ t + α t δ t (θ t )z 0:t . The vector z 0:t = [�] [t] k=0 [(][γλ][)] [k] [φ][(][s] [t][−][k] [)][ is called the eligibility trace which is up-]
dated incrementally as shown below in Algorithm 2. As the name suggests, the components of z 0:t roughly
capture the extent to which each feature is eligible for receiving credit or blame for an observed TD error

[Sutton and Barto, 1998, Seijen and Sutton, 2014].


Algorithm 2: Projected TD(λ) with linear function approximation

Input : radius R, initial guess {θ 0 : ∥θ 0 ∥ 2 ≤ R}, and step-size sequence {α t } t∈N
Initialize: θ [¯] 0 ← θ 0, z −1 = 0, λ ∈ [0, 1].
for t = 0, 1, . . . do

Observe tuple: O t = (s t, r t, s t+1 )
Get TD error: δ t (θ t ) = r t + γV θ t (s t+1 ) − V θ t (s t ) /* sample Bellman error */
Update eligibility trace: z 0:t = (γλ)z 0:t−1 + φ(s t ) /* Geometric weighting */
Compute update direction: x t (θ t, z 0:t ) = δ t (θ t )z 0:t
Take a projected update step: θ t+1 = Π 2,R (θ t + α t x t (θ t, z 0:t )) /* α t :step-size */

Update averaged iterate: θ [¯] t+1 ← � t+1t � θ¯ t + � t+11 � θ t+1 /* θ [¯] t+1 = t+11 � tℓ+1=1 [θ] [ℓ] [*/]

end


11 TD(0) corresponds to λ = 0.


23




Some new notation in Algorithm 2 should be highlighted. We use x t (θ, z 0:t ) = δ t (θ)z 0:t to denote the
update to the parameter vector θ at time t. This plays a role analogous to the negative gradient g t (θ) in TD(0).


9.2 Limiting behavior of TD(λ)


We now review results on the asymptotic convergence of TD(λ) due to Tsitsiklis and Van Roy [1997]. This
provides the foundation of our finite time analysis and also offers insight into how the algorithm differs from
TD(0).
Before giving any results, let us note that just as the true value function V µ (·) is the unique solution
to Bellman’s fixed point equation V µ = T µ V µ, it is also the unique solution to a k-step Bellman equation
V µ = T µ [(][k][)] [V] µ [. This can be written equivalently as]



�



V µ (s) = E



k
� γ [t] R(s t ) + γ [k][+1] V (s k+1 ) | s 0 = s
� t=0



∀s ∈ S,



where the expectation is over states sampled when policy µ is applied to the MDP. The asymptotic properties
of TD(λ) are closely tied to a geometrically weighted version of the k-step Bellman equations described
above. Define the averaged Bellman operator



�



k
� γ [t] R(s t ) + γ [k][+1] V (s k+1 ) | s 0 = s
� t=0



(T µ [(][λ][)] V )(s) = (1 − λ)



∞
� λ [k] E


k=0



. (28)



One interesting interpretation of this equation is as a k-step Bellman equation, but where the horizon k itself
is a random geometrically distributed random variable.

Tsitsiklis and Van Roy [1997] showed that under appropriate technical conditions, the approximate value
function V θ t = Φθ t estimated by TD(λ) converges almost surely to the unique solution, θ [∗] of the projected
fixed point equation
Φθ = Π D T µ [(][λ][)] Φθ.


TD(λ) is then interpreted as a stochastic approximation scheme for solving this fixed point equation. The
existence and uniqueness of such a fixed point Φθ [∗] is implied by the following lemma, which shows that
Π D T [λ] (·) is a contraction operator with respect to the steady-state weighted norm ∥·∥ D .


Lemma 12. [Tsitsiklis and Van Roy [1997]] Π D T µ [(][λ][)] [(][·][)][ is a contraction with respect to][ ∥·∥] D [with modulus]


κ = [γ][(1][ −] [λ][)]

1 − γλ [≤] [γ <][ 1][.]


As with TD(0), the limiting value function under TD(λ) comes with some competitive guarantees. A
short argument using Lemma 12 shows


1
∥V θ ∗ − V µ ∥ D ≤ (29)
√1 − κ [2] [ ∥][Π] [D] [V] [µ] [ −] [V] [µ] [∥] [D] [.]


See for example Chapter 6 of Bertsekas [2012] for a proof. It is important to note the distinction between the
convergence guarantee results for TD(λ) and TD(0) in terms of the contraction factors. The contraction factor
κ is always less than γ, the contraction factor under TD(0). In addition, as λ → 1, κ → 0 implying that the
limit point of TD(λ) for large enough λ will be arbitrarily close to Π D V µ, which minimizes the mean-square
error in value predictions among all value functions representable by the features. This calculation suggests
a choice of λ = 1 will offer the best performance. However, the rate of convergence also depends on λ, and
may degrade as λ grows. Disentangling such issues requires also a careful study of the statistical efficiency
of TD(λ), which we undertake in the following subsection.


24




9.3 Finite time bounds for Projected TD(λ)


Following Section 8, we establish three finite time bounds on the performance of the Projected TD(λ) algorithm. The first bound in part (a) does not depend on any special regularity of the problem instance but gives
a comparatively slow convergence rate of O [˜] (1/√T ). It applies with the robust (problem independent) and

aggressive step-size of 1/√T . Part (b) illustrates the exponential rate of convergence to within some radius

Oof the TD(˜(1/T ), but the step-size sequence requires knowledge of the minimum eigenvalueλ) fixed-point for sufficiently small step-sizes. Part (c) attains an improved dependence on ω of Σ. T of
Compared to the results for TD(0), our bounds depend on a slightly different definition of the mixing time
that takes into account the geometric weighting in the eligibility trace term. Define


τ λ [mix] (ǫ) = max{τ [MC] (ǫ), τ [Algo] (ǫ)}, (30)


where we denote τ [MC] (ǫ) = min{t ∈ N 0 | mρ [t] ≤ ǫ} and τ [Algo] (ǫ) = min{t ∈ N 0 | (γλ) [t] ≤ ǫ}. As we show
next, this definition of mixing time enables compact bounds for convergence rates of TD(λ).


Theorem 4. Suppose the Projected TD(λ) algorithm is applied with parameter R ≥∥θ [∗] ∥ 2 under the Markov
chain observation model with Assumption 1. Set B = [(][r] (1 [max] − [+2] γλ) [R][)] [. Then the following claims hold.]


(a) With a constant step-size α t = α 0 = 1/√T,



E ���V θ ∗ − V θ¯ T �� 2D



∥θ [∗] − θ 0 ∥ [2] 2 [+][ B] [2] [ �] 13 + 28τ λ [mix] (1/√
≤
�



2√



T (1 − κ) .



T )
�



(b) With a constant step-size α t = α 0 < 1/(2ω(1 − κ)) and T > 2τ λ [mix] (α 0 ),



.

�



E ∥θ [∗] − θ T ∥ [2] 2 ≤ e [−][2][α] [0] [(1][−][κ][)][ωT] [ �] ∥θ [∗] − θ 0 ∥ [2] 2 [+][ α] [0]
� � �


(c) With a decaying step-size α t = 1/(ω(t + 1)(1 − κ)),



B [2] [ �] 13 + 24τ λ [mix] (α 0 )�

2(1 − κ)ω

�



E ���V θ ∗ − V θ¯ T �� 2D



13 + 52τ λ [mix] (α T )�
≤ [B] [2] [ �] (1 + log T ) .
� T (1 − κ) [2] ω



Remark 2. As was the case for TD(0), the proof of part (c) also implies a O [˜] (1/T ) convergence rate for the
iterate θ T itself. Again, a different weighting of the iterates as shown in Lacoste-Julien et al. [2012] might
enable us to eliminate the log T term in the numerator of part (c) to give a O(1/T ) convergence rate. For
brevity, we do not pursue this direction.


We now compare the bounds for TD(λ) with that of TD(0) ignoring the constant terms. First, let us look
at the results for the constant step-size α t = 1/√T in part (a) of Theorems 3 and 4. Approximately, for the

B [2] G [2]
TD(λ) case, we have the term √T (1−κ) [vis-a-vis the term] √T (1−γ) [for the TD(0) case. A simple argument]



B [2] G [2]

T (1−κ) [vis-a-vis the term] √T (1



TD(λ) case, we have the term √T (1−κ) [vis-a-vis the term] √T (1−γ) [for the TD(0) case. A simple argument]

below clarifies the relationship between these two.



B [2]

√T (1



B [2] = (r max + 2R) [2]

T (1 − κ) √T (1 − κ)(1 −



(r max + 2R) [2] G [2]

T (1 − κ)(1 − γλ) [2] [ =] √T (1 − κ



T (1 − κ)(1 − γλ) [2]



G [2]
≥
√T (1 − κ



T (1 − γ) .



G [2] G [2]


=
T (1 − κ)(1 − γλ) √T (1



As we will see later, B is an upper bound to the norm of x t (θ t, z 0:t ), the update direction for TD(λ). Correspondingly, from Section 8, we know that G is the upper bound on gradient norm, g t (θ t ) for TD(0). Intuitively, for TD(λ), the bound B is larger (due to the presence of the eligibility trace term) and more so as
λ → 1. This dominates any benefit (in terms of statistical efficiency) from a smaller contraction factor, κ.


25




However, for decaying step-sizes of α t = 1/(ω(t + 1)(1 − κ)), the bounds are qualitatively the same.
This follows as the terms that dominate part (c) of Theorems 3 and 4 are equal:



B [2] (r max + 2R) [2]

T (1 − κ) [2] [ =] T (1 − κ) [2] (1 −



B [2]



(r max + 2R) [2] G [2]

T (1 − κ) [2] (1 − γλ) [2] [ =] T (1 − κ) [2]



G [2] G [2]

T (1 − κ) [2] (1 − γλ) [2] [ =] T (1 −



T (1 − γ) [2] [ .]



In conclusion, for constant step-sizes–which is often how TD algorithms as used in practice–our bounds
establish a faster convergence rate for TD(0) than for TD(λ). Or equivalently, according to our bounds, more
data is required to guarantee TD(λ) is close to its limit point. In this context, however, the trade-off we
remarked on in Section 9.2 is noteworthy as the fixed point for TD(λ) comes with a better error guarantee.

## 10 Extension: Q-learning for high dimensional Optimal Stopping


So far, this paper has dealt with the problem of approximating the value function of a fixed policy in a
computationally and statistically efficient manner. The Q-learning algorithm is one natural extension of
temporal-difference learning to control problems, where the goal is to learn an effective policy from data.
Although it is widely applied in reinforcement learning, in general Q-learning is unstable and its iterates
may oscillate forever. An important exception to this was discovered by Tsitsiklis and Van Roy [1999], who
showed that Q-learning converges asymptotically for optimal stopping problems. In this section, we show
how the techniques developed in Sections 7 and 8 can be applied in an identical manner to give finite time
bounds for Q-learning with linear function approximation applied to optimal-stopping problems with high
dimensional state spaces. To avoid repetition, we only state key properties satisfied by Q-learning in this
setting which establish exactly the same convergence bounds as shown in Theorems 2 and 3.


10.1 Problem formulation


The optimal stopping problem is that of determining the time to terminate a process to maximize cumulative
expected rewards accrued. Problems of this nature arise naturally in many settings, most notably in the pricing
of financial derivatives [Andersen and Broadie, 2004, Haugh and Kogan, 2004, Desai et al., 2012]. We first
give a brief formulation for a class of optimal stopping problems. A more detailed exposition can be found
in Tsitsiklis and Van Roy [1999], or Chapter 5 of the thesis work of Van Roy [1998].
Consider a discrete-time Markov chain {s t } t≥0 with finite state space S and unique stationary distribution distribution π. At each time t, the decision-maker observes the state s t and decides whether to stop or
continue. Let γ ∈ [0, 1) denote the discount factor and let u(·) and U (·) denote the reward functions associated with continuation and termination decisions respectively. Let the stopping time τ denote the (random)
time at which the decision-maker stops. The expected total discounted reward from initial state s associated
with the stopping time τ is



�



E



τ −1
� γ [t] u(s t ) + γ [τ] U (s τ ) s 0 = s
� t=0 ����



, (31)



where U (s τ ) is defined to be zero for τ = ∞. We seek an optimal stopping policy, which determines when
to stop as a function of the observed states so as to maximize (31).
For any Markov decision process, the optimal state-action value function Q [∗] : S × A → R specifies
the expected value to go from choosing an action a ∈A in a state s ∈S and following the optimal policy
in subsequent states. In optimal stopping problems, there are only two possible actions at every time step:
whether to terminate or to continue. The value of stopping in state s is just U (s), which allows us to simplify
notation by only representing the continuation value.
For the remainder of this section, we let Q [∗] : S → R denote the continuation-value function. It can be
shown that Q [∗] is the unique solution to the Bellman equation Q [∗] = FQ [∗], where the Bellman operator is
given by
FQ(s) = u(s) + γ � P (s [′] |s) max {U (s [′] ), Q(s [′] )}.

s [′] ∈S


26




                               Given the optimal continuation values Q [∗] ( ), the optimal stopping time is simply


τ [∗] = min {t | U (s t ) ≥ Q [∗] (s t )}. (32)


10.2 Q-Learning for high dimensional Optimal Stopping


In principle, one could generate the optimal stopping time using Equation (32) by applying exact dynamic
programming algorithms to compute the optimal Q-function. However, such methods are only implementable
for small state spaces. To scale to high dimensional state spaces, we consider a feature-based approximation
of the optimal continuation value function, Q [∗] . We focus on linear function approximation, where Q [∗] (s) is
approximated as
Q [∗] (s) ≈ Q θ (s) = φ(s) [⊤] θ,


where φ(s) ∈ R [d] is a fixed feature vector for state s and θ ∈ R [d] is a parameter vector that is shared across
states. As shown in Section 2, for a finite state space, S = {s 1, . . ., s n }, Q θ ∈ R [n] can be expressed
compactly as Q θ = Φθ, where Φ ∈ R [n][×][d] and θ ∈ R [d] . We also assume that the d features vectors {φ k } [d] k=1 [,]
forming the columns of Φ are linearly independent.
We consider the Q-learning approximation scheme in Algorithm 3. The algorithm starts with an initial
parameter estimate of θ 0 and observes a data tuple O t = (s t, u(s t ), s [′] t [)][. This is used to compute the target]
y t = u(s t ) + γ max {U (s [′] t [)][, Q] [θ] t [(][s] [′] t [)][}][, which is a sampled version of the][ F] [(][·][)][ operator applied to the current]
Q–function. The next iterate, θ t+1, is computed by taking a gradient step with respect to a loss function
measuring the distance between y t and predicted value-to-go. An important feature of this method is that
problem data is generated by the exploratory policy that chooses to continue at all time-steps.


Algorithm 3: Q-Learning for Optimal Stopping problems.

Input : initial guess θ 0, step-size sequence {α t } t∈N and radius R.
Initialize: θ [¯] 0 ← θ 0 .
for t = 0, 1, . . . do



Observe tuple: O t = (s t, u(s t ), s [′] t [)]
Define target: y t = u(s t ) + γ max{U (s [′] t [)][, Q] [θ] t [(][s] [′] t [)][}] /* sample Bellman operator */
Define loss function: [1] [(][y] [t] [ −] [Q] [θ] [(][s] [t] [))] [2] /* sample Bellman error */



Define loss function: 2 [1] [(][y] [t] [ −] [Q] [θ] [(][s] [t] [))] [2] /* sample Bellman error */

1

Compute negative gradient: g t (θ t ) = − [∂] [(][y] [t] [ −] [Q] [θ] [(][s] [t] [))] [2] [|] θ=θ



1

Compute negative gradient: g t (θ t ) = − ∂θ [∂] 2 [(][y] [t] [ −] [Q] [θ] [(][s] [t] [))] [2] [|] θ=θ t

Take a gradient step: θ t+1 = θ t + α t g t (θ t ) /* α t :step-size */



Update averaged iterate: θ [¯] t+1 ← � t+1t � θ¯ t + � t+11 � θ t+1 /* θ [¯] t+1 = t+11 � tℓ+1=1 [θ] [ℓ] [*/]

end


10.3 Asymptotic guarantees


Similar to the asymptotic results for TD algorithms, Tsitsiklis and Van Roy [1999] show that the variant of
Q-learning detailed above in Algorithm 3 converges to the unique solution, θ [∗], of the projected Bellman
equation,
Φθ = Π D F Φθ.


This results crucially relies on the fact that the projected Bellman operator Π D F (·) is a contraction with
respect to ∥·∥ D with modulus γ. The analogous result for our study of TD(0) was stated in Lemma 2.
Tsitsiklis and Van Roy [1999] also give error bounds for the limit of convergence with respect to Q [∗], the
optimal Q-function. In particular, it can be shown that


1
∥Φθ [∗] − Q [∗] ∥ D ≤
�1 − γ [2] [ ∥][Π] [D] [Q] [∗] [−] [Q] [∗] [∥] [D]


27




where the left hand side measures the error between the estimated and the optimal Q-function which is upper
bounded by the representational power of the linear approximation architecture, as given on the right hand
side. In particular, if Q [∗] can be represented as a linear combination of the feature vectors then there is no
approximation error and the algorithm converges to the optimal Q-function. Finally, one can ask whether the
stopping times suggested by this approximate continuation value function, Φθ [∗], are effective. Let ˜µ be the
policy that stops at the first time t when


U (s t ) ≥ (Φθ [∗] )(s t ).


Then, for an initial state s 0 drawn from the stationary distribution π,



2
E [V [∗] (s 0 )] − E [V µ˜ (s 0 )] ≤



1 − γ [2] [ ∥][Π] [D] [Q] [∗] [−] [Q] [∗] [∥] [D] [,]



(1 − γ)�



where V [∗] and V µ˜ denote the value functions corresponding, respectively, to the optimal stopping policy the
approximate stopping policy µ. Again, this error guarantee depends on the choice of feature representation.


10.4 Finite time analysis


In this section, we show how our results in Sections 7, 8 for TD(0) and its projected counterpart can be
extended, without any modification, to give convergence bounds for the Q-function approximation algorithm
described above. To this effect, we highlight that key lemmas that enable our analysis in Sections 7 and
8 also hold in this setting. The contraction property of the F (·) operator will be crucial to our arguments
here. Convergence rates for an i.i.d. noise model, mirroring those established for TD(0) in Theorem 2, can be
shown for Algorithm 3. Results for the Markov chain sampling model, mirroring those established for TD(0)
in Theorem 3, can be shown for a projected variant of Algorithm 3.
First, we give mathematical expressions for the negative gradient. As a general function of θ and tuple
O t = (s t, u(s t ), s [′] t [)][, the negative gradient can be written as]


g t (θ) = u(s t ) + γ max {U (s [′] t [)][, φ][(][s] [′] t [)] [⊤] [θ][} −] [φ][(][s] [t] [)] [⊤] [θ] φ(s t ). (33)
� �


The negative expected gradient, when the tuple (s t, u(s t ), s [′] t [)][ follows its steady-state behavior, can be written]

as

¯
g(θ) = � π(s)P(s [′] |s) �u(s) + γ max {U (s [′] ), φ(s [′] ) [⊤] θ} − φ(s) [⊤] θ� φ(s).

s,s [′] ∈S


Additionally, using [�] s [′] ∈S [P][(][s] [′] [|][s][)] �u(s) + γ max {U (s [′] ), φ(s [′] ) [⊤] θ}� = (F Φθ) (s), it is easy to show


g¯(θ) = Φ [⊤] D(F Φθ − Φθ).


Note the close similarity of this expression with its counterparts for TD learning (see Section 3 and Appendix
B); the only difference is that the appropriate Bellman operator(s) for TD learning, T µ (·), has been replaced
with the appropriate Bellman operator F (·) for this optimal stopping problem.


10.4.1 Analysis with i.i.d. noise


In this section, we show how to analyze the Q-learning algorithm under an i.i.d. observation model, where
the random tuples observed by the algorithm are sampled i.i.d. from the stationary distribution of the Markov
process. All our ideas follow the presentation in Section 7, a careful understanding of which reveals that
Lemmas 3 and 5 form the backbone of our results. Recall that Lemma 3 establishes how, at any iterate θ, TD
updates point in the descent direction of ∥θ [∗] − θ∥ 2 [2] [. Lemma][ 5][ bounds the expected norm of the stochastic]
gradient, thus giving a control over system noise.
In Lemmas 13 and 14, given below, we show how exactly the same results also hold for the Q-function
approximation algorithm under the i.i.d. sampling model. With these two key lemmas, convergence bounds
shown in Theorem 2 follows by repeating the analysis in Section 7.


28




Lemma 13. [Tsitsiklis and Van Roy [1999]] Let V θ ∗ be the unique fixed point of Π D F (·), i.e. V θ ∗ =
Π D FV θ ∗ . Then, for any θ ∈ R [d],


¯
(θ [∗] − θ) [⊤] g(θ) ≥ (1 − γ)∥V θ ∗ − V θ ∥ [2] D [.]


Proof. This property is a consequence of the fact that Π D F (·) is a contraction with respect to ∥·∥ D with
modulus γ. It was established by Tsitsiklis and Van Roy [1999] in the process of proving their Lemma 8. For
completeness, we provide a standalone proof in Appendix C.


Lemma 14. We use notation and proof strategy mirroring the proof of Lemma 5. For any fixed θ ∈ R [d],
E∥g t (θ)∥ 2 [2] [≤] [2][σ] [2] [ + 8][∥][V] [θ] [−] [V] [θ] [∗] [∥] [2] D [where][ σ] [2] [ =][ E] �∥g t (θ [∗] )∥ 2 [2] �.

Proof. For brevity of notation, set φ = φ(s t ), φ [′] = φ(s [′] t [)][ and][ U] [ ′] [ =][ U] [(][s] [′] [)][. Define][ ξ][ = (][θ] [∗] [−] [θ][)] [⊤] [φ][ and]
ξ [′] = (θ [∗] − θ) [⊤] φ [′] . By stationarity ξ and ξ [′] have the same marginal distribution and E[ξ [2] ] = ∥V θ ∗ − V θ ∥ [2] D [.]
Using the formula for g t (θ) in Equation (33), we have


E �∥g t (θ)∥ 2 [2] � ≤ 2E �∥g t (θ [∗] )∥ 2 [2] � + 2E �∥g t (θ) − g t (θ [∗] )∥ 2 [2] �



2
= 2σ [2] + 2E φ φ [⊤] (θ [∗] − θ) − γ max U [′], φ [′⊤] θ [∗] [�] − max U [′], φ [′⊤] θ
���� � � � � ������ 2 �


2
≤ 2σ [2] + 2E ����φ ���φ ⊤ (θ ∗ − θ)�� + γ���max �U [′], φ [′⊤] θ [∗] [�] − max �U [′], φ [′⊤] θ�������� 2 �



2
≤ 2σ [2] + 2E ����φ ���φ ⊤ (θ ∗ − θ)�� + γ���φ ′⊤ (θ ∗ − θ)������� 2

≤ 2σ [2] + 2E[|ξ + γξ [′] | [2] ]

≤ 2σ [2] + 4 �E �|ξ| [2] [�] + γ [2] E �|ξ [′] | [2] [��]



(34)
�



= 2σ [2] + 4(1 = γ [2] )∥V θ − V θ [∗] ∥ D [2] [≤] [2][σ] [2] [ + 8][∥][V] [θ] [−] [V] [θ] [∗] [∥] [2] D [,]


where we used the assumption that features are normalized so that ∥φ∥ [2] 2 [≤] [1][ almost surely. Additionally,]
in going to Equation (34), we used that |max (c 1, c 3 ) − max (c 2, c 3 )| ≤|c 1 − c 2 | for any scalars c 1, c 2 and

c 3 .


10.4.2 Analysis under the Markov chain model


Analogous to Section 8, we analyze a projected variant of Algorithm 3 under the Markov chain sampling
model. Let Θ R = {θ ∈ R [d] : ∥θ∥ 2 ≤ R}. Starting with an initial guess of θ 0 ∈ Θ R, the algorithm updates
to the next iterate by taking a gradient step followed by projection onto Θ R, so iterates satisfy the stochastic
recursion θ t+1 = Π 2,R (θ t + α t g t (θ t )). We make the similar structural assumptions to those in Section 8.
In particular, assume the feature vectors and the continuation, termination rewards to be uniformly bounded,
with ∥φ(s)∥ 2 ≤ 1 and max{|u(s)|, |U (s)|} ≤ r max for all s ∈S. We assume r max ≤ R, which can always
be ensured by rescaling rewards or the projection radius.
We first show a uniform bound on the gradient norm.


Lemma 15. Define G = (r max + 2R). With probability 1, ∥g t (θ)∥ 2 ≤ G for all θ ∈ Θ R .


Proof. We start with the mathematical expression for the stochastic gradient,


g t (θ) = u(s t ) + γ max {U (s [′] t [)][, φ][(][s] [′] t [)] [⊤] [θ][} −] [φ][(][s] [t] [)] [⊤] [θ] φ(s t ).
� �


As r max ≤ R, we have: max {U (s [′] t [)][, φ][(][s] [′] t [)] [⊤] [θ][} ≤] [max][ {][U] [(][s] [′] t [)][,][ ∥][φ][(][s] [′] t [)][∥] 2 [∥][θ][∥] 2 [} ≤] [R][. Then,]


2 2
∥g t (θ)∥ [2] 2 = �u(s t ) + γ max {U (s [′] t [)][, φ][(][s] [′] t [)] [⊤] [θ][} −] [φ][(][s] [t] [)] [⊤] [θ] � ∥φ(s t )∥


2
≤ �r max + γR − φ(s t ) [⊤] θ�

≤ (r max + γR + ∥φ(s t )∥ 2 ∥θ∥ 2 ) [2] ≤ (r max + 2R) [2] = G [2] .


29




We used here that the basis vectors are normalized, ∥φ(s t )∥ 2 ≤ 1 for all t.


If we assume the Markov process (s 0, s 1, . . .) satisfies Assumption 1, then Lemma 15 paves the way to
show exactly the same convergence bounds as given in Theorem 3. For this, we refer the readers to Section
8 and Appendix A, where we show all the key lemmas and a detailed proof of Theorem 3. One can mirror
the same proof, using Lemmas 13 and 15 in place of Lemmas 3 and 11, which apply to TD(0). In particular,
note that we can use Lemma 15 along with some basic algebraic inequalities to show the gradient bias, ζ t (θ),
to be Lipschitz and bounded. This, along with the information-theoretic arguments of Lemma 9 enables the
exact same upper bound on the gradient bias as shown in Lemma 11. Combining these with standard proof
techniques for SGD [Lacoste-Julien et al., 2012, Nemirovski et al., 2009] shows the convergence bounds for
Q-learning.

## 11 Conclusions


In this paper we provide a simple finite time analysis of a foundational and widely used algorithm known
as temporal difference learning. Although asymptotic convergence guarantees for the TD method were previously known, characterizing its data efficiency stands as an important open problem. Our work makes a
substantial advance in this direction by providing a number of explicit finite time bounds for TD, including in the much more complicated case where data is generated from a single trajectory of a Markov chain.
Our analysis inherits the simplicity of and elegance enjoyed by SGD analysis and can gracefully extend to
different variants of TD, for example TD learning with eligibility traces (TD(λ)) and Q-function approximation for optimal stopping problems. Owing to the close connection with SGD, we believe that optimization
researchers can further build on our techniques to develop principled improvements to TD.
There are a number of research directions one can take to extend our work. First, we use a projection step
for analysis under the Markov chain model, a choice we borrowed from the optimization literature to simplify
our analysis. It will be interesting to find alternative ways to add regularity to the TD algorithm and establish
similar convergence results; we think analysis without the projection step is possible if one can show that
the iterates remain bounded under additional regularity conditions. Second, the O [˜] (1/T ) convergence rate
we showed used step-sizes which crucially depends on the minimum eigenvalue ω of the feature covariance
matrix, which would need to be estimated from samples. While such results are common in optimization for
strongly convex functions, very recently Lakshminarayanan and Szepesv´ari [2018] showed TD(0) with iterate
averaging and universal constant step-sizes can attain an O [˜] (1/T ) convergence rate in the i.i.d. sampling
model. Extending our analysis for problem independent, robust step-size choices is a research direction
worth pursuing.

## References


Leif Andersen and Mark Broadie. Primal-dual simulation algorithm for pricing multidimensional american
options. Management Science, 50(9):1222–1234, 2004.


Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-residual
minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89–129,
2008.


Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence
rate o(1/n). In Advances in neural information processing systems 26, pages 773–781, 2013.


Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In Proceedings
of the 12th International Conference on Machine Learning, pages 30–37. 1995.


Albert Benveniste, Michel M´etivier, and Pierre Priouret. Adaptive algorithms and stochastic approximations,
volume 22. Springer Science & Business Media, 2012.


30




Dimitri P Bertsekas. Dynamic programming and optimal control, volume 2. Athena Scientific, 2012.


Dimitri P Bertsekas and Steven E Shreve. Stochastic optimal control: the discrete time case. 1978.


Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer, 2009.


Vivek S Borkar and Sean P Meyn. The ODE method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000.


L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning.
SIAM Review, 60(2):223–311, 2018.


Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning.
Machine learning, 22(1-3):33–57, 1996.


S´ebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends⃝ R in Machine
Learning, 8(3-4):231–357, 2015.


Gal Dalal, Balzs Szrnyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td(0) with function approximation, 2018a. URL
[https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16392.](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16392)


Gal Dalal, Gugan Thoppe, Balzs Szrnyi, and Shie Mannor. Finite sample analysis of two-timescale stochastic
approximation with applications to reinforcement learning. In Proceedings of the 31st Conference On
Learning Theory, pages 1199–1233, 2018b.


Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences: A survey
and comparison. The Journal of Machine Learning Research, 15(1):809–883, 2014.


Daniela Pucci De Farias and Benjamin Van Roy. The linear programming approach to approximate dynamic
programming. Operations research, 51(6):850–865, 2003.


Vijay V Desai, Vivek F Farias, and Ciamac C Moallemi. Pathwise optimization for optimal stopping problems. Management Science, 58(12):2292–2308, 2012.


Adithya M Devraj and Sean P Meyn. Zap q-learning. In Advances in Neural Information Processing Systems
30, pages 2235–2244, 2017.


Mohammad Ghavamzadeh, Alessandro Lazaric, Odalric Maillard, and R´emi Munos. LSTD with Random
Projections. In Advances in Neural Information Processing Systems 23, pages 721–729, 2010.


David A Goldberg and Yilun Chen. Beating the curse of dimensionality in options pricing and optimal
stopping. arXiv preprint arXiv:1807.02227, 2018.


L´aszl´o Gy¨orfi and Harro Walk. On the averaged stochastic approximation for linear regression. SIAM Journal
on Control and Optimization, 34(1):31–61, 1996.


Martin B Haugh and Leonid Kogan. Pricing american options: A duality approach. Operations Research, 52
(2):258–270, 2004.


Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. Convergence of stochastic iterative dynamic
programming algorithms. In Advances in neural information processing systems 7, pages 703–710, 1994.


Prateek Jain and Purushottam Kar. Non-convex optimization for machine learning. Foundations and Trends⃝ R
in Machine Learning, 10(3-4):142–336, 2017.


Vijay R Konda. Actor-Critic Algorithms. PhD thesis, Massachusetts Institute of Technology, 2002.


Nathaniel Korda and Prashanth La. On TD(0) with function approximation: Concentration bounds and a
centered variant with exponential convergence. In Proceedings of the 32nd International Conference on
Machine Learning, pages 626–634, 2015.


Harold Kushner. Stochastic approximation: A survey. Wiley Interdisciplinary Reviews: Computational
Statistics, 2(1):87–96, 2010.


31




Harold Kushner and Gang G Yin. Stochastic approximation and recursive algorithms and applications,
volume 35. Springer Science & Business Media, 2003.


Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002, 2012.


Chandrashekar Lakshminarayanan and Csaba Szepesv´ari. Finite Time Bounds for Temporal Difference
Learning with Function Approximation: Problems with some “state-of-the-art” results, 2017. URL
[https://sites.ualberta.ca/˜szepesva/papers/TD-issues17.pdf.](https://sites.ualberta.ca/~szepesva/papers/TD-issues17.pdf)


Chandrashekar Lakshminarayanan and Csaba Szepesv´ari. Linear stochastic approximation: How far does
constant step-size and iterate averaging go? In Proceedings of the 21st International Conference on
Artificial Intelligence and Statistics, pages 1347–1355, 2018.


Alessandro Lazaric, Mohammad Ghavamzadeh, and R´emi Munos. Finite-sample analysis of LSTD. In
Proceedings of the 27th International Conference on Machine Learning, pages 615–622, 2010.


David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathematical
Society, 2017.


Bo Liu, Mohammad Liu, Ji ]and Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik. Finite-sample analysis of proximal gradient td algorithms. In Proceedings of the 31st Conference on Uncertainty in Artificial
Intelligence, pages 504–513, 2015.


Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529, 2015.


Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.

Bernardo A Pires and Csaba Szepesv´ari. Statistical linear estimation with penalized estimators: An appli- [´]
cation to reinforcement learning. In Proceedings of the 29th International Coference on International
Conference on Machine Learning, pages 1755–1762, 2012.


La Prashanth, Nathaniel Korda, and R´emi Munos. Fast LSTD using stochastic approximation: Finite time
analysis and application to traffic control. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 66–81, 2013.


Robert E Schapire and Manfred K Warmuth. On the worst-case analysis of temporal-difference learning
algorithms. Machine Learning, 22(1-3):95–121, 1996.


Harm Seijen and Richard S Sutton. True online td (lambda). In Proceedings of the 31st International
Conference on Machine Learning, pages 692–700, 2014.


Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):9–44,
1988.


Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998.


Richard S Sutton, Hamid R Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesv´ari, and
Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function approximation. In Proceedings of the 26th International Conference on Machine Learning, pages 993–1000,
2009a.


Richard S Sutton, Hamid R Maei, and Csaba Szepesv´ari. A convergent o(n) temporal-difference algorithm
for off-policy learning with linear function approximation. In Advances in neural information processing
systems 21, pages 1609–1616, 2009b.


Ahmed Touati, Pierre-Luc Bacon, Doina Precup, and Pascal Vincent. Convergent TREE BACKUP and
RETRACE with function approximation. In Proceedings of the 35th International Conference on Machine
Learning, pages 4962–4971, 2018.


32




John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674 – 690, 1997.


John N Tsitsiklis and Benjamin Van Roy. Optimal stopping of markov processes: Hilbert space theory,
approximation algorithms, and an application to pricing high-dimensional financial derivatives. IEEE
Transactions on Automatic Control, 44(10):1840 – 1851, 1999.


Stephen Tu and Benjamin Recht. Least-squares temporal difference learning for the linear quadratic regulator.
In Proceedings of the 35th International Conference on Machine Learning, pages 5012–5021, 2018.


Benjamin Van Roy. Learning and value function approximation in complex decision processes. PhD thesis,
Massachusetts Institute of Technology, 1998.


Huizhen Yu and Dimitri P Bertsekas. Convergence results for some temporal difference methods based on
least squares. IEEE Transactions on Automatic Control, 54(7):1515–1531, 2009.


33




## A Analysis of Projected TD(0) under the Markov chain sampling model

In this section, we complete the proof of Theorem 3. The first subsection restates the theorem, as well as the
two key lemmas from Section 8 that underly the proof. The second subsection contains a proof of Theorem
3. Finally, Subsection A.3 contains the proof of a technical result, Lemma 10, which was omitted from the
main text but we need for the proof.


A.1 Restatement of the theorem and key lemmas from the main text


Theorem 3. Suppose the Projected TD algorithm is applied with parameter R ≥∥θ [∗] ∥ 2 under the Markov
chain observation model with Assumption 1. Set G = (r max + 2R). Then the following claims hold.


(a) With a constant step-size sequence α 0 = · · · = α T = 1/√T,



E ���V θ ∗ − V θ¯ T �� 2D



∥θ [∗] − θ 0 ∥ [2] 2 [+][ G] [2] [ �] 9 + 12τ [mix] (1/√
≤
�



2√



T (1 − γ) .



T )
�



(b) With a constant step-size sequence α 0 = · · · = α T < 1/(2ω(1 − γ)),



�



E ∥θ [∗] − θ T ∥ [2] 2 ≤ e [−][2][α] [0] [(1][−][γ][)][ωT] [�] ∥θ [∗] − θ 0 ∥ [2] 2 [+][ α] [0]
� � �



G [2] [ �] 9 + 12τ [mix] (α 0 )�

2(1 − γ)ω

�



.



(c) With a decaying step-size sequence α t = 1/(ω(t + 1)(1 − γ)) for all t ∈ N 0,



E ���V θ ∗ − V θ¯ T �� 2D



9 + 24τ [mix] (α T )�
≤ [G] [2] [ �] (1 + log T ),
� T (1 − γ) [2] ω



The key to our proof is the following lemmas, which were establish in Section 8. Recall the definition of
the gradient error ζ t (θ) ≡ (g t (θ) − g¯(θ)) [⊤] (θ − θ [∗] ).


Lemma 8. With probability 1, for every t ∈ N 0,


∥θ [∗] − θ t+1 ∥ 2 [2] [≤∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 [−][2][α] [t] [(1][ −] [γ][)][∥][V] [θ] [∗] [−] [V] [θ] t [∥] D [2] [+2][α] [t] [ζ] [t] [(][θ] [t] [) +][ α] [2] t [G] [2] [.]


Lemma 11 (Bound on gradient bias). Consider a non-increasing step-size sequence, α 0 ≥ α 1 . . . ≥ α T . Fix
any t < T, and set t [∗] ≡ max{0, t − τ [mix] (α T )}. Then,


E [ζ t (θ t )] ≤ G [2] [ �] 4 + 6τ [mix] (α T )� α t ∗ .


The following bound also holds:

t−1
E [ζ t (θ t )] ≤ 6G [2] � α i .


i=0


A.2 Proof of Theorem 3.


We now complete the proof of Theorem 3. The proof directly uses Lemma 8 and Lemma 11.


Proof. From Lemma 8, we have


E ∥θ [∗] − θ t+1 ∥ [2] 2 ≤ E ∥θ [∗] − θ t ∥ [2] 2 − 2α t (1 − γ)E ∥V θ ∗ − V θ t ∥ [2] D + 2α t E [ζ t (θ t )] + α [2] t [G] [2] [.] (35)
� � � � � �


34




Proof of part (a): We first show the analysis for a constant step-size and iterate averaging. Considering
α t = α 0 = 1/√T in Equation (35), rearranging terms and summing from t = 0 to t = T − 1, we get



T −1
� E [ζ t (θ t )] .


t=0



T −1
� �E �∥θ [∗] − θ t ∥ [2] 2 � − E �∥θ [∗] − θ t+1 ∥ [2] 2 �� + G [2] + 2α 0

t=0



2α 0 (1 − γ)



T −1
� E �∥V θ [∗] − V θ t ∥ [2] D � ≤

t=0



Using Lemma 11 (in which α t [∗] = α 0 in this case) and simplifying, we find



T −1
�



− θ 0 ∥ 2 [+][ G] [2] + [T][ ·][ 2][G] [2] [(2 + 3][τ] [ mix] [(1][/] √

2α 0 (1 − γ) (1 − γ)




[τ] [ mix] [(1][/] √T ))α 0

(1 − γ)



T −1
� t=0 E �∥V θ ∗ − V θ t ∥ [2] D � ≤ ∥θ [∗] 2α− 0 θ(1 0 ∥ − [2] 2 [+] γ) [ G] [2]



T �∥θ [∗] − θ 0 ∥ [2] 2 [+][ G] [2] [�]



√



(2 + 3τ (1/√T ))


.
(1 − γ)



√



T · 2G [2] (2 + 3τ [mix] (1/√



=



+
2(1 − γ)



This gives us our desired result,



2√



T (1 − γ) .



T )
�



T −1 ∥θ [∗] − θ 0 ∥ [2] 2 [+][ G] [2] [ �] 9 + 12τ [mix] (1/√
� t=0 E �∥V θ ∗ − V θ t ∥ [2] D � ≤ 2√T (1 − γ)



T −1
�



E ���V θ ∗ − V θ¯ T �� 2D



� ≤ T [1]



Proof of part (b): The proof is analogous to part (b) of Theorem 2. Consider a constant step-size of
α 0 < 1/(2ω(1 − γ)). Starting with Equation (35) and applying Lemma 1, which showed ∥V θ [∗] − V θ ∥ [2] D [≥]
ω∥θ [∗] − θ∥ [2] 2 [for all][ θ][, we get]


E ∥θ [∗] − θ t+1 ∥ [2] 2 ≤ (1 − 2α 0 (1 − γ)ω) E ∥θ [∗] − θ t ∥ [2] 2 + α [2] 0 [G] [2] [ + 2][α] [0] [E][ [][ζ] [t] [(][θ] [t] [)]]
� � � �

≤ (1 − 2α 0 (1 − γ)ω) E �∥θ [∗] − θ t ∥ [2] 2 � + α [2] 0 [G] [2] [ �] 9 + 12τ [mix] (α 0 )�,


where we used Lemma 11 to go to the second inequality. Iterating over this inequality gives us our final
result. For any T ∈ N 0,

E �∥θ [∗] − θ T ∥ [2] 2 � ≤ (1 − 2α 0 (1 − γ)ω) [T] ∥θ [∗] − θ 0 ∥ [2] 2 [+][ α] 0 [2] [G] [2] [ �] 9 + 12τ [mix] (α 0 )� � [∞] (1 − 2α 0 (1 − γ)ω) [t]

t=0

≤ e [−][2][α] [0] [(1][−][γ][)][ωT] [ �] ∥θ [∗] − θ 0 ∥ [2] 2 [+][ α] [0] [G] [2] [ �] 9 + 12τ [mix] (α 0 )� .
� 2(1 − γ)ω


The second inequality above follows by solving the geometric series and then using the fact that (1 − 2α 0 (1 − γ)ω) ≤
e [−][2][α] [0] [(1][−][γ][)][ω] .


Proof of part (c): We now show the analysis for a linearly decaying step-size using Equation (35) as our
starting point. We again use Lemma 1, which showed ∥V θ ∗ − V θ ∥ [2] D [≥] [ω][∥][θ] [∗] [−] [θ][∥] [2] 2 [for all][ θ][, to get,]



1
E ∥V θ ∗ − V θ t ∥ [2] D ≤
� � (1 − γ)α t



(1 − (1 − γ)ωα t )E ∥θ [∗] − θ t ∥ [2] 2 − E ∥θ [∗] − θ t+1 ∥ [2] 2 + α [2] t [G] [2] [�] +
� � � � �


2
(1 − γ) [E][ [][ζ] [t] [(][θ] [t] [)]][ .]



1
Consider a decaying step-size α t = ω(t+1)(1−γ) [, simplify and sum from][ t][ = 0][ to][ T][ −] [1][ to get]



T −1
�



G [2]
+



T −1
�


t=0



1 2
t + 1 [+] (1 − γ)



T −1
� E [ζ t (θ t )] .


t=0


(36)



� E �∥V θ [∗] − V θ t ∥ [2] D � ≤ −ωT E �∥θ [∗] − θ T ∥ [2] 2 �

t=0 � �



� �� �
<0



G [2]
+

ω(1 − γ) [2]



35




To simplify notation, for the remainder of the proof put τ = τ [mix] (α T ). We can decompose the sum of
gradient errors as



T −1
� E [ζ t (θ t )] =


t=0



τ
� E [ζ t (θ t )] +


t=0



τ
�



T −1
� E [ζ t (θ t )] . (37)

t=τ +1



T −1
�



1
We will upper bound each term. In each case we use that, since α t = ω(t+1)(1−γ) [,]



T −1

1

� t=0 α t = ω(1 − γ)



T −1
�


t=0



1
(t + 1) [≤] [1 + log] ω(1 − γ [ T] ) [.]



Combining this with Lemma 11 gives,



6G [2] τ
≤
ω(1 − γ) [(1 + log][ T][ )][.]

�



�



τ
� E [ζ t (θ t )] ≤


t=0



τ
�


t=0



�



t−1
6G [2] α i
�


i=0



t−1
6G [2] �



≤ τ



�



T −1
6G [2] α i
�


i=0



T −1
6G [2] �



Similarly, using Lemma 11, we have


T −1
� E [ζ t (θ t )] ≤ 2G [2] (2 + 3τ )

t=τ +1


Combining the two parts, we get



T −1
� α t−τ ≤ 2G [2] (2 + 3τ )

t=τ +1



T −1
�



(1 + log T ) .
ω(1 − γ)



−
� t=1 α t ≤ [2][G] ω [2] (1 [ (2 + 3] − γ) [τ] [)]



T −1
�



(1 + log T ).
ω(1 − γ)



−
� t=0 E [ζ t (θ t )] ≤ [4][G] ω [2] (1 [ (1 + 3] − γ) [τ] [)]



Using this in conjunction with Equation (36) we give final result.



G 2

ωT (1 − γ) [2] [ (1 + log][ T][ ) +] T (1 − γ)



T −1
� E [ζ t (θ t )] .


t=0



T −1
�



− G [2]
� t=0 E �∥V θ t − V θ ∗ ∥ [2] D � ≤ ωT (1



E ���V θ ∗ − V θ¯ T �� 2D



� ≤ T [1]



Simplifying and substituting τ = τ [mix] (α T ), we get



(1 + log T )
ωT (1 − γ) [2]



E ���V θ ∗ − V θ¯ T �� 2D



G [2]
≤
� ωT



G [2] 1 + 3τ [mix] (α T )�

ωT (1 − γ) [2] [ (1 + log][ T][ ) + 8][G] [2] [ �] ωT (1 − γ) [2]



G [2] [ �] 9 + 24τ [mix] (α T )�
≤ (1 + log T ) .

ωT (1 − γ) [2]



Additionally, Equation (36) also gives us a convergence rate of O(log T/T ) for the iterate θ T itself:

E ∥θ [∗] − θ T ∥ [2] 2 ≤ G [2] [ �] 9 + 24τ [mix] (α T )� (1 + log T ) .
� � ω [2] T (1 − γ) [2]


A.3 Proof of Lemma 10


Lemma 10 (Gradient error is bounded and Lipschitz). With probability 1,


|ζ t (θ)|≤ 2G [2] for all θ ∈ Θ R


and

′
|ζ t (θ) − ζ t (θ [′] )| ≤ 6G (θ − θ ) for all θ, θ [′] ∈ Θ R .
��� ��� 2


36




Proof. The first claim follows from a simple argument using Lemma 6.


|ζ t (θ)| = (g t (θ) − g¯(θ)) ⊤ (θ − θ ∗ ) ≤ (∥g t (θ)∥ 2 + ∥g¯(θ)∥ 2 ) (∥θ∥ 2 + ∥θ ∗ ∥ 2 ) ≤ 4GR ≤ 2G 2,
��� ���


where the first inequality follows from the triangle inequality and the Cauchy-Schwartz inequality, and the
final inequality uses that R ≤ G/2 by definition of G = r max + 2R.
To establish the second claim, consider the following inequality for any vectors (a 1, b 1, a 2, b 2 ):

��a ⊤1 [b] [1] [−] [a] [⊤] 2 [b] [2] �� = ��a ⊤1 [(][b] [1] [−] [b] [2] [) +][ b] [⊤] 2 [(][a] [1] [−] [a] [2] [)] �� ≤∥a 1 ∥∥b 1 − b 2 ∥ + ∥b 2 ∥∥a 1 − a 2 ∥.


This follows as a direct application of Cauchy-Schwartz. It implies that for any θ, θ [′] ∈ Θ R,


|ζ t (θ) − ζ t (θ [′] )| = (g t (θ) − g¯(θ)) ⊤ (θ − θ ∗ ) − (g t (θ ′ ) − g¯(θ ′ )) ⊤ (θ ′ − θ ∗ )
��� ���


¯ ¯ ¯
≤ ∥g t (θ) − g(θ)∥ 2 ∥θ − θ [′] ∥ 2 + ∥θ [′] − θ [∗] ∥ 2 ∥(g t (θ) − g(θ)) − (g t (θ [′] ) − g(θ [′] ))∥ 2
≤ 2G∥θ − θ [′] ∥ 2 + 2R (∥g t (θ) − g t (θ [′] )∥ 2 + ∥g¯(θ) − g¯(θ [′] )∥ 2 )

≤ 2G∥θ − θ [′] ∥ 2 + 8R∥θ − θ [′] ∥ 2
≤ 6G∥θ − θ [′] ∥ 2 .


where we used that R ≤ G/2 by the definition of G. We also used that both g t (·) and ¯g(·) are 2-Lipschitz
functions which is easy to see. Starting with g t (θ) = (r t + γφ(s [′] t [)] [⊤] [θ][ −] [φ][(][s] [t] [)] [⊤] [θ][)][φ][(][s] [t] [)][, consider]


′
∥g t (θ) − g t (θ [′] )∥ 2 = φ(s t ) (γφ(s t [)][ −] [φ][(][s] [t] [))] [⊤] [(][θ][ −] [θ] [′] [)]
��� ��� 2
≤ ∥φ(s t )∥ 2 ∥(γφ(s [′] t [)][ −] [φ][(][s] [t] [))][∥] 2 [∥][(][θ][ −] [θ] [′] [)][∥] 2
≤ 2∥(θ − θ [′] )∥ 2 .


Similarly, following Equation (2), we have ∥g¯(θ) − g¯(θ [′] )∥ 2 = E [φ (γφ ′ − φ)] ⊤ (θ − θ ′ )
��� ��� 2 [, where][ φ][ =]

φ(s) is the feature vector of a random initial state s ∼ π, φ [′] = φ(s [′] ) is the feature vector of a random next
state drawn according to s [′] ∼P(· | s). Therefore,


¯ ¯ ′ ⊤ ′ ′
∥g(θ) − g(θ [′] )∥ 2 ≤ φ (γφ − φ) (θ − θ ) ≤ 2∥(θ − θ )∥ 2 .
��� ���


37




## B Analysis of Projected TD(λ) under Markov chain observation model

In this section, we give a detailed proof of the convergence bounds presented in Theorem 4. Subsection B.1
details our proof strategy along with key lemmas which come together in Subsection B.2 to establish the
results. We begin by providing mathematical expressions for TD(λ) updates.


Stationary distribution of TD(λ) updates: Recall that the projected TD(λ) update at time t is given by:


θ t+1 = Π 2,R (θ t + α t x t (θ t, z 0:t ))


where Π 2,R (·) denotes the projection operator onto a norm ball of radius R < ∞ and x t (θ t, z 0:t ) is the
update direction. Let us now give explicit mathematical expressions for x t (θ, z 0:t ) and its steady-state mean
x¯(θ). Note that these are analogous to the expressions for the negative gradient g t (θ) and its steady-state
expectation ¯g(θ) for TD(0). At time t, as a general function of (non-random) θ and the tuple O t = (s t, r t, s [′] t [)]
along with the eligibility trace term z 0:t, we have


x t (θ, z 0:t ) = �r t + γφ(s [′] t [)] [⊤] [θ][ −] [φ][(][s] [t] [)] [⊤] [θ] � z 0:t = δ t (θ)z 0:t ∀ θ ∈ R [d] .


The asymptotic convergence of TD(λ) is closely related to the expected value of x t (θ, z 0:t ) under the steadystate behavior of (O t, z 0:t ),
x¯(θ) = lim
t→∞ [E][ [][δ] [t] [(][θ][)][z] [0:][t] []][ .]


Rather than take this limit, it will be helpful in our analysis to think of an equivalent backward view by constructing a stationary process with mean ¯x(θ). Consider a stationary sequence of states (. . ., s −1, s 0, s 1, . . .)
and set z −∞:t = [�] [∞] k=0 [(][γλ][)] [k] [φ][(][s] [t][−][k] [)][. Then the sequence][ (][x] [0] [(][θ, z] [−∞][:0] [)][, x] [1] [(][θ, z] [−∞][:1] [)][, . . .][)][ is stationary,]
and we have


x¯(θ) = E [δ t (θ)z −∞:t ] . (38)


It should be emphasized that ¯x(θ) and the states (. . ., s −2, s −1 ) are introduced only for the purposes of our
analysis and are never used by the algorithm itself. However, this turns out to be quite useful as it is easy to
show (Van Roy, 1998) that


x¯(θ) = Φ [⊤] D �T µ [(][λ][)] Φθ − Φθ�, (39)


where Φ is the feature matrix and (T µ [(][λ][)] [Φ][θ][ −] [Φ][θ][)][ denotes the Bellman error defined with respect to the]
Bellman operator T µ [(][λ][)] [(][·][)][, corresponding to a policy][ µ][. Careful readers will notice the stark similarity between]
Equation (39) and Equation (3). Exploiting the property that Π D T µ [λ] [(][·][)][ is also a contraction operator, one can]
easily show a result equivalent to Lemma 3, thus quantifying the progress we make by taking steps in the
direction of ¯x(θ). The rest of our proof essentially shows how to control for the observation noise, i.e. the
fact that we use x t (θ, z 0:t ) rather than ¯x(θ) to make updates. To remind the readers of the results, we first
restate Theorem 4 below.


Theorem 4. Suppose the Projected TD(λ) algorithm is applied with parameter R ≥∥θ [∗] ∥ 2 under the Markov
chain observation model with Assumption 1. Set B = [(][r] (1 [max] − [+2] γλ) [R][)] [. Then the following claims hold.]


(a) With a constant step-size α t = α 0 = 1/√T,



E ���V θ ∗ − V θ¯ T �� 2D



∥θ [∗] − θ 0 ∥ [2] 2 [+][ B] [2] [ �] 13 + 28τ λ [mix] (1/√
≤
�



2√



T (1 − κ) .



T )
�



38




(b) With a constant step-size α t = α 0 < 1/(2ω(1 − κ)) and T > 2τ λ [mix] (α 0 ),



.

�



E ∥θ [∗] − θ T ∥ [2] 2 ≤ e [−][2][α] [0] [(1][−][κ][)][ωT] [ �] ∥θ [∗] − θ 0 ∥ [2] 2 [+][ α] [0]
� � �


(c) With a decaying step-size α t = 1/(ω(t + 1)(1 − κ)),



B [2] [ �] 13 + 24τ λ [mix] (α 0 )�

2(1 − κ)ω

�



E ���V θ ∗ − V θ¯ T �� 2D



13 + 52τ λ [mix] (α T )�
≤ [B] [2] [ �] (1 + log T ) .
� T (1 − κ) [2] ω



B.1 Proof strategy and key lemmas


We now describe our proof strategy and give key lemmas used to establish Theorem 4. Throughout, we
consider the Markov chain observation model with Assumption 1 and study the Projected TD (λ) algorithm
applied with parameter R ≥∥θ [∗] ∥ 2 and step-size sequence (α 0, . . ., α T ). To simplify our exposition, we
introduce some notation below.


Notation: We specify the notation used throughout this section. Define the set Θ R = {θ ∈ R [d] : ∥θ∥ 2 ≤
R}, so θ t ∈ Θ R for each t because of the algorithm’s projection step. Next, we generically define z l:t =
� tk−=0l [(][γλ][)] [k] [φ][(][s] [t][−][k] [)][ for any lower limit][ l][ ≤] [t][. Thus,][ z] [l][:][t] [ denotes the eligibility trace as a function of the]
states (s l, . . ., s t ). Next, we define ζ t (θ, z l:t ) as a general function of θ and z l:t,


¯
ζ t (θ, z l:t ) = (δ t (θ)z l:t − x(θ)) [⊤] (θ − θ [∗] ). (40)


Here, the subscript t in ζ t encodes the dependence on the tuple O t = (s t, r t, s [′] t [)][ which is used to compute]
the Bellman error, δ t (·) at time t. Finally, we set B := (r max + 2R)/(1 − γλ) which implies B > R/2, a
fact we use many times in our proofs to simplify constant terms. As a reminder, note that our bounds depend
on the mixing time, which we defined in Section 9 as


τ λ [mix] (ǫ) = max{τ [MC] (ǫ), τ [Algo] (ǫ)},


where τ [MC] (ǫ) = min{t ∈ N 0 | mρ [t] ≤ ǫ} and τ [Algo] (ǫ) = min{t ∈ N 0 | (γλ) [t] ≤ ǫ}.


Proof outline: The analysis for TD(λ) can be broadly divided into three parts and closely mimics the steps
used to prove TD(0) results.


1. As a first step, we do an error decomposition, similar to the result shown in Lemma 8. This is enabled
by two key lemmas, which are analogues of Lemma 3 and Lemma 6 for Projected TD(0). The first
one spells out a clear relationship of how the updates following ¯x(θ) point in the descent direction
of ∥θ [∗] − θ∥ 2 [2] [while the second one upper bounds the norm of the update direction,][ x] [t] [(][θ, z] [0:][t] [)][, by the]
constant B (as defined above).


2. The error decomposition that we obtain from Step 1 can be stated as:


E[∥θ [∗] − θ t+1 ∥ [2] 2 []][ ≤] [E][[][∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 []][ −] [2][α] [t] [(1][ −] [κ][)][E][[][∥][V] [θ] [∗] [−] [V] [θ] [t] [∥] [2] D [] + 2][α] [t] [E][[][ζ] [t] [(][θ] [t] [, z] [0:][t] [)] +][ α] t [2] [B] [2] [.]


In the second step, we establish an upper bound on the bias term, E [ζ t (θ t, z 0:t )], which is the main
challenge in our proof. Recall that the dependent nature of the state transitions may result in strong
coupling between the tuples O t−1 and O t under the Markov chain observation model. Therefore, this
bias in update direction can potentially be non-zero. Presence of the eligibility trace term, z 0:t, which
is a function of the entire history of states, (s 0, . . ., s t ), further complicates the analysis by introducing
subtle dependencies.


39




To control for this, we use information-theoretic techniques shown in Lemma 9 which exploit the
geometric ergodicity of the MDP, along with the geometric weighting of state features in the eligibility
trace term. Our result essentially shows that the bias scales the noise in update direction by a factor of
the mixing time. Mathematically, for a constant step-size α, we show that E [αζ t (θ t, z 0:t )] ≈ B [2] (6 +
12τ λ [mix] (α))α [2] . We show a similar result for decaying step-sizes as well.


3. In the final step, we combine the error decomposition from Step 1 and the bound on the bias from
Step 2, to establish finite time bounds on the performance of Projected TD(λ) for different step-size
choices. We closely mimic the analysis of Nemirovski et al. (2009) for a constant, aggressive step-size
of (1/√T ) and the proof ideas of Lacoste-Julien et al. (2012) for decaying step-sizes.


B.1.1 Error decomposition under Projected TD(λ)


We first prove Lemmas 16 and 17 which enable the error decomposition shown in Lemma 18.


Lemma 16. [Tsitsiklis and Van Roy (1997)] Let V θ ∗ be the unique fixed point of Π D T µ [(][λ][)] (·) i.e. V θ ∗ =
ΠT µ [(][λ][)] V θ ∗ . For any θ ∈ R [d],

¯
(θ [∗] − θ) [⊤] x(θ) ≥ (1 − κ)∥V θ ∗ − V θ ∥ [2] D [.]


Proof. We use the definition of ¯x(θ) = ⟨Φ [⊤], T µ [(][λ][)] Φθ − Φθ⟩ D as shown in Equation (39) along with the
fact that Π D T µ [(][λ][)] [(][·][)][ is a contraction with respect to][ ∥·∥] D [with modulus][ κ][. See Appendix][ C][ for a complete]
proof.


Lemma 17. For all θ ∈ Θ R, ∥x t (θ, z 0:t )∥ 2 ≤ B with probability 1. Additionally, ∥x¯(θ)∥ 2 ≤ B.


Proof. See Subsection B.3 for a complete proof.


The above two lemmas can be easily combined to establish a recursion for the error under projected
TD(λ) that holds for each sample path.


Lemma 18. With probability 1, for every t ∈ N 0,


∥θ [∗] − θ t+1 ∥ [2] 2 [≤∥][θ] [∗] [−] [θ] [t] [∥] [2] 2 [−] [2][α] [t] [(1][ −] [κ][)][∥][V] [θ] [∗] [−] [V] [θ] [t] [∥] [2] D [+ 2][α] [t] [ζ] [t] [(][θ] [t] [, z] [0:][t] [) +][ α] t [2] [B] [2] [.]


Proof. The Projected TD(λ) algorithm updates the parameter as: θ t+1 = Π 2,R [θ t + α t x t (θ t, z 0:t )] ∀ t ∈ N 0 .
This implies,


∥θ [∗] − θ t+1 ∥ [2] 2 = ∥θ [∗] − Π 2,R (θ t + α t x t (θ, z 0:t ))∥ [2] 2
= ∥Π 2,R (θ [∗] ) − Π 2,R (θ t + α t x t (θ t, z 0:t ))∥ [2] 2
≤ ∥θ [∗] − θ t − α t x t (θ t, z 0:t )∥ [2] 2
= ∥θ [∗] − θ t ∥ [2] 2 [−] [2][α] [t] [x] [t] [(][θ] [t] [, z] [0:][t] [)] [⊤] [(][θ] [∗] [−] [θ] [t] [) +][ α] t [2] [∥][x] [t] [(][θ] [t] [, z] [0:][t] [)][∥] [2] 2
≤ ∥θ [∗] − θ t ∥ [2] 2 [−] [2][α] [t] [x] [t] [(][θ] [t] [, z] [0:][t] [)] [⊤] [(][θ] [∗] [−] [θ] [t] [) +][ α] t [2] [B] [2]

= ∥θ [∗] − θ t ∥ 2 [2] [−][2][α] [t] [x][¯][(][θ] [t] [)] [⊤] [(][θ] [∗] [−] [θ] [t] [) + 2][α] [t] [ζ] [t] [(][θ] [t] [, z] [0:][t] [) +][ α] [2] t [G] [2] [.]

≤ ∥θ [∗] − θ t ∥ [2] 2 [−] [2][α] [t] [(1][ −] [κ][)][∥][V] [θ] [∗] [−] [V] [θ] [∥] [2] D [+ 2][α] [t] [ζ] [t] [(][θ] [t] [, z] [0:][t] [) +][ α] t [2] [B] [2] [.]


The first inequality used that orthogonal projection operators onto a convex set are non-expansive, the second
used Lemma 17 together with the fact ∥θ t ∥ 2 ≤ R due to projection, and the third used Lemma 16. Note that
we used ζ t (θ t, z 0:t ) to simplify the notation for the error in the update direction. Recall the definition of the
error function from Equation (40) which implies,


¯ ¯
ζ t (θ t, z 0:t ) = (δ t (θ t )z 0:t − x(θ t )) [⊤] (θ t − θ [∗] ) = (x t (θ t, z 0:t ) − x(θ t )) [⊤] (θ t − θ [∗] ).


40




B.1.2 Upper bound on the bias in update direction.


We give an upper bound on the expected error in the update direction, E[ζ t (θ t, z 0:t )], which as explained
above, is the key challenge for our analysis. For this, we first establish some basic regularity properties of the
error function ζ t (·, ·) in Lemma 19 below. In particular, part (a) shows boundedness, part (b) shows that it is
Lipschitz in the first argument and part (c) bounds the error due to truncation of the eligibility trace. Recall
that z l:t denotes the eligibility trace as a function of the states (s l, . . ., s t ).


Lemma 19. Consider any l ≤ t and any θ, θ [′] ∈ Θ R . With probability 1,


(a) |ζ t (θ, z l:t )| ≤ 2B [2] .


′
(b) |ζ t (θ, z l:t ) − ζ t (θ [′], z l:t )| ≤ 6B (θ − θ )
��� ��� 2 [.]


(c) The following two bounds also hold,


|ζ t (θ, z 0:t ) − ζ t (θ, z t−τ :t )| ≤ B [2] (γλ) [τ] for all τ ≤ t,
|ζ t (θ, z 0:t ) − ζ t (θ, z −∞:t )| ≤ B [2] (γλ) [t] .


Proof. We essentially use the uniform bound on x t (θ, z 0:t ) and ¯x(θ) as stated in Lemma 17 to show this
result. See Subsection B.3 for a detailed proof.


Lemma 19 can be combined with Lemma 9 to give an upper bound on the bias term, E [ζ t (θ t, z 0:t )], as
shown below.


Lemma 20. Consider a non-increasing step-size sequence, α 0 ≥ α 1 . . . ≥ α T . Then the following hold.


(a) For 2τ λ [mix] (α T ) < t ≤ T,


E [ζ t (θ t, z 0:t )] ≤ 6B [2] (1 + 2τ λ [mix] (α T ))α t−2τ mixλ (α T ) [.]


(b) For 0 ≤ t ≤ 2τ λ [mix] (α T ),


E [ζ t (θ t, z 0:t )] ≤ 6B [2] [ �] 1 + 2τ λ [mix] (α T )� α 0 + B [2] (γλ) [t] .


(c) For all t ∈ N 0,


t−1
E [ζ t (θ t, z 0:t )] ≤ 6B [2] � α i + B [2] (γλ) [t]


i=0


Proof. We proceed in two cases below. Throughout the proof, results from Lemma 19 are applied using the
fact that θ t ∈ Θ R, because of the algorithm’s projection step.


Case (a): Let t > 2τ and consider the following decomposition for all τ ∈{0, 1, . . ., t/2}. We show an
upper bound on each of the three terms separately.


E [ζ t (θ t, z 0:t )] ≤|E [ζ t (θ t, z 0:t )] − E[ζ t (θ t−2τ, z 0:t )]| + |E[ζ t (θ t−2τ, z 0:t )] − E[ζ t (θ t−2τ, z t−τ :t )]| + |E[ζ t (θ t−2τ, z t−τ :t )]|.


Step 1: Use regularity properties of the error function to bound first two terms.

We relate ζ t (θ t, z 0:t ) and ζ t (θ t−τ, z 0:t ) using the Lipschitz property shown in part (b) of Lemma 19 to get,


t−1
|ζ t (θ t, z 0:t ) − ζ t (θ t−2τ, z 0:t )| = 6B∥θ t − θ t−2τ ∥ 2 ≤ 6B [2] � α i . (41)


i=t−2τ


41




Taking expectations on both sides gives us the desired bound on the first term. The last inequality used the
norm bound on update direction as shown in Lemma 17 to simplify,



t−1
� α i .


i=t−2τ



t−1
� α i ∥x i (θ i, z 0:i )∥ 2 ≤ B


i=t−2τ



∥θ t − θ t−2τ ∥ 2 ≤



t−1
� ∥Π 2,R (θ i+1 + α i x i (θ i, z 0:i )) − θ i ∥ 2 ≤


i=t−2τ



Similarly, by part (c) of Lemma 19, we have a bound on the second term.


|E[ζ t (θ t−2τ, z 0:t )] − E[ζ t (θ t−2τ, z t−τ :t )]| ≤ B [2] (γλ) [τ] . (42)


Step 2: Use information-theoretic arguments to upper bound E[ζ t (θ t−2τ, z t−τ :t )].
We will essentially use Lemma 9 to upper bound E[ζ t (θ t−2τ, z t−τ :t )]. We first introduce some notation to
highlight subtle dependency issues. Note that ζ t (θ t−2τ, z t−τ :t ) is a function of (θ t−2τ, s t−τ, . . ., s t−1, O t ).
To simplify, let Y t−τ :t = (s t−τ, . . ., s t−1, O t ). Define,


f (θ t−2τ, Y t−τ :t ) := ζ t (θ t−2τ, z t−τ :t ).


Consider random variables θ t [′] −2τ [and][ Y] t [ ′] −τ :t [drawn independently from the marginal distributions of][ θ] [t][−][2][τ]
and Y t−τ :t, so P(θ t [′] −2τ [=][ ·][, Y] t [ ′] −τ :t [=][ ·][) =][ P][(][θ] [t][−][τ] [=][ ·][)][ ⊗] [P][(][Y] [t][−][τ] [:][t] [=][ ·][)][. By Lemma][ 19][ we have that]
|f (θ, Y t−τ :t )| ≤ 2B [2] for all θ ∈ Θ R with probability 1. As


θ t−2τ → s t−2τ → s t−τ → s t → O t


form a Markov chain, a direct application of Lemma 9 gives us:

′ 2 τ
��E[f (θ t−2τ, Y t−τ :t )] − E[f (θ t−2τ [, Y] t [ ′] −τ :t [)]] �� ≤ 4B mρ . (43)


We also have the following bound for all fixed θ ∈ Θ R . Using ¯x(θ) = E[δ t (θ)z −∞:t ], we get


¯ ⊤ ∗ 2 τ
E[f (θ, Y t−τ :t )] = (E[δ t (θ)z t−τ :t ] − x(θ)) [⊤] (θ − θ [∗] ) ≤ (δ t (θ)z −∞:t−τ ) (θ − θ ) ≤ B (γλ)
��� ���


Combining the above with Equation (43), we get


|E[ζ t (θ t−2τ, z t−τ :t )]| = |E[f (θ t−2τ, Y t−τ :t )]|

′ ′
≤ ��E[f (θ t−2τ, Y t−τ :t )] − E[f (θ t−2τ [, Y] t [ ′] −τ :t [)]] �� + ��E[f (θ t−2τ [, Y] t [ ′] −τ :t [)]] ��

≤ 4B [2] mρ [τ] + ��E[E[f (θ t′−2τ [, Y] t [ ′] −τ :t [)][|][θ] t [′] −2τ []]] ��

≤ 4B [2] mρ [τ] + B [2] (γλ) [τ] . (44)


Step 3. Combine terms to show part (a) of our claim.


Taking τ = τ λ [mix] (α T ) and combining Equations (41), (42) and (44) establishes the first claim.


t−1
E [ζ t (θ t, z 0:t )] ≤ 6B [2] � α i + 4B [2] mρ [τ] + 2B [2] (γλ) [τ] ≤ 12B [2] τ λ [mix] (α T )α t−2τ mixλ (α T ) [+ 6][B] [2] [α] [T]

i=t−2τ

≤ 6B [2] (1 + 2τ [mix] (α T ))α t−2τ mixλ (α T ) [.]


Here we used that letting τ = τ λ [mix] (α T ) implies: max{mρ [τ], (γλ) [τ] } ≤ α T . Two additional facts which we
also use follow from a non-increasing step-size sequence, [�] [t] i= [−] t [1] −2τ [α] [i] [ ≤] [2][τα] [t][−][2][τ] [ and][ α] [T] [ ≤] [α] [t][−][2][τ] [.]


42




Case (b): Consider the following decomposition for all t ∈ N 0,


E [ζ t (θ t, z 0:t )] ≤|E [ζ t (θ t, z 0:t )] − E[ζ t (θ 0, z 0:t )]| + |E[ζ t (θ 0, z 0:t )] − E[ζ t (θ 0, z −∞:t )]| + |E[ζ t (θ 0, z −∞:t )]|.


Step 1: Use regularity properties of the error function to upper bound the first two terms.

Using parts (b), (c) of Lemma 19 and following the arguments shown in Step 1, 2 of case (a) above, we get


t−1
|E [ζ t (θ t, z 0:t )] − E[ζ t (θ 0, z 0:t )]| + |E[ζ t (θ 0, z 0:t )] − E[ζ t (θ 0, z −∞:t )]| ≤ 6B [2] � α i + B [2] (γλ) [t] . (45)


i=0


Step 2: Characterizing E[ζ t (θ, z −∞:t )] for any fixed (non-random) θ.
Recall the definition of ¯x(θ) from Equation (38). For any fixed (non-random) θ, we have ¯x(θ) = E[δ t (θ)z −∞:t ].
Therefore,


E[ζ t (θ 0, z −∞:t )] = (E[δ t (θ 0 )z −∞:t ] − x¯(θ 0 )) [⊤] (θ 0 − θ [∗] ) = 0. (46)


Step 3. Combine terms to show parts (b), (c) of our claim.

Combining Equations (45) and (46) establishes part (c) which states,


t−1
E [ζ t (θ t, z 0:t )] ≤ 6B [2] � α i + B [2] (γλ) [t] ∀ t ∈ N 0 .


i=0


We establish part (b) by using that the step-size sequence is non-increasing which implies: [�] [t] i=0 [−][1] [α] [i] [ ≤] [tα] [0] [.]
For all t ≤ 2τ λ [mix] (α T ), we have the following loose upper bound.

E [ζ t (θ t, z 0:t )] ≤ 6B [2] tα 0 + B [2] (γλ) [t] ≤ 6B [2] [ �] 1 + 2τ λ [mix] (α T ))� α 0 + B [2] (γλ) [t] .


B.2 Proof of Theorem 4


In this subsection, we establish convergence bounds for Projected TD(λ) as stated in Theorem 4 using Lemmas 18 and 20. From Lemma 18 we have,


E ∥θ [∗] − θ t+1 ∥ [2] 2 ≤ E ∥θ [∗] − θ t ∥ [2] 2 − 2α t (1 − κ)E ∥V θ ∗ − V θ t ∥ [2] D + 2α t E [ζ t (θ t, z 0:t )] + α [2] t [B] [2] [.] (47)
� � � � � �


Equation (47) will be used as a starting point for analyzing different step-size choices.


Proof of part (a): Fix a constant step-size of α 0 = . . . = α t = 1/√T in Equation (47), rearrange terms

and sum from t = 0 to t = T − 1, we get



T −1
�


t=0



T −1
� E [ζ t (θ t, z 0:t )] .


t=0



2α 0 (1 − κ)



T −1
� E �∥V θ ∗ − V θ t ∥ [2] D � ≤

t=0



E ∥θ [∗] − θ t ∥ [2] 2 − E ∥θ [∗] − θ t+1 ∥ [2] 2 + B [2] + 2α 0
� � � � ��



Using Lemma 20 where α t−2τ mixλ (α T ) [=][ α] [0] [along with the fact that][ (][γλ][)][ <][ 1][, we simplify to get]



T −1
�



− θ 0 ∥ [2] 2 [+][ B] [2] λ (1/√

+ [T][ ·][ 6][B] [2] [(1 + 2][τ] [ mix]
2α 0 (1 − κ) (1 − κ)




[τ] λ [ mix] (1/√T ))α 0 + 1

(1 − κ) (1 − κ)



T −1
� t=0 E �∥V θ [∗] − V θ t ∥ [2] D � ≤ ∥θ [∗] 2α− 0 θ(1 0 ∥ − [2] 2 [+] κ) [ B] [2]



2τ λ [mix] (1/√



(1/√T )

�



� B [2] (γλ) [t]


t=0



√



λ [ mix] (1/√T )


.
(1 − κ)



τ λ [mix] (1/√T )) + [2][B] [2] [τ] λ [ mix] (1/√

(1 − κ) (1 − κ)



≤



T �∥θ [∗] − θ 0 ∥ [2] 2 [+][ B] [2] [�]



+
2(1 − κ)



√



T · 6B [2] (1 + 2τ λ [mix] (1/√



Adding these terms, we conclude



2√



T (1 − κ) .



T )
�



T −1 ∥θ [∗] − θ 0 ∥ [2] 2 [+][ B] [2] [ �] 13 + 28τ λ [mix] (1/√
� t=0 E �∥V θ ∗ − V θ t ∥ [2] D � ≤ 2√T (1 − κ)



T −1
�



E ���V θ ∗ − V θ¯ T �� 2D



� ≤ T [1]



43




Proof of part (b): For a constant step-size of α 0 < 1/(2ω(1 − κ)), we show that the expected distance
between the iterate θ T and the TD(λ) limit point, θ [∗] converges at an exponential rate below some level that
depends on the choice of step-size and λ. Starting with Equation (47) and applying Lemma 1 which shows
that ∥V θ ∗ − V θ ∥ [2] D [≥] [w][∥][θ] [∗] [−] [θ][∥] [2] 2 [for any][ θ][, we have that for all][ t >][ 2][τ] λ [ mix] (α 0 ),


E ∥θ [∗] − θ t+1 ∥ [2] 2 ≤ (1 − 2α 0 (1 − κ)ω) E ∥θ [∗] − θ t ∥ [2] 2 + α [2] 0 [B] [2] [ + 2][α] [0] [E][ [][ζ] [t] [(][θ] [t] [, z] [0:][t] [)]]
� � � �

≤ (1 − 2α 0 (1 − κ)ω) E �∥θ [∗] − θ t ∥ [2] 2 � + α [2] 0 [B] [2] [ �] 13 + 24τ λ [mix] (α 0 )�,


where we used part (a) of Lemma 20 for the second inequality. Iterating over it gives us our final result. For
any T > 2τ λ [mix] (α 0 ),


E �∥θ [∗] − θ T ∥ [2] 2 � ≤ (1 − 2α 0 (1 − κ)ω) [T] ∥θ [∗] − θ 0 ∥ [2] 2 [+][ B] [2] [�] α [2] 0 �13 + 24τ λ [mix] (α 0 )� [�] � [∞] (1 − 2α 0 (1 − κ)ω) [t]

t=0

B [2] [�] α 0 �13 + 24τ λ [mix] (α 0 )� [�]
≤ e [−][2][α] [0] [(1][−][κ][)][ωT] [ �] ∥θ [∗] − θ 0 ∥ [2] 2 [+] .
� 2(1 − κ)ω


The second inequality follows by solving the geometric series and using that (1 − 2α 0 (1 − κ)ω) ≤ e [−][2][α] [0] [(1][−][κ][)][ω] .


Proof of part (c): Consider a decaying step-size of α t = 1/(ω(t + 1)(1 − κ)). We start with Equation (47)

and use Lemma 1 which showed E ∥V θ [∗] − V θ ∥ [2] D ≥ wE ∥θ [∗] − θ∥ [2] 2 for all θ to get,
� � � �



1
E ∥V θ ∗ − V θ t ∥ [2] D ≤
� � (1 − κ)α t



(1 − (1 − κ)ωα t )E ∥θ [∗] − θ t ∥ [2] 2 − E ∥θ [∗] − θ t+1 ∥ [2] 2 + α [2] t [B] [2] [�] +
� � � � �


2
(1 − κ) [E][ [][ζ] [t] [(][θ] [t] [, z] [0:][t] [)]][ .]



1
Substituting α t = ω(t+1)(1−κ) [, simplify and sum from][ t][ = 0][ to][ T][ −] [1][ to get,]



T −1
�



ω(1 − κ) [2]



− B [2]
� t=0 E �∥V θ [∗] − V θ t ∥ [2] D � ≤ −ωT E �∥θ [∗] − θ T ∥ [2] 2 � + ω(1 −



T −1
� E [ζ t (θ t, z 0:t )]


t=0



T −1
�


t=0



1 2
t + 1 [+] (1 − κ)



T −1
� E [ζ t (θ t, z 0:t )], (48)


t=0



≤ −ωT E ∥θ [∗] − θ T ∥ [2] 2
� �

� �� �
<0



2

+ [B] [2] [(1 + log][ T][ )] +

ω(1 − κ) [2] (1 − κ)



where we used that [�] [T] t=0 [ −][1] t+11 [≤] [(1 + log][ T][ )][. To simplify notation, we put][ τ][ =][ τ] λ [ mix] (α T ) for the remainder

of the proof. We use Lemma 20 to upper bound the total bias, [�] [T] t=0 [ −][1] [E][ [][ζ] [t] [(][θ] [t] [, z] [0:][t] [)]][ which can be decomposed]

as:



T −1
� E [ζ t (θ t, z 0:t )] =


t=0



2τ
� E [ζ t (θ t, z 0:t )] +


t=0



T −1
� E [ζ t (θ t, z 0:t )] . (49)

t=2τ +1



1
First, note that for a decaying step-size α t = ω(t+1)(1−γ) [we have]



T −1

1

� t=0 α t = ω(1 − γ)



T −1
�


t=0


44



1
(t + 1) [≤] [1 + log] ω(1 − γ [ T] ) [.]




We will combine this with Lemma 20 to upper bound each term separately. First,



+

�



2τ
� B [2] (γλ) [t]


t=0



2τ
� E [ζ t (θ t, z 0:t )] ≤


t=0



2τ
�


t=0



�



t−1
6B [2] � α i


i=0



t−1
6B [2] �



6B [2]
≤

ω(1 − κ)



2τ T −1
� �


t=0 i=0



1 14B [2] τ
(i + 1) [+ 2][B] [2] [τ][ ≤] ω(1 − κ) [(1 + log][ T][ )][,]



where we used the fact that ω, κ, (γλ) < 1. Similarly,



T −1
� E [ζ t (θ t, z 0:t )] ≤ 6B [2] (1 + 2τ )

t=2τ +1


Combining the two parts, we get



T −1
� α t−2τ ≤ 6B [2] (1 + 2τ )

t=2τ +1



T −1
�



T −1
�



−
� t=0 α t ≤ [6][B] ω [2] (1 [(1 + 2] − κ) [τ] [)]



(1 + log T ) .
ω(1 − κ)



T −1
�



(1 + log T ).
ω(1 − κ)



−
� t=0 E [ζ t (θ t, z 0:t )] ≤ [B] ω [2] [(6 + 26] (1 − κ) [τ] [)]



Using this in conjunction with Equation (48) we get our final result,



B 2

ωT (1 − κ) [2] [ (1 + log][ T][ ) +] T (1 − κ)



T −1
� E [ζ t (θ t, z 0:t )] .


t=0



T −1
�



− B [2]
� t=0 E �∥V θ t − V θ ∗ ∥ [2] D � ≤ ωT (1



E ���V θ ∗ − V θ¯ T �� 2D



� ≤ T [1]



Simplifying and putting back τ = τ λ [mix] (α T ), we get



(1 + log T )
ωT (1 − κ) [2]



E ���V θ ∗ − V θ¯ T �� 2D



B [2]
≤
� ωT



B [2] 6 + 26τ λ [mix] (α T )�

ωT (1 − κ) [2] [ (1 + log][ T][ ) + 2][B] [2] [ �] ωT (1 − κ) [2]



B [2] [ �] 13 + 52τ [mix] (α T )�
≤ (1 + log T ).

ωT (1 − κ) [2]



Additionally, Equation (48) implies a convergence rate of O(log T/T ) for the iterate θ T itself:

E ∥θ [∗] − θ T ∥ [2] 2 ≤ B [2] [ �] 13 + 52τ λ [mix] (α T )� (1 + log T ) .
� � ω [2] T (1 − κ) [2]


B.3 Proof of supporting lemmas.


In this subsection, we provide standalone proofs of Lemma 17 and 19 used above.


Lemma 17. For all θ ∈ Θ R, ∥x t (θ, z 0:t )∥ 2 ≤ B with probability 1. Additionally, ∥x¯(θ)∥ 2 ≤ B.


Proof. We start with the mathematical expression for x t (θ, z 0:t ).


x t (θ, z 0:t ) = δ t (θ)z 0:t ⇒∥x t (θ, z 0:t )∥ 2 = |δ t (θ)|∥z 0:t ∥ 2 .


We give an upper bound on both |δ t (θ)| and ∥z 0:t ∥ 2 . Starting with the definition of δ t (θ) and using that
∥φ(s t )∥ 2 ≤ 1 ∀ t along with ∥θ∥ 2 ≤ R, we get


′ ′
|δ t (θ)| = ��r t + γφ(s t [)] [⊤] [θ][ −] [φ][(][s] [t] [)] [⊤] [θ] �� ≤ r max + ∥φ(s t [)][∥] 2 [∥][θ][∥] 2 [+][ ∥][φ][(][s] [t] [)][∥] 2 [∥][θ][∥] 2 [≤] [(][r] [max] [+ 2][R][)][ .]


Next,



�����



2



∥z 0:t ∥ [2] 2 [=]



�����



t
�



�(γλ) [k] φ(s t−k )


k=0



≤

2



t 2
�(γλ) [k]
� k=0 �


45



≤



∞ 2 1


=

�� k=0 (γλ) [k] � (1 − γλ) [2] [ .]




Combining these two implies the first part of our claim.


∥x t (θ, z 0:t )∥ 2 = |δ t (θ)|∥z 0:t ∥ 2 ≤ [(][r] [max] [ + 2][R][)] = B.

(1 − γλ)


Note that can easily show an upper bound ∥δ t (θ)z l:t ∥ 2 ≤ B for any pair (θ, z l:t ) with l ≤ t. Consider,



∥z l:t ∥ [2] 2 ≤ ∥z −∞:t ∥ [2] 2 [≤]



∞ 2
�(γλ) [k]
� k=0 �



1

=
(1 − γλ) [2]



⇒∥δ t (θ)z l:t ∥ 2 = |δ t (θ)|∥z l:t ∥ 2 ≤ [(][r] [max] [ + 2][R][)] = B.

(1 − γλ)



Taking l →−∞ implies that ∥δ t (θ)z −∞:t ∥ 2 ≤ B. As ¯x(θ) = E [δ t (θ)z −∞:t ], we also have a uniform norm
bound on the expected updates, ∥x¯(θ)∥ 2 ≤ B, as claimed.


Lemma 19. Consider any l ≤ t and any θ, θ [′] ∈ Θ R . With probability 1,


(a) |ζ t (θ, z l:t )| ≤ 2B [2] .


′
(b) |ζ t (θ, z l:t ) − ζ t (θ [′], z l:t )| ≤ 6B (θ − θ )
��� ��� 2 [.]


(c) The following two bounds also hold,


|ζ t (θ, z 0:t ) − ζ t (θ, z t−τ :t )| ≤ B [2] (γλ) [τ] for all τ ≤ t,
|ζ t (θ, z 0:t ) − ζ t (θ, z −∞:t )| ≤ B [2] (γλ) [t] .


Proof. Throughout, we use the assumption that basis vectors are normalized i.e. ∥φ(s t )∥ 2 ≤ 1 ∀ t.


Part (a): We show a uniform norm bound on ζ t (θ, z l:t ) ∀ θ ∈ Θ R . First consider the following:



∥δ t (θ)z l:t ∥ 2 = |δ t (θ)|∥z l:t ∥ 2 ≤ ��r t + γφ(s ′t [)] [⊤] [θ][ −] [φ][(][s] [t] [)] [⊤] [θ] ��

�����



t−l
�(γλ) [k] φ(s t−k )


k=0



t−l
�



����� 2



����� 2



≤ |r t + ∥φ(s [′] t [)][∥] 2 [∥][θ][∥] 2 [+][ ∥][φ][(][s] [t] [)][∥] 2 [∥][θ][∥] 2 [|]



�����



∞
�



�(γλ) [k] φ(s t−k )


k=0



≤ (r max + 2R) = B.

(1 − γλ)


¯
Using this along with the fact that ∥θ − θ [∗] ∥ 2 ≤ 2R ≤ B and ∥x(θ)∥ 2 ≤ B for all θ ∈ Θ R, we get


¯ ⊤ ∗ ¯
|ζ t (θ, z l:t )| = (δ t (θ)z l:t − x(θ)) (θ − θ ) ≤ ∥δ t (θ)z l:t − x(θ)∥ 2 ∥(θ − θ [∗] )∥ 2
��� ���


¯
≤ (∥δ t (θ)z l:t ∥ 2 + ∥x(θ)∥ 2 ) ∥(θ − θ [∗] )∥ 2
≤ 2B∥(θ − θ [∗] )∥ 2 ≤ 2B [2] .


Part (b): To show that ζ t (·, z l:t ) is L-Lipschitz, consider the following inequality for any four vectors
(a 1, b 1, a 2, b 2 ), which follows as a direct application of Cauchy-Schwartz.

��a ⊤1 [b] [1] [−] [a] [⊤] 2 [b] [2] �� = ��a ⊤1 [(][b] [1] [−] [b] [2] [) +][ b] [⊤] 2 [(][a] [1] [−] [a] [2] [)] �� ≤∥a 1 ∥ 2 ∥b 1 − b 2 ∥ 2 + ∥b 2 ∥ 2 ∥a 1 − a 2 ∥ 2 .


46




This implies,


|ζ t (θ, z l:t ) − ζ t (θ [′], z l:t )| = (δ t (θ)z l:t − x¯(θ)) ⊤ (θ − θ ∗ ) − (δ t (θ ′ )z l:t − x¯(θ ′ )) ⊤ (θ ′ − θ ∗ )
��� ���


¯ ¯ ¯
≤ ∥δ t (θ)z l:t − x(θ)∥ 2 ∥θ − θ [′] ∥ 2 + ∥θ [′] − θ [∗] ∥ 2 ∥(δ t (θ)z l:t − x(θ)) − (δ t (θ [′] )z l:t − x(θ [′] ))∥ 2


≤ 2B∥θ − θ [′] ∥ 2 + 2R ∥z l:t (δ t (θ) − δ t (θ [′] ))∥ 2 + ∥x¯(θ) − x¯(θ [′] )∥ 2
� �


8R
≤ 2B∥θ − θ [′] ∥ 2 +
(1 − γλ) [∥][θ][ −] [θ] [′] [∥] [2]

≤ 6B∥θ − θ [′] ∥ 2,


R
where the last inequality follows as 1−γλ [≤] [B/][2][ by definition. In the penultimate inequality, we used that]
2
∥z l:t (δ t (θ) − δ t (θ [′] ))∥ 2 ≤ (1−γλ) [∥][θ][ −] [θ] [′] [∥] 2 [which is easy to prove. Consider,]


∥z l:t (δ t (θ) − δ t (θ [′] ))∥ 2 ≤ ∥z l:t ∥ 2 |(δ t (θ) − δ t (θ [′] ))|



|(δ t (θ) − δ t (θ [′] ))|
����� 2



≤



�����



∞
�



�(γλ) [k] φ(s t−k )


k=0



1
≤
(1 − γλ)



′
(γφ(s t [)][ −] [φ][(][s] [t] [))] [⊤] [(][θ][ −] [θ] [′] [)]
��� ���



(∥φ(s [′] t [)][∥] 2 [+][ ∥][φ][(][s] [t] [)][∥] 2 [)] 2
≤ ∥θ − θ [′] ∥ 2 ≤

(1 − γλ) (1 − γλ) [∥][θ][ −] [θ] [′] [∥] [2] [.]



As ¯x(θ) = E [δ t (θ)z −∞:t ], this also implies that ∥x¯(θ) − x¯(θ [′] )∥ 2 ≤ (1−2γλ) [∥][θ][ −] [θ] [′] [∥] 2 [which completes the]
proof.


Part (c): To show that |ζ t (θ, z 0:t ) − ζ t (θ, z t−τ :t )| ≤ B [2] (γλ) [τ] for all θ ∈ Θ R and τ ≤ t, we use that
∥θ − θ [∗] ∥ 2 ≤ 2R ≤ B.


|ζ t (θ, z 0:t ) − ζ t (θ, z t−τ :t )| = (δ t (θ)z 0:t − δ t (θ)z t−τ :t ) ⊤ (θ − θ ∗ )
��� ���

≤ |δ t (θ)|∥z 0:t − z t−τ :t ∥ 2 ∥θ − θ [∗] ∥ 2



B
����� 2



′
≤ ��r t + γφ(s t [)] [⊤] [θ][ −] [φ][(][s] [t] [)] [⊤] [θ] ��



�����



∞
�



�(γλ) [k] φ(s t−k )


k=τ



Similarly,



(γλ) [τ]
≤ |r t + 2∥θ∥ 2 | ·

(1 − γλ) [·][ B]

≤ B [(][r] [max] [ + 2][R][)] (γλ) [τ] = B [2] (γλ) [τ] .

(1 − γλ)


⊤ ∗
|ζ t (θ, z 0:t ) − ζ t (θ, z −∞:t )| ≤ ��δ t (θ)(z 0:t − z −∞:t ) (θ − θ )��

≤ |δ t (θ)|∥z 0:t − z −∞:t ∥ 2 ∥θ − θ [∗] ∥ 2



�����



B
����� 2



≤ ���r t + γφ(s [′] t [)] [⊤] [θ][ −] [φ][(][s] [t] [)] [⊤] [θ] ���



∞
�



�(γλ) [k] φ(s t−k )


k=t



≤ B [(][r] [max] [ + 2][R][)] (γλ) [t] ≤ B [2] (γλ) [t] .

(1 − γλ)


47




## C Proofs of Lemmas 13 and 16

In this section, we give a combined proof of Lemmas 13 and 16 which quantify the progress of the expected
updates towards the limit point θ [∗] for TD(λ) and the Q-function approximation algorithm. These lemmas can
be stated more generally as shown below, instead of using the Bellman operators F (·) and T [(][λ][)] (·).


Lemma 21. Let Π D H(·) be a contraction with respect to ∥·∥ D with modulus γ and let V θ ∗ be the unique
fixed point of Π D H(·), i.e. V θ ∗ = Π D HV θ ∗ . Define ¯g(θ) = Φ [⊤] D (HΦθ − Φθ) for all θ ∈ R [d] to be the
expected update. Then,

¯
(θ [∗] − θ) [⊤] g(θ) ≥ (1 − γ)∥V θ ∗ − V θ ∥ [2] D [.]


Proof. We have


¯
(θ [∗] − θ) [⊤] g(θ) = (θ [∗] − θ) [⊤] Φ [⊤] D (HΦθ − Φθ)
= ⟨Φ(θ [∗] − θ), (HΦθ − Φθ)⟩ D

= ⟨Π D Φ(θ [∗] − θ), (HΦθ − Φθ)⟩ D (50)
= ⟨Φ(θ [∗] − θ), Π D (HΦθ − Φθ)⟩ D (51)


= ⟨Φ(θ [∗] − θ), Π D HΦθ − Φθ⟩ D
= ⟨Φ(θ [∗] − θ), Π D HΦθ − Φθ [∗] + Φθ [∗] − Φθ⟩ D
= ∥Φ(θ [∗] − θ)∥ D [2] [−⟨][Φ(][θ] [∗] [−] [θ][)][,][ Φ][θ] [∗] [−] [Π] [D] [H][Φ][θ][⟩] [D]
≥ ∥Φ(θ [∗] − θ)∥ [2] D [−∥][Φ(][θ] [∗] [−] [θ][)][∥] [D] [·∥][Π] [D] [H][Φ][θ][ −] [Φ][θ] [∗] [∥] [D]
≥ ∥Φ(θ [∗] − θ)∥ D [2] [−][γ][ · ∥][Φ(][θ] [∗] [−] [θ][)][∥] [2] D (52)

= (1 − γ) · ∥Φ(θ [∗] − θ)∥ [2] D [= (1][ −] [γ][)][ · ∥][V] [θ] [∗] [−] [V] [θ] [∥] D [2] [,]


where in going to Equation (50), we used that ∀ x ∈ Span(Φ), we have Π D x = x. In Equation (51), we
used that the projection matrix Π D is symmetric. In going to Equation (52), we used that that Π D H(·) is a
contraction operator with modulus γ with Φθ [∗] as its fixed point, which implies that ∥Π D HΦθ − Φθ [∗] ∥ D =
∥Π D HΦθ − Π D HΦθ [∗] ∥ D ≤ γ∥Φθ − Φθ [∗] ∥ D .


48


