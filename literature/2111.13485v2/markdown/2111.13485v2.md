Published as a conference paper at ICLR 2022

## L EARNING L ONG -T ERM R EWARD R EDISTRIBUTION VIA R ANDOMIZED R ETURN D ECOMPOSITION


**Zhizhou Ren** [1] **, Ruihan Guo** [2] **, Yuan Zhou** [3] **, Jian Peng** [145]

1 University of Illinois at Urbana-Champaign 2 Shanghai Jiao Tong University
3 BIMSA 4 AIR, Tsinghua University 5 HeliXon Limited
zhizhour@illinois.edu, guoruihan@sjtu.edu.cn
timzhouyuan@gmail.com, jianpeng@illinois.edu


A BSTRACT


Many practical applications of reinforcement learning require agents to learn from
sparse and delayed rewards. It challenges the ability of agents to attribute their actions to future outcomes. In this paper, we consider the problem formulation of
episodic reinforcement learning with trajectory feedback. It refers to an extreme
delay of reward signals, in which the agent can only obtain one reward signal at
the end of each trajectory. A popular paradigm for this problem setting is learning
with a designed auxiliary dense reward function, namely proxy reward, instead
of sparse environmental signals. Based on this framework, this paper proposes a
novel reward redistribution algorithm, randomized return decomposition (RRD),
to learn a proxy reward function for episodic reinforcement learning. We establish
a surrogate problem by Monte-Carlo sampling that scales up least-squares-based
reward redistribution to long-horizon problems. We analyze our surrogate loss
function by connection with existing methods in the literature, which illustrates
the algorithmic properties of our approach. In experiments, we extensively evaluate our proposed method on a variety of benchmark tasks with episodic rewards
and demonstrate substantial improvement over baseline algorithms.


1 I NTRODUCTION


Scaling reinforcement learning (RL) algorithms to practical applications has become the focus of
numerous recent studies, including resource management (Mao et al., 2016), industrial control (Hein
et al., 2017), drug discovery (Popova et al., 2018), and recommendation systems (Chen et al., 2018).
One of the challenges in these real-world problems is the sparse and delayed environmental rewards.
For example, in the molecular structure design problem, the target molecule property can only be
evaluated after completing the whole sequence of modification operations (Zhou et al., 2019b). The
sparsity of environmental feedback would complicate the attribution of rewards on agent actions and
therefore can hinder the efficiency of learning (Rahmandad et al., 2009). In practice, it is a common
choice to formulate the RL objective with a meticulously designed reward function instead of the
sparse environmental rewards. The design of such a reward function is crucial to the performance of
the learned policies. Most standard RL algorithms, such as temporal difference learning and policy
gradient methods, prefer dense reward functions that can provide instant feedback for every step
of environment transitions. Designing such dense reward functions is not a simple problem even
with domain knowledge and human supervision. It has been widely observed in prior works that
handcrafted heuristic reward functions may lead to unexpected and undesired behaviors (Randløv
& Alstrøm, 1998; Bottou et al., 2013; Andrychowicz et al., 2017). The agent may find a shortcut
solution that formally optimizes the given objective but deviates from the desired policies (Dewey,
2014; Amodei et al., 2016). The reward designer can hardly anticipate all potential side effects of
the designed reward function, which highlights the difficulty of reward engineering.


To avoid the unintended behaviors induced by misspecified reward engineering, a common paradigm
is considering the reward design as an online problem within the trial-and-error loop of reinforcement learning (Sorg et al., 2010). This algorithmic framework contains two components, namely
reward modeling and policy optimization. The agent first learns a proxy reward function from the
experience data and then optimizes its policy based on the learned per-step rewards. By iterating this


1




Published as a conference paper at ICLR 2022


procedure and interacting with the environment, the agent is able to continuously refine its reward
model so that the learned proxy reward function can better approximate the actual objective given by
the environmental feedback. More specifically, this paradigm aims to reshape the sparse and delayed
environmental rewards to a dense Markovian reward function while trying to avoid misspecifying
the goal of given tasks.


In this paper, we propose a novel reward redistribution algorithm based on a classical mechanism
called _return decomposition_ (Arjona-Medina et al., 2019). Our method is built upon the leastsquares-based return decomposition (Efroni et al., 2021) whose basic idea is training a regression
model that decomposes the trajectory return to the summation of per-step proxy rewards. This
paradigm is a promising approach to redistributing sparse environmental feedback. Our proposed
algorithm, _randomized return decomposition_ (RRD), establish a surrogate optimization of return decomposition to improve the scalability in long-horizon tasks. In this surrogate problem, the reward
model is trained to predict the episodic return from a random subsequence of the agent trajectory,
i.e., we conduct a structural constraint that the learned proxy rewards can approximately reconstruct
environmental trajectory return from a small subset of state-action pairs. This design enables us to
conduct return decomposition effectively by mini-batch training. Our analysis shows that our surrogate loss function is an upper bound of the original loss of deterministic return decomposition,
which gives a theoretical interpretation of this randomized implementation. We also present how
the surrogate gap can be controlled and draw connections to another method called uniform reward
redistribution. In experiments, we demonstrate substantial improvement of our proposed approach
over baseline algorithms on a suite of MuJoCo benchmark tasks with episodic rewards.


2 B ACKGROUND


2.1 E PISODIC R EINFORCEMENT L EARNING WITH T RAJECTORY F EEDBACK


In standard reinforcement learning settings, the environment model is usually formulated by a
_Markov decision process_ (MDP; Bellman, 1957), defined as a tuple _M_ = _⟨S, A, P, R, µ⟩_, where
_S_ and _A_ denote the spaces of environment states and agent actions. _P_ ( _s_ _[′]_ _|s, a_ ) and _R_ ( _s, a_ ) denote
the unknown environment transition and reward functions. _µ_ denotes the initial state distribution.
The goal of reinforcement learning is to find a policy _π_ : _S →A_ maximizing cumulative rewards.
More specifically, a common objective is maximizing infinite-horizon discounted rewards based on
a pre-defined discount factor _γ_ as follows:



_s_ 0 _∼_ _µ, s_ _t_ +1 _∼_ _P_ ( _· | s_ _t_ _, π_ ( _s_ _t_ ))
����� �



(standard objective) _J_ ( _π_ ) = E



_∞_
� _γ_ _[t]_ _R_ ( _s_ _t_ _, π_ ( _s_ _t_ ))
� _t_ =0



_._ (1)



In this paper, we consider the episodic reinforcement learning setting with trajectory feedback, in
which the agent can only obtain one reward feedback at the end of each trajectory. Let _τ_ denote
an agent trajectory that contains all experienced states and behaved actions within an episode. We
assume all trajectories terminate in finite steps. The episodic reward function _R_ ep ( _τ_ ) is defined on
the trajectory space, which represents the overall performance of trajectory _τ_ . The goal of episodic
reinforcement learning is to maximize the expected trajectory return:


(episodic objective) _J_ ep ( _π_ ) = E _R_ ep ( _τ_ ) _s_ 0 _∼_ _µ, a_ _t_ = _π_ ( _s_ _t_ ) _, τ_ = _⟨s_ 0 _, a_ 0 _, s_ 1 _, · · ·, s_ _T_ _⟩_ _._ (2)
� ���� �


In general, the episodic-reward setting is a particular form of _partially observable Markov decision_
_processes_ (POMDPs) where the reward function is non-Markovian. The worst case may require the
agent to enumerate the entire exponential-size trajectory space for recovering the episodic reward
function. In practical problems, the episodic environmental feedback usually has structured representations. A common structural assumption is the existence of an underlying Markovian reward
function _R_ [�] ( _s, a_ ) that approximates the episodic reward _R_ ep ( _τ_ ) by a sum-form decomposition,



(sum-decomposable episodic reward) _R_ ep ( _τ_ ) _≈_ _R_ [�] ep ( _τ_ ) =



_T −_ 1


�

� _R_ ( _s_ _t_ _, a_ _t_ ) _._ (3)


_t_ =0



This structure is commonly considered by both theoretical (Efroni et al., 2021) and empirical studies
(Liu et al., 2019; Raposo et al., 2021) on long-horizon episodic rewards. It models the situations


2




Published as a conference paper at ICLR 2022


where the agent objective is measured by some metric with additivity properties, e.g., the distance
of robot running, the time cost of navigation, or the number of products produced in a time interval.


2.2 R EWARD R EDISTRIBUTION


The goal of _reward redistribution_ is constructing a proxy reward function _R_ [�] ( _s_ _t_ _, a_ _t_ ) that transforms
the episodic-reward problem stated in Eq. (2) to a standard dense-reward setting. By replacing
environmental rewards with such a Markovian proxy reward function _R_ [�] ( _s_ _t_ _, a_ _t_ ), the agent can be
trained to optimize the discounted objective in Eq. (1) using any standard RL algorithms. Formally,
the proxy rewards _R_ [�] ( _s_ _t_ _, a_ _t_ ) form a sum-decomposable reward function _R_ [�] ep ( _τ_ ) = [�] _[T]_ _t_ =0 _[ −]_ [1] _[R]_ [�][(] _[s]_ _[t]_ _[, a]_ _[t]_ [)]
that is expected to have high correlation to the environmental reward _R_ ep ( _τ_ ). Here, we introduce
two branches of existing reward redistribution methods, _return decomposition_ and _uniform reward_
_redistribution_, which are the most related to our proposed approach. We defer the discussions of
other related work to section 5.


**Return Decomposition.** The idea of _return decomposition_ is training a reward model that predicts the trajectory return with a given state-action sequence (Arjona-Medina et al., 2019). In this
paper, without further specification, we focus on the least-squares-based implementation of return
decomposition (Efroni et al., 2021). The reward redistribution is given by the learned reward model,
i.e., decomposing the environmental episodic reward� _R_ ep ( _τ_ ) to a Markovian proxy reward function
_R_ ( _s, a_ ). In practice, the reward modeling is formulated by optimizing the following loss function:



_T −_ 1 2 [�]


�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )

_t_ =0 �



_L_ RD ( _θ_ ) = E
_τ_ _∼D_



_R_ ep ( _τ_ ) _−_
��



_,_ (4)



where _R_ [�] _θ_ denotes the parameterized proxy reward function, _θ_ denotes the parameters of the learned
reward model, and _D_ denotes the experience dataset collected by the agent. Assuming the sumdecomposable structure stated in Eq. (3), _R_ [�] _θ_ ( _s, a_ ) is expected to asymptotically concentrate near the
ground-truth underlying rewards _R_ [�] ( _s, a_ ) when Eq. (4) is properly optimized (Efroni et al., 2021).


One limitation of the least-squares-based return decomposition method specified by Eq. (4) is its
scalability in terms of the computation costs. Note that the trajectory-wise episodic reward is the
only environmental supervision for reward modeling. Computing the loss function _L_ RD ( _θ_ ) with a
single episodic reward label requires to enumerate all state-action pairs along the whole trajectory.
This computation procedure can be expensive in numerous situations, e.g., when the task horizon _T_
is quite long, or the state space _S_ is high-dimensional. To address this practical barrier, recent works
focus on designing reward redistribution mechanisms that can be easily integrated in complex tasks.
We will discuss the implementation subtlety of existing methods in section 4.


**Uniform Reward Redistribution.** To pursue a simple but effective reward redistribution mechanism, IRCR (Gangwani et al., 2020) considers _uniform reward redistribution_ which assumes all
state-action pairs equally contribute to the return value. It is designed to redistribute rewards in the
absence of any prior structure or information. More specifically, the proxy reward _R_ [�] IRCR ( _s, a_ ) is
computed by averaging episodic return values over all experienced trajectories containing ( _s, a_ ),



�
_R_ IRCR ( _s, a_ ) = E
_τ_ _∼D_



� _R_ ep ( _τ_ ) _|_ ( _s, a_ ) _∈_ _τ_ � _._ (5)



In this paper, we will introduce a novel reward redistribution mechanism that bridges between return
decomposition and uniform reward redistribution.


3 R EWARD R EDISTRIBUTION VIA R ANDOMIZED R ETURN D ECOMPOSITION


In this section, we introduce our approach, randomized return decomposition (RRD), which sets
up a surrogate optimization problem of the least-squares-based return decomposition. The proposed
surrogate objective allows us to conduct return decomposition on short subsequences of agent trajectories, which is scalable in long-horizon tasks. We provide analyses to characterize the algorithmic
property of our surrogate objective function and discuss connections to existing methods.


3




Published as a conference paper at ICLR 2022


3.1 R ANDOMIZED R ETURN D ECOMPOSITION WITH M ONTE -C ARLO R ETURN E STIMATION


One practical barrier to apply least-squares-based return decomposition methods in long-horizon
tasks is the computation costs of the regression loss in Eq. (4), i.e., it requires to enumerate all stateaction pairs within the agent trajectory. To resolve this issue, we consider a randomized method that
uses a Monte-Carlo estimator to compute the predicted episodic return _R_ [�] ep _,θ_ ( _τ_ ) as follows:



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�
_R_ ep _,θ_ ( _τ_ ) =



_T −_ 1


�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t_ =0



= E
_I∼ρ_ _T_ ( _·_ )



_≈_ _[T]_

_|I|_



�



_T_

_|I|_
�



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�



_,_ (6)



~~�~~ ~~�~~ � ~~�~~
Deterministic Computation



~~�~~ ~~�~~ � ~~�~~

Monte-Carlo Estimation



where _I_ denotes a subset of indices. _ρ_ _T_ ( _·_ ) denotes an unbiased sampling distribution where each
index _t_ has the same probability to be included in _I_ . In this paper, without further specification,
_ρ_ _T_ ( _·_ ) is constructed by uniformly sampling _K_ distinct indices.

_ρ_ _T_ ( _·_ ) = Uniform �� _I ⊆_ Z _T_ : _|I|_ = _K_ �� _,_ (7)
where _K_ is a hyper-parameter. In this sampling distribution, each timestep _t_ has the same probability
to be covered by the sampled subsequence _I ∼_ _ρ_ _T_ ( _·_ ) so that it gives an unbiased Monte-Carlo
estimation of the episodic summation _R_ [�] ep _,θ_ ( _τ_ ).


**Randomized Return Decomposition.** Based on the idea of using Monte-Carlo estimation shown
in Eq. (6), we introduce our approach, _randomized return decomposition_ (RRD), to improve the scalability of least-squares-based reward redistribution methods. The objective function of our approach
is formulated by the randomized return decomposition loss _L_ Rand-RD ( _θ_ ) stated in Eq. (8), in which
the parameterized proxy reward function _R_ [�] _θ_ is trained to predict the episodic return _R_ ep ( _τ_ ) given a
random subsequence of the agent trajectory. In other words, we integrate the Monte-Carlo estimator
(see Eq. (6)) into the return decomposition loss to obtain the following surrogate loss function:



2 [��]

�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )

_t∈I_ �



_L_ Rand-RD ( _θ_ ) = E
_τ_ _∼D_



�



E
_I∼ρ_ _T_ ( _·_ )



_R_ ep ( _τ_ ) _−_ _|I|_ _[T]_
��



_._ (8)



In practice, the loss function _L_ Rand-RD ( _θ_ ) can be estimated by sampling a mini-batch of trajectory
subsequences instead of computing _R_ [�] _θ_ ( _s_ _t_ _, a_ _t_ ) for the whole agent trajectory, and thus the implementation of randomized return decomposition is adaptive and flexible in long-horizon tasks.


3.2 A NALYSIS OF R ANDOMIZED R ETURN D ECOMPOSITION


The main purpose of our approach is establishing a surrogate loss function to improve the scalability of least-squares-based return decomposition in practice. Our proposed method, randomized
return decomposition, is a trade-off between the computation complexity and the estimation error
induced by the Monte-Carlo estimator. In this section, we show that our approach is an interpolation
between between the return decomposition paradigm and uniform reward redistribution, which can
be controlled by the hyper-parameter _K_ used in the sampling distribution (see Eq. (7)). We present
Theorem 1 as a formal characterization of our proposed surrogate objective function.
**Theorem 1** (Loss Decomposition) **.** _The surrogate loss function L_ _Rand-RD_ ( _θ_ ) _can be decomposed to_
_two terms interpolating between return decomposition and uniform reward redistribution._



_L_ _Rand-RD_ ( _θ_ ) = _L_ _RD_ ( _θ_ ) + E
_τ_ _∼D_

�



�



�



_Var_
_I∼ρ_ _T_ ( _·_ )



_T_

_|I|_
�



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�



~~�~~ ~~�~~ � �
_variance of the Monte-Carlo estimator_



� ~~��~~ ~~�~~
_uniform reward redistribution_



(9)


_,_ (10)



�



�



1 _−_ _[K][ −]_ [1]
� _T −_ 1



�
_R_ _θ_ ( _s_ _t_ _, a_ _t_ ) _·_ [1]
� � _K_



= _L_ _RD_ ( _θ_ ) + E
~~����~~ _τ_ _∼D_
_return decomposition_



�



_T_ [2] _·_ _Var_
( _s_ _t_ _,a_ _t_ ) _∼τ_



� ~~��~~ ~~�~~
_interpolation weight_



_where K denotes the length of sampled subsequences defined in Eq._ (7) _._


The proof of Theorem 1 is based on the bias-variance decomposition formula of mean squared error
(Kohavi & Wolpert, 1996). The detailed proofs are deferred to Appendix A.


4




Published as a conference paper at ICLR 2022


**Interpretation as Regularization.** As presented in Theorem 1, the randomized decomposition
loss can be decomposed to two terms, the deterministic return decomposition loss _L_ RD ( _θ_ ) and a
variance penalty term (see the second term of Eq. (9)). The variance penalty term can be regarded
as a regularization that is controlled by hyper-parameter _K_ . In practical problems, the objective
of return decomposition is ill-posed, since the number of trajectory labels is dramatically less than
the number of transition samples. There may exist solutions of reward redistribution that formally
optimize the least-squares regression loss but serve little functionality to guide the policy learning.
Regarding this issue, the variance penalty in randomized return decomposition is a regularizer for
reward modeling. It searches for smooth proxy rewards that has low variance within the trajectory.
This regularization effect is similar to the mechanism of uniform reward redistribution (Gangwani
et al., 2020), which achieves state-of-the-art performance in the previous literature. In section 4,
our experiments demonstrate that the variance penalty is crucial to the empirical performance of
randomized return decomposition.


**A Closer Look at Loss Decomposition.** In addition to the intuitive interpretation of regularization, we will present a detailed characterization of the loss decomposition shown in Theorem 1. We
interpret this loss decomposition as below:


1. Note that the Monte-Carlo estimator used by randomized return decomposition is an unbiased estimation of the proxy episodic return _R_ [�] ep _,θ_ ( _τ_ ) (see Eq. (6)). This unbiased property
gives the first component of the loss decomposition, i.e, the original return decomposition
loss _L_ RD ( _θ_ ).

2. Although the Monte-Carlo estimator is unbiased, its variance would contribute to an additional loss term induced by the mean-square operator, i.e., the second component of loss
decomposition presented in Eq. (10). This additional term penalizes the variance of the
learned proxy rewards under random sampling. This penalty expresses the same mechanism as uniform reward redistribution (Gangwani et al., 2020) in which the episodic return
is uniformly redistributed to the state-action pairs in the trajectory.


Based on the above discussions, we can analyze the algorithmic properties of randomized return
decomposition by connecting with previous studies.


3.2.1 S URROGATE O PTIMIZATION OF R ETURN D ECOMPOSITION


Randomized return decomposition conducts a surrogate optimization of the actual return decomposition. Note that the variance penalty term in Eq. (10) is non-negative, our loss function _L_ Rand-RD ( _θ_ )
serves an upper bound estimation of the original loss _L_ RD ( _θ_ ) as the following statement.
**Proposition 1** (Surrogate Upper Bound) **.** _The randomized return decomposition loss L_ _Rand-RD_ ( _θ_ ) _is_
_an upper bound of the actual return decomposition loss function L_ _RD_ ( _θ_ ) _, i.e., L_ _Rand-RD_ ( _θ_ ) _≥L_ _RD_ ( _θ_ ) _._


Proposition 1 suggests that optimizing our surrogate loss _L_ Rand-RD ( _θ_ ) guarantees to optimize an upper bound of the actual return decomposition loss _L_ RD ( _θ_ ). According to Theorem 1, the gap between
_L_ Rand-RD ( _θ_ ) and _L_ RD ( _θ_ ) refers to the variance of subsequence sampling. The magnitude of this gap
can be controlled by the hyper-parameter _K_ that refers to the length of sampled subsequences.

**Proposition 2** (Objective Gap) **.** _Let L_ [(] _Rand-RD_ _[K]_ [)] [(] _[θ]_ [)] _[ denote the randomized return decomposition loss]_
_that samples subsequences with length K. The gap between L_ [(] _Rand-RD_ _[K]_ [)] [(] _[θ]_ [)] _[ and][ L]_ _[RD]_ [(] _[θ]_ [)] _[ can be reduced]_
_by using larger values of hyper-parameter K._

_∀θ,_ _L_ [(1)] _Rand-RD_ [(] _[θ]_ [)] _[ ≥L]_ [(2)] _Rand-RD_ [(] _[θ]_ [)] _[ ≥· · · ≥L]_ [(] _Rand-RD_ _[T][ −]_ [1)] [(] _[θ]_ [)] _[ ≥L]_ [(] _Rand-RD_ _[T]_ [ )] [(] _[θ]_ [) =] _[ L]_ _[RD]_ [(] _[θ]_ [)] _[.]_ (11)


This gap can be eliminated by choosing _K_ = _T_ in the sampling distribution (see Eq. (7)) so that our
approach degrades to the original deterministic implementation of return decomposition.


3.2.2 G ENERALIZATION OF U NIFORM R EWARD R EDISTRIBUTION


The reward redistribution mechanism of randomized return decomposition is a generalization of
uniform reward redistribution. To serve intuitions, we start the discussions with the simplest case
using _K_ = 1 in subsequence sampling, in which our approach degrades to the uniform reward
redistribution as the following statement.


5




Published as a conference paper at ICLR 2022


**Proposition 3** (Uniform Reward Redistribution) **.** _Assume all trajectories have the same length and_
_the parameterization space of θ serves universal representation capacity. The optimal solution θ_ _[⋆]_ _of_
_minimizing L_ [(1)] _Rand-RD_ [(] _[θ]_ [)] _[ is stated as follows:]_



�
_R_ _θ_ _⋆_ ( _s, a_ ) = E
_τ_ _∼D_



� _R_ _ep_ ( _τ_ ) _/T |_ ( _s, a_ ) _∈_ _τ_ � _,_ (12)



_where L_ [(1)] _Rand-RD_ [(] _[θ]_ [)] _[ denotes the randomized return decomposition loss with][ K]_ [ = 1] _[.]_


A minor difference between Eq. (12) and the proxy reward designed by Gangwani et al. (2020) (see
Eq. (5)) is a multiplier scalar 1 _/T_ . Gangwani et al. (2020) interprets such a proxy reward mechanism
as a trajectory-space smoothing process or a non-committal reward redistribution. Our analysis can
give a mathematical characterization to illustrate the objective of uniform reward redistribution. As
characterized by Theorem 1, uniform reward redistribution conducts an additional regularizer to
penalize the variance of per-step proxy rewards. In the view of randomized return decomposition,
the functionality of this regularizer is requiring the reward model to reconstruct episodic return from
each single-step transition.


By using larger values of hyper-parameter _K_, randomized return decomposition is trained to reconstruct the episodic return from a subsequence of agent trajectory instead of the single-step transition
used by uniform reward redistribution. This mechanism is a generalization of uniform reward redistribution, in which we equally assign rewards to subsequences generated by uniformly random
sampling. It relies on the concentratability of random sampling, i.e., the average of a sequence can
be estimated by a small random subset with sub-linear size. The individual contribution of each
transition within the subsequence is further attributed by return decomposition.


3.3 P RACTICAL I MPLEMENTATION OF R ANDOMIZED R ETURN D ECOMPOSITION


In Algorithm 1, we integrate randomized return decomposition with policy optimization. It follows
an iterative paradigm that iterates between the rewarding modeling and policy optimization modules.


**Algorithm 1** Policy Optimization with Randomized Return Decomposition


1: Initialize _D ←∅_
2: **for** _ℓ_ = 1 _,_ 2 _, · · ·_ **do**
3: Collect a rollout trajectory _τ_ using the current policy.
4: Store trajectory _τ_ and feedback _R_ ep ( _τ_ ) into the replay buffer _D ←D ∪{_ ( _τ, R_ ep ( _τ_ )) _}_ .
5: **for** _i_ = 1 _,_ 2 _, · · ·_ **do**
6: Sample _M_ trajectories _{τ_ _j_ _∈D}_ _[M]_ _j_ =1 [from the replay buffer.]
7: Sample subsequences _{I_ _j_ _⊆_ Z _T_ _j_ _}_ _[M]_ _j_ =1 [for these trajectories.]

8: Estimate randomized return decomposition loss _L_ [�] Rand-RD ( _θ_ ),







� _R_ ep ( _τ_ _j_ ) _−_ _|I_ _[T]_ _j_ _[j]_




�
_L_ Rand-RD ( _θ_ ) = [1]

_M_



_M_
�

_j_ =1



_|I_ _j_ _|_



2

�
_R_ _θ_ ( _s_ _j,t_ _, a_ _j,t_ )  _,_ (13)
�





�

_t∈I_ _j_



where _T_ _j_ denotes the length of trajectory _τ_ _j_ = _⟨s_ _j,_ 1 _, a_ _j,_ 1 _, · · ·, s_ _j,T_ _j_ _⟩_ .

9: Perform a gradient update on the reward model _R_ [�] _θ_,


_θ ←_ _θ −_ _α∇_ _θ_ _L_ [�] Rand-RD ( _θ_ ) _,_ (14)


where _α_ denotes the learning rate.
10: Perform policy optimization using the learned proxy reward function _R_ [�] _θ_ ( _s, a_ ).


As presented in Eq. (13) and Eq. (14), the optimization of our loss function _L_ Rand-RD ( _θ_ ) can be easily
conducted by mini-batch gradient descent. This surrogate loss function only requires computations
on short-length subsequences. It provides a scalable implementation for return decomposition that
can be generalized to long-horizon tasks with manageable computation costs. In section 4, we will
show that this simple implementation can also achieve state-of-the-art performance in comparison
to other existing methods.


6




Published as a conference paper at ICLR 2022


4 E XPERIMENTS


In this section, we investigate the empirical performance of our proposed methods by conducting experiments on a suite of MuJoCo benchmark tasks with episodic rewards. We compare our approach
with several baseline algorithms in the literature and conduct an ablation study on subsequence
sampling that is the core component of our algorithm.


4.1 P ERFORMANCE E VALUATION ON M U J O C O B ENCHMARK WITH E PISODIC R EWARDS


**Experiment Setting.** We adopt the same experiment setting as Gangwani et al. (2020) to compare
the performance of our approach with baseline algorithms. The experiment environments is based on
the MuJoCo locomotion benchmark tasks created by OpenAI Gym (Brockman et al., 2016). These
tasks are long-horizon with maximum trajectory length _T_ = 1000, i.e., the task horizon is definitely
longer than the batch size used by the standard implementation of mini-batch gradient estimation.
We modify the reward function of these environments to set up an episodic-reward setting. Formally,
on non-terminal states, the agent will receive a zero signal instead of the per-step dense rewards.
The agent can obtain the episodic feedback _R_ ep ( _τ_ ) at the last step of the rollout trajectory, in which
_R_ ep ( _τ_ ) is computed by the summation of per-step instant rewards given by the standard setting. We
evaluate the performance of our proposed methods with the same configuration of hyper-parameters
in all environments. A detailed description of implementation details is included in Appendix B.


We evaluate two implementations of randomized return decomposition (RRD):


   - **RRD (ours)** denotes the default implementation of our approach. We train a reward model
using randomized return decomposition loss _L_ Rand-RD, in which we sample subsequences with
length _K_ = 64 in comparison to the task horizon _T_ = 1000. The reward model _R_ [�] _θ_ is
parameterized by a two-layer fully connected network. The policy optimization module is
implemented by soft actor-critic (SAC; Haarnoja et al., 2018a). [1]


   - **RRD-** _L_ **RD** **(ours)** is an alternative implementation that optimizes _L_ RD instead of _L_ Rand-RD .
Note that Theorem 1 gives a closed-form characterization of the gap between _L_ Rand-RD ( _θ_ ) and
_L_ RD ( _θ_ ), which is represented by the variance of the learned proxy rewards. By subtracting an
unbiased variance estimation from loss function _L_ Rand-RD ( _θ_ ), we can estimate loss function
_L_ RD ( _θ_ ) by sampling short subsequences. It gives a computationally efficient way to optimize _L_ RD ( _θ_ ). We include this alternative implementation to reveal the functionality of the
regularization given by variance penalty. A detailed description is deferred to Appendix B.3.


We compare with several existing methods for episodic or delayed reward settings:


   - **IRCR** (Gangwani et al., 2020) is an implementation of uniform reward redistribution. The
reward redistribution mechanism of IRCR is non-parametric, in which the proxy reward of a
transition is set to be the normalized value of corresponding trajectory return. It is equivalent
to use a Monte-Carlo estimator of Eq. (5). Due to the ease of implementation, this method
achieves state-of-the-art performance in the literature.

   - **RUDDER** (Arjona-Medina et al., 2019) is based on the idea of return decomposition but does
not directly optimize _L_ RD ( _θ_ ). Instead, it trains a return predictor based on trajectory, and the
step-wise credit is assigned by the prediction difference between two consecutive states. By
using the warm-up technique of LSTM, this transform prevents its training computation costs
from depending on the task horizon _T_ so that it is adaptive to long-horizon tasks.

   - **GASIL** (Guo et al., 2018), generative adversarial self-imitation learning, is a generalization
of GAIL (Ho & Ermon, 2016). It formulates an imitation learning framework by imitating
best trajectories in the replay buffer. The proxy rewards are given by a discriminator that is
trained to classify the agent and expert trajectories.

   - **LIRPG** (Zheng et al., 2018) aims to learn an intrinsic reward function from sparse environment feedback. Its policy is trained to optimized the sum of the extrinsic and intrinsic rewards.
The parametric intrinsic reward function is updated by meta-gradients to optimize the actual
extrinsic rewards achieved by the policy


1 [The source code of our implementation is available at https://github.com/Stilwell-Git/](https://github.com/Stilwell-Git/Randomized-Return-Decomposition)
[Randomized-Return-Decomposition.](https://github.com/Stilwell-Git/Randomized-Return-Decomposition)


7




Published as a conference paper at ICLR 2022














|000<br>000<br>000<br>000<br>000<br>000<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br>||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br>||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br>||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br>||||||






|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||


|000<br>000<br>000<br>000<br>000<br>000<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps<br>Swimmer-v2<br>360<br>300<br>240<br>180<br>120<br>60<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>60<br>120<br>180<br>240<br>300<br>360<br>Swimmer~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>60<br>120<br>180<br>240<br>300<br>360<br>Swimmer~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>60<br>120<br>180<br>240<br>300<br>360<br>Swimmer~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>60<br>120<br>180<br>240<br>300<br>360<br>Swimmer~~-~~v2|||||||


|600<br>000<br>400<br>800<br>200<br>600<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>600<br>200<br>800<br>400<br>000<br>600<br>||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>600<br>200<br>800<br>400<br>000<br>600<br>||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>600<br>200<br>800<br>400<br>000<br>600<br>||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>600<br>200<br>800<br>400<br>000<br>600<br>||||||


|6000<br>5000<br>4000<br>3000<br>2000<br>1000<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps<br>HumanoidStandup-v2<br>50000<br>25000<br>00000<br>75000<br>50000<br>25000<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>25000<br>50000<br>75000<br>00000<br>25000<br>50000<br>HumanoidStandup~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>25000<br>50000<br>75000<br>00000<br>25000<br>50000<br>HumanoidStandup~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>25000<br>50000<br>75000<br>00000<br>25000<br>50000<br>HumanoidStandup~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>25000<br>50000<br>75000<br>00000<br>25000<br>50000<br>HumanoidStandup~~-~~v2|||||||



Figure 1: Learning curves on a suite of MuJoCo benchmark tasks with episodic rewards. All curves
for MuJoCo benchmark are plotted from 30 runs with random initializations. The shaded region
indicates the standard deviation. To make the comparison more clear, the curves are smoothed by
averaging 10 most recent evaluation points. We set up an evaluation point every 10 [4] timesteps.


**Overall Performance Comparison.** As presented in Figure 1, randomized return decomposition
generally outperforms baseline algorithms. Our approach can achieve higher sample efficiency and
produce better policies after convergence. RUDDER is an implementation of return decomposition
that represents single-step rewards by the differences between the return predictions of two consecutive states. This implementation maintains high computation efficiency but long-term return predic
|6000<br>5000<br>4000<br>3000<br>2000<br>1000<br>0<br>0M 0.5M<br>0<br>80<br>160<br>240<br>320<br>400<br>0M 0.5M<br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>verall P<br>nerally<br>oduce b<br>at repre<br>e states|Col2|Col3|Col4|
|---|---|---|---|
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||~~ 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>Reacher~~-~~v2|~~ 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>Reacher~~-~~v2|
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||||
|~~0M 0.5M~~<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M~~<br>400<br>320<br>240<br>160<br>80<br>0<br><br>gure 1:<br>r MuJo<br>dicates<br>eraging<br>**verall P**<br>nerally<br>oduce b<br>at repre<br>e states||~~ 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>Learning curve<br>Co benchmark<br>the standard de<br> 10 most recent<br>**erformance C**<br>outperforms ba<br>etter policies af<br>sents single-ste<br>. This impleme|~~ 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>Learning curve<br>Co benchmark<br>the standard de<br> 10 most recent<br>**erformance C**<br>outperforms ba<br>etter policies af<br>sents single-ste<br>. This impleme|

tion is a hard optimization problem and requires on-policy samples. In comparison, RRD is a more
scalable and stable implementation which can better integrate with off-policy learning for improving
sample efficiency. The uniform reward redistribution considered by IRCR is simple to implement
but cannot extract the temporal structure of episodic rewards. Thus the final policy quality produced
by RRD is usually better than that of IRCR. GASIL and LIRPG aim to construct auxiliary reward
functions that have high correlation to the environmental return. These two methods cannot achieve
high sample efficiency since their objectives require on-policy training.


**Variance Penalty as Regularization.** Figure 1 also compares two implementations of randomized
return decomposition. In most testing environments, RRD optimizing _L_ Rand-RD outperforms the unbiased implementation RRD- _L_ RD . We consider RRD using _L_ Rand-RD as our default implementation
since it performs better and its objective function is simpler to implement. As discussed in section 3.2, the variance penalty conducted by RRD aims to minimize the variance of the Monte-Carlo
estimator presented in Eq. (6). It serves as a regularization to restrict the solution space of return
decomposition, which gives two potential effects: (1) RRD prefers smooth proxy rewards when the
expressiveness capacity of reward network over-parameterizes the dataset. (2) The variance of minibatch gradient estimation can also be reduced when the variance of Monte-Carlo estimator is small.
In practice, this regularization would benefit the training stability. As presented in Figure 1, RRD
achieves higher sample efficiency than RRD- _L_ RD in most testing environments. The quality of the
learned policy of RRD is also better than that of RRD- _L_ RD . It suggests that the regularized reward
redistribution can better approximate the actual environmental objective.


4.2 A BLATION S TUDIES


We conduct an ablation study on the hyper-parameter _K_ that represent the length of subsequences
used by randomized return decomposition. As discussed in section 3.2, the hyper-parameter _K_
controls the interpolation ratio between return decomposition and uniform reward redistribution.
It trades off the accuracy of return reconstruction and variance regularization. In Figure 2, we
evaluate RRD with a set of choices of hyper-parameter _K ∈{_ 1 _,_ 8 _,_ 16 _,_ 32 _,_ 64 _,_ 128 _}_ . The experiment
results show that, although the sensitivity of this hyper-parameter depends on the environment, a
relatively large value of _K_ generally achieves better performance, since it can better approximate
the environmental objective. In this experiment, we ensure all runs use the same input size for minibatch training, i.e., using larger value of _K_ leads to less number of subsequences in the mini-batch.


8




Published as a conference paper at ICLR 2022














|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||


|6000 n Ant-v2|Col2|Col3|An|nt-v2|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br>||||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br>||||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br>||||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br>||||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br>||||||||


|6000<br>5000<br>4000<br>3000<br>2000<br>1000<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000|||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000|||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000|||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000|||||||


|6000<br>5000<br>4000<br>3000<br>2000<br>1000<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000|||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000|||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000|||||||
|~~0M~~<br>~~0.5M~~<br>~~1M~~<br>~~1.5M~~<br>~~2M~~<br>~~2.5M~~<br>~~3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000|||||||



Figure 2: Learning curves of RRD with different choices of hyper-parameter _K_ . The curves with
_K_ = 64 correspond to the default implementation of RRD presented in Figure 1.


More specifically, in this ablation study, all algorithm instances estimate the loss function _L_ Rand-RD
using a mini-batch containing 256 transitions. We consider _K_ = 64 as our default configuration. As
presented in Figure 2, larger values give marginal improvement in most environments. The benefits
of larger values of _K_ can only be observed in HalfCheetah-v2. We note that HalfCheetah-v2 does
not have early termination and thus has the longest average horizon among these locomotion tasks. It
highlights the trade-off between the weight of regularization and the bias of subsequence estimation.


5 R ELATED W ORK


**Reward Design.** The ability of RL agents highly depends on the designs of reward functions. It is
widely observed that reward shaping can accelerate learning (Mataric, 1994; Ng et al., 1999; Devlin
et al., 2011; Wu & Tian, 2017; Song et al., 2019). Many previous works study how to automatically
design auxiliary reward functions for efficient reinforcement learning. A famous paradigm, inverse
RL (Ng & Russell, 2000; Fu et al., 2018), considers to recover a reward function from expert demonstrations. Another branch of work is learning an intrinsic reward function that guides the agent to
maximize extrinsic objective (Sorg et al., 2010; Guo et al., 2016). Such an intrinsic reward function
can be learned through meta-gradients (Zheng et al., 2018; 2020) or self-imitation (Guo et al., 2018;
Gangwani et al., 2019). A recent work (Abel et al., 2021) studies the expressivity of Markov rewards
and proposes algorithms to design Markov rewards for three notions of abstract tasks.


**Temporal Credit Assignment.** Another methodology for tackling long-horizon sequential decision problems is assigning credits to emphasize the contribution of each single step over the temporal
structure. These methods directly consider the specification of the step values instead of manipulating the reward function. The simplest example is studying how the choice of discount factor _γ_
affects the policy learning (Petrik & Scherrer, 2008; Jiang et al., 2015; Fedus et al., 2019). Several
previous works consider to extend the _λ_ -return mechanism (Sutton, 1988) to a more generalized
credit assignment framework, such as adaptive _λ_ (Xu et al., 2018) and pairwise weights (Zheng
et al., 2021). RUDDER (Arjona-Medina et al., 2019) proposes a return-equivalent formulation for
the credit assignment problem and establish theoretical analyses (Holzleitner et al., 2021). AlignedRUDDER (Patil et al., 2020) considers to use expert demonstrations for higher sample efficiency.
Harutyunyan et al. (2019) opens up a new family of algorithms, called hindsight credit assignment,
that attributes the credits from a backward view. In Appendix F, we cover more topics of related
work and discuss the connections to the problem focused by this paper.


6 C ONCLUSION


In this paper, we propose randomized return decomposition (RRD), a novel reward redistribution
algorithm, to tackle the episodic reinforcement learning problem with trajectory feedback. RRD
uses a Monte-Carlo estimator to establish a surrogate optimization problem of return decomposition. This surrogate objective implicitly conducts a variance reduction penalty as regularization. We
analyze its algorithmic properties by connecting with previous studies in reward redistribution. Our
experiments demonstrate that RRD outperforms previous methods in terms of both sample efficiency
and policy quality. The basic idea of randomized return decomposition can potentially generalize to
other related problems with sum-decomposition structure, such as preference-based reward modeling (Christiano et al., 2017) and multi-agent value decomposition (Sunehag et al., 2018). It is also
promising to consider non-linear decomposition as what is explored in multi-agent value factorization (Rashid et al., 2018). We leave these investigations as our future work.


9




Published as a conference paper at ICLR 2022


A CKNOWLEDGMENTS


The authors would like to thank Kefan Dong for insightful discussions. This work is supported by
the National Science Foundation under Grant CCF-2006526.


R EFERENCES


David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, and
Satinder Singh. On the expressivity of Markov reward. _Advances in Neural Information Process-_
_ing Systems_, 34, 2021.


Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Concrete problems in AI safety. _arXiv preprint arXiv:1606.06565_, 2016.


Saidhiraj Amuru and R Michael Buehrer. Optimal jamming using delayed learning. In _2014 IEEE_
_Military Communications Conference_, pp. 1528–1533. IEEE, 2014.


Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In
_Advances in Neural Information Processing Systems_, pp. 5048–5058, 2017.


Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with Bellmanresidual minimization based fitted policy iteration and a single sample path. _Machine Learning_,
71(1):89–129, 2008.


Jose A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, and Sepp Hochreiter. RUDDER: Return decomposition for delayed rewards. In _Advances_
_in Neural Information Processing Systems_, volume 32, 2019.


Richard Bellman. Dynamic programming. _Princeton University Press_, 89:92, 1957.


Wendelin B¨ohmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In _International_
_Conference on Machine Learning_, pp. 980–991. PMLR, 2020.


L´eon Bottou, Jonas Peters, Joaquin Qui˜nonero-Candela, Denis X Charles, D Max Chickering, Elon
Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning
systems: The example of computational advertising. _Journal of Machine Learning Research_, 14
(11), 2013.


Yann Bouteiller, Simon Ramstedt, Giovanni Beltrame, Christopher Pal, and Jonathan Binas. Reinforcement learning with random delays. In _International Conference on Learning Representa-_
_tions_, 2021.


Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI gym. _arXiv preprint arXiv:1606.01540_, 2016.


Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G Bellemare. Dopamine: A research framework for deep reinforcement learning. _arXiv preprint_
_arXiv:1812.06110_, 2018.


Niladri Chatterji, Aldo Pacchiano, Peter Bartlett, and Michael Jordan. On the theory of reinforcement learning with once-per-episode feedback. _Advances in Neural Information Processing Sys-_
_tems_, 34, 2021.


Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang, and Hai-Hong Tang. Stabilizing
reinforcement learning in dynamic environment with application to online recommendation. In
_Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &_
_Data Mining_, pp. 1187–1196, 2018.


Xinshi Chen, Yu Li, Ramzan Umarov, Xin Gao, and Le Song. Rna secondary structure prediction
by learning unrolled algorithms. In _International Conference on Learning Representations_, 2020.


10




Published as a conference paper at ICLR 2022


Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. _Advances in neural information processing sys-_
_tems_, 30, 2017.


Edward H Clarke. Multipart pricing of public goods. _Public choice_, pp. 17–33, 1971.


Sam Devlin, Daniel Kudenko, and Marek Grze´s. An empirical study of potential-based reward
shaping and advice in complex, multi-agent systems. _Advances in Complex Systems_, 14(02):
251–278, 2011.


Daniel Dewey. Reinforcement learning and the reward engineering principle. In _2014 AAAI Spring_
_Symposium Series_, 2014.


Yali Du, Lei Han, Meng Fang, Tianhong Dai, Ji Liu, and Dacheng Tao. LIIR: learning individual intrinsic reward in multi-agent reinforcement learning. In _Advances in Neural Information_
_Processing Systems_, pp. 4403–4414, 2019.


Yonathan Efroni, Nadav Merlis, and Shie Mannor. Reinforcement learning with trajectory feedback.
In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 7288–7295,
2021.


William Fedus, Carles Gelada, Yoshua Bengio, Marc G Bellemare, and Hugo Larochelle. Hyperbolic discounting and learning over multiple horizons. _arXiv preprint arXiv:1902.06865_, 2019.


Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforcement learning. In _International Conference on Learning Representations_, 2018.


Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In _International Conference on Machine Learning_, pp. 1587–1596, 2018.


Tanmay Gangwani, Qiang Liu, and Jian Peng. Learning self-imitating diverse policies. In _Interna-_
_tional Conference on Learning Representations_, 2019.


Tanmay Gangwani, Yuan Zhou, and Jian Peng. Learning guidance rewards with trajectory-space
smoothing. In _Advances in Neural Information Processing Systems_, volume 33, pp. 822–832,
2020.


Theodore Groves. Incentives in teams. _Econometrica: Journal of the Econometric Society_, pp.
617–631, 1973.


Xiaoxiao Guo, Satinder P Singh, Richard L Lewis, and Honglak Lee. Deep learning for reward
design to improve Monte Carlo tree search in ATARI games. In _Proceedings of the Twenty-Fifth_
_International Joint Conference on Artificial Intelligence_, pp. 1519–1525, 2016.


Yijie Guo, Junhyuk Oh, Satinder Singh, and Honglak Lee. Generative adversarial self-imitation
learning. _arXiv preprint arXiv:1812.00950_, 2018.


Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In _International confer-_
_ence on machine learning_, pp. 1861–1870. PMLR, 2018a.


Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. _arXiv preprint arXiv:1812.05905_, 2018b.


Beining Han, Zhizhou Ren, Zuofan Wu, Yuan Zhou, and Jian Peng. Off-policy reinforcement learning with delayed rewards. _arXiv preprint arXiv:2106.11854_, 2021.


Anna Harutyunyan, Will Dabney, Thomas Mesnard, Mohammad Gheshlaghi Azar, Bilal Piot, Nicolas Heess, Hado P van Hasselt, Gregory Wayne, Satinder Singh, Doina Precup, et al. Hindsight
credit assignment. In _Advances in neural information processing systems_, volume 32, pp. 12488–
12497, 2019.


11




Published as a conference paper at ICLR 2022


Daniel Hein, Stefan Depeweg, Michel Tokic, Steffen Udluft, Alexander Hentschel, Thomas A Runkler, and Volkmar Sterzing. A benchmark environment motivated by industrial control problems.
In _2017 IEEE Symposium Series on Computational Intelligence (SSCI)_, pp. 1–8. IEEE, 2017.


Am´elie H´eliou, Panayotis Mertikopoulos, and Zhengyuan Zhou. Gradient-free online learning in
continuous games with delayed rewards. In _International Conference on Machine Learning_, pp.
4172–4181. PMLR, 2020.


Todd Hester and Peter Stone. TEXPLORE: real-time sample-efficient reinforcement learning for
robots. _Machine learning_, 90(3):385–429, 2013.


Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In _Advances in neural_
_information processing systems_, volume 29, pp. 4565–4573, 2016.


Markus Holzleitner, Lukas Gruber, Jos´e Arjona-Medina, Johannes Brandstetter, and Sepp Hochreiter. Convergence proof for actor-critic methods applied to PPO and RUDDER. In _Transactions_
_on Large-Scale Data-and Knowledge-Centered Systems XLVIII_, pp. 105–130. Springer, 2021.


Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning
horizon on model accuracy. In _Proceedings of the 2015 International Conference on Autonomous_
_Agents and Multiagent Systems_, pp. 1181–1189. Citeseer, 2015.


Konstantinos V Katsikopoulos and Sascha E Engelbrecht. Markov decision processes with delays
and asynchronous cost collection. _IEEE transactions on automatic control_, 48(4):568–574, 2003.


Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International_
_Conference on Learning Representations_, 2015.


Ron Kohavi and David Wolpert. Bias plus variance decomposition for zero-one loss functions. In
_Proceedings of the Thirteenth International Conference on International Conference on Machine_
_Learning_, pp. 275–283, 1996.


Kimin Lee, Laura Smith, and Pieter Abbeel. PEBBLE: Feedback-efficient interactive reinforcement
learning via relabeling experience and unsupervised pre-training. In _International Conference on_
_Machine Learning_, 2021.


Lei Lei, Yue Tan, Kan Zheng, Shiwen Liu, Kuan Zhang, and Xuemin Shen. Deep reinforcement
learning for autonomous internet of things: Model, applications and challenges. _IEEE Communi-_
_cations Surveys & Tutorials_, 22(3):1722–1760, 2020.


Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In _Inter-_
_national Conference on Learning Representations_, 2016.


Yang Liu, Yunan Luo, Yuanyi Zhong, Xi Chen, Qiang Liu, and Jian Peng. Sequence modeling of
temporal credit assignment for episodic reinforcement learning. _arXiv preprint arXiv:1905.13420_,
2019.


Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search provides a competitive
approach to reinforcement learning. _arXiv preprint arXiv:1803.07055_, 2018.


Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource management
with deep reinforcement learning. In _Proceedings of the 15th ACM workshop on hot topics in_
_networks_, pp. 50–56, 2016.


Maja J Mataric. Reward functions for accelerated learning. _Machine learning_, pp. 181–189, 1994.


Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. _Nature_, 518(7540):529–533, 2015.


Roger B Myerson. Optimal auction design. _Mathematics of operations research_, 6(1):58–73, 1981.


12




Published as a conference paper at ICLR 2022


Somjit Nath, Mayank Baranwal, and Harshad Khadilkar. Revisiting state augmentation methods
for reinforcement learning with stochastic delays. In _Proceedings of the 30th ACM International_
_Conference on Information & Knowledge Management_, pp. 1346–1355, 2021.


Andrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In _Proceedings of_
_the Seventeenth International Conference on Machine Learning_ . Citeseer, 2000.


Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In _Proceedings of the Sixteenth International_
_Conference on Machine Learning_, pp. 278–287. Morgan Kaufmann Publishers Inc., 1999.


Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective multiagent RL with global rewards. In _Advances in Neural Information Processing Systems_, volume 31,
pp. 8102–8113, 2018.


Johan Nilsson, Bo Bernhardsson, and Bj¨orn Wittenmark. Stochastic analysis and control of real-time
systems with random time delays. _Automatica_, 34(1):57–64, 1998.


Vihang P Patil, Markus Hofmarcher, Marius-Constantin Dinu, Matthias Dorfer, Patrick M Blies,
Johannes Brandstetter, Jose A Arjona-Medina, and Sepp Hochreiter. Align-RUDDER: Learning
from few demonstrations by reward redistribution. _arXiv preprint arXiv:2009.14108_, 2020.


Marek Petrik and Bruno Scherrer. Biasing approximate dynamic programming with a lower discount
factor. _Advances in neural information processing systems_, 21:1265–1272, 2008.


Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo
drug design. _Science advances_, 4(7):eaap7885, 2018.


Hazhir Rahmandad, Nelson Repenning, and John Sterman. Effects of feedback delay on learning.
_System Dynamics Review_, 25(4):309–338, 2009.


Jette Randløv and Preben Alstrøm. Learning to drive a bicycle using reinforcement learning and
shaping. In _International Conference on Machine Learning_, pp. 463–471. Citeseer, 1998.


David Raposo, Sam Ritter, Adam Santoro, Greg Wayne, Theophane Weber, Matt Botvinick, Hado
van Hasselt, and Francis Song. Synthetic returns for long-term credit assignment. _arXiv preprint_
_arXiv:2102.12425_, 2021.


Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In _International Conference on Machine Learning_, pp. 4295–4304. PMLR,
2018.


Erik Schuitema, Lucian Bus¸oniu, Robert Babuˇska, and Pieter Jonker. Control delay in reinforcement
learning for real-time dynamic systems: a memoryless approach. In _2010 IEEE/RSJ International_
_Conference on Intelligent Robots and Systems_, pp. 3226–3231. IEEE, 2010.


Sarjinder Singh. _Advanced Sampling Theory With Applications: How Michael”” Selected”” Amy_,
volume 2. Springer Science & Business Media, 2003.


Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. QTRAN:
Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In
_International Conference on Machine Learning_, pp. 5887–5896. PMLR, 2019.


Shihong Song, Jiayi Weng, Hang Su, Dong Yan, Haosheng Zou, and Jun Zhu. Playing FPS games
with environment-aware hierarchical reinforcement learning. In _Proceedings of the Twenty-Eighth_
_International Joint Conference on Artificial Intelligence_, pp. 3475–3482, 2019.


Jonathan Sorg, Richard L Lewis, and Satinder Singh. Reward design via online gradient ascent. In
_Advances in Neural Information Processing Systems_, volume 23, pp. 2190–2198, 2010.


Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In _Proceedings of the 17th_
_International Conference on Autonomous Agents and MultiAgent Systems_, pp. 2085–2087, 2018.


13




Published as a conference paper at ICLR 2022


Richard S Sutton. Learning to predict by the methods of temporal differences. _Machine learning_, 3
(1):9–44, 1988.


Wei Tang, Chien-Ju Ho, and Yang Liu. Bandit learning with delayed impact of actions. _Advances_
_in Neural Information Processing Systems_, 34, 2021.


Arash Tavakoli, Mehdi Fatemi, and Petar Kormushev. Learning to represent action values as a
hypergraph on the action vertices. In _International Conference on Learning Representations_,
2021.


William Vickrey. Counterspeculation, auctions, and competitive sealed tenders. _The Journal of_
_finance_, 16(1):8–37, 1961.


Thomas J Walsh, Ali Nouri, Lihong Li, and Michael L Littman. Learning and planning in environments with delayed feedback. _Autonomous Agents and Multi-Agent Systems_, 18(1):83–105,
2009.


Jianhao Wang, Zhizhou Ren, Beining Han, Jianing Ye, and Chongjie Zhang. Towards understanding
cooperative multi-agent q-learning with value factorization. In _Thirty-Fifth Conference on Neural_
_Information Processing Systems_, 2021a.


Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. QPLEX: Duplex dueling
multi-agent q-learning. In _International Conference on Learning Representations_, 2021b.


Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. Shapley Q-value: a local reward
approach to solve global reward games. In _Proceedings of the AAAI Conference on Artificial_
_Intelligence_, volume 34, pp. 7285–7292, 2020.


Christian Wirth, Johannes F¨urnkranz, and Gerhard Neumann. Model-free preference-based reinforcement learning. In _Thirtieth AAAI Conference on Artificial Intelligence_, 2016.


Yuxin Wu and Yuandong Tian. Training agent for first-person shooter game with actor-critic curriculum learning. In _International Conference on Learning Representations_, 2017.


Zhongwen Xu, Hado P van Hasselt, and David Silver. Meta-gradient reinforcement learning. In
_Advances in Neural Information Processing Systems_, volume 31, pp. 2396–2407, 2018.


Zeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient
methods. In _Advances in Neural Information Processing Systems_, volume 31, pp. 4644–4654,
2018.


Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado Van Hasselt, David
Silver, and Satinder Singh. What can learned intrinsic rewards capture? In _International Confer-_
_ence on Machine Learning_, pp. 11436–11446. PMLR, 2020.


Zeyu Zheng, Risto Vuorio, Richard Lewis, and Satinder Singh. Pairwise weights for temporal credit
assignment. _arXiv preprint arXiv:2102.04999_, 2021.


Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter Glynn, Yinyu Ye, Li-Jia Li,
and Li Fei-Fei. Distributed asynchronous optimization with unbounded delays: How slow can
you go? In _International Conference on Machine Learning_, pp. 5970–5979. PMLR, 2018.


Zhengyuan Zhou, Renyuan Xu, and Jose Blanchet. Learning in generalized linear contextual bandits
with stochastic delays. In _Advances in Neural Information Processing Systems_, volume 32, pp.
5197–5208, 2019a.


Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of
molecules via deep reinforcement learning. _Scientific reports_, 9(1):1–10, 2019b.


14




Published as a conference paper at ICLR 2022


A O MITTED P ROOFS


**Theorem 1** (Loss Decomposition) **.** _The surrogate loss function L_ _Rand-RD_ ( _θ_ ) _can be decomposed to_
_two terms interpolating between return decomposition and uniform reward redistribution._



_L_ _Rand-RD_ ( _θ_ ) = _L_ _RD_ ( _θ_ ) + E
_τ_ _∼D_

�



�



�



_Var_
_I∼ρ_ _T_ ( _·_ )



_T_

_|I|_
�



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�



~~�~~ ~~�~~ � �
_variance of the Monte-Carlo estimator_



� ~~��~~ ~~�~~
_uniform reward redistribution_



(9)


_,_ (10)



�



�



1 _−_ _[K][ −]_ [1]
� _T −_ 1



�
_R_ _θ_ ( _s_ _t_ _, a_ _t_ ) _·_ [1]
� � _K_



= _L_ _RD_ ( _θ_ )
~~����~~
_return decomposition_



�



+ E
_τ_ _∼D_



_T_ [2] _·_ _Var_
( _s_ _t_ _,a_ _t_ ) _∼τ_



� ~~��~~ ~~�~~
_interpolation weight_



_where K denotes the length of sampled subsequences defined in Eq._ (7) _._


_Proof._ First, we note that random sampling serves an unbiased estimation. i.e.,



�



E
_I∼ρ_ _T_ ( _·_ )



_T_

_|I|_
�



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�



=



_T −_ 1
� _R_ � _θ_ ( _s_ _t_ _, a_ _t_ ) = � _R_ ep _,θ_ ( _τ_ ) _._


_t_ =0



We can decompose our loss function _L_ Rand-RD ( _θ_ ) as follows:



2 [�][�]

�

_R_ ep ( _τ_ ) _−_ _[T]_ � _R_ _θ_ ( _s_ _t_ _, a_ _t_ )
�� _|I|_ _t∈I_ �



_L_ Rand-RD ( _θ_ ) = E
_τ_ _∼D_


= E
_τ_ _∼D_


= E
_τ_ _∼D_



E

� _I∼ρ_ _T_ ( _·_ )


E

� _I∼ρ_ _T_ ( _·_ )


E

� _I∼ρ_ _T_ ( _·_ )



_R_ ep ( _τ_ ) _−_ _R_ [�] ep _,θ_ ( _τ_ ) + _R_ [�] ep _,θ_ ( _τ_ ) _−_ _|I|_ _[T]_
��


2 [��]
_R_ ep ( _τ_ ) _−_ _R_ [�] ep _,θ_ ( _τ_ )
�� �



2 [��]

�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )

_t∈I_ �



� ~~�~~ � ~~�~~
= _L_ RD ( _θ_ )



�



+ E
_τ_ _∼D_

�


+ E
_τ_ _∼D_

�



�

E _R_ ep _,θ_ ( _τ_ ) _−_ _[T]_
_I∼ρ_ _T_ ( _·_ ) �� _|I|_



�
_R_ ep _,θ_ ( _τ_ ) _−_ _|I|_ _[T]_
��



�
_R_ ep _,θ_ ( _τ_ ) _−_ _[T]_
��



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )

_t∈I_ � [�]



2� _R_ ep ( _τ_ ) _−_ _R_ [�] ep _,θ_ ( _τ_ )� _·_ E
_I∼ρ_ _T_ ( _·_ )



~~�~~ �� �
=0



2 [�]

�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )

_t∈I_ �



�



~~�~~ ~~��~~ ~~�~~



=Var [ ( _T/|I|_ ) _·_ [�] _t∈I_ _R_ [�] _θ_ ( _s_ _t_ _,a_ _t_ ) ]



_T_

_|I|_
�



_._

��



= _L_ RD ( _θ_ ) + E
_τ_ _∼D_



�



Var
_I∼ρ_ _T_ ( _·_ )



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�



_._



Note our sampling distribution defined in Eq. (7) refers to “ _sampling without replacement_ ” (Singh,
2003) whose variance can be further decomposed as follows:



_T_

_|I|_
�



��



_L_ Rand-RD ( _θ_ ) = _L_ RD ( _θ_ ) + E
_τ_ _∼D_


= _L_ RD ( _θ_ ) + E
_τ_ _∼D_



�

�



_T_ [2] _·_ Var
( _s_ _t_ _,a_ _t_ ) _∼τ_


15



Var
_I∼ρ_ _T_ ( _·_ )



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�



_._

� [�]



1 _−_ _[K][ −]_ [1]
� _T −_ 1



�
_R_ _θ_ ( _s_ _t_ _, a_ _t_ ) _·_ [1]
� � _K_




Published as a conference paper at ICLR 2022


The proof of Theorem 1 follows a particular form of bias-variance decomposition formula (Kohavi
& Wolpert, 1996). Similar decomposition form can also be found in other works in the literature of
reinforcement learning (Antos et al., 2008).


**Proposition 1** (Surrogate Upper Bound) **.** _The randomized return decomposition loss L_ _Rand-RD_ ( _θ_ ) _is_
_an upper bound of the actual return decomposition loss function L_ _RD_ ( _θ_ ) _, i.e., L_ _Rand-RD_ ( _θ_ ) _≥L_ _RD_ ( _θ_ ) _._


_Proof._ Note that the second term of Eq. (9) in Theorem 1 expresses the variance of a Monte-Carlo
estimator which is clearly non-negative. It directly gives _L_ Rand-RD ( _θ_ ) _≥L_ RD ( _θ_ ).


An alternative proof of Proposition 1 can be directly given by Jensen’s inequality.

**Proposition 2** (Objective Gap) **.** _Let L_ [(] _Rand-RD_ _[K]_ [)] [(] _[θ]_ [)] _[ denote the randomized return decomposition loss]_
_that samples subsequences with length K. The gap between L_ [(] _Rand-RD_ _[K]_ [)] [(] _[θ]_ [)] _[ and][ L]_ _[RD]_ [(] _[θ]_ [)] _[ can be reduced]_
_by using larger values of hyper-parameter K._


_∀θ,_ _L_ [(1)] _Rand-RD_ [(] _[θ]_ [)] _[ ≥L]_ [(2)] _Rand-RD_ [(] _[θ]_ [)] _[ ≥· · · ≥L]_ [(] _Rand-RD_ _[T][ −]_ [1)] [(] _[θ]_ [)] _[ ≥L]_ [(] _Rand-RD_ _[T]_ [ )] [(] _[θ]_ [) =] _[ L]_ _[RD]_ [(] _[θ]_ [)] _[.]_ (11)



_Proof._ In Eq. (10) of Theorem 1, the last term _K_ [1] 1 _−_ _[K]_ _T −_ _[−]_ 1 [1] monotonically decreases as the hyper
� �

parameter _K_ increases. When _K_ = _T_, this coefficient is equal to zero. It derives Eq. (11) in the
given statement.




[1] 1 _−_ _[K][−]_ [1]

_K_ _T −_ 1
�



_Proof._ In Eq. (10) of Theorem 1, the last term [1]



**Proposition 3** (Uniform Reward Redistribution) **.** _Assume all trajectories have the same length and_
_the parameterization space of θ serves universal representation capacity. The optimal solution θ_ _[⋆]_ _of_
_minimizing L_ [(1)] _Rand-RD_ [(] _[θ]_ [)] _[ is stated as follows:]_



�
_R_ _θ_ _⋆_ ( _s, a_ ) = E � _R_ _ep_ ( _τ_ ) _/T |_ ( _s, a_ ) _∈_ _τ_ � _,_ (12)
_τ_ _∼D_



_where L_ [(1)] _Rand-RD_ [(] _[θ]_ [)] _[ denotes the randomized return decomposition loss with][ K]_ [ = 1] _[.]_


_Proof._ Note that we assume all trajectories have the same length. The optimal solution of this leastsquares problem is given by



( _R_ ep ( _τ_ ) _−_ _T · r_ ) [2] [��] ( _s, a_ ) _∈_ _τ_
� � �



�
_R_ _θ_ _⋆_ ( _s, a_ ) = min E
_r∈_ R _τ_ _∼D_


= min E
_r∈_ R _τ_ _∼D_



1
� _T_ [2] [(] _[R]_ [ep] [(] _[τ]_ [)] _[/T][ −]_ _[r]_ [)] [2] [���] [(] _[s, a]_ [)] _[ ∈]_ _[τ]_ �



1
= min E
_r∈_ R _T_ [2] _τ_ _∼D_



( _R_ ep ( _τ_ ) _/T −_ _r_ ) [2] [��] ( _s, a_ ) _∈_ _τ_
� � �



= min E ( _R_ ep ( _τ_ ) _/T −_ _r_ ) [2] [��] ( _s, a_ ) _∈_ _τ_
_r∈_ R _τ_ _∼D_ � � �



= E
_τ_ _∼D_



� _R_ ep ( _τ_ ) _/T |_ ( _s, a_ ) _∈_ _τ_ � _,_



which depends on the trajectory distribution in dataset _D_ .


If we relax the assumption that all trajectories have the same length, the solution of the above leastsquares problem would be a weighted expectation as follows:



�
_R_ _θ_ _⋆_ ( _s, a_ ) = min E ( _R_ ep ( _τ_ ) _−_ _T_ _τ_ _· r_ ) [2] [��] ( _s, a_ ) _∈_ _τ_
_r∈_ R _τ_ _∼D_ � � �

= min _r∈_ R _τ_ _∼D_ E � _T_ _τ_ [2] _[·]_ [ (] _[R]_ [ep] [(] _[τ]_ [)] _[/T]_ _[τ]_ _[−]_ _[r]_ [)] [2] [���] [(] _[s, a]_ [)] _[ ∈]_ _[τ]_ �



= � _τ_ _∈D_ ~~�~~ : _τ_ ( _∈Ds,a_ ):( _∈s,aτ_ _[T]_ ) _∈_ _[τ]_ _[ ·]_ _τ_ _[ R][T]_ [ 2] _τ_ [ep] [(] _[τ]_ [)] _,_


where _T_ _τ_ denotes the length of trajectory _τ_ . This solution can still be interpreted as a uniform reward
redistribution, in which the dataset distribution is prioritized by the trajectory length.


16




Published as a conference paper at ICLR 2022


B E XPERIMENT S ETTINGS AND I MPLEMENTATION D ETAILS


B.1 M U J O C O B ENCHMARK WITH E PISODIC R EWARDS


**MuJoCo Benchmark with Episodic Rewards.** We adopt the same experiment setting as Gangwani et al. (2020) and compare our approach with baselines in a suite of MuJoCo locomotion benchmark tasks with episodic rewards. This experiment setting is commonly used in the literature (Mania
et al., 2018; Guo et al., 2018; Liu et al., 2019; Arjona-Medina et al., 2019; Gangwani et al., 2019;
2020). The environment simulator is based on OpenAI Gym (Brockman et al., 2016). These tasks
are long-horizon with maximum trajectory length _T_ = 1000. We modify the reward function of
these environments to set up an episodic-reward setting. Formally, on non-terminal states, the agent
will receive a zero signal instead of the per-step dense rewards. The agent can obtain the episodic
feedback _R_ ep ( _τ_ ) at the last step of the rollout trajectory, in which _R_ ep ( _τ_ ) is computed by the summation of per-step instant rewards given by the standard setting.


**Hyper-Parameter Configuration For MuJoCo Experiments.** In MuJoCo experiments, the policy optimization module of RRD is implemented based on soft actor-critic (SAC; Haarnoja et al.,
2018a). We evaluate the performance of our proposed methods with the same configuration of hyperparameters in all environments. The hyper-parameters of the back-end SAC follow the official technical report (Haarnoja et al., 2018b). We summarize our default configuration of hyper-parameters
as the following table:


Hyper-Parameter Default Configuration


discount factor _γ_ 0.99


# hidden layers (all networks) 2
# neurons per layer 256
activation ReLU
optimizer (all losses) Adam (Kingma & Ba, 2015)
learning rate 3 _·_ 10 _[−]_ [4]


initial temperature _α_ init 1.0
target entropy _−_ dim( _A_ )
Polyak-averaging coefficient 0.005
# gradient steps per environment step 1
# gradient steps per target update 1


# transitions in replay buffer 10 [6]
# transitions in mini-batch for training SAC 256
# transitions in mini-batch for training _R_ [�] _θ_ 256
# transitions per subsequence ( _K_ ) 64
# subsequences in mini-batch for training _R_ [�] _θ_ 4


Table 1: The hyper-parameter configuration of RRD in MuJoCo experiments.


In addition to SAC, we also provide the implementations upon DDPG (Lillicrap et al., 2016) and
[TD3 (Fujimoto et al., 2018) in our Github repository.](https://github.com/Stilwell-Git/Randomized-Return-Decomposition)


B.2 A TARI B ENCHMARK WITH E PISODIC R EWARDS


**Atari Benchmark with Episodic Rewards.** In addition, we conduct experiments in a suite of
Atari games with episodic rewards. The environment simulator is based on OpenAI Gym (Brockman
et al., 2016). Following the standard Atari pre-processing proposed by Mnih et al. (2015), we rescale
each RGB frame to an 84 _×_ 84 luminance map, and the observation is constructed as a stack of 4
recent luminance maps. We modify the reward function of these environments to set up an episodicreward setting. Formally, on non-terminal states, the agent will receive a zero signal instead of the
per-step dense rewards. The agent can obtain the episodic feedback _R_ ep ( _τ_ ) at the last step of the
rollout trajectory, in which _R_ ep ( _τ_ ) is computed by the summation of per-step instant rewards given
by the standard setting.


17




Published as a conference paper at ICLR 2022


**Hyper-Parameter Configuration For Atari Experiments.** In Atari experiments, the policy optimization module of RRD is implemented based on deep Q-network (DQN; Mnih et al., 2015). We
evaluate the performance of our proposed methods with the same configuration of hyper-parameters
in all environments. The hyper-parameters of the back-end DQN follow the technical report (Castro
et al., 2018). We summarize our default configuration of hyper-parameters as the following table:


Hyper-Parameter Default Configuration


discount factor _γ_ 0.99
# stacked frames in agent observation 4
# noop actions while starting a new episode 30


network architecture DQN (Mnih et al., 2015)
optimizer for Q-values Adam (Kingma & Ba, 2015)
learning rate for Q-values 6 _._ 25 _·_ 10 _[−]_ [5]

optimizer for _R_ [�] _θ_ Adam (Kingma & Ba, 2015)
learning rate for _R_ [�] _θ_ 3 _·_ 10 _[−]_ [4]


exploration strategy _ϵ_ -greedy
_ϵ_ decaying range - start value 1.0
_ϵ_ decaying range - end value 0.01
# timesteps for _ϵ_ decaying schedule 250000
# gradient steps per environment step 0.25
# gradient steps per target update 8000


# transitions in replay buffer 10 [6]
# transitions in mini-batch for training DQN 32
# transitions in mini-batch for training _R_ [�] _θ_ 32
# transitions per subsequence ( _K_ ) 32
# subsequences in mini-batch for training _R_ [�] _θ_ 1


Table 2: The hyper-parameter configuration of RRD in Atari experiments.


B.3 A N A LTERNATIVE I MPLEMENTATION OF R ANDOMIZED R ETURN D ECOMPOSITION


Recall that the major practical barrier of the least-squares-based return decomposition method specified by _L_ RD ( _θ_ ) is its scalability in terms of the computation costs. The trajectory-wise episodic
reward is the only environmental supervision for reward modeling. Computing the loss function
_L_ RD ( _θ_ ) with a single episodic reward label requires to enumerate all state-action pairs along the
whole trajectory.


Theorem 1 motivates an unbiased implementation of randomized return decomposition that optimizes _L_ RD ( _θ_ ) instead of _L_ Rand-RD ( _θ_ ). By rearranging the terms of Eq. (10), we can obtain the
difference between _L_ Rand-RD ( _θ_ ) and _L_ RD ( _θ_ ) as follows:



�



_L_ Rand-RD ( _θ_ ) _−L_ RD ( _θ_ ) = Var
_I∼ρ_ _T_ ( _·_ )



_T_

_|I|_
�



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�



_._



Note our sampling distribution _ρ_ _T_ ( _·_ ) defined in Eq. (7) refers to “ _sampling without replacement_ ”
(Singh, 2003) whose variance can be estimated as follows:






_,_




�



= _T_ [2] _·_ E
_I∼ρ_ _T_ ( _·_ )




_·_
_T_







_T_

 _[T][ −]_ _[K]_



Var
_I∼ρ_ _T_ ( _·_ )



_T_

_|I|_
�



�

� _R_ _θ_ ( _s_ _t_ _, a_ _t_ )


_t∈I_



�



� ¯ 2

� _t∈I_ � _R_ _θ_ ( _s_ _t_ _, a_ _t_ ) _−_ _R_ _θ_ ( _I_ ; _τ_ )�

_K_ ( _K −_ 1)



where _R_ [¯] _θ_ ( _I_ ; _τ_ ) = _|I|_ 1 � _t∈I_ _[R]_ [�] _[θ]_ [(] _[s]_ _[t]_ _[, a]_ _[t]_ [)][. Thus we can obtain an unbiased estimation of this variance]

penalty by sampling a subsequence _I_ . By subtracting this estimation from _L_ [�] Rand-RD ( _θ_ ), we can


18




Published as a conference paper at ICLR 2022


obtain an unbiased estimation of _L_ RD ( _θ_ ). More specifically, we can use the following sample-based
loss function to substitute Eq. (13) in implementation:



� 1
_L_ RD ( _θ_ ) =
_M_



_M_
�

_j_ =1







� _R_ ep ( _τ_ _j_ ) _−_ _|I_ _[T]_ _j_ _[j]_




_|I_ _j_ _|_



2

�
_R_ _θ_ ( _s_ _j,t_ _, a_ _j,t_ ) 
�





�

_t∈I_ _j_



~~�~~ ~~�~~ � ~~�~~
_L_ � Rand-RD ( _θ_ )



_t∈I_ _j_



�

_t∈I_ _j_



_−_ [1]

_M_



_M_
�

_j_ =1







_|I_ _j_ _|_ ( _|I_ _j_ _| −_

 _[T]_ _[j]_ [(] _[T]_ _[j]_ _[ −]_ _[K]_ [)]



_|I_ _j_ _[j]_ _|_ ( _|I_ _[j]_ _[ −]_ _j_ _| −_ 1) _[·]_ �



2

�
_R_ _θ_ ( _s_ _j,t_ _, a_ _j,t_ )  _._
�





� 1
� _R_ _θ_ ( _s_ _j,t_ _, a_ _j,t_ ) _−_ _|I_ _j_ _|_



The above loss function can be optimized through the same mini-batch training paradigm as what is
presented in Algorithm 1.


C E XPERIMENTS ON A TARI B ENCHMARK WITH E PISODIC R EWARDS













Figure 3: Learning curves on a suite of Atari benchmark tasks with episodic rewards. These curves
are plotted from 5 runs with random initializations. The shaded region indicates the standard deviation. To make the comparison more clear, the curves are smoothed by averaging 10 most recent
evaluation points. We set up an evaluation point every 5 _·_ 10 [4] timesteps.


Note that our proposed method does not restricts its usage to continuous control problems. It can
also be integrated in DQN-based algorithms to solve problems with discrete-action space. We evaluate the performance of our method built upon DQN in several famous Atari games. The reward
redistribution problem in these tasks is more challenging than that in MuJoCo locomotion benchmark since the task horizon of Atari is much longer. For example, the maximum task horizon in
Pong can exceed 20000 steps in a single trajectory. This setting highlights the scalability advantage
of our method, i.e., the objective of RRD can be optimized by sampling short subsequences whose
computation cost is manageable. The experiment results are presented in Figure 3. Our method
outperforms all baselines in 3 out of 4 tasks. We note that IRCR outperforms RRD in RoadRunner.
It may be because IRCR is non-parametric and thus does not suffer from the difficulty of processing
visual observations.


D V ISUALIZING THE P ROXY R EWARDS OF RRD















Figure 4: Visualization of the proxy rewards learned by RRD in MuJoCo locomotion tasks.


19




Published as a conference paper at ICLR 2022


In MuJoCo locomotion tasks, the goal of agents is running towards a fixed direction. In Figure 4,
we visualize the correlation between per-step forward distance and the assigned proxy reward. We
uniformly collected 10 [3] samples during the first 10 [6] training steps. “Agent Forward Motion” denotes
the forward distance at a single step. “Proxy Reward _R_ [�] _θ_ ( _s, a_ )” denotes the immediate proxy reward
assigned at that step. It shows that the learned proxy reward has high correlation to the forward
distance at that step.


E A N A BLATION S TUDY ON THE H YPER -P ARAMETERS OF IRCR


We note that Gangwani et al. (2020) uses a different hyper-parameter configuration from the standard
SAC implementation (Haarnoja et al., 2018b). The differences exist in two hyper-parameters:


Hyper-Parameter Default Configuration


Polyak-averaging coefficient 0.001
# transitions in replay buffer 3 _·_ 10 [5]
# transitions in mini-batch for training SAC 512


Table 3: The hyper-parameters used by Gangwani et al. (2020) in MuJoCo experiments.


To establish a rigorous comparison, we evaluate the performance of IRCR with the hyper-parameter
configuration proposed by Haarnoja et al. (2018b), so that IRCR and RRD use the same hyperparameters in their back-end SAC agents. The experiment results are presented in Figure 5.














|000<br>000<br>000<br>000<br>000<br>000<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|
|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br>||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br>||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br>||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br>||||






|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||


|000<br>000<br>000<br>000<br>000<br>000<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps<br>Swimmer-v2<br>360<br>300<br>240<br>180<br>120<br>60<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>60<br>120<br>180<br>240<br>300<br>360<br>Swimmer~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>60<br>120<br>180<br>240<br>300<br>360<br>Swimmer~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>60<br>120<br>180<br>240<br>300<br>360<br>Swimmer~~-~~v2|||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>000<br>000<br>000<br>000<br>000<br>000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>60<br>120<br>180<br>240<br>300<br>360<br>Swimmer~~-~~v2|||||||


|600<br>000<br>400<br>800<br>200<br>600<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|
|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>600<br>200<br>800<br>400<br>000<br>600<br>||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>600<br>200<br>800<br>400<br>000<br>600<br>||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>600<br>200<br>800<br>400<br>000<br>600<br>||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>600<br>200<br>800<br>400<br>000<br>600<br>||||


|6000<br>5000<br>4000<br>3000<br>2000<br>1000<br>0<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps<br>HumanoidStandup-v2<br>50000<br>25000<br>00000<br>75000<br>50000<br>25000<br>0M 0.5M 1M 1.5M 2M 2.5M 3M<br>Timesteps|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>25000<br>50000<br>75000<br>00000<br>25000<br>50000<br>HumanoidStandup~~-~~v2||||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>25000<br>50000<br>75000<br>00000<br>25000<br>50000<br>HumanoidStandup~~-~~v2||||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>25000<br>50000<br>75000<br>00000<br>25000<br>50000<br>HumanoidStandup~~-~~v2||||||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>25000<br>50000<br>75000<br>00000<br>25000<br>50000<br>HumanoidStandup~~-~~v2||||||||



Figure 5: An ablation study on the hyper-parameter configuration of IRCR. The curves of “IRCR”
refer to the performance of IRCR using the hyper-parameter setting proposed by Gangwani et al.
(2020). The curves of “IRCR (ablation)” refer to the performance of IRCR using the hyperparameters stated in Table 1. All curves are plotted from 30 runs with random initializations.


As shown in Figure 5, the hyper-parameters tuned by Gangwani et al. (2020) is more stable in most
environments. Although using hyper-parameters stated in Table 1 can improve the performance in
some cases, the overall performance cannot outperform RRD.

|n Ant-v2|Col2|Ant-v|v2|
|---|---|---|---|
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||||
|~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>0<br>1000<br>2000<br>3000<br>4000<br>5000<br>6000<br><br><br>~~0M 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br>400<br>320<br>240<br>160<br>80<br>0<br><br>Reacher~~-~~v2<br>gure 5: An ablation stu<br>er to the performance<br>020).<br>The curves of<br>rameters stated in Tabl<br> shown in Figure 5, the<br>vironments. Although<br>me cases, the overall pe||~~ 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br> 5: An ablation stu<br>o the performance<br>.<br>The curves of<br>eters stated in Tabl<br>wn in Figure 5, the<br>nments. Although<br>cases, the overall pe|~~ 0.5M 1M 1.5M 2M 2.5M 3M~~<br>Timesteps<br> 5: An ablation stu<br>o the performance<br>.<br>The curves of<br>eters stated in Tabl<br>wn in Figure 5, the<br>nments. Although<br>cases, the overall pe|



F R ELATED W ORK


**Delayed Feedback.** Tackling environmental delays is a long-lasting problem in reinforcement
learning and control theory (Nilsson et al., 1998; Walsh et al., 2009; Zhou et al., 2018; 2019a; H´eliou
et al., 2020; Nath et al., 2021; Tang et al., 2021). In real-world applications, almost all environmental
signals have random delays (Schuitema et al., 2010; Hester & Stone, 2013; Amuru & Buehrer, 2014;
Lei et al., 2020), which is a fundamental challenge for the designs of RL algorithms. A classical
method to handle delayed signals is stacking recent observations within a small sliding window as


20




Published as a conference paper at ICLR 2022


the input for decision-making (Katsikopoulos & Engelbrecht, 2003). This simple transformation
can establish a Markovian environment formulation, which is widely used to deal with short-term
environmental delays (Mnih et al., 2015). Many recent works focus on establishing sample-efficient
off-policy RL algorithm that is adaptive to delayed environmental signals (Bouteiller et al., 2021;
Han et al., 2021). In this paper, we consider an extreme delay of reward signals, which is a harder
problem setting than short-term random delays.


**Reward Design.** The ability of reinforcement learning agents highly depends on the designs of
reward functions. It is widely observed that reward shaping can accelerate learning (Mataric, 1994;
Ng et al., 1999; Devlin et al., 2011; Wu & Tian, 2017; Song et al., 2019). Many previous works
study how to automatically design auxiliary reward functions for efficient reinforcement learning. A
famous paradigm, inverse reinforcement learning (Ng & Russell, 2000; Fu et al., 2018), considers
to recover a reward function from expert demonstrations. Several works consider to learn a reward
function from expert labels of preference comparisons (Wirth et al., 2016; Christiano et al., 2017;
Lee et al., 2021), which is a form of weak supervision. Another branch of work is learning an
intrinsic reward function from experience that guides the agent to maximize extrinsic objective (Sorg
et al., 2010; Guo et al., 2016). Such an intrinsic reward function can be learned through metagradients (Zheng et al., 2018; 2020) or self-imitation (Guo et al., 2018; Gangwani et al., 2019). A
recent work (Abel et al., 2021) studies the expressivity of Markov rewards and proposes algorithms
to design Markov rewards for three notions of abstract tasks.


**Temporal Credit Assignment.** Another methodology for tackling long-horizon sequential decision problems is assigning credits to emphasize the contribution of each single step over the temporal
structure. These methods directly consider the specification of the step values instead of manipulating the reward function. The simplest example is studying how the choice of discount factor _γ_
affects the policy learning (Petrik & Scherrer, 2008; Jiang et al., 2015; Fedus et al., 2019). Several
previous works consider to extend the _λ_ -return mechanism (Sutton, 1988) to a more generalized
credit assignment framework, such as adaptive _λ_ (Xu et al., 2018) and pairwise weights (Zheng
et al., 2021). RUDDER (Arjona-Medina et al., 2019) proposes a return-equivalent formulation for
the credit assignment problem and establish theoretical analyses (Holzleitner et al., 2021). AlignedRUDDER (Patil et al., 2020) considers to use expert demonstrations for higher sample efficiency.
Harutyunyan et al. (2019) opens up a new family of algorithms, called hindsight credit assignment,
that attributes the credits from a backward view.


**Value Decomposition.** This paper follows the paradigm of reward redistribution that aims to decompose the return value to step-wise reward signals. The simplest mechanism in the literature
is the uniform reward redistribution considered by Gangwani et al. (2020). It can be effectively
integrated with off-policy reinforcement learning and thus achieves state-of-the-art performance in
practice. Least-squares-based reward redistribution is investigated by Efroni et al. (2021) from a
theoretical point of view. Chatterji et al. (2021) extends the theoretic results to the logistic reward
model. In game theory and multi-agent reinforcement learning, a related problem is how to attribute
a global team reward to individual rewards (Nguyen et al., 2018; Du et al., 2019; Wang et al., 2020),
which provide agents incentives to optimize the global social welfare (Vickrey, 1961; Clarke, 1971;
Groves, 1973; Myerson, 1981). A promising paradigm for multi-agent credit assignment is using
structural value representation (Sunehag et al., 2018; Rashid et al., 2018; Son et al., 2019; B¨ohmer
et al., 2020; Wang et al., 2021a;b), which supports end-to-end temporal difference learning. This
paradigm transforms the value decomposition to the structured prediction problem. A future work
is integrating prior knowledge of the decomposition structure as many previous works for structured
prediction (Chen et al., 2020; Tavakoli et al., 2021).


21


