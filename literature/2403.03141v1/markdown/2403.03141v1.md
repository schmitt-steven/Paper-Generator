## **Language Guided Exploration for RL Agents in Text Environments**

**Hitesh Golchha** _[‚Ä¢]_ **, Sahil Yerawar** _[‚Ä¢]_ **, Dhruvesh Patel** _[‚Ä¢]_ **, Soham Dan** _[‚ñ≥]_ **, Keerthiram Murugesan** _[‚ñ≥]_

_‚Ä¢_ Manning College of Information & Computer Sciences, University of Massachusetts Amherst

_‚ñ≥_ IBM Research

{hgolchha,syerawar,dhruveshpate}@cs.umass.edu
{soham.dan,keerthiram.murugesan}@ibm.com


**Abstract**



Real-world sequential decision making is characterized by sparse rewards and large decision
spaces, posing significant difficulty for experiential learning systems like _tabula rasa_ reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world
knowledge, can help RL agents learn quickly
and adapt to distribution shifts. In this work, we
introduce Language Guided Exploration (LGE)
framework, which uses a pre-trained language
model (called G UIDE ) to provide decision-level
guidance to an RL agent (called E XPLORER ).
We observe that on ScienceWorld (Wang et al.,
2022), a challenging text environment, LGE
outperforms vanilla RL agents significantly
and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision
Transformer.


**1** **Introduction**


Reinforcement Learning (RL) has been used with
great success for sequential decision making tasks.
AI assistants whether text based (Li et al., 2022;
Huang et al., 2022) or multi-modal (Chang et al.,
2020; Patel et al., 2023), have to work with large
action spaces and sparse rewards. In such settings,
the approach of random exploration is inadequate.
One needs to look for ways to use external information either to create a dense reward model or to

reduce the size of action space. In this work we
focus on the latter approach.
We make a simple observation that, in many
cases, the textual description of the task or goal
contains enough information to completely rule
out certain actions, thereby greatly reducing the
size of the effective action space. For example,
as shown in Fig.1, if the task description is _‚ÄúDe-_
_termine if a metal fork is electrically conductive‚Äù_,
then one can safely rule out actions that involve
objects like sink, apple, and actions like eat, smell,
etc. Motivated by this observation, we introduce


|Task: Determine if a metal fork is electrically<br>conductive‚Ä¶<br>available objects<br>templates at time ùíï<br>connect _ to _ sink<br>metal fork<br>eat __ thermometer<br>look in __ apple<br>battery<br>pick up __<br>wire<br>Combinatorially large<br>space of actions<br>Observation|Guide<br>‚Ä¶.<br>Task ‚Ä¶.<br>pick up,<br>connect,,<br>eat ‚Ä¶.<br>pick up wire pick up metal fork|
|---|---|
|thermometer<br>sink<br>metal fork<br>wire<br>apple<br>battery<br>_connect _ to __<br>_eat ___<br>_pick up ___<br>_look in ___<br>Combinatorially large<br>space of actions<br>templates<br>available objects<br>at timeùíï<br>**Task**: Determine if a metal fork is electrically<br>conductive‚Ä¶<br>Observation|_connect_wire_to_batter<br>|



Figure 1: The Language Guided Exploration (LGE)
Framework: The _Guide_ uses contrastive learning to produce a set of feasible action given the task description
thereby reducing substantially the space of possible actions. The _Explorer_, an RL agent, then uses the set of
actions provided by the _Guide_ to learn a policy and pick
a suitable action using it.


the **L** anguage **G** uided **E** xploration (LGE) framework that uses an RL agent but augments it with
a _Guide_ model that uses world knowledge to rule
out large number of actions that are infeasible or
highly unlikely. Along with removing irrelevant
actions, the frameworks supports generalization in
unseen environments where new objects may appear. For example, if the model observed an apple
in the environment during training, at test time, the
environment may contain an orange instead. But
the guide, which posses commonsense may understand that all fruits are equally relevant or irrelevant
for the given task.
To test our framework, we use the highly challenging benchmark called S CIENCE W ORLD (Wang
et al., 2022), which consists of a purely text based
environment where the observations, actions, and
inventory are expressed using natural language text.
S CIENCE W ORLD embodies the major challenges
faced by RL agents in realy world applications: the
template based actions with slots for verbs and ob



jects produce a combinatorially large action space,
the long natural language based observations make
for a challenging state representation, and the rewards signals based mainly on the completion of
challenging tasks create a delayed and sparse reward signal. Following are the main contributions
of our work:

We propose a novel way to allow language guided
exploration for RL agents. The task instructions
are used to identify relevant actions using a contrastively trained LM. The proposed G UIDE model
that uses contrastive learning has not been explored
for text environments before.

We demonstrate significantly stronger results on
the S CIENCE W ORLD environment when com
pared to methods that use Reinforcement Learning,
and more sophisticated methods like Behaviour
Cloning (Wang et al., 2023) and Text Decision
Transformer (Chen et al., 2021).


**2** **Related Work**


Text-based environments (Lebling et al., 1979; Yin
and May, 2019; Murugesan et al., 2020; C√¥t√© et al.,
2019) provide a low-cost alternative to complex
2D/3D environments, and real world scenarios, for
the development of the high-level learning and
navigation capabilities of the AI agents. Due to
the complexity of these environments, _tabula rasa_
RL agents (He et al., 2016; Zahavy et al., 2018;
Yao et al., 2020) struggle to learn anything useful. Therefore several methods like imitation learn
ing, use of knowledge graphs (Ammanabrolu and
Hausknecht, 2020), Case-Based Reasoning (Atzeni
et al., 2022), behavior cloning (Chen et al., 2021),
intrinsically motivated RL, and language motivated
RL (Du et al., 2023; Adeniji et al., 2023) have been
proposed. The main aim of all these methods is
to use external knowledge or a handful of gold trajectories to guide the learning. In our work, we
address the same issue in a much direct and generalizable manner by reducing the size of the action
space using an auxiliary model called the Guide.


**3** **Methodology**


**Notation:** The text environment, a partially observable Markov decision process (POMDP) consists of ( _S, T, A, R,_ _O,_ [Àú] ‚Ñ¶) . In S CIENCE W ORLD,
along with the description of the current state,
the observation also consists of a task description
_œÑ ‚ààT_ that stays fixed throughout the evolution of
a single trajectory, i.e., _O_ [Àú] = _O √ó T_, where _O_ is



the set of textual descriptions of the state and _T_ is
the set of tasks (including different variations of
each task). Note that the set of tasks are divided
into different types and each type of task has differ_V_ _Œ≥_
ent variations, i.e., _T_ = [ÔøΩ] [Œì] _Œ≥_ =1 ÔøΩ _v_ =1 _[œÑ]_ _[Œ≥,v]_ [, where] [ Œì]
is the number of task types and _V_ _Œ≥_ is the number
of variations for the task type _Œ≥_ . Gold trajectories
_G_ _Œ≥,v_ = _{a_ 1 _, a_ 2 _, .., a_ _T_ _}_ are available for each _Œ≥_, _v_ .


**3.1** **The LGE framework**


We propose a Language Guided Exploration Framework (LGE), which consists of an an RL agent
called the E XPLORER, and an auxiliary model
that scores each action called the G UIDE . The E X 

PLORER starts in some state sampled from initial
state distribution _d_ 0 . At any time step _t_, a set of all
valid actions _A_ _Œ≥,v,t_ is provided by the environment.
This set, constructed using the cross product of
action templates and the set of objects (see Fig.1)
is extremely large, typically in thousands. The
G UIDE uses the task description _œÑ_ _Œ≥,v_, to produce a
set of most relevant actions _A_ [ÀÜ] _Œ≥,v,t_ _‚äÇ_ _A_ _Œ≥,v,t_ . With a
probability 1 _‚àí_ _œµ_ (resp. _œµ_ ), the E XPLORER samples
an action from _A_ [ÀÜ] _Œ≥,v,t_ using its policy _œÄ_ ( _a|s_ _t_ ) (resp.,
from _A_ _Œ≥,v,t_ ). Algorithm 1 in Appendix A.1 outlines the steps involved in the LGE framework
using a DRRN (He et al., 2016) based E XPLORER .


**3.1.1** **E** **XPLORER**


The E XPLORER learns a separate policy _œÄ_ _Œ≥_ for
each task type _Œ≥ ‚àà_ Œì by exploring the the environment. [1] We use the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016) as our
E XPLORER, as it has shown to be the strongest
baseline in Wang et al. (2022). However, our framework allows to swap the DRRN with any other RL
agent. The DRRN uses Q-learning with with prioritized experience replay to perform policy improvement using a parametric approximation of the
action value function _Q_ ( _s, a_ ) . [2] The current state
_s_ _t_ is represented by concatenating the representations of the past observation _o_ _t‚àí_ 1, inventory _i_ _t_ and
look around _l_ _t_, each encoded by separate GRUs,
i.e., _h_ _s_ _t_ = ( _f_ _Œ∏_ _o_ ( _o_ _t‚àí_ 1 ) : _f_ _Œ∏_ _i_ ( _i_ _t_ ) : _f_ _Œ∏_ _l_ ( _l_ _t_ ) ) _._ Each
relevant action _a ‚àà_ _A_ rel _,t_ is encoded in the same
manner: _h_ _a_ _t_ = _f_ _Œ∏_ _a_ ( _a_ _t_ ) _._ Here _f_ _‚àó_ are the respective GRU encoders, _Œ∏_ _‚àó_ their parameters and ‚Äú : ‚Äù


1 The agent learns a separate policy of each task type but
this policy is common across all variations for that particular
task type.
2 We follow the implementation of DRRN provided in
Hausknecht et al. (2019).




denotes concatenation. The value function _Q_ ( _s, a_ )
is represented using a linear layer over the concatenation of the action and state representations
_Q_ ( _s_ _t_ _, a_ _t_ _|Œ∏_ ) = _W_ _[T]_ _¬∑_ ( _h_ _s_ _t_ : _h_ _a_ _t_ ) + _b,_ where _Œ∏_ is
a collection of _Œ∏_ _o_, _Œ∏_ _i_, _Œ∏_ _l_, _Œ∏_ _a_, _W_ and _b_ . During
training, a stochastic policy based on the value
function is used: ÀÜ _a ‚àº_ _œÄ_ ( _a|s_ ) _‚àù_ _Q_ ( _s, a|Œ∏_ ), while
at inference time we use greedy sampling: ÀÜ _a_ =
arg max _a_ _Q_ ( _s, a|Œ∏_ ).


**3.1.2** **G** **UIDE**


While LLMs are capable of scoring the relevant actions without any finetuning, we observed that due
to the idiosyncrasies of the S CIENCE W ORLD environment, it is beneficial to perform some finetuning.
We use SimCSE (Gao et al., 2021), a contrastive
learning framework, to finetune the G UIDE LM.
The training data _{œÑ_ _i_ _, G_ _i_ _}_ _[M]_ _i_ =1 [, which consists of]
task descriptions _œÑ_ _i_ = _œÑ_ _Œ≥,v_ _‚ààT_ along with the
set of corresponding gold actions _G_ _i_ = _G_ _Œ≥,v_ . The
G UIDE model _g_ _œï_ is used to embed the actions and
the task descriptions into a shared representation
space where the similarity score of a task and an

_[g]_ _[œï]_ [(] _[a]_ [)]
action is expressed as _s_ ( _œÑ, a_ ) = _[g]_ _[œï]_ [(] _[œÑ]_ [)] _[ ¬∑]_ _Œª_, with

_Œª_ being the temperature parameter. The training
objective is such that the embeddings of a task are
close to those of the corresponding relevant actions,
expressed using the following loss function:



_e_ _[s]_ [(] _[œÑ]_ _[i]_ _[, a]_ [+] [)]
_l_ ( _œï_ ; _œÑ_ _i_ _, G_ _i_ ) = _‚àí_ log

[+]



_e_ _[s]_ [(] _[œÑ]_ _[i]_ _[,a]_ [+] [)] + ~~ÔøΩ~~




_[,]_

~~ÔøΩ~~ _e_ _[s]_ [(] _[œÑ,a]_ _[‚àí]_ [)]

_a_ _[‚àí]_ _‚ààN_ _i_



where _a_ [+] _‚àº_ _G_ _i_ is a relevant action and _N_ _i_ is a
fixed sized subset of irrelevant actions. [3]

Note that since we only have access to a small
amount of gold trajectories (3442) for training, we
take special steps to avoid overfitting, which is the
main issue plaguing the imitation learning based
methods. First, we only provide the task description to the G UIDE and not the full state information.

Second, unlike the E XPLORER, which uses different policy for each task type, we train a common
G UIDE across all tasks.


**4** **Experiments and Results**


As done in Wang et al. (2022), the variations of
each task type are divided into training, validation
and test sets. Both G UIDE and E XPLORER are

trained only using the training variations.


3 Details of the models used and the training data are provided in Appendix A.1.



**Model** **top-k** **RSR** **MAP** **GAR** **GAR (%)** **GARR**


_G_ _g_ 50 0.71 0.52 N/A N/A N/A
_G_ _œÑ_ 50 0.9 0.66


50 0.99 0.68

Guide 20 0.94 0.67 7.4 ¬± 16.2 1.8 ¬± 9.4 0.31 ¬± 2.3

10 0.79 0.61


Table 1: Various metrics used to evaluate the G UIDE
in isolation. Note that for the baselines _G_ _g_ and _G_ _œÑ_, we
cannot compute GAR.


**4.1** **Evaluating the G** **UIDE**


Before the joint evaluation, we evaluate the G UIDE
in isolation. We sample 5 variations from the validation set for each task type and compute the three
metrics: GAR, RST and MAP. We use the following two intuitive but strong baselines:
**(1) Gold per-task (** _G_ _œÑ_ **)** : We create a set of 50
most most used actions in gold trajectories of all
training variations of a particular task. The Gold
per-task baseline, predicts an action to be relevant
if it belongs to this set.
**(2) Gold Global (** _G_ _g_ **)** : Similar to Gold per-task
but we use 50 most used actions in Gold trajectories
of all training variations for all tasks.


**Gold Action Rank (GAR):** At any time step _t_,
_GAR_ ( _Œ≥, v, t_ ) is defined as the rank of the gold _a_ _t_ in
the set of valid actions _A_ _Œ≥,v,t_, and the Gold Action
Reciprocal Rank (GARR) is defined as 1/GAR.
Since the size of _A_ _Œ≥,v,t_ is variable for every _t_, we
also report percent GAR. As seen in Table 1, the
gold action gets an average rank of 7 _._ 42, which is
impressive because _|A_ _Œ≥,v,t_ _|_ averages around 2000.


**Relevant Set Recall (RSR):** GAR ranks a single
optimal action at any time, but multiple valid action
sequences may exist for task completion. Although
all viable paths are not directly accessible, we estimate them. For each time step _t_ in variation _œÑ_ _Œ≥,v_,
a set of gold relevant actions _A_ [Àú] _Œ≥,v,t_ is identified
by intersecting the gold trajectory _G_ _Œ≥,v_ with valid
actions at _t_, so _A_ [Àú] _Œ≥,v,t_ = _{a | a ‚àà_ _G_ _Œ≥,v_ _‚à©_ _A_ _Œ≥,v,t_ _}_ .
The G UIDE ‚Äôs effectiveness is measured by its recall
of this set, considering its top-k predicted actions _A_ ÀÜ _Œ≥,v,t_ . Relevant Set Recall (RSR) is calculated as

_A_ _Œ≥,v,t_ _‚à©A_ [Àú] _Œ≥,v,t_ _|_
_RSR_ ( _Œ≥, v, t_ ) = _[|]_ [ ÀÜ] _|A_ [Àú] _Œ≥,v,t_ _|_ _._ As seen in Table 1,
the G UIDE has almost perfect average recall of 0.99
while selecting top 50 actions for the E XPLORER
at every step of the episode.


**Mean Avg. Precision (MAP):** The G UIDE also
functions as a binary classifier, predicting the rele



|Relevant Gold Actions|Selected By GUIDE|
|---|---|
|open cupboard,focus on soap in kitchen,<br>move soap in kitchen to metal pot,<br>move metal pot to stove,<br>go to outside,wait1,<br>pick up thermometer,<br>pick up metal pot,look around,activate stove|pick up metal pot,move metal pot to sink,pour metal pot into metal pot,<br>open cupboard, activate stove,move metal pot to stove, pick up thermometer,<br>open freezer, wait, go to outside, open glass jar,look around, open drawer in cupboard,<br>open drawer in counter,open oven,move ceramic cup to sink,pick up ceramic cup,open fridge,<br>open door to hallway, activate sink, mix metal pot, pour ceramic cup into ceramic cup,<br>pick up sodium chloride, wait1, focus on metal pot, pick up soap in kitchen|



Table 2: Column 1 shows the relevant gold actions for the task ‚ÄúChange of State (variation 1 from the dev set)‚Äù, and
column two shows the set of actions selected by the G UIDE . The missed gold actions are in Red, while selected
gold actions are in Green



vance of each action in _A_ _Œ≥,v,t_ . Using a thresholdfree metric like average precision score (Pedregosa
et al., 2011), the G UIDE achieves a superior average precision score of 0.68 compared to baselines. Coupled with perfect recall at 50, this indicates the G UIDE ‚Äôs strong generalization ability
on new variations and robust performance across
various thresholds. We observe that the threshold

that produces best MAP is 0.52, which corresponds
to _|A_ [ÀÜ] _Œ≥,v,t_ _|_ = 28 on average. So, to be conservative,
we use _k_ = 50 in the subsequent evaluations. Table
5 shows an example of the set of actions selected
by G UIDE for the task ‚ÄúChange of state‚Äù.


**4.2** **Evaluating LGE**


We follow the same evaluation protocol as (Wang
et al., 2022) and evaluate two versions of the LGE
framework, one with a fixed _œµ_ of 0.1 and the other
with _œµ_ increasing from 0 to 1. Table 3 reports the
means returns for each task.

**LGE improves significantly over the RL base-**
**line.** The DRRN agent, which only uses RL, performs the best among the baselines. The proposed
LGE framework (last two columns), improves the
performance of DRRN on 18 out of 30 tasks. On
average the LGE with _œµ_ = 0 _._ 1, improves the mean
returns by 35% (0 _._ 17 _‚Üí_ 0 _._ 23).
**LGE is better than much more complex, special-**
**ized methods.** The behaviour cloning (BC) model,
uses a Macaw (Tafjord and Clark, 2021) model finetuned on the gold trajectories to predict the next action. The Text Decision Transformer (TDT) (Chen
et al., 2021) models the complete POMDP trajectories as a sequence and is capable of predicting
actions that maximize long-term reward. As seen in
Table 3, the simpler LGE framework outperforms
both TDT and BC. This shows the importance of
having an RL agent in the framework that can adapt
to the peculiarities of the environment.
**Increasing** _œµ_ **does not always help.** _œµ_ = 1 corresponds using only the E XPLORER ‚Äîideal once the
policy is trained well. However, we observe that



**Task** DRRN* BC* TDT* LGE inc LGE fix Delta


T4 0.08 0.01 0.02 0.08 0.08 0.00

T5 0.06 0.01 0.02 0.06 **0.07** 0.01( _‚Üë_ )
T6 0.10 0.04 0.04 0.08 **0.11** 0.01( _‚Üë_ )

T11 0.19 0.21 0.19 **0.39** 0.19 0.20( _‚Üë_ )
T12 0.26 0.29 0.16 0.18 **0.56** 0.30( _‚Üë_ )
T13 0.56 0.19 0.17 0.55 **0.60** 0.04( _‚Üë_ )
T14 0.19 0.17 0.19 0.19 **0.67** 0.48( _‚Üë_ )

T17 0.20 0.06 0.10 0.21 **0.25** 0.05( _‚Üë_ )
T18 0.29 0.16 0.20 **0.30** 0.27 0.01( _‚Üë_ )

T19 0.11 0.05 0.07 0.11 0.11 0.00

T23 0.10 0.02 0.07 0.17 **0.18** 0.08( _‚Üë_ )
T24 0.09 0.04 0.02 **0.16** 0.05 0.07( _‚Üë_ )

T28 0.19 0.06 0.06 0.19 **0.22** 0.03( _‚Üë_ )

T29 0.17 0.13 0.05 0.17 0.16 0.00


**Avg.** 0.17 0.08 0.08 0.20 **0.23** 0.06( _‚Üë_ )


Table 3: Zero-shot performance of the agents on test
variations of across all tasks. The columns with * are

reported from Wang et al. (2022). The Delta column is
the difference between DRRN and the best LGE model.

The names of the tasks are in Table 4 in Appendix.


the actions provided by the G UIDE almost always
contain the right action and increasing _œµ_ does not
always help.


**5** **Conclusion**


We proposed a simple and effective framework for
using the knowledge in LMs to guide RL agents in
text environments, and showed its effectiveness on

the S CIENCE W ORLD environment when used with

DRRN. Our framework is generic and can extend
to work with other RL agents. We believe that the
positive results observed in our work will pave the
way for future work in this area.




**6** **Limitations**


Our work is the first to use a pre-trained language
model as a guide for RL agents in text environments. This paper focuses on the ScienceWorld
environment, which is an English only environment. Moreover, it focuses mainly on scientific
concepts and skills. To explore other environments
in different languages with different RL agents will
be an interesting future work.


**References**


Ademi Adeniji, Amber Xie, Carmelo Sferrazza, Young[gyo Seo, Stephen James, and P. Abbeel. 2023. Lan-](https://api.semanticscholar.org/CorpusID:261075941)
[guage reward modulation for pretraining reinforce-](https://api.semanticscholar.org/CorpusID:261075941)
[ment learning.](https://api.semanticscholar.org/CorpusID:261075941) _ArXiv_, abs/2308.12270.


Prithviraj Ammanabrolu and Matthew Hausknecht.
[2020. Graph constrained reinforcement learning for](https://openreview.net/forum?id=B1x6w0EtwH)
[natural language action spaces. In](https://openreview.net/forum?id=B1x6w0EtwH) _International Con-_
_ference on Learning Representations_ .


Mattia Atzeni, Shehzaad Zuzar Dhuliawala, Keerthiram Murugesan, and MRINMAYA SACHAN. 2022.
[Case-based reasoning for better generalization in tex-](https://openreview.net/forum?id=ZDaSIkWT-AP)
[tual reinforcement learning. In](https://openreview.net/forum?id=ZDaSIkWT-AP) _International Confer-_
_ence on Learning Representations_ .


Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan
Adeli, Li Fei-Fei, and Juan Carlos Niebles. 2020.
Procedure planning in instructional videos. In _Eu-_
_ropean Conference on Computer Vision_, pages 334‚Äì
350. Springer.


Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,
Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. Decision
transformer: Reinforcement learning via sequence
modeling. _arXiv preprint arXiv:2106.01345_ .


Marc-Alexandre C√¥t√©, √Åkos K√°d√°r, Xingdi Yuan, Ben
Kybartas, Tavian Barnes, Emery Fine, James Moore,
Matthew Hausknecht, Layla El Asri, Mahmoud
Adada, Wendy Tay, and Adam Trischler. 2019.
[Textworld: A learning environment for text-based](https://doi.org/10.1007/978-3-030-24337-1_3)
[games.](https://doi.org/10.1007/978-3-030-24337-1_3) _Computer Games_, page 41‚Äì75.


Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understanding. _ArXiv_, abs/1810.04805.


Yuqing Du, Olivia Watkins, Zihan Wang, C√©dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta,
[and Jacob Andreas. 2023. Guiding pretraining in](https://proceedings.mlr.press/v202/du23f.html)
[reinforcement learning with large language models.](https://proceedings.mlr.press/v202/du23f.html)
In _Proceedings of the 40th International Conference_
_on Machine Learning_, volume 202 of _Proceedings_
_of Machine Learning Research_, pages 8657‚Äì8677.
PMLR.



Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
[SimCSE: Simple contrastive learning of sentence em-](https://doi.org/10.18653/v1/2021.emnlp-main.552)
[beddings. In](https://doi.org/10.18653/v1/2021.emnlp-main.552) _Proceedings of the 2021 Conference_
_on Empirical Methods in Natural Language Process-_
_ing_, pages 6894‚Äì6910, Online and Punta Cana, Dominican Republic. Association for Computational
Linguistics.


Matthew J. Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre C√¥t√©, and Xingdi Yuan. 2019. Interactive
fiction games: A colossal adventure. In _AAAI Con-_
_ference on Artificial Intelligence_ .


Ji He, Mari Ostendorf, Xiaodong He, Jianshu Chen,
[Jianfeng Gao, Lihong Li, and Li Deng. 2016. Deep](https://doi.org/10.18653/v1/D16-1189)
[reinforcement learning with a combinatorial action](https://doi.org/10.18653/v1/D16-1189)
[space for predicting popular Reddit threads. In](https://doi.org/10.18653/v1/D16-1189) _Pro-_
_ceedings of the 2016 Conference on Empirical Meth-_
_ods in Natural Language Processing_, pages 1838‚Äì
1848, Austin, Texas. Association for Computational
Linguistics.


Wenlong Huang, Pieter Abbeel, Deepak Pathak, and
Igor Mordatch. 2022. Language models as zero-shot
planners: Extracting actionable knowledge for embodied agents. In _International Conference on Ma-_
_chine Learning_, pages 9118‚Äì9147. PMLR.


[Lebling, Blank, and Anderson. 1979. Special feature](https://doi.org/10.1109/MC.1979.1658697)
[zork: A computerized fantasy simulation game.](https://doi.org/10.1109/MC.1979.1658697) _Com-_
_puter_, 12(4):51‚Äì59.


Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin
Aky√ºrek, Anima Anandkumar, et al. 2022. Pretrained language models for interactive decisionmaking. _Advances in Neural Information Processing_
_Systems_, 35:31199‚Äì31212.


Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald
Tesauro, Kartik Talamadupula, Mrinmaya Sachan,
[and Murray Campbell. 2020. Text-based rl agents](http://arxiv.org/abs/2010.03790)
[with commonsense knowledge: New challenges, en-](http://arxiv.org/abs/2010.03790)
[vironments and baselines.](http://arxiv.org/abs/2010.03790)


Dhruvesh Patel, Hamid Eghbalzadeh, Nitin Kamra,
Michael Louis Iuzzolino, Unnat Jain, and Ruta Desai.
2023. Pretrained language models as visual planners for human assistance. In _Proceedings of the_
_IEEE/CVF International Conference on Computer_
_Vision (ICCV)_, pages 15302‚Äì15314.


F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in
Python. _Journal of Machine Learning Research_,
12:2825‚Äì2830.


Oyvind Tafjord and Peter Clark. 2021. Generalpurpose question-answering with Macaw. _ArXiv_,
abs/2109.02593.




Ruoyao Wang, Peter Jansen, Marc-Alexandre C√¥t√©, and
Prithviraj Ammanabrolu. 2023. [Behavior cloned](https://doi.org/10.18653/v1/2023.eacl-main.204)
[transformers are neurosymbolic reasoners. In](https://doi.org/10.18653/v1/2023.eacl-main.204) _Pro-_
_ceedings of the 17th Conference of the European_
_Chapter of the Association for Computational Lin-_
_guistics_, pages 2777‚Äì2788, Dubrovnik, Croatia. Association for Computational Linguistics.


Ruoyao Wang, Peter Alexander Jansen, MarcAlexandre C√¥t√©, and Prithviraj Ammanabrolu. 2022.
Scienceworld: Is your agent smarter than a 5th
grader? In _Conference on Empirical Methods in_
_Natural Language Processing_ .


Shunyu Yao, Rohan Rao, Matthew Hausknecht, and
[Karthik Narasimhan. 2020. Keep CALM and ex-](https://doi.org/10.18653/v1/2020.emnlp-main.704)
[plore: Language models for action generation in text-](https://doi.org/10.18653/v1/2020.emnlp-main.704)
[based games. In](https://doi.org/10.18653/v1/2020.emnlp-main.704) _Proceedings of the 2020 Conference_
_on Empirical Methods in Natural Language Process-_
_ing (EMNLP)_, pages 8736‚Äì8754, Online. Association
for Computational Linguistics.


[Xusen Yin and Jonathan May. 2019. Learn how to cook](http://arxiv.org/abs/1908.04777)
[a new recipe in a new house: Using map familiar-](http://arxiv.org/abs/1908.04777)
[ization, curriculum learning, and bandit feedback to](http://arxiv.org/abs/1908.04777)
[learn families of text-based adventure games.](http://arxiv.org/abs/1908.04777)


Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J.
Mankowitz, and Shie Mannor. 2018. Learn what not
to learn: Action elimination with deep reinforcement
learning. In _Proceedings of the 32nd International_
_Conference on Neural Information Processing Sys-_
_tems_, NIPS‚Äô18, page 3566‚Äì3577, Red Hook, NY,
USA. Curran Associates Inc.




**A** **Appendix**


**A.1** **Implementation details**


**A.1.1** **G** **UIDE** **‚Äôs architecture**

We use a BERT-base model (Devlin et al., 2019)
as the G UIDE . We also performed a rudimentary
experiment of fine-tuning the Encoder part of the
770M Macaw (Tafjord and Clark, 2021) model (T5
Large model pretrained on Question Answering
datasets in Science Domain), but could not achieve
the same quality of pruning post training as the
smaller BERT-base model. This could be attributed

to two reasons:


1. The size of the training dataset may not be
enough to train the large number of parameters in the bigger Macaw model (thus leading
to underfitting).


2. We used a smaller batch size for training the
Macaw model using similar compute as the
BERT-base model (16GB GPU memory). As
the contrastive loss depends on in-batch examples for negative samples, the smaller batchsize could mean less effective signal to train
the model. We would explore a fairer comparison with similar training settings as the
BERT model in future work.


The code for this work is available at at this

repository: [https://github.com/hitzkrieg/](https://github.com/hitzkrieg/drrn-scienceworld-clone)

[drrn-scienceworld-clone](https://github.com/hitzkrieg/drrn-scienceworld-clone)


**A.1.2** **Training the G** **UIDE**


The supervised contrastive loss framework in (Gao
et al., 2021) needs a dataset consisting of example
triplets of form ( _x_ _i_, _x_ [+] _i_ [and] _[ x]_ _[‚àí]_ _i_ [) where] _[ x]_ _[i]_ [ and] _[ x]_ [+] _i_
are semantically related and _x_ _[‚àí]_ _i_ [is an example of]
a hard negative (semantically unrelated to _x_ _i_, but
more still more similar than any random sample).
For training the Guide, we want to anchor the
task descriptions closer in some embedding space
to relevant actions and away from irrelevant actions.
Thus we prepare a training data _{_ ( _œÑ_ _i_ _, a_ [+] _i_ _[, a]_ _[‚àí]_ _i_ [)] _[}]_ _i_ _[M]_ =1 [,]
consists of tuples of task descriptions _œÑ_ _i_ = _œÑ_ _Œ≥,v_ _‚àà_
_T_ along with a relevant action _a_ [+] _i_ _‚àº_ _G_ _Œ≥,v_ and
an irrelevant action _a_ _[‚àí]_ _i_ _‚àºN_ _Œ≥_ (fixed size set of
irrelevant actions for every task _Œ≥_ ).
Preparing _N_ _Œ≥_ : We simulate gold trajectories
from 10 random training variations for each tasktype _Œ≥ ‚àà_ Œì, and keep taking a union of the
valid actions at each time step to create a large
union of valid actions for that task-type. _N_ _Œ≥_ =



TaskID Task Name


T0 Changes of State (Boiling)
T1 Changes of State (Any)
T2 Changes of State (Freezing)
T3 Changes of State (Melting)
T4 Measuring Boiling Point (known)
T5 Measuring Boiling Point (unknown)
T6 Use Thermometer

T7 Create a circuit

T8 Renewable vs Non-renewable Energy
T9 Test Conductivity (known)
T10 Test Conductivity (unknown)
T11 Find an animal

T12 Find a living thing
T13 Find a non-living thing
T14 Find a plant
T15 Grow a fruit

T16 Grow a plant
T17 Mixing (generic)
T18 Mixing paints (secondary colours)
T19 Mixing paints (tertiary colours)
T20 Identify longest-lived animal
T21 Identify longest-then-shortest-lived animal
T22 Identify shortest-lived animal
T23 Identify life stages (animal)
T24 Identify life stages (plant)
T25 Inclined Planes (determine angle)
T26 Task 26 Friction (known surfaces)
T27 Friction (unknown surfaces)
T28 Mendelian Genetics (known plants)
T29 Mendelian Genetics (unknown plants)


Table 4: List of Task Names with their task ID‚Äôs


ÔøΩ 10 _v_ =1 ÔøΩ _t_ _[A]_ _[Œ≥,v,t]_ [. Now, this set is used for sampling]
hard negatives for a given task description. For a
batch of size N, the loss is computed as:



_l_ ( _œï_ ) = _‚àí_



_N_ _e_ _[s]_ [(] _[œÑ]_ _[i]_ _[, a]_ _i_ [+] [)]
ÔøΩ _i_ =1 log ~~ÔøΩ~~ _Nj_ =1 _[e]_ _[s]_ [(] _[œÑ]_ _[i]_ _[,a]_ _j_ _[‚àí]_ [)] + _e_ _[s]_ [(] _[œÑ]_ _[i]_ _[,a]_ _j_ [+] [)] _[,]_

(1)



The final training dataset to train the G UIDE
LM on 30 task-types consisting of 3442 training
variations had 214535 tuples. The LM was trained
with a batch size of 128, on 10 epochs and with a
learning rate of 0.00005.


**A.1.3** **Training and evaluating the Explorer**


We use similar approach as (Wang et al., 2022) to
train and evaluate the Explorer. The DRRN architecture is trained with embedding size and hidden
size = 128, learning rate = 0.0001, memory size
= 100k, priority fraction (for experience replay) =




0.5. The model is trained simultaneously on 8 environment threads at 100k steps per thread. Episodes
are reset if they reach 100 steps, or success/failure

state.

After every 1000 training steps, evaluation is performed on 10 randomly chosen test variations. The
final numbers reported in table 4 are the average
score of last 10% test step scores.


**A.2** **More examples**


Table 2 shows an example of the out of the G UIDE .




**Algorithm 1** Training Algorithm: L ANGUAGE G UIDED E XPLORATION F RAMEWORK


Initialize replay memory _D_ to capacity _C_
Initialize Explorer‚Äôs Q-network with random weights _Œ∏_
Initialize _updateFrequency_, _totalSteps_
**for** episode = 1 to _M_ **do**

_env, v, d ‚Üê_ sampleRandomEnv(‚Äôtrain‚Äô, _T_ )
Sample initial state _s_ 1 from _d_ 0 and get _A_ valid _,_ 1
**for** _t_ = 1 to _N_ **do**


_totalSteps_ += 1
Identify _A_ ÀÜ relevant _,t_ _k ‚Üê_ most relevant actions using Guide:Guide.top_k( _A_ valid _,t_ _, k, d_ _T,v_ )

_randomNumber ‚àº_ Uniform(0 _,_ 1)
**if** _randomNumber > œµ_ **then**

_a_ _t_ _‚àº_ Multinomial(softmax( _{Q_ ( _s_ _t_ _, a|Œ∏_ ) for _a ‚àà_ _A_ [ÀÜ] relevant _,t_ _}_ ))
**else**

_a_ _t_ _‚àº_ Multinomial(softmax( _{Q_ ( _s_ _t_ _, a|Œ∏_ ) for _a ‚àà_ _A_ valid _,t_ _}_ ))


Execute _a_ _t_, observe _r_ _t_ +1, _s_ _t_ +1, _A_ valid _,t_ +1
Store ( _s_ _t_ _, a_ _t_ _, r_ _t_ +1 _, s_ _t_ +1 _, A_ valid _,t_ +1 ) in _D_
**if** _totalSteps_ mod _updateFrequency_ = 0 **then**

Sample batch from _D_
_L_ cumulative = 0
**for** each ( _s, a, r, s_ _[‚Ä≤]_ _, A_ _[‚Ä≤]_ ) in batch **do**

_Œ¥_ = _r_ + _Œ≥_ max _a_ _‚Ä≤_ _‚ààA_ _‚Ä≤_ _Q_ ( _s_ _[‚Ä≤]_ _, a_ _[‚Ä≤]_ _|Œ∏_ ) _‚àí_ _Q_ ( _s, a|Œ∏_ )
Compute Huber loss _L_ :



_L_ =



12 _[Œ¥]_ [2] if _|Œ¥| <_ 1
ÔøΩ _|Œ¥| ‚àí_ 2 [1] otherwise



_|Œ¥| ‚àí_ 2 otherwise

_L_ cumulative += _L_

Update _Œ∏_ with Adam optimizer:
_Œ∏ ‚Üê_ AdamOptimizer( _Œ∏, ‚àá_ _Œ∏_ _L_ cumulative )

Update state: _s_ _t_ _‚Üê_ _s_ _t_ +1




|Relevant Gold Actions|Selected By Pruner|
|---|---|
|look around,<br>move block to inclined plane with a steel surface,<br>focus on inclined plane with a steel surface,go to hallway,<br>wait1,look at inclined plane with a soapy water surface,<br>move block to inclined plane with a soapy water surface,<br>look at inclined plane with a steel surface|focus on inclined plane with a soapy water surface,look at inclined plane with a soapy water surface,<br>move block to inclined plane with a soapy water surface, look at inclined plane with a steel surface,<br>move block to inclined plane with a steel surface, focus on inclined plane with a steel surface,<br>go to hallway, look around,wait1connect red wire terminal 2 to anode in green light bulb,<br>connect red wire terminal 2 to cathode in green light bulb,<br>connect battery cathode to red wire terminal 1,<br>connect black wire terminal 2 to anode in green light bulb,<br>connect red wire terminal 2 to anode in red light bulb,<br>connect black wire terminal 2 to cathode in green light bulb,<br>connect battery cathode to black wire terminal 1,<br>connect red wire terminal 2 to cathode in red light bulb,<br>connect black wire terminal 2 to anode in red light bulb,<br>connect black wire terminal 2 to cathode in red light bulb, open freezer, wait, pick up red wire<br>focus on red light bulb,pick up black wire, focus on green light bulb, pick up green light bulb,<br>pick up black wire, focus on green light bulb, pick up green light bulb|



Table 5: Qualitative analysis of Validation set trajectories for the ScienceWorld Task "Friction Known Surfaces" for variation 0
at step 17. Note: Missed gold actions are in Red, while selected gold actions are in Green


